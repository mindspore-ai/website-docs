<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.operations.nn_ops &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script><script src="../../../../_static/js/training.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.9/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.9/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">常见问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/typical_api_comparision.html">与PyTorch典型区别</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/use_third_party_op.html">基于自定义算子接口调用第三方算子库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">Module code</a> &raquo;</li>
      <li>mindspore.ops.operations.nn_ops</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.ops.operations.nn_ops</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators for nn.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">_check_3d_int_or_tuple</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">signature</span> <span class="k">as</span> <span class="n">sig</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Rel</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common._decorator</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">Primitive</span><span class="p">,</span> <span class="n">PrimitiveWithInfer</span><span class="p">,</span> <span class="n">PrimitiveWithCheck</span><span class="p">,</span> <span class="n">prim_attr_register</span>


<span class="k">def</span> <span class="nf">_check_positive_int_or_tuple</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">ret_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strict_positive</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether an argument is a positive int or tuple with 2 or 4(when allow_four is True) positive int elements.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_raise_message</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; must be an positive int number or a tuple of two &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;or four &#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">allow_four</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">positive int numbers, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_return_value</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">)</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="n">arg_value</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_four</span><span class="p">:</span>
                <span class="n">_raise_message</span><span class="p">()</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">arg_value</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_raise_message</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">ret_value</span> <span class="o">=</span> <span class="n">_get_return_value</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ret_value</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">strict_positive</span> <span class="ow">and</span> <span class="n">item</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
        <span class="n">_raise_message</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ret_value</span>


<span class="k">def</span> <span class="nf">_check_shape</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether an shape dims is a positive int elements.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_raise_message</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; dims elements must be positive int numbers, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">arg_value</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">_raise_message</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">arg_value</span>


<span class="k">def</span> <span class="nf">_update_attr_by_format</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_format</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    If the format is NHWC, should modify the strides or dilation shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">arg_value</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">arg_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">ret</span>


<span class="k">class</span> <span class="nc">CeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes CeLU (Continuously differentiable exponential linear units) of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{CeLU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</span>

<span class="sd">    It returns :math:`\max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))` element-wise.</span>

<span class="sd">    The picture about CeLU looks like this `CeLU &lt;https://arxiv.org/abs/1704.07483&gt;`_.</span>


<span class="sd">    Args:</span>
<span class="sd">        alpha (float): The :math:`\alpha` value for the Celu formulation. Default: 1.0</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with dtype of float16 and float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `alpha` is not a float.</span>
<span class="sd">        ValueError: If `alpha` has the value of 0.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of &#39;input_x&#39; is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-2.0, -1.0, 1.0, 2.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; celu = ops.CeLU(alpha=1.0)</span>
<span class="sd">        &gt;&gt;&gt; output = celu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.86466473 -0.63212055  1.          2.        ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CeLU&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">NE</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>


<div class="viewcode-block" id="Flatten"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Flatten.html#mindspore.ops.Flatten">[文档]</a><span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flattens a tensor without changing its batch size on the 0-th axis.</span>

<span class="sd">    Refer to :func:`mindspore.ops.flatten` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; flatten = ops.Flatten()</span>
<span class="sd">        &gt;&gt;&gt; output = flatten(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 24)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span></div>


<span class="k">class</span> <span class="nc">AdaptiveAvgPool3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AdaptiveAvgPool3D operation.</span>

<span class="sd">    Refer to :func:`mindspore.ops.adaptive_avg_pool3d` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import nn, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations.nn_ops import AdaptiveAvgPool3D</span>
<span class="sd">        &gt;&gt;&gt; class AdaptiveAvgPool3DNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, output_size):</span>
<span class="sd">        ...         super(AdaptiveAvgPool3DNet, self).__init__()</span>
<span class="sd">        ...         self.output_size_ = output_size</span>
<span class="sd">        ...         self.adaptive_avg_pool_3d = AdaptiveAvgPool3D(self.output_size_)</span>
<span class="sd">        ...     def construct(self, x_):</span>
<span class="sd">        ...         return self.adaptive_avg_pool_3d(x_)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; output_size=(1,1,1)</span>
<span class="sd">        &gt;&gt;&gt; input_x_val = np.zeros((1,1,2,2,2))</span>
<span class="sd">        &gt;&gt;&gt; input_x_val[:,:,0,:,:]  += 1</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(input_x_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; adaptive_avg_pool_3d = AdaptiveAvgPool3DNet(output_size)</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_avg_pool_3d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[0.5]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;cust_aicpu&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">3</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">output_size</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output_size[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output_size[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">val</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="AdaptiveAvgPool2D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AdaptiveAvgPool2D.html#mindspore.ops.AdaptiveAvgPool2D">[文档]</a><span class="k">class</span> <span class="nc">AdaptiveAvgPool2D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2D adaptive average pooling for temporal data.</span>

<span class="sd">    Refer to :func:`mindspore.ops.adaptive_avg_pool2d` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(None, 2)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; adaptive_avg_pool_2d = ops.AdaptiveAvgPool2D((None, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_avg_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.5 2.5]</span>
<span class="sd">           [4.5 5.5]</span>
<span class="sd">           [7.5 8.5]]</span>
<span class="sd">          [[1.5 2.5]</span>
<span class="sd">           [4.5 5.5]</span>
<span class="sd">           [7.5 8.5]]</span>
<span class="sd">          [[1.5 2.5]</span>
<span class="sd">           [4.5 5.5]</span>
<span class="sd">           [7.5 8.5]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=2</span>
<span class="sd">        &gt;&gt;&gt; adaptive_avg_pool_2d = ops.AdaptiveAvgPool2D(2)</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_avg_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[3. 4.]</span>
<span class="sd">           [6. 7.]]</span>
<span class="sd">          [[3. 4.]</span>
<span class="sd">           [6. 7.]]</span>
<span class="sd">          [[3. 4.]</span>
<span class="sd">           [6. 7.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; adaptive_avg_pool_2d = ops.AdaptiveAvgPool2D((1, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_avg_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[4.5 5.5]]</span>
<span class="sd">          [[4.5 5.5]]</span>
<span class="sd">          [[4.5 5.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AdaptiveAvgPool2D.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;length of output_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">output_size</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input_x </span><span class="si">{}</span><span class="s2"> dimension must be larger than output_size </span><span class="si">{}</span><span class="s2"> &quot;</span>
                             <span class="s2">&quot;dimension&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LT</span><span class="p">,</span> <span class="s1">&#39;input_x_dimensions&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">input_x_dimension</span> <span class="ow">in</span> <span class="n">x_shape</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">input_x_dimension</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="s1">&#39;input_x dimension&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">):])</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">j</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">zipped</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">out_size</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;item of output_size&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">out_size</span><span class="p">)</span>
        <span class="n">output_shape</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[:</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">out_size</span><span class="p">)]</span> <span class="o">+</span> <span class="n">out_size</span>
        <span class="k">return</span> <span class="n">output_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">],</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<span class="k">class</span> <span class="nc">AdaptiveMaxPool2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AdaptiveMaxPool2D operation.</span>

<span class="sd">    This operator applies a 2D adaptive max pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    That is, for any input size, the size of the specified output is H x W.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    The input and output data format can be &quot;NCHW&quot; and &quot;CHW&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    H is the feature height, and W is the feature width.</span>

<span class="sd">    For max adaptive pool2d:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{align}</span>
<span class="sd">        h_{start} &amp;= floor(i * H_{in} / H_{out})\\</span>
<span class="sd">        h_{end} &amp;= ceil((i + 1) * H_{in} / H_{out})\\</span>
<span class="sd">        w_{start} &amp;= floor(j * W_{in} / W_{out})\\</span>
<span class="sd">        w_{end} &amp;= ceil((j + 1) * W_{in} / W_{out})\\</span>
<span class="sd">        Output(i,j) &amp;= {\max Input[h_{start}:h_{end}, w_{start}:w_{end}]}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    Note:</span>
<span class="sd">        In Ascend, the second output `argmax` is invalid, please ignore it.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size is H x W.</span>
<span class="sd">            ouput_size can be a tuple, or a single H for H x H, and H and W can be int or None</span>
<span class="sd">            which means the output size is the same as the input.</span>

<span class="sd">        return_indices (bool): If `return_indices` is True, the indices of max value would be output.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of AdaptiveMaxPool2D, which is a 3D or 4D tensor,</span>
<span class="sd">          with float16, float32 or float64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type as the `input_x`.</span>

<span class="sd">        Shape of the output is `input_x_shape[:len(input_x_shape) - len(out_shape)] + out_shape`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `output_size` is not int or tuple.</span>
<span class="sd">        TypeError: If `input_x` is not a tensor.</span>
<span class="sd">        TypeError: If `return_indices` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>
<span class="sd">        ValueError: If `output_size` is a tuple and the length of `output_size` is not 2.</span>
<span class="sd">        ValueError: If the dimension of `input_x` is not NCHW or CHW.</span>
<span class="sd">        ValueError: If `output_size` is less than -1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(None, 2)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; adaptive_max_pool_2d = ops.AdaptiveMaxPool2D((None, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_max_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=2</span>
<span class="sd">        &gt;&gt;&gt; adaptive_max_pool_2d = ops.AdaptiveMaxPool2D(2)</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_max_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[5. 6.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; adaptive_max_pool_2d = ops.AdaptiveMaxPool2D((1, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_max_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[8. 9.]]</span>
<span class="sd">          [[8. 9.]]</span>
<span class="sd">          [[8. 9.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AdaptiveMaxPool2D.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;return_indices&quot;</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                                <span class="s1">&#39;length of output_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AdaptiveMaxPool3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D adaptive max pooling over an input signal composed of several input planes.</span>

<span class="sd">    Refer to :func:`mindspore.ops.adaptive_max_pool3d` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Dynamic output size</span>
<span class="sd">        &gt;&gt;&gt; class AdaptiveMaxPool3DNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(AdaptiveMaxPool3DNet, self).__init__()</span>
<span class="sd">        ...         self.adaptive_max_pool_3d = ops.AdaptiveMaxPool3D()</span>
<span class="sd">        ...     def construct(self, x_, output_size_):</span>
<span class="sd">        ...         return self.adaptive_max_pool_3d(x_, output_size_)</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(0,36).reshape((1, 3, 3, 4)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_size = np.array([1, 1, 2], dtype=np.int32)</span>
<span class="sd">        &gt;&gt;&gt; net = AdaptiveMaxPool3DNet()</span>
<span class="sd">        &gt;&gt;&gt; output = net(Tensor(x), Tensor(output_size))</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].asnumpy())</span>
<span class="sd">        [[[[33. 35.]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1].asnumpy())</span>
<span class="sd">        [[[[33 35]]]]</span>

<span class="sd">        &gt;&gt;&gt; # case 2: Constant output size</span>
<span class="sd">        &gt;&gt;&gt; class ConstAdaptiveMaxPool3DNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, output_size):</span>
<span class="sd">        ...         super(ConstAdaptiveMaxPool3DNet, self).__init__()</span>
<span class="sd">        ...         self.output_size_ = output_size</span>
<span class="sd">        ...         self.adaptive_max_pool_3d = ops.AdaptiveMaxPool3D()</span>
<span class="sd">        ...     def construct(self, x_):</span>
<span class="sd">        ...         return self.adaptive_max_pool_3d(x_, self.output_size_)</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(0,36).reshape((1, 3, 3, 4)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_size = np.array([1, 1, 2], dtype=np.int32)</span>
<span class="sd">        &gt;&gt;&gt; net = ConstAdaptiveMaxPool3DNet(Tensor(output_size))</span>
<span class="sd">        &gt;&gt;&gt; output = net(Tensor(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].asnumpy())</span>
<span class="sd">        [[[[33. 35.]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1].asnumpy())</span>
<span class="sd">        [[[[33 35]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Softmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Softmax.html#mindspore.ops.Softmax">[文档]</a><span class="k">class</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Softmax operation to the input tensor on the specified axis.</span>

<span class="sd">    Refer to :func:`mindspore.ops.softmax` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softmax = ops.Softmax()</span>
<span class="sd">        &gt;&gt;&gt; output = softmax(logits)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.01165623 0.03168492 0.08612854 0.23412167 0.6364086 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Softmax.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">axis</span><span class="p">,))</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;item of axis&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="LogSoftmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogSoftmax.html#mindspore.ops.LogSoftmax">[文档]</a><span class="k">class</span> <span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Log Softmax activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.log_softmax` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; log_softmax = ops.LogSoftmax()</span>
<span class="sd">        &gt;&gt;&gt; output = log_softmax(logits)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-4.4519143 -3.4519143 -2.4519143 -1.4519144 -0.4519144]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LogSoftmax.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Softplus"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Softplus.html#mindspore.ops.Softplus">[文档]</a><span class="k">class</span> <span class="nc">Softplus</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus activation function.</span>

<span class="sd">    Softplus is a smooth approximation to the ReLU function.</span>
<span class="sd">    It can be used to constrain the output of a machine to always be positive.</span>
<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output} = \log(1 + \exp(\text{x})),</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input_x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``  ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softplus = ops.Softplus()</span>
<span class="sd">        &gt;&gt;&gt; output = softplus(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.3132615 2.126928  3.0485873 4.01815   5.0067153]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Softplus&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Softsign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Softsign.html#mindspore.ops.Softsign">[文档]</a><span class="k">class</span> <span class="nc">Softsign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softsign activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.softsign` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations.nn_ops import Softsign</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0, -1, 2, 30, -30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softsign = Softsign()</span>
<span class="sd">        &gt;&gt;&gt; output = softsign(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.        -0.5         0.6666667  0.9677419 -0.9677419]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Softsign&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="ReLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReLU.html#mindspore.ops.ReLU">[文档]</a><span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit activation function) of input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.relu` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu = ops.ReLU()</span>
<span class="sd">        &gt;&gt;&gt; output = relu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">ReLUV3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLUV3 (Rectified Linear Unit activation function) of input tensors element-wise.</span>

<span class="sd">    It returns max(x, 0) element-wise. Specially, the neurons with the negative output</span>
<span class="sd">    will be suppressed and the active neurons will stay the same.</span>

<span class="sd">    .. math::</span>

<span class="sd">        ReLUV3(x) = (x)^+ = max(0, x)</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r1.9/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, *)`, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu_v3 = ops.ReLUV3()</span>
<span class="sd">        &gt;&gt;&gt; output = relu_v3(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLUV3&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Mish"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Mish.html#mindspore.ops.Mish">[文档]</a><span class="k">class</span> <span class="nc">Mish</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes MISH(A Self Regularized Non-Monotonic Neural Activation Function) of input tensors element-wise.</span>

<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output} = x * \tanh(\log(1 + \exp(\text{x})))</span>

<span class="sd">    See more details in `A Self Regularized Non-Monotonic Neural Activation Function</span>
<span class="sd">    &lt;https://arxiv.org/abs/1908.08681&gt;`_.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mish = ops.Mish()</span>
<span class="sd">        &gt;&gt;&gt; output = mish(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.3034014  3.9974129 -0.0026832]</span>
<span class="sd">         [ 1.9439590  -0.0033576 9.0000000]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Mish&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;Mish&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="SeLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SeLU.html#mindspore.ops.SeLU">[文档]</a><span class="k">class</span> <span class="nc">SeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Activation function SeLU (Scaled exponential Linear Unit).</span>

<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        E_{i} =</span>
<span class="sd">        scale *</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x_{i}, &amp;\text{if } x_{i} \geq 0; \cr</span>
<span class="sd">        \text{alpha} * (\exp(x_i) - 1), &amp;\text{otherwise.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`alpha` and :math:`scale` are pre-defined constants(:math:`alpha=1.67326324`</span>
<span class="sd">    and :math:`scale=1.05070098`).</span>

<span class="sd">    See more details in `Self-Normalizing Neural Networks &lt;https://arxiv.org/abs/1706.02515&gt;`_.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of any dimension.</span>
<span class="sd">          The data type is int8, int32, float16, float32, float64(only CPU, GPU).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is not int8, int32, float16, float32, float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations.nn_ops import SeLU</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; selu = SeLU()</span>
<span class="sd">        &gt;&gt;&gt; output = selu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.1113307 4.202804 -1.7575096]</span>
<span class="sd">        [ 2.101402 -1.7462534 9.456309 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SeLU&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;SeLU&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="ReLU6"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReLU6.html#mindspore.ops.ReLU6">[文档]</a><span class="k">class</span> <span class="nc">ReLU6</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit) upper bounded by 6 of input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.relu6` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu6 = ops.ReLU6()</span>
<span class="sd">        &gt;&gt;&gt; result = relu6(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLU6&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ReLUV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The ReLUV2 interface is deprecated, please use the :class:`mindspore.ops.ReLU` instead.</span>

<span class="sd">    Rectified Linear Unit activation function.</span>

<span class="sd">    It returns element-wise :math:`\max(0, x)`, specially, the neurons with the negative output</span>
<span class="sd">    will be suppressed and the active neurons will stay the same.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ReLU}(x) = (x)^+ = \max(0, x)</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor must be a 4-D tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - Has the same type and shape as the `input_x`.</span>
<span class="sd">        - **mask** (Tensor) - A tensor, but it is meaningless.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of `input_x` is not 4-D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        deprecated</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1, -2], [-3, 4]], [[-5, 6], [7, -8]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu_v2 = ops.ReLUV2()</span>
<span class="sd">        &gt;&gt;&gt; output, _= relu_v2(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1. 0.]</span>
<span class="sd">           [0. 4.]]</span>
<span class="sd">          [[0. 6.]</span>
<span class="sd">           [7. 0.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLUV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="s1">&#39;mask&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Elu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Elu.html#mindspore.ops.Elu">[文档]</a><span class="k">class</span> <span class="nc">Elu</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential Linear Uint activation function.</span>

<span class="sd">    Applies the exponential linear unit function element-wise.</span>
<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ELU}(x)= \left\{</span>
<span class="sd">        \begin{array}{align}</span>
<span class="sd">            \alpha(e^{x}  - 1) &amp; \text{if } x \le 0\\</span>
<span class="sd">            x &amp; \text{if } x \gt 0\\</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    The picture about ELU looks like this `ELU &lt;https://en.wikipedia.org/wiki/</span>
<span class="sd">    Activation_function#/media/File:Activation_elu.svg&gt;`_ .</span>

<span class="sd">    Args:</span>
<span class="sd">        alpha (float): The alpha value of ELU, the data type is float. Only support &#39;1.0&#39; currently. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of ELU is a Tensor of any dimension with data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `alpha` is not a float.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `alpha` is not equal to 1.0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; elu = ops.Elu()</span>
<span class="sd">        &gt;&gt;&gt; output = elu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.63212055  4.         -0.99966455]</span>
<span class="sd">         [ 2.         -0.99326205  9.        ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Elu&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="s1">&#39;mask&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="HSwish"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.HSwish.html#mindspore.ops.HSwish">[文档]</a><span class="k">class</span> <span class="nc">HSwish</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard swish activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.hardswish` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; hswish = ops.HSwish()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; result = hswish(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [-0.3333  -0.3333  0  1.666  0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize HSwish.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Sigmoid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sigmoid.html#mindspore.ops.Sigmoid">[文档]</a><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sigmoid activation function.</span>
<span class="sd">    Refer to :func:`mindspore.ops.sigmoid` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sigmoid = ops.Sigmoid()</span>
<span class="sd">        &gt;&gt;&gt; output = sigmoid(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7310586  0.880797   0.95257413 0.98201376 0.9933072 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Sigmoid.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="HSigmoid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.HSigmoid.html#mindspore.ops.HSigmoid">[文档]</a><span class="k">class</span> <span class="nc">HSigmoid</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard sigmoid activation function.</span>

<span class="sd">    Applies hard sigmoid activation element-wise. The input is a Tensor with any valid shape.</span>

<span class="sd">    Hard sigmoid is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{hsigmoid}(x_{i}) = max(0, min(1, \frac{x_{i} + 3}{6})),</span>

<span class="sd">    where :math:`x_i` is an element of the input Tensor.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; hsigmoid = ops.HSigmoid()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; result = hsigmoid(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [0.3333 0.1666 0.5    0.8335 0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize HSigmoid.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Tanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Tanh.html#mindspore.ops.Tanh">[文档]</a><span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tanh activation function.</span>

<span class="sd">    Computes hyperbolic tangent of input element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tanh` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; tanh = ops.Tanh()</span>
<span class="sd">        &gt;&gt;&gt; output = tanh(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7615941 0.9640276 0.9950547 0.9993293 0.9999092]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Tanh&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">FusedBatchNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The FusedBatchNorm interface is deprecated, please use the BatchNorm interface.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The FusedBatchNorm interface is deprecated, please use the BatchNorm interface.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FusedBatchNormEx</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The FusedBatchNormEx interface is deprecated, please use the BatchNorm interface.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;FusedBatchnormEx interface is deprecated, please use BatchNorm interface.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">InstanceNorm</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Instance Normalization over a 4D input.</span>

<span class="sd">    This operator applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with</span>
<span class="sd">    additional channel dimension) as described in the paper `Instance Normalization: The Missing Ingredient for</span>
<span class="sd">    Fast Stylization &lt;https://arxiv.org/abs/1607.08022&gt;`_. It rescales and recenters the feature using a mini-batch</span>
<span class="sd">    of data and the learned parameters which can be described in the following formula.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-5.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for running_mean and running_var</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = momentum * running\_mean + (1 - momentum) * current\_mean`).</span>
<span class="sd">            Momentum value must be [0, 1]. Default: 0.1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of InstanceNorm, Tensor of shape :math:`(N, C)`,</span>
<span class="sd">          data type: float16 or float32.</span>
<span class="sd">        - **gamma** (Parameter) - Scale, Tensor of shape :math:`(C,)`,</span>
<span class="sd">          data type: float32.</span>
<span class="sd">        - **beta** (Parameter) - Bias, Tensor of shape :math:`(C,)`,</span>
<span class="sd">          data type: float32.</span>
<span class="sd">        - **mean** (Parameter) - Mean value, Tensor of shape :math:`(C,)`, data type: float32.</span>
<span class="sd">        - **variance** (Parameter) - Variance value, Tensor of shape :math:`(C,)`, data type: float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, the normalized input, the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The output of InstanceNorm, same type and shape as the `input_x`.</span>
<span class="sd">        - **updated_moving_mean** (Tensor) - Updated mean value, Tensor of shape :math:`(NC,)`, data type: float32.</span>
<span class="sd">        - **updated_moving_variance** (Tensor) - Updated variance value, Tensor of shape :math:`(NC,)`,</span>
<span class="sd">          data type: float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `epsilon` or `momentum` is not a float.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `gamma`, `beta` or `mean` is not float32.</span>
<span class="sd">        ValueError: If `epsilon` is not in the range of [0, 1).</span>
<span class="sd">        ValueError: If `momentum` is not in the range of [0, 1].</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class InstanceNormNet(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(InstanceNormNet, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.instance_norm = ops.InstanceNorm()</span>
<span class="sd">        &gt;&gt;&gt;         self.gamma = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;gamma&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.beta = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;beta&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.mean = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.variance = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;variance&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, input_x):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.instance_norm(input_x, self.gamma, self.beta, self.mean, self.variance)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([128, 64, 32, 64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = InstanceNormNet()</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x)</span>
<span class="sd">        &gt;&gt;&gt; result = output[0].shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (128, 64, 32, 64)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InstanceNorm.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;save_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;save_variance&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">momentum</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_parameter</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BNTrainingReduce</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The BNTrainingReduce interface is deprecated, please use the :class:`mindspore.ops.BatchNorm` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.5&quot;</span><span class="p">,</span> <span class="s2">&quot;ops.BatchNorm&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BNTrainingReduce.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;square_sum&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BNTrainingUpdate</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The BNTrainingUpdate interface is deprecated, please use the :class:`mindspore.ops.BatchNorm` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.5&quot;</span><span class="p">,</span> <span class="s2">&quot;ops.BatchNorm&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">isRef</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BNTrainingUpdate.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;square_sum&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;running_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;save_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;save_inv_variance&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;isRef&quot;</span><span class="p">,</span> <span class="n">isRef</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;factor&quot;</span><span class="p">,</span> <span class="n">factor</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;BNTrainingUpdate&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">factor</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;factor&#39;</span><span class="p">,</span> <span class="s1">&#39;BNTrainingUpdate&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>


<div class="viewcode-block" id="BatchNorm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BatchNorm.html#mindspore.ops.BatchNorm">[文档]</a><span class="k">class</span> <span class="nc">BatchNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Batch Normalization for input data and updated parameters.</span>

<span class="sd">    Batch Normalization is widely used in convolutional neural networks. This operation</span>
<span class="sd">    applies Batch Normalization over inputs to avoid internal covariate shift as described</span>
<span class="sd">    in the paper `Batch Normalization: Accelerating Deep Network Training by Reducing Internal</span>
<span class="sd">    Covariate Shift &lt;https://arxiv.org/abs/1502.03167&gt;`_. It rescales and recenters the</span>
<span class="sd">    features using a mini-batch of data and the learned parameters can be described</span>
<span class="sd">    in the following formula,</span>

<span class="sd">    .. math::</span>

<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon, :math:`mean` is the mean of x,</span>
<span class="sd">    :math:`variance` is the variance of x.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If the operation is used for inference, and outputs &quot;reserve_space_1&quot; and &quot;reserve_space_2&quot; are available,</span>
<span class="sd">          then &quot;reserve_space_1&quot; has the same value as &quot;mean&quot; and &quot;reserve_space_2&quot; has the same value as &quot;variance&quot;.</span>
<span class="sd">        - For Ascend 310, the result accuracy fails to reach 1‰ due to the square root instruction.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_training (bool): If `is_training` is True, `mean` and `variance` are computed during training.</span>
<span class="sd">            If `is_training` is False, they&#39;re loaded from checkpoint during inference. Default: False.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-5.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for running_mean and running_var</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = (1 - momentum) * running\_mean + momentum * current\_mean`).</span>
<span class="sd">            Momentum value must be [0, 1]. Default: 0.1.</span>
<span class="sd">        data_format (str): The optional value for data format, is &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &quot;NCHW&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        If `is_training` is False, inputs are Tensors.</span>

<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C)`, with float16 or float32 data type.</span>
<span class="sd">        - **scale** (Tensor) - Tensor of shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`(C,)`, has the same data type with `scale`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor of shape :math:`(C,)`, has the same data type with `scale`.</span>
<span class="sd">        - **variance** (Tensor) - Tensor of shape :math:`(C,)`, has the same data type with `scale`.</span>

<span class="sd">        If `is_training` is True, `scale`, `bias`, `mean` and `variance` are Parameters.</span>

<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C)`, with float16 or float32 data type.</span>
<span class="sd">        - **scale** (Parameter) - Parameter of shape :math:`(C,)`, with float16 or float32 data type.</span>
<span class="sd">        - **bias** (Parameter) - Parameter of shape :math:`(C,)`, has the same data type with `scale`.</span>
<span class="sd">        - **mean** (Parameter) - Parameter of shape :math:`(C,)`, has the same data type with `scale`.</span>
<span class="sd">        - **variance** (Parameter) - Parameter of shape :math:`(C,)`, has the same data type with `scale`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 5 Tensors, the normalized inputs and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The same type and shape as the input_x. The shape is :math:`(N, C)`.</span>
<span class="sd">        - **batch_mean** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **batch_variance** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **reserve_space_1** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **reserve_space_2** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `is_training` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `epsilon` or `momentum` is not float.</span>
<span class="sd">        TypeError: If `data_format` is not a str.</span>
<span class="sd">        TypeError: If `input_x`, `scale`, `bias`, `mean` or `variance` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x`, `scale` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; scale = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; variance = Tensor(np.ones([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; batch_norm = ops.BatchNorm()</span>
<span class="sd">        &gt;&gt;&gt; output = batch_norm(input_x, scale, bias, mean, variance)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BatchNorm.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">is_training</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_signatures</span><span class="p">(</span><span class="nb">tuple</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;is_training&#39;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">momentum</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="s1">&#39;offset&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_variance&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve_space_1&#39;</span><span class="p">,</span> <span class="s1">&#39;reserve_space_2&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Conv2D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Conv2D.html#mindspore.ops.Conv2D">[文档]</a><span class="k">class</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2D convolution layer.</span>

<span class="sd">    Refer to :func:`mindspore.ops.conv2d` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv2d = ops.Conv2D(out_channel=32, kernel_size=3)</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2D&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero when &#39;pad_mode&#39; is not &#39;pad&#39;, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got &#39;pad&#39;: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39;: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;and platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">DataFormatVecPermute</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Permute input tensor from src_format to dst_format.</span>

<span class="sd">    Args:</span>
<span class="sd">        src_format (str): An optional value for source data format. The format can be &#39;NHWC&#39; and &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NHWC&#39;.</span>
<span class="sd">        dst_format (str): An optional value for destination data format. The format can be &#39;NHWC&#39; and &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A Tensor of shape (4, ) or (4, 2) in source data format. Only supports int32 and int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same data type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `src_format` or `dst_format` is not a str in [&#39;NHWC&#39;, &#39;NCHW&#39;].</span>
<span class="sd">        ValueError: If input_x shape is not (4, ) or (4, 2).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, src_format=&quot;NHWC&quot;, dst_format=&quot;NCHW&quot;):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.op = P.nn_ops.DataFormatVecPermute(src_format, dst_format)</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         return self.op(x)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 4 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="n">dst_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DataFormatVecPermute.&quot;&quot;&quot;</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;NCHW&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">src_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;src_format&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">dst_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;dst_format&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">DepthwiseConv2dNative</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DepthwiseConv2dNative will be deprecated in the future. Please use :class:`mindspore.nn.Conv2d` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">channel_multiplier</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DepthwiseConv2dNative&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;WARN_DEPRECATED: The usage of DepthwiseConv2dNative is deprecated.&quot;</span>
                       <span class="s2">&quot; Please use nn.Conv2D.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The height and width of &#39;stride&#39; must be equal,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got height:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">,  width:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The height and width of &#39;dilation&#39; must be equal,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got height:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">,  width:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero or (0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">channel_multiplier</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">channel_multiplier</span><span class="p">,</span> <span class="s2">&quot;channel_multiplier&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s2">&quot;group&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;offset_a&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;weight rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;w_shape[2:4]&#39;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">kernel_size_n</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kernel_size_h</span><span class="p">,</span> <span class="n">kernel_size_w</span> <span class="o">=</span> <span class="n">w_shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dilation_h</span><span class="p">,</span> <span class="n">dilation_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span>
        <span class="k">if</span> <span class="n">kernel_size_n</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the batch of &#39;weight&#39; must be 1, but got </span><span class="si">{</span><span class="n">kernel_size_n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>

            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">h_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">w_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>

            <span class="n">h_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_top</span> <span class="o">+</span> <span class="n">pad_bottom</span> <span class="o">-</span> <span class="n">kernel_size_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                    <span class="o">/</span> <span class="n">stride_h</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_left</span> <span class="o">+</span> <span class="n">pad_right</span> <span class="o">-</span> <span class="n">kernel_size_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                    <span class="o">/</span> <span class="n">stride_w</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">h_out</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">w_out</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>

        <span class="n">out_channel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">channel_multiplier</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">()</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>


<span class="k">class</span> <span class="nc">_Pool</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs max/avg pooling operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the kernel, that must be a tuple</span>
<span class="sd">           of two `int` for height and width. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The stride of the window, that must be</span>
<span class="sd">            a tuple of two `int` for height and width. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>
<span class="sd">        data_format (str): The optional value for data format, is &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &quot;NCHW&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _Pool.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;MaxPoolWithArgmax&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">x_shape_norm</span> <span class="o">=</span> <span class="n">x_shape</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape_norm</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">input_h</span><span class="p">,</span> <span class="n">input_w</span> <span class="o">=</span> <span class="n">x_shape_norm</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;VALID&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_h</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">out_h</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">input_w</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">out_w</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_h</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">out_h</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_h</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">input_w</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">out_w</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_w</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">,</span> <span class="n">channel</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">shape_value</span> <span class="ow">in</span> <span class="n">out_shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">shape_value</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">shape_value</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the each element of the output shape must be larger than 0, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got output shape: </span><span class="si">{</span><span class="n">out_shape</span><span class="si">}</span><span class="s2">. The input shape: </span><span class="si">{</span><span class="n">x_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;kernel size: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="si">}</span><span class="s2">, strides: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="si">}</span><span class="s2">.&quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;Please check the official api documents for &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;more information about the output.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>


<div class="viewcode-block" id="MaxPool"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaxPool.html#mindspore.ops.MaxPool">[文档]</a><span class="k">class</span> <span class="nc">MaxPool</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Max pooling operation.</span>

<span class="sd">    Applies a 2D max pooling over an input Tensor which can be regarded as a composition of 2D planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            not only the height of movement but also the width of movement, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value of pad mode is &quot;same&quot; or &quot;valid&quot;.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top, bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>
<span class="sd">        data_format (str) : The optional value for data format, is &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; nor &#39;NHWC&#39;.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape((1, 3, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maxpool_op = ops.MaxPool(pad_mode=&quot;VALID&quot;, kernel_size=2, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; output = maxpool_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 5.  6.  7.]</span>
<span class="sd">           [ 9. 10. 11.]]</span>
<span class="sd">          [[17. 18. 19.]</span>
<span class="sd">           [21. 22. 23.]]</span>
<span class="sd">          [[29. 30. 31.]</span>
<span class="sd">           [33. 34. 35.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxPool.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MaxPool</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">MaxPoolV1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Maxpooling operation.</span>

<span class="sd">    Applies a 2D maxpooling over an input Tensor which can be regarded as a composition of 2D planes.</span>

<span class="sd">    Typically, the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPoolV1</span>
<span class="sd">    outputs regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_h, s_w)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_h \times h + m, s_w \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the max value,</span>
<span class="sd">            is an integer that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two integers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an integer that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two integers that</span>
<span class="sd">            represent height and width of movement, respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions, and evenly distributed to top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of the</span>
<span class="sd">              output will be returned without padding. Extra pixels will be discarded.</span>
<span class="sd">        data_format (str) : The optional value for data format, is &#39;NCHW&#39; or &#39;NHWC&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NHWC&#39; nor &#39;NCHW&#39;.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If the length of shape of `input` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape((1, 3, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maxpoolv1_op = ops.MaxPoolV1(pad_mode=&quot;VALID&quot;, kernel_size=2, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; output_ = maxpoolv1_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output_)</span>
<span class="sd">        [[[[ 5.  6.  7.]</span>
<span class="sd">           [ 9. 10. 11.]]</span>
<span class="sd">          [[17. 18. 19.]</span>
<span class="sd">           [21. 22. 23.]]</span>
<span class="sd">          [[29. 30. 31.]</span>
<span class="sd">           [33. 34. 35.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxPoolV1.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">kernel_size_adapted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s1">&#39;NCHW&#39;</span> <span class="k">else</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">strides_adapted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s1">&#39;NCHW&#39;</span> <span class="k">else</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size_adapted</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides_adapted</span><span class="p">)</span>


<div class="viewcode-block" id="MaxPoolWithArgmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaxPoolWithArgmax.html#mindspore.ops.MaxPoolWithArgmax">[文档]</a><span class="k">class</span> <span class="nc">MaxPoolWithArgmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs max pooling on the input Tensor and returns both max values and indices.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value and argmax</span>
<span class="sd">            value, is an int number that represents height and width of the kernel, or a tuple of</span>
<span class="sd">            two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            not only the height of movement but also the width of movement, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &quot;same&quot; or &quot;valid&quot;.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top, bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>

<span class="sd">        data_format (str) : The optional value for data format, is &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, representing the maxpool result and where the max values are generated.</span>

<span class="sd">        - **output** (Tensor) -  Maxpooling result, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">          It has the same data type as `x`.</span>
<span class="sd">        - **mask** (Tensor) -  Max values&#39; index represented by the mask. Data type is int32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data type of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape((1, 3, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maxpool_arg_op = ops.MaxPoolWithArgmax(pad_mode=&quot;VALID&quot;, kernel_size=2, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor, argmax = maxpool_arg_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output_tensor)</span>
<span class="sd">        [[[[ 5.  6.  7.]</span>
<span class="sd">           [ 9. 10. 11.]]</span>
<span class="sd">          [[17. 18. 19.]</span>
<span class="sd">           [21. 22. 23.]]</span>
<span class="sd">          [[29. 30. 31.]</span>
<span class="sd">           [33. 34. 35.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxPoolWithArgmax.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="s1">&#39;mask&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaxPool3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaxPool3D.html#mindspore.ops.MaxPool3D">[文档]</a><span class="k">class</span> <span class="nc">MaxPool3D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    3D max pooling operation.</span>

<span class="sd">    Applies a 3D max pooling over an input Tensor which can be regarded as a composition of 3D planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1, s_2)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \max_{l=0, \ldots, d_{ker}-1} \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents depth, height and width of the kernel, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            not only the depth, height of movement but also the width of movement,, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value of pad mode is &quot;same&quot;, &quot;valid&quot; or &quot;pad&quot;.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to top, bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the bottom and the right side.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input in depth, height and width. The number of &quot;pad&quot; will</span>
<span class="sd">              be padded to the input Tensor borders. &quot;pad_list&quot; must be greater than or equal to 0.</span>

<span class="sd">        pad_list (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings</span>
<span class="sd">            of head, tail, top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of six</span>
<span class="sd">            integers, the padding of head, tail, top, bottom, left and right equals to pad[0], pad[1], pad[2],</span>
<span class="sd">            pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        ceil_mode (Union[bool, None]): Whether to use ceil instead of floor to calculate output shape.</span>
<span class="sd">            Only effective in &quot;pad&quot; mode.</span>
<span class="sd">            When &quot;pad_mode&quot; is &quot;pad&quot; and &quot;ceil_mode&quot; is &quot;None&quot;, &quot;ceil_mode&quot; will be set as &quot;False&quot;. Default: None.</span>
<span class="sd">        data_format (str) : The optional value for data format. Currently only support &#39;NCDHW&#39;. Default: &#39;NCDHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})`. Has the data type of `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `pad_mode` or `data_format` is not a string.</span>
<span class="sd">        ValueError: If numbers in `kernel_size` or `strides` are not positive.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad_mode` is &#39;same&#39; or &#39;valid&#39;, &#39;ceil_mode&#39; is not None.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is a tuple whose length is not equal to 3.</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 2 * 2 * 2 * 3).reshape((1, 2, 2, 2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; max_pool3d = ops.MaxPool3D(kernel_size=2, strides=1, pad_mode=&quot;valid&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = max_pool3d(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[10. 11.]]]</span>
<span class="sd">          [[[22. 23.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">,</span> <span class="n">pad_list</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxPool3D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="s1">&#39;PAD&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;PAD&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="s2">&quot;CALCULATED&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ceil_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s2">&quot;CALCULATED&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;When the &#39;pad_mode&#39; is &#39;same&#39; or &#39;valid&#39;, the &#39;ceil_mode&#39; only supports &#39;None&#39;.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ceil_mode&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span><span class="p">))</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="n">pad_list</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad_list</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pad_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pad_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pad_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pad_list</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">pad_list</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;pad_list&#39; must be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;three or six positive int numbers, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span><span class="si">}</span><span class="s2"> numbers.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;CALCULATED&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad_list&#39; must be zero or (0, 0, 0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad_list&#39; is </span><span class="si">{</span><span class="n">pad_list</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;CALCULATED&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad_list item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_list&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">input_d</span><span class="p">,</span> <span class="n">input_h</span><span class="p">,</span> <span class="n">input_w</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kernel_d</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_d</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;VALID&quot;</span><span class="p">:</span>
            <span class="n">out_d</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_d</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_d</span><span class="p">)</span>
            <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="n">out_d</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_d</span> <span class="o">/</span> <span class="n">stride_d</span><span class="p">)</span>
            <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_h</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_w</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out_d</span> <span class="o">=</span> <span class="p">((</span><span class="n">input_d</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span>
                      <span class="p">(</span><span class="n">kernel_d</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride_d</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">out_h</span> <span class="o">=</span> <span class="p">((</span><span class="n">input_h</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span>
                      <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">out_w</span> <span class="o">=</span> <span class="p">((</span><span class="n">input_w</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">-</span>
                      <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span><span class="p">:</span>
                <span class="n">out_d</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">out_d</span><span class="p">)</span>
                <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">out_h</span><span class="p">)</span>
                <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">out_w</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_d</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">out_d</span><span class="p">)</span>
                <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">out_h</span><span class="p">)</span>
                <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">out_w</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">out_d</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">]</span>

        <span class="n">_check_shape</span><span class="p">(</span><span class="s1">&#39;output&#39;</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span></div>


<span class="k">class</span> <span class="nc">MaxUnpool2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a partial inverse of MaxUnpool2D.</span>

<span class="sd">    MaxUnpool2D is not fully invertible, since the non-maximal values are lost.</span>

<span class="sd">    MaxUnpool2D takes in as input the output of MaxUnpool2D including the indices of the maximal values</span>
<span class="sd">    and computes a partial inverse in which all non-maximal values are set to zero. Typically the input</span>
<span class="sd">    is of shape :math:`(N, C, H_{in}, W_{in})`, the output is of shape :math:`(N, C, H_{out}, W_{out})`,</span>
<span class="sd">    the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        H_{out} = (H{in} - 1) \times strides[0] - 2 \times pads[0] + ksize[0] \\</span>
<span class="sd">        W_{out} = (W{in} - 1) \times strides[1] - 2 \times pads[1] + ksize[1] \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively.</span>
<span class="sd">            If strides is 0 or (0, 0), then strides equal to ksize. Default: 0.</span>
<span class="sd">        pads (Union[int, tuple[int]]): The pad value to be filled. Default: 0. If `pads` is an integer,</span>
<span class="sd">            the paddings of height and width are the same, equal to pads. If `pads` is a tuple of two</span>
<span class="sd">            integers, the padding of height and width equal to pads[0] and pads[1] correspondingly.</span>
<span class="sd">        output_shape (tuple[int]) : The target output size is an optional input. Default: ().</span>
<span class="sd">            If output_shape == (), then the shape of output computed by kszie, strides and pads.</span>
<span class="sd">            If output_shape != (), then output_shape must be :math:`(N, C, H, W)` or</span>
<span class="sd">            :math:`(N, H, W, C)` and output_shape must belong to</span>
<span class="sd">            :math:`[(N, C, H_{out} - strides[0], W_{out} - strides[1]),</span>
<span class="sd">            (N, C, H_{out} + strides[0], W_{out} + strides[1])]`.</span>
<span class="sd">        data_format (str) : The optional value for data format.</span>
<span class="sd">            Currently support &#39;NCHW&#39; and &#39;NHWC&#39;. Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor to invert.</span>
<span class="sd">          Tensor of shape :math:`(N, C, H_{in}, W_{in})` or :math:`(N, H_{in}, W_{in}, C)`.</span>
<span class="sd">        - **argmax** (Tensor) - Max values&#39; index represented by the argmax.</span>
<span class="sd">          Tensor of shape must be same with input &#39;x&#39;.</span>
<span class="sd">          Values of argmax must belong to :math:`[0, H_{in} \times W_{in} - 1]`.</span>
<span class="sd">          Data type must be in int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, H_{out}, W_{out})` or :math:`(N, H_{out}, W_{out}, C)`.</span>
<span class="sd">        Has the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` or `argmax` is not supported.</span>
<span class="sd">        TypeError: If `ksize`, `strides` or `pads` is neither int nor tuple.</span>
<span class="sd">        ValueError: If numbers in `strides` (also support 0 and (0, 0)) or `ksize` is not positive.</span>
<span class="sd">        ValueError: If numbers in `pads` is negative.</span>
<span class="sd">        ValueError: If `ksize`, `strides` or `pads` is a tuple whose length is not equal to 2.</span>
<span class="sd">        ValueError: If `data_format` is not a str or is neither `NCHW` nor `NHWC`.</span>
<span class="sd">        ValueError: If `output_shape` whose length is neither 0 or 4.</span>
<span class="sd">        ValueError: If `output_shape` is not close to output size</span>
<span class="sd">                    computed by attr `ksize, strides, pads`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[[0, 1], [8, 9]]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; argmax = Tensor(np.array([[[[0, 1], [2, 3]]]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; maxunpool2d = P.MaxUnpool2D(ksize=1, strides=1, pads=0)</span>
<span class="sd">        &gt;&gt;&gt; output = maxunpool2d(x, argmax)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [[[[0. 1.]</span>
<span class="sd">            [8. 9.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pads</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxUnpool2D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">strides</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)):</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="n">ksize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">strict_positive</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">output_shape</span>


<span class="k">class</span> <span class="nc">MaxUnpool3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a partial inverse of MaxUnpool3D.</span>

<span class="sd">    MaxUnpool3D is not fully invertible, since the non-maximal values are lost.</span>

<span class="sd">    MaxUnpool3D takes in as input the output of MaxUnpool3D including the indices of the maximal</span>
<span class="sd">    values and computes a partial inverse in which all non-maximal values are set to zero.</span>
<span class="sd">    Typically the input is of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`, the output is of</span>
<span class="sd">    shape :math:`(N, C, D_{out}, H_{out}, W_{out})`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        D_{out} = (D{in} - 1) \times strides[0] - 2 \times pads[0] + ksize[0] \\</span>
<span class="sd">        H_{out} = (H{in} - 1) \times strides[1] - 2 \times pads[1] + ksize[1] \\</span>
<span class="sd">        W_{out} = (W{in} - 1) \times strides[2] - 2 \times pads[2] + ksize[2] \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents depth, height and width of the kernel, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively.</span>
<span class="sd">            If strides is 0 or (0, 0, 0), then strides equal to ksize. Default: 0.</span>
<span class="sd">        pads (Union[int, tuple[int]]): The pad value to be filled. Default: 0. If `pads` is an integer,</span>
<span class="sd">            the paddings of depth, height and width are the same, equal to pads. If `pads` is a tuple of three integers,</span>
<span class="sd">            the padding of depth, height and width equal to pads[0], pads[1] and pads[2] correspondingly.</span>
<span class="sd">        output_shape (tuple[int]) : The target output size is an optional input. Default: ().</span>
<span class="sd">            If output_shape == (), then the shape of output computed by kszie, strides and pads.</span>
<span class="sd">            If output_shape != (), then output_shape must be :math:`(N, C, D, H, W)` or</span>
<span class="sd">            :math:`(N, D, H, W, C)` and output_shape must belong to</span>
<span class="sd">            :math:`[(N, C, D_{out} - strides[0], H_{out} - strides[1], W_{out} - strides[2]),</span>
<span class="sd">            (N, C, D_{out} + strides[0], H_{out} + strides[1], W_{out} + strides[2])]`.</span>
<span class="sd">        data_format (str) : The optional value for data format. Currently support &#39;NCDHW&#39; and &#39;NDHWC&#39;. Default: &#39;NCDHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor to invert.</span>
<span class="sd">          Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(N, D_{in}, H_{in}, W_{in}, C)`.</span>
<span class="sd">        - **argmax** (Tensor) - Max values&#39; index represented by the argmax.</span>
<span class="sd">          Tensor of shape must be same with input &#39;x&#39;.</span>
<span class="sd">          Values of argmax must belong to :math:`[0, D_{in} \times H_{in} \times W_{in} - 1]`.</span>
<span class="sd">          Data type must be in int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(N, D_{out}, H_{out}, W_{out}, C)`.</span>
<span class="sd">        Has the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` or `argmax` is not supported.</span>
<span class="sd">        TypeError: If `ksize`, `strides` or `pads` is neither int nor tuple.</span>
<span class="sd">        ValueError: If numbers in `strides` (also support 0 and (0, 0, 0)) or `ksize` is not positive.</span>
<span class="sd">        ValueError: If numbers in `pads` is negative.</span>
<span class="sd">        ValueError: If `ksize`, `strides` or `pads` is a tuple whose length is not equal to 3.</span>
<span class="sd">        ValueError: If `data_format` is not a str or is neither `NCDHW` nor `NDHWC`.</span>
<span class="sd">        ValueError: If `output_shape` whose length is neither 0 or 5.</span>
<span class="sd">        ValueError: If `output_shape` is not close to output size</span>
<span class="sd">                    computed by attr `ksize, strides, pads`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[[[0, 1], [8, 9]]]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; argmax = Tensor(np.array([[[[[0, 1], [2, 3]]]]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; maxunpool3d = P.MaxUnpool3D(ksize=1, strides=1, pads=0)</span>
<span class="sd">        &gt;&gt;&gt; output = maxunpool3d(x, argmax)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [[[[[0. 1.]</span>
<span class="sd">            [8. 9.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pads</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxUnpool3D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">strides</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)):</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="n">ksize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">greater_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NDHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NDHWC&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">output_shape</span>


<div class="viewcode-block" id="AvgPool"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AvgPool.html#mindspore.ops.AvgPool">[文档]</a><span class="k">class</span> <span class="nc">AvgPool</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average pooling operation.</span>

<span class="sd">    Refer to :func:`mindspore.ops.avg_pool2d` for more detail.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the average value,</span>
<span class="sd">            is an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &#39;same&#39; or &#39;valid&#39;.</span>
<span class="sd">            Default: &#39;valid&#39;.</span>

<span class="sd">            - same: The height and width of the output are the same as the input divided by &#39;strides&#39;</span>
<span class="sd">              and rounded up.</span>

<span class="sd">            - valid: Returns the output of the valid calculation without filling. Redundant pixels that</span>
<span class="sd">              do not satisfy the calculation will be discarded.</span>
<span class="sd">        data_format (str): The format of input and output data. It should be &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; nor &#39;NHWC&#39;.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.avgpool_op = ops.AvgPool(pad_mode=&quot;VALID&quot;, kernel_size=2, strides=1)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         result = self.avgpool_op(x)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape(1, 3, 3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 2.5   3.5   4.5]</span>
<span class="sd">           [ 6.5   7.5   8.5]]</span>
<span class="sd">          [[14.5  15.5  16.5]</span>
<span class="sd">           [18.5  19.5  20.5]]</span>
<span class="sd">          [[26.5  27.5  28.5]</span>
<span class="sd">           [30.5  31.5  32.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AvgPool.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AvgPool</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">AvgPoolV1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average-pooling operation.</span>

<span class="sd">    Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, AvgPoolV1 outputs</span>
<span class="sd">    regional average in the :math:`(H_{in}, W_{in})`-dimension. Given window size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and strides :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \frac{1}{h_{ker} * w_{ker}} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - Only single input and single output are supported.</span>
<span class="sd">        - Global average pooling is supported.</span>
<span class="sd">        - The height of &quot;kernel_size&quot; and the weight of &quot;kernel_size&quot; are positive integers within the range [1, 255].</span>
<span class="sd">          ksize_h * ksize_w &lt; 256.</span>
<span class="sd">        - Due to instruction restrictions, the values of &quot;strides_h&quot; and &quot;strides_w&quot; are</span>
<span class="sd">          positive integers within the range [1, 64).</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the kernel used to take the average value,</span>
<span class="sd">            is an integer that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two integers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an integer that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two integers that</span>
<span class="sd">            represent height and width of movement, respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, should be one of &quot;same&quot; or &quot;valid&quot;.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated horizontally and vertically,</span>
<span class="sd">              and evenly distributed to top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from bottom and right.</span>

<span class="sd">            - valid: Adopts the way of discarding. The largest possible height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded.</span>
<span class="sd">        data_format (str): The format of input and output data. Should be &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; nor &#39;NHWC&#39;.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 2 * 4 * 4).reshape((1, 2, 4, 4)), mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; avgpoolv1_op = ops.AvgPoolV1(pad_mode=&quot;VALID&quot;, kernel_size=3, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; _output = avgpoolv1_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(_output)</span>
<span class="sd">        [[[[ 5.  6.]</span>
<span class="sd">           [ 9. 10.]]</span>
<span class="sd">          [[21. 22.]</span>
<span class="sd">           [25. 26.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AvgPoolV1.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># adapt data_format</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size_adapted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size_adapted</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides_adapted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides_adapted</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Conv2DBackpropInput</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Conv2DBackpropInput interface is deprecated, please refer to :class:`mindspore.ops.Conv2DTranspose` if you</span>
<span class="sd">    want to do unsampling.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_sizes&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2DBackpropInput&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="s1">&#39;input_sizes&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_update_attr_by_format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_update_attr_by_format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero or (0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">pad_mode</span> <span class="o">=</span> <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_list</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pad_list</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;element of pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad_list</span>


<span class="k">class</span> <span class="nc">MaxPool3DWithArgmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a 3D max pooling on the input Tensor and returns both max values and indices.</span>

<span class="sd">    Typically the input is a Tensor with shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})`, outputs</span>
<span class="sd">    regional maximum in the :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given `ksize`</span>
<span class="sd">    :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and `strides` :math:`s = (s_0, s_1, s_2)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \max_{l=0, \ldots, d_{ker}-1} \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of kernel used to take the maximum value and arg</span>
<span class="sd">            value, is an int number that represents depth, height and width of the kernel, or a tuple of</span>
<span class="sd">            three int numbers that represent depth, height and width respectively.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents the depth,</span>
<span class="sd">            height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively.</span>
<span class="sd">        pads (Union[int, tuple[int]]): An int number that represents the depth, height and width of movement are both</span>
<span class="sd">            strides, or a tuple of three int numbers that represent depth, height and width of movement respectively.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): Default: &#39;(1, 1, 1)&#39;.</span>
<span class="sd">        ceil_mode (bool): Whether to use ceil instead of floor to calculate output shape. Default: False.</span>
<span class="sd">        data_format (str) : The optional value for data format. Currently only support &#39;NCDHW&#39;. Default: &#39;NCDHW&#39;.</span>
<span class="sd">        argmax_type (mindspore.dtype) : The dtype for argmax. Default: mstype.int64.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})` with data type of int8,</span>
<span class="sd">          int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, representing the maxpool result and where the max values are generated.</span>

<span class="sd">        - **output** (Tensor) - Maxpooling result, with shape :math:`(N_{out}, C_{out}, D_{out}, H_{out}, W_{out})`.</span>
<span class="sd">          It has the same data type as `x`.</span>
<span class="sd">        - **argmax** (Tensor) - Index corresponding to the maximum value. Data type is int32 or int64.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 5.</span>
<span class="sd">        TypeError: If `ksize` , `strides` , `pads` or `dilation` is not int or tuple.</span>
<span class="sd">        ValueError: If `ksize` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If `pads` is less than 0.</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>
<span class="sd">        ValueError: If `argmax_type` is not mindspore.int64 or mindspore.int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(2 * 1 * 2 * 2 * 2).reshape((2, 1, 2, 2, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; max_pool3d_with_arg_op = ops.MaxPool3DWithArgmax(ksize=2, strides=1, pads=1)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor, argmax = max_pool3d_with_arg_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output_tensor.shape)</span>
<span class="sd">        (2, 1, 3, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(argmax.shape)</span>
<span class="sd">        (2, 1, 3, 3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">,</span> <span class="n">argmax_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxPool3DWithArgmax.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;argmax_type&quot;</span><span class="p">,</span> <span class="n">argmax_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">argmax_type_valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span>
            <span class="s2">&quot;argmax_type&quot;</span><span class="p">,</span> <span class="n">argmax_type</span><span class="p">,</span> <span class="n">argmax_type_valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">argmax_type</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;argmax_type&#39;</span><span class="p">,</span> <span class="s1">&#39;int32&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">argmax_type</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;argmax_type&#39;</span><span class="p">,</span> <span class="s1">&#39;int64&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;argmax_type&#39; must be mstype.int32 or mstype.int64, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">argmax_type</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;pads&quot;</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">greater_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>


<div class="viewcode-block" id="Conv2DTranspose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Conv2DTranspose.html#mindspore.ops.Conv2DTranspose">[文档]</a><span class="k">class</span> <span class="nc">Conv2DTranspose</span><span class="p">(</span><span class="n">Conv2DBackpropInput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute a 2D transposed convolution, which is also known as a deconvolution</span>
<span class="sd">    (although it is not an actual deconvolution).</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimensionality of the output space.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the convolution window.</span>
<span class="sd">        pad_mode (str): Modes to fill padding. It could be &quot;valid&quot;, &quot;same&quot;, or &quot;pad&quot;. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (Union[int, tuple[int]]): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">                    top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of four integers, the</span>
<span class="sd">                    padding of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3] correspondingly.</span>
<span class="sd">        pad_list (Union[str, None]): The pad list like (top, bottom, left, right). Default: None.</span>
<span class="sd">        mode (int): Modes for different convolutions. The value is currently not used. Default: 1.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The stride to be applied to the convolution filter. Default: 1.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): Specifies the dilation rate to be used for the dilated convolution.</span>
<span class="sd">            Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>
<span class="sd">        data_format (str): The format of input and output data. It should be &#39;NHWC&#39; or &#39;NCHW&#39;，\</span>
<span class="sd">            default is &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **dout** (Tensor) - the gradients with respect to the output of the convolution.</span>
<span class="sd">          The shape conforms to the default data_format :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(K_1, K_2)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}, K_1, K_2)`.</span>
<span class="sd">        - **input_size** (Tensor) - A tuple describes the shape of the input which conforms to the format</span>
<span class="sd">          :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients with respect to the input of convolution. It has the same shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        ValueError: If `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 4.</span>
<span class="sd">        ValueError: If `pad_mode` it not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; nor &#39;NHWC&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([10, 32, 30, 30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]))</span>
<span class="sd">        &gt;&gt;&gt; conv2d_transpose_input = ops.Conv2DTranspose(out_channel=32, kernel_size=3)</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d_transpose_input(dout, weight, ops.shape(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 32, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2DTranspose.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2DTranspose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span>
                                              <span class="n">pad_list</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>


<div class="viewcode-block" id="BiasAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BiasAdd.html#mindspore.ops.BiasAdd">[文档]</a><span class="k">class</span> <span class="nc">BiasAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the sum of the input Tensor and the bias Tensor. Before adding, the bias Tensor will be broadcasted to be</span>
<span class="sd">    consistent with the shape of the input Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        data_format (str): The format of input and output data. It should be &#39;NHWC&#39;, &#39;NCHW&#39; or &#39;NCDHW&#39;.</span>
<span class="sd">            Default is &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The shape can be 2-5 dimensions.</span>
<span class="sd">          The data type should be float16 or float32.</span>
<span class="sd">        - **bias** (Tensor) - The bias tensor, with shape :math:`(C)`. C must be the same as channel dimension C of</span>
<span class="sd">          `input_x`. The data type should be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `data_format` is not a str.</span>
<span class="sd">        ValueError: If value of `data_format` is not in the range of [&#39;NHWC&#39;,&#39;NCHW&#39;,&#39;NCDHW&#39;].</span>
<span class="sd">        TypeError: If `input_x` or `bias` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `bias` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `bias` is inconsistent.</span>
<span class="sd">        TypeError: If dimension of `input_x` is not in the range [2, 5].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.random.random(3).reshape((3,)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias_add = ops.BiasAdd()</span>
<span class="sd">        &gt;&gt;&gt; output = bias_add(input_x, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BiasAdd.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span></div>


<div class="viewcode-block" id="NLLLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NLLLoss.html#mindspore.ops.NLLLoss">[文档]</a><span class="k">class</span> <span class="nc">NLLLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the negative log likelihood loss between logits and labels.</span>

<span class="sd">    The nll loss with reduction=none can be described as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top},</span>
<span class="sd">        \quad l_{n}=-w_{t_{n}} x_{n, t_{n}},</span>
<span class="sd">        \quad w_{c}=\text { weight }[c] \cdot 1</span>

<span class="sd">    where :math:`x` is the logits, :math:`t` is the labels, :math:`w` is the weight,</span>
<span class="sd">    N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">    If reduction is not &#39;none&#39; (default &#39;mean&#39;), then</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=\left\{\begin{array}{ll}</span>
<span class="sd">        \sum_{n=1}^{N} \frac{1}{\sum_{n=1}^{N} w_{t n}} l_{n}, &amp; \text { if reduction }=\text { &#39;mean&#39;; } \\</span>
<span class="sd">        \sum_{n=1}^{N} l_{n}, &amp; \text { if reduction }=\text { &#39;sum&#39; }</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, or &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type only supports float32 or float16.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N,)`. Data type only supports int32.</span>
<span class="sd">        - **weight** (Tensor) - The rescaling weight to each class, with shape :math:`(C,)` and data type only</span>
<span class="sd">          supports float32 or float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors composed with `loss` and `total_weight`.</span>

<span class="sd">        - **loss** (Tensor) - When `reduction` is &#39;none&#39; and `logits` is a 2D tensor, the `loss` shape is :math:`(N,)`.</span>
<span class="sd">          Otherwise, the `loss` is a scalar. The data type is the same with `input&#39;s`.</span>
<span class="sd">        - **total_weight** (Tensor) - The `total_weight` is a scalar. The data type is the same with `weight&#39;s`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits` or `weight` is neither float16 nor float32, `labels` is not int32.</span>
<span class="sd">        ValueError: If `logits` is not a one or two dimension tensor, `labels` and `weight` are not</span>
<span class="sd">                    one dimension tensors.</span>
<span class="sd">                    When `logits` is a two dimension tensor, the first dimension of `logits` is not equal to `labels`,</span>
<span class="sd">                    and second dimension of `logits` is not equal to `weight`.</span>
<span class="sd">                    When `logits` is a one dimension tensor, the dimensions of `logits`, `labels`</span>
<span class="sd">                    and `weight` should be equal to each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[0.5488135, 0.71518934],</span>
<span class="sd">        ...                           [0.60276335, 0.5448832],</span>
<span class="sd">        ...                           [0.4236548, 0.6458941]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0, 0, 0]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.3834415, 0.79172504]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; nll_loss = ops.NLLLoss(reduction=&quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt; loss, weight = nll_loss(logits, labels, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        -0.52507716</span>
<span class="sd">        &gt;&gt;&gt; print(weight)</span>
<span class="sd">        1.1503246</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NLLLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="s1">&#39;total_weight&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SoftmaxCrossEntropyWithLogits.html#mindspore.ops.SoftmaxCrossEntropyWithLogits">[文档]</a><span class="k">class</span> <span class="nc">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the softmax cross-entropy value between logits and labels with one-hot encoding.</span>

<span class="sd">    The updating formulas of SoftmaxCrossEntropyWithLogits algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{\exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)} \\</span>
<span class="sd">            loss_{ij} = -\sum_j{Y_{ij} * ln(p_{ij})}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`X` represents `logits`.</span>
<span class="sd">    :math:`Y` represents `label`.</span>
<span class="sd">    :math:`loss` represents `output`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N, C)`, has the same data type with `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors(loss, dlogits), the `loss` shape is :math:`(N,)`,</span>
<span class="sd">        and the `dlogits` with the same shape as `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `logits` or `labels` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor([[2, 4, 1, 4, 5], [2, 1, 2, 4, 3]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([[0, 0, 0, 0, 1], [0, 0, 0, 1, 0]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softmax_cross = ops.SoftmaxCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; loss, dlogits = softmax_cross(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        [0.5899297  0.52374405]</span>
<span class="sd">        &gt;&gt;&gt; print(dlogits)</span>
<span class="sd">        [[ 0.02760027  0.20393994  0.01015357  0.20393994 -0.44563377]</span>
<span class="sd">         [ 0.08015892  0.02948882  0.08015892 -0.4077012   0.21789455]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="SparseSoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseSoftmaxCrossEntropyWithLogits.html#mindspore.ops.SparseSoftmaxCrossEntropyWithLogits">[文档]</a><span class="k">class</span> <span class="nc">SparseSoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the softmax cross-entropy value between logits and sparse encoding labels.</span>

<span class="sd">    Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{\exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)} \\</span>
<span class="sd">            loss_{ij} = \begin{cases} -ln(p_{ij}), &amp;j = y_i \cr 0, &amp; j \neq y_i \end{cases} \\</span>
<span class="sd">            loss = \sum_{ij} loss_{ij}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        is_grad (bool): If true, this operation returns the computed gradient. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N)`.</span>
<span class="sd">          Data type must be int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, if `is_grad` is False, the output tensor is the value of loss which is a scalar tensor;</span>
<span class="sd">        if `is_grad` is True, the output tensor is the gradient of input with the same shape as `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `is_grad` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `labels` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If logits.shape[0] != labels.shape[0].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor([[2, 3, 1, 4, 5], [2, 1, 2, 4, 3]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([0, 1], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sparse_softmax_cross = ops.SparseSoftmaxCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; loss = sparse_softmax_cross(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        3.4878292</span>
<span class="sd">        &gt;&gt;&gt; sparse_softmax_cross_grad = ops.SparseSoftmaxCrossEntropyWithLogits(is_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; loss_grad = sparse_softmax_cross_grad(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss_grad)</span>
<span class="sd">        [[-0.48415753  0.04306427  0.00582811  0.11706084  0.3182043 ]</span>
<span class="sd">         [ 0.04007946 -0.4852556   0.04007946  0.2961494   0.10894729]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseSoftmaxCrossEntropyWithLogits.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;is_grad&#39;</span><span class="p">,</span> <span class="n">is_grad</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;features&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_grad</span> <span class="o">=</span> <span class="n">is_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;sens&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int_sequence</span><span class="p">(</span><span class="n">logits_shape</span><span class="p">,</span> <span class="s1">&#39;dims&#39;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int_sequence</span><span class="p">(</span><span class="n">labels_shape</span><span class="p">,</span> <span class="s1">&#39;dims&#39;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;logits_shape[0]&quot;</span><span class="p">,</span> <span class="n">logits_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;labels_shape[0]&quot;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">loss_shape</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_grad</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">logits_shape</span>
        <span class="k">return</span> <span class="n">loss_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;logits&quot;</span><span class="p">,</span> <span class="n">logits_type</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits_type</span></div>


<span class="k">class</span> <span class="nc">SparseSoftmaxCrossEntropyWithLogitsV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the softmax cross-entropy value between logits and sparse encoding labels.</span>

<span class="sd">    Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{\exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)} \\</span>
<span class="sd">            loss_{ij} = \begin{cases} -ln(p_{ij}), &amp;j = y_i \cr 0, &amp; j \neq y_i \end{cases}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N)`.</span>
<span class="sd">          Data type must be int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **loss** (Tensor) - With the same shape as `labels`, the same type as `logits`.</span>
<span class="sd">        - **backprop** (Tensor) - With the same shape and same type as `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `labels` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If logits.shape is not [batch x classes] or labels.shape is not [batch].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor([[2, 3, 1, 4, 5], [2, 1, 2, 4, 3]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([0, 1], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sparse_softmax_cross = ops.SparseSoftmaxCrossEntropyWithLogitsV2()</span>
<span class="sd">        &gt;&gt;&gt; loss, backprop = sparse_softmax_cross(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        [3.4519143 3.523744 ]</span>
<span class="sd">        &gt;&gt;&gt; print(backprop)</span>
<span class="sd">        [[-0.96831506  0.08612854  0.01165623  0.23412165  0.6364086 ]</span>
<span class="sd">         [ 0.08015893 -0.9705112   0.08015893  0.5922988   0.21789455]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseSoftmaxCrossEntropyWithLogitsV2.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;features&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="s1">&#39;backprop&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="ApplyMomentum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyMomentum.html#mindspore.ops.ApplyMomentum">[文档]</a><span class="k">class</span> <span class="nc">ApplyMomentum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the Momentum algorithm.</span>

<span class="sd">    Refer to the paper `On the importance of initialization and momentum in deep</span>
<span class="sd">    learning &lt;https://dl.acm.org/doi/10.5555/3042817.3043064&gt;`_  for more details.</span>

<span class="sd">    Inputs of `variable`, `accumulation` and `gradient` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Refer to :class:`mindspore.nn.Momentum` for more details about the formula and usage.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumulation tensors</span>
<span class="sd">                            from being updated. Default: False.</span>
<span class="sd">        use_nesterov (bool): Enable Nesterov momentum. Default: False.</span>
<span class="sd">        gradient_scale (float): The scale of the gradient. Default: 1.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **variable** (Parameter) - Weights to be updated. Data type must be float.</span>
<span class="sd">        - **accumulation** (Parameter) - Accumulated gradient value by moment weight,</span>
<span class="sd">          has the same data type with `variable`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same data type as `variable`.</span>
<span class="sd">        - **momentum** (Union[Number, Tensor]) - Momentum, must be a float number or</span>
<span class="sd">          a scalar tensor with float data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the `use_locking` or `use_nesterov` is not a bool or `gradient_scale` is not a float.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        Please refer to the usage in :class:`mindspore.nn.Momentum`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accumulation&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">gradient_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyMomentum.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_nesterov</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">use_nesterov</span><span class="p">,</span> <span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">use_locking</span><span class="p">,</span> <span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;gradient_scale&#39;</span><span class="p">,</span> <span class="n">gradient_scale</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="s1">&#39;accumulation&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="SmoothL1Loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SmoothL1Loss.html#mindspore.ops.SmoothL1Loss">[文档]</a><span class="k">class</span> <span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Refer to :func:`mindspore.ops.smooth_l1_loss` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.SmoothL1Loss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SmoothL1Loss.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">MultiMarginLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a criterion that optimizes a multi-class classification hinge</span>
<span class="sd">    loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`) and</span>
<span class="sd">    output :math:`y` (which is a 1D tensor of target class indices,</span>
<span class="sd">    :math:`0 \leq y \leq \text{x.size}(1)-1`):</span>

<span class="sd">    For each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar</span>
<span class="sd">    output :math:`y` is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`</span>
<span class="sd">    and :math:`i \neq y`.</span>

<span class="sd">    Optionally, you can give non-equal weighting on the classes by passing</span>
<span class="sd">    a 1D input `weight` tensor w into the constructor.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (int): Optional. The norm degree for pairwise distance. Should be 1 or 2. Default: 1.</span>
<span class="sd">        margin (float): Optional. A parameter to change pairwise distance. Default: 1.0.</span>
<span class="sd">        reduction (str): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;. Default: &quot;mean&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input x, with shape :math:`(N, C)`. Data type only support float32, float16 or float64.</span>
<span class="sd">        - **target** (Tensor) - Ground truth labels, with shape :math:`(N,)`. Data type only support int64. The</span>
<span class="sd">          value of target should be non-negative, less than C.</span>
<span class="sd">        - **weight** (Tensor, optional) - The rescaling weight to each class with shape :math:`(C,)`. Data type only</span>
<span class="sd">          support float32, float16 or float64. Default: None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, When `reduction` is &#39;none&#39;, the shape is :math:`(N,)`.</span>
<span class="sd">        Otherwise, it is a scalar. Has the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `p` or `target` is not int.</span>
<span class="sd">        TypeError: If dtype of `margin` is not float.</span>
<span class="sd">        TypeError: If dtype of `reduction` is not str.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float or float64.</span>
<span class="sd">        TypeError: If dtype of `weight` and `x` is not the same.</span>
<span class="sd">        ValueError: If &#39;p&#39; is not 1 or 2.</span>
<span class="sd">        ValueError: If &#39;reduction&#39; is not one of {&#39;none&#39;,&#39;sum&#39;,&#39;mean&#39;}.</span>
<span class="sd">        ValueError: If shape[0] of `x` is not equal to shape[0] of `target`.</span>
<span class="sd">        ValueError: If shape[1] of `x` is not equal to shape[0] of `weight`.</span>
<span class="sd">        ValueError: IF rank of `weight` is not 1.</span>
<span class="sd">        ValueError: If rank of `x` is not 2 or rank of &#39;target&#39; is not 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones(shape=[3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor(np.array([1, 2, 1]), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.MultiMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(x, target, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.6666667</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MultiMarginLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">},</span> <span class="n">Rel</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="SoftMarginLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SoftMarginLoss.html#mindspore.ops.SoftMarginLoss">[文档]</a><span class="k">class</span> <span class="nc">SoftMarginLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SoftMarginLoss operation.</span>

<span class="sd">    Creates a criterion that optimizes a two-class classification</span>
<span class="sd">    logistic loss between input tensor :math:`x` and target tensor :math:`y`</span>
<span class="sd">    (containing 1 or -1).</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</span>

<span class="sd">    where :math:`x.nelement()` is the number of elements of x.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;. Default: &quot;mean&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Predict data. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth data, with the same type and shape as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &quot;none&quot;, its shape is the same as `logits`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits` or `labels` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.SoftMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[0.3, 0.7], [0.5, 0.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[-1, 1], [1, -1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.6764238</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SoftMarginLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;predict&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="L2Loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.L2Loss.html#mindspore.ops.L2Loss">[文档]</a><span class="k">class</span> <span class="nc">L2Loss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates half of the L2 norm, but do not square the result.</span>

<span class="sd">    Set input as x and output as loss.</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss = \frac{\sum x ^ 2}{2}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor for computing the L2 norm. Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has a Scalar Tensor with the same data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; l2_loss = L2Loss()</span>
<span class="sd">        &gt;&gt;&gt; output = l2_loss(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        7.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize L2Loss&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="DataFormatDimMap"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.DataFormatDimMap.html#mindspore.ops.DataFormatDimMap">[文档]</a><span class="k">class</span> <span class="nc">DataFormatDimMap</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the dimension index in the destination data format given in the source data format.</span>

<span class="sd">    Args:</span>
<span class="sd">        src_format (str): An optional value for source data format. The format can be &#39;NHWC&#39; and &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NHWC&#39;.</span>
<span class="sd">        dst_format (str): An optional value for destination data format. The format can be &#39;NHWC&#39; and &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A Tensor, each element is used as a dimension index of the source data format.</span>
<span class="sd">          The suggested values are in the range [-4, 4). Only supports int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, Return the dimension index in the given target data format,</span>
<span class="sd">        has the same data type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `src_format` or `dst_format` is not a str.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor whose dtype is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([0, 1, 2, 3], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dfdm = ops.DataFormatDimMap()</span>
<span class="sd">        &gt;&gt;&gt; output = dfdm(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 3 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="n">dst_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DataFormatDimMap.&quot;&quot;&quot;</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;NCHW&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">src_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;src_format&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">dst_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;dst_format&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="RNNTLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.RNNTLoss.html#mindspore.ops.RNNTLoss">[文档]</a><span class="k">class</span> <span class="nc">RNNTLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the RNNTLoss and its gradient with respect to the softmax outputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank_label (int): blank label. Default: 0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **acts** (Tensor) - Tensor of shape :math:`(B, T, U, V)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape :math:`(B, U-1)`. Data type is int32.</span>
<span class="sd">        - **input_lengths** (Tensor) - Tensor of shape :math:`(B,)`. Data type is int32.</span>
<span class="sd">        - **label_lengths** (Tensor) - Tensor of shape :math:`(B,)`. Data type is int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **costs** (Tensor) - Tensor of shape :math:`(B,)`. Data type is int32.</span>
<span class="sd">        - **grads** (Tensor) - Has the same shape and dtype as `acts`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `acts`, `labels`, `input_lengths` or `label_lengths` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `acts` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `labels`, `input_lengths` or `label_lengths` is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; B, T, U, V = 1, 2, 3, 5</span>
<span class="sd">        &gt;&gt;&gt; blank = 0</span>
<span class="sd">        &gt;&gt;&gt; acts = np.random.random((B, T, U, V)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = np.array([[1, 2]]).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_length = np.array([T] * B).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; label_length = np.array([len(l) for l in labels]).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; rnnt_loss = ops.RNNTLoss(blank_label=0)</span>
<span class="sd">        &gt;&gt;&gt; costs, grads = rnnt_loss(Tensor(acts), Tensor(labels), Tensor(input_length), Tensor(label_length))</span>
<span class="sd">        &gt;&gt;&gt; print(costs.shape)</span>
<span class="sd">        (1,)</span>
<span class="sd">        &gt;&gt;&gt; print(grads.shape)</span>
<span class="sd">        (1, 2, 3, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank_label</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize RNNTLoss.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;blank_label&#39;</span><span class="p">,</span> <span class="n">blank_label</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acts&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="s1">&#39;input_length&#39;</span><span class="p">,</span> <span class="s1">&#39;label_length&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;costs&#39;</span><span class="p">,</span> <span class="s1">&#39;grads&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">,</span> <span class="n">input_length_shape</span><span class="p">,</span> <span class="n">label_length_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">acts_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;acts_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;labels_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_length_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;input_length_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label_length_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;label_length_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;labels shape[0]&#39;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;labels shape[1]&#39;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;acts shape[2]-1&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;input_length size&#39;</span><span class="p">,</span> <span class="n">input_length_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;label_length size&#39;</span><span class="p">,</span> <span class="n">label_length_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">costs_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span>
        <span class="k">return</span> <span class="n">costs_shape</span><span class="p">,</span> <span class="n">acts_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">acts_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">,</span> <span class="n">input_length_type</span><span class="p">,</span> <span class="n">label_length_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;acts_type&quot;</span><span class="p">,</span> <span class="n">acts_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span>
                          <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,),</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="s2">&quot;input_length&quot;</span><span class="p">,</span> <span class="s2">&quot;label_length&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">labels_type</span><span class="p">,</span> <span class="n">input_length_type</span><span class="p">,</span> <span class="n">label_length_type</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">acts_type</span><span class="p">,</span> <span class="n">acts_type</span></div>


<div class="viewcode-block" id="SGD"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SGD.html#mindspore.ops.SGD">[文档]</a><span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the stochastic gradient descent. Momentum is optional.</span>

<span class="sd">    Nesterov momentum is based on the formula from paper `On the importance of</span>
<span class="sd">    initialization and momentum in deep learning &lt;http://proceedings.mlr.press/v28/sutskever13.html&gt;`_.</span>

<span class="sd">    Note:</span>
<span class="sd">        If parameters are not grouped, the `weight_decay` in optimizer will be applied on the network parameters without</span>
<span class="sd">        &#39;beta&#39; or &#39;gamma&#39; in their names. Users can group parameters to change the strategy of decaying weight. When</span>
<span class="sd">        parameters are grouped, each group can set `weight_decay`. If not, the `weight_decay` in optimizer will be</span>
<span class="sd">        applied.</span>
<span class="sd">        For more details, please refer to :class:`mindspore.nn.SGD`.</span>

<span class="sd">    Args:</span>
<span class="sd">        dampening (float): The dampening for momentum. Default: 0.0.</span>
<span class="sd">        weight_decay (float): Weight decay (L2 penalty). Default: 0.0.</span>
<span class="sd">        nesterov (bool): Enable Nesterov momentum. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **parameters** (Tensor) - Parameters to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, with float16 or float32 data type.</span>
<span class="sd">        - **learning_rate** (Tensor) - Learning rate, a scalar tensor with float16 or float32 data type.</span>
<span class="sd">          e.g. Tensor(0.1, mindspore.float32)</span>
<span class="sd">        - **accum** (Tensor) - Accum(velocity) to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **momentum** (Tensor) - Momentum, a scalar tensor with float16 or float32 data type.</span>
<span class="sd">          e.g. Tensor(0.1, mindspore.float32).</span>
<span class="sd">        - **stat** (Tensor) - States to be updated with the same shape as gradient, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dampening` or `weight_decay` is not a float.</span>
<span class="sd">        TypeError: If `nesterov` is not a bool.</span>
<span class="sd">        TypeError: If `parameters`, `gradient`, `learning_rate`, `accum`, `momentum` or `stat` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `parameters`, `gradient`, `learning_rate`, `accum`, `momentum` or `stat` is neither</span>
<span class="sd">                   float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; sgd = ops.SGD()</span>
<span class="sd">        &gt;&gt;&gt; parameters = Tensor(np.array([2, -0.5, 1.7, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([1, -1, 0.5, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; learning_rate = Tensor(0.01, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; accum = Tensor(np.array([0.1, 0.3, -0.2, -0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; momentum = Tensor(0.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; stat = Tensor(np.array([1.5, -0.3, 0.2, -0.7]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = sgd(parameters, gradient, learning_rate, accum, momentum, stat)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [1.99 -0.4903 1.695 3.9801]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dampening</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SGD.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;nesterov&quot;</span><span class="p">,</span> <span class="n">nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">nesterov</span> <span class="ow">and</span> <span class="n">dampening</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;dampening&#39; must be 0 when &#39;nesterov&#39; is True, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got &#39;dampening&#39; is </span><span class="si">{</span><span class="n">dampening</span><span class="si">}</span><span class="s2"> and &#39;nesterov&#39; is </span><span class="si">{</span><span class="n">nesterov</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;stat&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_shape</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">learning_rate_shape</span><span class="p">,</span>
                    <span class="n">accum_shape</span><span class="p">,</span> <span class="n">momentum_shape</span><span class="p">,</span> <span class="n">stat_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gradient_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;gradient rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">learning_rate_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;learning rate rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">momentum_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;momentum rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">stat_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;stat rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;gradient shape&quot;</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="s2">&quot;stat shape&quot;</span><span class="p">,</span> <span class="n">stat_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span>
                    <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">stat_dtype</span><span class="p">):</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span>
                          <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;parameters&quot;</span><span class="p">,</span> <span class="s2">&quot;gradient&quot;</span><span class="p">,</span> <span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="s2">&quot;accum&quot;</span><span class="p">,</span> <span class="s2">&quot;momentum&quot;</span><span class="p">,</span> <span class="s2">&quot;stat&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">parameters_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">stat_dtype</span><span class="p">)))</span></div>


<div class="viewcode-block" id="ApplyRMSProp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyRMSProp.html#mindspore.ops.ApplyRMSProp">[文档]</a><span class="k">class</span> <span class="nc">ApplyRMSProp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the Root Mean Square prop(RMSProp) algorithm.</span>
<span class="sd">    Please refer to the usage in source code of :class:`mindspore.nn.RMSProp`.</span>

<span class="sd">    The updating formulas of ApplyRMSProp algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            s_{t+1} = \rho s_{t} + (1 - \rho)(\nabla Q_{i}(w))^2 \\</span>
<span class="sd">            m_{t+1} = \beta m_{t} + \frac{\eta} {\sqrt{s_{t+1} + \epsilon}} \nabla Q_{i}(w) \\</span>
<span class="sd">            w = w - m_{t+1}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`w` represents `var`, which will be updated.</span>
<span class="sd">    :math:`s_{t+1}` represents `mean_square`, :math:`s_{t}` is the last moment of :math:`s_{t+1}`,</span>
<span class="sd">    :math:`m_{t+1}` represents `moment`, :math:`m_{t}` is the last moment of :math:`m_{t+1}`.</span>
<span class="sd">    :math:`\rho` represents `decay`. :math:`\beta` is the momentum term, represents `momentum`.</span>
<span class="sd">    :math:`\epsilon` is a smoothing term to avoid division by zero, represents `epsilon`.</span>
<span class="sd">    :math:`\eta` represents `learning_rate`. :math:`\nabla Q_{i}(w)` represents `grad`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Note that in dense implementation of this algorithm, &quot;mean_square&quot; and &quot;moment&quot; will update even if &quot;grad&quot; is 0,</span>
<span class="sd">        but in this sparse implementation, &quot;mean_square&quot; and &quot;moment&quot; will not update</span>
<span class="sd">        in iterations during which &quot;grad&quot; is 0.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumulation tensors</span>
<span class="sd">                            from being updated. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be updated.</span>
<span class="sd">        - **mean_square** (Tensor) - Mean square gradients, must be the same type as `var`.</span>
<span class="sd">        - **moment** (Tensor) - Delta of `var`, must be the same type as `var`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. Must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - Gradient, must be the same type as `var`.</span>
<span class="sd">        - **decay** (float) - Decay rate. Only constant value is allowed.</span>
<span class="sd">        - **momentum** (float) - Momentum. Only constant value is allowed.</span>
<span class="sd">        - **epsilon** (float) - Ridge term. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `var`, `mean_square`, `moment` or `decay` is not a Tensor.</span>
<span class="sd">        TypeError: If `learning_rate` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `decay`, `momentum` or `epsilon` is not float.</span>
<span class="sd">        TypeError: If dtype of `learning_rate` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `decay`, `momentum` or `epsilon` is not a constant value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_rms_prop = ops.ApplyRMSProp()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, mean_square, moment, grad, decay, momentum, epsilon, lr):</span>
<span class="sd">        ...         out = self.apply_rms_prop(self.var, mean_square, moment, lr, grad, decay, momentum, epsilon)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; mean_square = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; moment = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(mean_square, moment, grad, 0.0, 1e-10, 0.001, 0.01)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.990005  0.990005]</span>
<span class="sd">         [0.990005  0.990005]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyRMSProp.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;mean_square&#39;</span><span class="p">,</span> <span class="s1">&#39;moment&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyCenteredRMSProp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyCenteredRMSProp.html#mindspore.ops.ApplyCenteredRMSProp">[文档]</a><span class="k">class</span> <span class="nc">ApplyCenteredRMSProp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the centered RMSProp algorithm.</span>
<span class="sd">    Please refer to the usage in source code of :class:`mindspore.nn.RMSProp`.</span>

<span class="sd">    The updating formulas of ApplyCenteredRMSProp algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            g_{t+1} = \rho g_{t} + (1 - \rho)\nabla Q_{i}(w) \\</span>
<span class="sd">            s_{t+1} = \rho s_{t} + (1 - \rho)(\nabla Q_{i}(w))^2 \\</span>
<span class="sd">            m_{t+1} = \beta m_{t} + \frac{\eta} {\sqrt{s_{t+1} - g_{t+1}^2 + \epsilon}} \nabla Q_{i}(w) \\</span>
<span class="sd">            w = w - m_{t+1}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`w` represents `var`, which will be updated.</span>
<span class="sd">    :math:`g_{t+1}` represents `mean_gradient`, :math:`g_{t}` is the last moment of :math:`g_{t+1}`.</span>
<span class="sd">    :math:`s_{t+1}` represents `mean_square`, :math:`s_{t}` is the last moment of :math:`s_{t+1}`,</span>
<span class="sd">    :math:`m_{t+1}` represents `moment`, :math:`m_{t}` is the last moment of :math:`m_{t+1}`.</span>
<span class="sd">    :math:`\rho` represents `decay`. :math:`\beta` is the momentum term, represents `momentum`.</span>
<span class="sd">    :math:`\epsilon` is a smoothing term to avoid division by zero, represents `epsilon`.</span>
<span class="sd">    :math:`\eta` represents `learning_rate`. :math:`\nabla Q_{i}(w)` represents `grad`.</span>

<span class="sd">    Note:</span>
<span class="sd">        The difference between `ApplyCenteredRMSProp` and `ApplyRMSProp` is that the former</span>
<span class="sd">        uses the centered RMSProp algorithm, and the centered RRMSProp algorithm uses an estimate of the centered second</span>
<span class="sd">        moment(i.e., the variance) for normalization, as opposed to regular RMSProp, which uses the (uncertained)</span>
<span class="sd">        second moment. This often helps with training, but is slightly more expensive in terms of computation and</span>
<span class="sd">        memory.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        In dense implementation of this algorithm, `mean_gradient`, `mean_square`, and `moment` will update</span>
<span class="sd">        even if the `grad` is zero. But in this sparse implementation, `mean_gradient`, `mean_square`, and `moment`</span>
<span class="sd">        will not update in iterations during which the `grad` is zero.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumulation tensors</span>
<span class="sd">                            from being updated. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be updated.</span>
<span class="sd">        - **mean_gradient** (Tensor) - Mean gradients, must be the same type as `var`.</span>
<span class="sd">        - **mean_square** (Tensor) - Mean square gradients, must be the same type as `var`.</span>
<span class="sd">        - **moment** (Tensor) - Delta of `var`, must be the same type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient, must be the same type as `var`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. Must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **decay** (float) - Decay rate.</span>
<span class="sd">        - **momentum** (float) - Momentum.</span>
<span class="sd">        - **epsilon** (float) - Ridge term.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `var`, `mean_gradient`, `mean_square`, `moment` or `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If `learing_rate` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `learing_rate` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `decay`, `momentum` or `epsilon` is not a float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_centerd_rms_prop = ops.ApplyCenteredRMSProp()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, mean_grad, mean_square, moment, grad, decay, momentum, epsilon, lr):</span>
<span class="sd">        ...         out = self.apply_centerd_rms_prop(self.var, mean_grad, mean_square, moment, grad,</span>
<span class="sd">        ...                                           lr, decay, momentum, epsilon)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; mean_grad = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; mean_square = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; moment = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(mean_grad, mean_square, moment, grad, 0.0, 1e-10, 0.001, 0.01)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.68377227  0.68377227]</span>
<span class="sd">         [0.68377227  0.68377227]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyCenteredRMSProp.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="LayerNorm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LayerNorm.html#mindspore.ops.LayerNorm">[文档]</a><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Layer Normalization to the input tensor.</span>

<span class="sd">    This operator will normalize the input tensor on given axis. LayerNorm is described in the paper</span>
<span class="sd">    `Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        begin_norm_axis (int): The begin axis of the `input_x` to apply LayerNorm,</span>
<span class="sd">            the value must be in [-1, rank(input)). Default: 1.</span>
<span class="sd">        begin_params_axis (int): The begin axis of the parameter input (`gamma`, `beta`) to</span>
<span class="sd">            apply LayerNorm, the value must be in [-1, rank(input)). Default: 1.</span>
<span class="sd">        epsilon (float): A value added to the denominator for numerical stability. Default: 1e-7.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)`.</span>
<span class="sd">          The input of LayerNorm.</span>
<span class="sd">        - **gamma** (Tensor) - Tensor of shape :math:`(P_0, \ldots, P_\text{begin_params_axis})`.</span>
<span class="sd">          The learnable parameter `gamma` as the scale on norm.</span>
<span class="sd">        - **beta** (Tensor) - Tensor of shape :math:`(P_0, \ldots, P_\text{begin_params_axis})`.</span>
<span class="sd">          The learnable parameter `beta` as the scale on norm.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], tuple of 3 tensors, the normalized input and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The normalized input, has the same type and shape as the `input_x`.</span>
<span class="sd">          The shape is :math:`(N, C)`.</span>
<span class="sd">        - **mean** (Tensor) - Tensor of shape :math:`(C,)`.</span>
<span class="sd">        - **variance** (Tensor) - Tensor of shape :math:`(C,)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `begin_norm_axis` or `begin_params_axis` is not an int.</span>
<span class="sd">        TypeError: If `epsilon` is not a float.</span>
<span class="sd">        TypeError: If `input_x`, `gamma` or `beta` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gamma = Tensor(np.ones([3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta = Tensor(np.ones([3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; layer_norm = ops.LayerNorm()</span>
<span class="sd">        &gt;&gt;&gt; output, mean, variance = layer_norm(input_x, gamma, beta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.2247448  1.         2.2247448]</span>
<span class="sd">         [-0.2247448  1.         2.2247448]]</span>
<span class="sd">        &gt;&gt;&gt; print(mean)</span>
<span class="sd">        [[2.]</span>
<span class="sd">         [2.]]</span>
<span class="sd">        &gt;&gt;&gt; print(variance)</span>
<span class="sd">        [[0.6666667]</span>
<span class="sd">         [0.6666667]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LayerNorm.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;begin_norm_axis&#39;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;begin_params_axis&#39;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="L2Normalize"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.L2Normalize.html#mindspore.ops.L2Normalize">[文档]</a><span class="k">class</span> <span class="nc">L2Normalize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    L2 Normalization Operator.</span>

<span class="sd">    This operator will normalize the input using the given axis. The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \displaylines{{\text{output} = \frac{x}{\sqrt{\text{max}( \sum_{i}^{}\left | x_i  \right | ^2, \epsilon)}}}}</span>

<span class="sd">    where :math:`\epsilon` is epsilon and :math:`\sum_{i}^{}\left | x_i  \right | ^2` calculate the sum of squares of</span>
<span class="sd">    the input `x` along the dimension `axis`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[list(int), tuple(int), int]): Specify the axis for calculating the L2 norm. Default: 0.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: 1e-4.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input to compute the normalization. Tensor of shape :math:`(N, \ldots)`.</span>
<span class="sd">          Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not one of the following: list, tuple or int.</span>
<span class="sd">        TypeError: If `epsilon` is not a float.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If dimension of `x` is not greater than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; l2_normalize = ops.L2Normalize()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randint(-256, 256, (2, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = l2_normalize(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize L2Normalize.&quot;&quot;&quot;</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">axis</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_attrs</span><span class="p">[</span><span class="s1">&#39;axis&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the length of &#39;axis&#39; must be 1, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;later will support multiple axis!&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span></div>


<span class="k">class</span> <span class="nc">DropoutGenMask</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The DropoutGenMask interface is deprecated, please use the :class:`mindspore.ops.Dropout` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.5&quot;</span><span class="p">,</span> <span class="s2">&quot;ops.Dropout&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Seed0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DropoutGenMask.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="s1">&#39;keep_prob&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Seed0&quot;</span><span class="p">,</span> <span class="n">Seed0</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Seed1&quot;</span><span class="p">,</span> <span class="n">Seed1</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_hidden&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">DropoutDoMask</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The DropoutDoMask interface is deprecated, please use the :class:`mindspore.ops.Dropout` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.5&quot;</span><span class="p">,</span> <span class="s2">&quot;ops.Dropout&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>


<div class="viewcode-block" id="ResizeBilinear"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ResizeBilinear.html#mindspore.ops.ResizeBilinear">[文档]</a><span class="k">class</span> <span class="nc">ResizeBilinear</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes an image to a certain size using the bilinear interpolation.</span>

<span class="sd">    The resizing only affects the lower two dimensions which represent the height and width. The input images</span>
<span class="sd">    can be represented by different data types, but the data types of output images are always float32.</span>

<span class="sd">    For general resize, refer to :func:`mindspore.ops.interpolate` for more detail.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This interface does not support dynamic shape and is subject to change or deletion,</span>
<span class="sd">        use :func:`mindspore.ops.interpolate` instead.</span>

<span class="sd">    Args:</span>
<span class="sd">        size (Union[tuple[int], list[int]]): A tuple or list of 2 int elements :math:`(new\_height, new\_width)`,</span>
<span class="sd">            the new size of the images.</span>
<span class="sd">        align_corners (bool): If true, rescale input by :math:`(new\_height - 1) / (height - 1)`,</span>
<span class="sd">                       which exactly aligns the 4 corners of images and resized images. If false,</span>
<span class="sd">                       rescale by :math:`new\_height / height`. Default: False.</span>
<span class="sd">        half_pixel_centers (bool): Whether half pixel center. If set to True, `align_corners` should be False.</span>
<span class="sd">                           Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Image to be resized. Input images must be a 4-D tensor with shape</span>
<span class="sd">          :math:`(batch, channels, height, width)`, with data type of float32 or float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, resized image. 4-D with shape :math:`(batch, channels, new\_height, new\_width)`,</span>
<span class="sd">        with the same data type as input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `size` is neither a tuple nor list.</span>
<span class="sd">        TypeError: If `align_corners` is not a bool.</span>
<span class="sd">        TypeError: If `half_pixel_centers` is not a bool.</span>
<span class="sd">        TypeError: If `align_corners` and `half_pixel_centers` are all True.</span>
<span class="sd">        TypeError: If `half_pixel_centers` is True and device_target not Ascend.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; resize_bilinear = ops.ResizeBilinear((5, 5))</span>
<span class="sd">        &gt;&gt;&gt; output = resize_bilinear(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ResizeBilinear.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;size len&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">size</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;size item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;size item&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span>
                                                             <span class="n">half_pixel_centers</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">half_pixel_centers</span> <span class="ow">and</span> <span class="n">align_corners</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;If half_pixel_centers is True, align_corners must be False, but got </span><span class="si">{</span><span class="n">align_corners</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">half_pixel_centers</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">!=</span> <span class="s2">&quot;ascend&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Currently `half_pixel_centers`=True only support in Ascend device_target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">th value of size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;dimension of input&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
            <span class="n">out_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;input_dtype&#39;</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_dtype</span></div>


<span class="k">class</span> <span class="nc">UpsampleTrilinear3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs upsampling with trilinear interpolation across 3dims for 5dim inputs.</span>

<span class="sd">    This operator scale up the volumetric input with specified `output_size` or `scales` factors,</span>
<span class="sd">    using trilinear upscaling algorithm.</span>

<span class="sd">    Note:</span>
<span class="sd">        One of `scales` and `output_size` MUST be specified and it is an error if both are specified.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size (Union[tuple[int], list[int]]):  A tuple or list of 3 int</span>
<span class="sd">            elements :math:`(output\_depth, output\_height, output\_width)`.</span>
<span class="sd">            Defaults to None. Only one of `scales` and `output_size` can be specified.</span>
<span class="sd">        scales (Union[tuple[float], list[float]]): A tuple or list of 3 float</span>
<span class="sd">           elements :math:`(scale\_depth, scale\_height, scale\_width)`. Defaults to None.</span>
<span class="sd">        align_corners (bool): An optional bool. Defaults to false.</span>
<span class="sd">            If true, the input and output tensors are aligned by the center points of their corner pixels,</span>
<span class="sd">            preserving the values at the corner pixels.</span>
<span class="sd">            If false, the input and output tensors are aligned by the corner points of their corner pixels,</span>
<span class="sd">            and the interpolation use edge value padding for out of boundary values.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 5-D input tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Must be one of the following types: float16, float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Upsampled output with the same data type as `x`.</span>
<span class="sd">          Tensor of shape :math:`(N, C, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: When `output_size` is not none and `output_size` is not list[int] or tuple[int].</span>
<span class="sd">        TypeError: When `scales` is not none and `scales` is not list[float] or tuple[float].</span>
<span class="sd">        TypeError: If dtype of `x` is not in [float16, float32, float64].</span>
<span class="sd">        TypeError: If type of `align_corners` is not bool.</span>
<span class="sd">        ValueError: If any value of `output_size` is negative or zero when `output_size` is not empty.</span>
<span class="sd">        ValueError: If any value of `scales` is negative or zero when `scales` is not empty.</span>
<span class="sd">        ValueError: If shape of `x` is not 5D.</span>
<span class="sd">        ValueError: If none of `scales` and `output_size` is specified or both specified.</span>
<span class="sd">        ValueError: If size of `scales` is not equal 3 when `scales` is specified.</span>
<span class="sd">        ValueError: If size of `output_size` is not equal 3 when `output_size` is specified.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; ops = P.UpsampleTrilinear3D(output_size=[4, 64, 48])</span>
<span class="sd">        &gt;&gt;&gt; out = ops(Tensor(input_data=np.random.randn(2, 3, 4, 512, 256)))</span>
<span class="sd">        &gt;&gt;&gt; print(out.shape)</span>
<span class="sd">        (2, 3, 4, 64, 48)</span>

<span class="sd">        &gt;&gt;&gt; ops = P.UpsampleTrilinear3D(output_size=[2, 4, 4])</span>
<span class="sd">        &gt;&gt;&gt; in_x = Tensor(np.arange(1, 5, dtype=np.float32).reshape((1, 1, 1, 2, 2)))</span>
<span class="sd">        &gt;&gt;&gt; out = ops(in_x)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[[[[1.   1.25 1.75 2.  ]</span>
<span class="sd">            [1.5  1.75 2.25 2.5 ]</span>
<span class="sd">            [2.5  2.75 3.25 3.5 ]</span>
<span class="sd">            [3.   3.25 3.75 4.  ]]</span>
<span class="sd">           [[1.   1.25 1.75 2.  ]</span>
<span class="sd">            [1.5  1.75 2.25 2.5 ]</span>
<span class="sd">            [2.5  2.75 3.25 3.5 ]</span>
<span class="sd">            [3.   3.25 3.75 4.  ]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize UpsampleTrilinear3D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scales</span> <span class="o">=</span> <span class="p">[]</span> <span class="k">if</span> <span class="n">scales</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">scales</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span> <span class="o">=</span> <span class="n">align_corners</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;scales&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scales</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int_sequence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scales</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float_sequence</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scales</span><span class="p">,</span> <span class="s2">&quot;scales&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;scales&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scales</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;align_corners&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span>


<div class="viewcode-block" id="OneHot"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.OneHot.html#mindspore.ops.OneHot">[文档]</a><span class="k">class</span> <span class="nc">OneHot</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a one-hot tensor.</span>

<span class="sd">    The locations represented by indices in `indices` take value `on_value`, while all</span>
<span class="sd">    other locations take value `off_value`.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the input indices is rank `N`, the output will have rank `N+1`. The new axis is created at dimension `axis`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Position to insert the value. e.g. If shape of `indices` is :math:`(N, C)`, and `axis` is -1,</span>
<span class="sd">            the output shape will be :math:`(N, C, D)`, If `axis` is 0, the output shape will be :math:`(D, N, C)`.</span>
<span class="sd">            Default: -1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices. Tensor of shape :math:`(X_0, \ldots, X_n)`.</span>
<span class="sd">          Data type must be uint8, int32 or int64.</span>
<span class="sd">        - **depth** (int) - A scalar defining the depth of the one-hot dimension.</span>
<span class="sd">        - **on_value** (Tensor) - A value to fill in output when `indices[j] = i`.</span>
<span class="sd">          Support uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64,</span>
<span class="sd">          bool, complex64, complex128.</span>
<span class="sd">        - **off_value** (Tensor) - A value to fill in output when `indices[j] != i`.</span>
<span class="sd">          Has the same data type as `on_value`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, one-hot tensor. Tensor of shape :math:`(X_0, \ldots, X_{axis}, \text{depth} ,X_{axis+1}, \ldots, X_n)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` or `depth` is not an int.</span>
<span class="sd">        TypeError: If dtype of `indices` is not uint8, int32 or int64.</span>
<span class="sd">        TypeError: If `indices`, `on_value` or `off_value` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is not in range [-1, len(indices_shape)].</span>
<span class="sd">        ValueError: If `depth` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; depth, on_value, off_value = 3, Tensor(1.0, mindspore.float32), Tensor(0.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; onehot = ops.OneHot()</span>
<span class="sd">        &gt;&gt;&gt; output = onehot(indices, depth, on_value, off_value)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0. 0.]</span>
<span class="sd">         [0. 1. 0.]</span>
<span class="sd">         [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize OneHot.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;depth&#39;</span><span class="p">,</span> <span class="s1">&#39;on_value&#39;</span><span class="p">,</span> <span class="s1">&#39;off_value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Gelu</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as operator GeLU. Gelu will be deprecated in the future.</span>
<span class="sd">    Please use GeLU instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.1&quot;</span><span class="p">,</span> <span class="s2">&quot;GeLU&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Gelu&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span>


<div class="viewcode-block" id="GeLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GeLU.html#mindspore.ops.GeLU">[文档]</a><span class="k">class</span> <span class="nc">GeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Error Linear Units activation function.</span>

<span class="sd">    GeLU is described in the paper `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_.</span>
<span class="sd">    And also please refer to `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>
<span class="sd">    &lt;https://arxiv.org/abs/1810.04805&gt;`_.</span>

<span class="sd">    GeLU is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        GELU(x_i) = x_i*P(X &lt; x_i)</span>

<span class="sd">    where :math:`P` is the cumulative distribution function of the standard Gaussian distribution,</span>
<span class="sd">    :math:`x_i` is the input element.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input of the activation function GeLU, the data type is float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gelu = ops.GeLU()</span>
<span class="sd">        &gt;&gt;&gt; result = gelu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [0.841192  1.9545976  2.9963627]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GeLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">FastGelu</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Same as operator FastGeLU. FastGelu will be deprecated in the future.</span>
<span class="sd">    Please use FastGeLU instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.1&quot;</span><span class="p">,</span> <span class="s2">&quot;FastGeLU&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FastGelu.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">input_x</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;input_x&quot;</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">input_x</span>


<div class="viewcode-block" id="FastGeLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FastGeLU.html#mindspore.ops.FastGeLU">[文档]</a><span class="k">class</span> <span class="nc">FastGeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fast Gaussian Error Linear Units activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.fast_gelu` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; fast_gelu = ops.FastGeLU()</span>
<span class="sd">        &gt;&gt;&gt; output = fast_gelu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.5418735e-01  3.9921875e+00 -9.7473649e-06]</span>
<span class="sd">         [ 1.9375000e+00 -1.0052517e-03  8.9824219e+00]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FastGeLU.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="GetNext"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GetNext.html#mindspore.ops.GetNext">[文档]</a><span class="k">class</span> <span class="nc">GetNext</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the next element in the dataset queue.</span>

<span class="sd">    Note:</span>
<span class="sd">        The GetNext operation needs to be associated with network and it also depends</span>
<span class="sd">        on the &#39;dataset&#39; interface, For example, please refer to :class:`mindspore.dataset.MnistDataset` .</span>
<span class="sd">        it can&#39;t be used directly as a single operation.</span>
<span class="sd">        For details, please refer to :class:`mindspore.connect_network_with_dataset` source code.</span>

<span class="sd">    Args:</span>
<span class="sd">        types (list[:class:`mindspore.dtype`]): The type of the outputs.</span>
<span class="sd">        shapes (list[tuple[int]]): The dimensionality of the outputs.</span>
<span class="sd">        output_num (int): The output number, length of `types` and `shapes`.</span>
<span class="sd">        shared_name (str): Queue name to fetch the data.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        No inputs.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], the output of dataset. The shape is described in `shapes`</span>
<span class="sd">        and the type is described in `types`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dataset as ds</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.common import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; data_path = &quot;/path/to/MNIST_Data/train/&quot;</span>
<span class="sd">        &gt;&gt;&gt; train_dataset = ds.MnistDataset(data_path, num_samples=10)</span>
<span class="sd">        &gt;&gt;&gt; dataset_helper = mindspore.DatasetHelper(train_dataset, dataset_sink_mode=True)</span>
<span class="sd">        &gt;&gt;&gt; dataset = dataset_helper.iter.dataset</span>
<span class="sd">        &gt;&gt;&gt; dataset_types, dataset_shapes = dataset_helper.types_shapes()</span>
<span class="sd">        &gt;&gt;&gt; queue_name = dataset.__transfer_dataset__.queue_name</span>
<span class="sd">        &gt;&gt;&gt; get_next = ops.GetNext(dataset_types, dataset_shapes, len(dataset_types), queue_name)</span>
<span class="sd">        &gt;&gt;&gt; data, label = get_next()</span>
<span class="sd">        &gt;&gt;&gt; relu = ops.ReLU()</span>
<span class="sd">        &gt;&gt;&gt; result = relu(data.astype(mstype.float32))</span>
<span class="sd">        &gt;&gt;&gt; print(result.shape)</span>
<span class="sd">        (28, 28, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GetNext.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;types&quot;</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shapes&quot;</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;types length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">types</span><span class="p">),</span> <span class="s2">&quot;shapes length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_num&quot;</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="PReLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.PReLU.html#mindspore.ops.PReLU">[文档]</a><span class="k">class</span> <span class="nc">PReLU</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric Rectified Linear Unit activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.prelu` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.prelu = ops.PReLU()</span>
<span class="sd">        ...     def construct(self, x, weight):</span>
<span class="sd">        ...         result = self.prelu(x, weight)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(-6, 6).reshape((2, 3, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.1, 0.6, -0.3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[-0.60 -0.50]</span>
<span class="sd">          [-2.40 -1.80]</span>
<span class="sd">          [ 0.60  0.30]]</span>
<span class="sd">         [[ 0.00  1.00]</span>
<span class="sd">          [ 2.00  3.00]</span>
<span class="sd">          [ 4.0   5.00]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="LSTM"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LSTM.html#mindspore.ops.LSTM">[文档]</a><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the Long Short-Term Memory (LSTM) on the input.</span>

<span class="sd">    For detailed information, please refer to :class:`mindspore.nn.LSTM`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Number of features of input.</span>
<span class="sd">        hidden_size (int):  Number of features of hidden layer.</span>
<span class="sd">        num_layers (int): Number of layers of stacked LSTM.</span>
<span class="sd">        has_bias (bool): Whether the cell has bias `b_ih` and `b_hh`.</span>
<span class="sd">        bidirectional (bool): Specifies whether it is a bidirectional LSTM.</span>
<span class="sd">        dropout (float): If not 0, append `Dropout` layer on the outputs of each</span>
<span class="sd">            LSTM layer except the last layer. The range of dropout is [0.0, 1.0].</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape (seq_len, batch_size, `input_size`) or</span>
<span class="sd">          (batch_size, seq_len, `input_size`).</span>
<span class="sd">        - **h** (tuple) - Tensor of shape (num_directions * `num_layers`, batch_size, `hidden_size`).</span>
<span class="sd">        - **c** (tuple) - Tensor of shape (num_directions * `num_layers`, batch_size, `hidden_size`).</span>
<span class="sd">        - **w** (Tensor) - A weight Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, a tuple contains (`output`, `h_n`, `c_n`, `reserve`, `state`).</span>

<span class="sd">        - **output** (Tensor) - Tensor of shape (seq_len, batch_size, num_directions * `hidden_size`).</span>
<span class="sd">        - **h_n** (Tensor) - Tensor of shape (num_directions * `num_layers`, batch_size, `hidden_size`).</span>
<span class="sd">        - **c_n** (Tensor) - Tensor of shape (num_directions * `num_layers`, batch_size, `hidden_size`).</span>
<span class="sd">        - **reserve** (Tensor) - Tensor of shape (r, 1).</span>
<span class="sd">        - **state** (Tensor) - Random number generator state and its shape is (s, 1).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_size`, `hidden_size` or `num_layers` is not an int.</span>
<span class="sd">        TypeError: If `has_bias` or `bidirectional` is not a bool.</span>
<span class="sd">        TypeError: If `dropout` is not a float.</span>
<span class="sd">        ValueError: If `dropout` is not in range [0.0, 1.0].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_size = 10</span>
<span class="sd">        &gt;&gt;&gt; hidden_size = 2</span>
<span class="sd">        &gt;&gt;&gt; num_layers = 1</span>
<span class="sd">        &gt;&gt;&gt; seq_len = 5</span>
<span class="sd">        &gt;&gt;&gt; batch_size = 2</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = ops.LSTM(input_size, hidden_size, num_layers, True, False, 0.0)</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones([seq_len, batch_size, input_size]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; h0 = Tensor(np.ones([num_layers, batch_size, hidden_size]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; c0 = Tensor(np.ones([num_layers, batch_size, hidden_size]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; w = Tensor(np.ones([112, 1, 1]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output, hn, cn, _, _ = net(input_tensor, h0, c0, w)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[0.9640267  0.9640267 ]</span>
<span class="sd">          [0.9640267  0.9640267 ]]</span>
<span class="sd">         [[0.9950539  0.9950539 ]</span>
<span class="sd">          [0.9950539  0.9950539 ]]</span>
<span class="sd">         [[0.99932843 0.99932843]</span>
<span class="sd">          [0.99932843 0.99932843]]</span>
<span class="sd">         [[0.9999084  0.9999084 ]</span>
<span class="sd">          [0.9999084  0.9999084 ]]</span>
<span class="sd">         [[0.9999869  0.9999869 ]</span>
<span class="sd">          [0.9999869  0.9999869 ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LSTM.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="s2">&quot;input_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="s2">&quot;num_layers&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;has_bias&quot;</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;bidirectional&quot;</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bidirectional</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="p">,</span> <span class="s2">&quot;x[2]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="c1"># h and c should be same shape</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;h rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="s2">&quot;c_shape&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;h[1]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">h_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h[2]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span><span class="p">)</span>

        <span class="c1"># set arbitrary shape for reserved space</span>
        <span class="n">reserved_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">state_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">reserved_shape</span><span class="p">,</span> <span class="n">state_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;h&#39;</span><span class="p">:</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">x_dtype</span></div>


<div class="viewcode-block" id="SigmoidCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SigmoidCrossEntropyWithLogits.html#mindspore.ops.SigmoidCrossEntropyWithLogits">[文档]</a><span class="k">class</span> <span class="nc">SigmoidCrossEntropyWithLogits</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses the given logits to compute sigmoid cross entropy between the logits and the label.</span>

<span class="sd">    Measures the distribution error in discrete classification tasks where each class is independent</span>
<span class="sd">    and not mutually exclusive using cross entropy loss.</span>

<span class="sd">    Sets input logits as :math:`X`, input label as :math:`Y`, output as :math:`loss`. Then,</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}} \\</span>
<span class="sd">            loss_{ij} = -[Y_{ij} * ln(p_{ij}) + (1 - Y_{ij})ln(1 - p_{ij})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits. Tensor of shape :math:`(N, *)` where :math:`*` means, any number</span>
<span class="sd">          of additional dimensions.</span>
<span class="sd">        - **label** (Tensor) - Ground truth label. With the same shape and type as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and type as input `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits` or `label` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[-0.8, 1.2, 0.7], [-0.1, -0.4, 0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[0.3, 0.8, 1.2], [-0.6, 0.1, 2.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; sigmoid = ops.SigmoidCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; output = sigmoid(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.6111007   0.5032824   0.26318604]</span>
<span class="sd">         [ 0.58439666  0.5530153  -0.4368139 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SigmoidCrossEntropyWithLogits&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;predict&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="BCEWithLogitsLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BCEWithLogitsLoss.html#mindspore.ops.BCEWithLogitsLoss">[文档]</a><span class="k">class</span> <span class="nc">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds sigmoid activation function to input `logits`, and uses the given logits to compute binary cross entropy</span>
<span class="sd">    between the logits and the label.</span>

<span class="sd">    Sets input logits as :math:`X`, input label as :math:`Y`, input weight as :math:`W`, output as :math:`L`. Then,</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}} \\</span>
<span class="sd">            L_{ij} = -[Y_{ij} * log(p_{ij}) + (1 - Y_{ij})log(1 - p_{ij})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`i` indicates the :math:`i^{th}` sample, :math:`j` indicates the category. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`\ell` indicates the method of calculating the loss. There are three methods:</span>
<span class="sd">    the first method is to provide the loss value directly,</span>
<span class="sd">    the second method is to calculate the average value of all losses,</span>
<span class="sd">    and the third method is to calculate the sum of all losses.</span>

<span class="sd">    This operator will multiply the output by the corresponding weight.</span>
<span class="sd">    The tensor weight assigns different weights to each piece of data in the batch,</span>
<span class="sd">    and the tensor pos_weight adds corresponding weights to the positive examples of each category.</span>

<span class="sd">    In addition, it can trade off recall and precision by adding weights to positive examples.</span>
<span class="sd">    In the case of multi-label classification the loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij,c} = sigmoid(X_{ij,c}) = \frac{1}{1 + e^{-X_{ij,c}}} \\</span>
<span class="sd">            L_{ij,c} = -[P_{c}Y_{ij,c} * log(p_{ij,c}) + (1 - Y_{ij,c})log(1 - p_{ij,c})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where c is the class number (c&gt;1 for multi-label binary classification, c=1 for single-label binary classification),</span>
<span class="sd">    n is the number of the sample in the batch and :math:`p_c` is the weight of the positive answer for the class c.</span>
<span class="sd">    :math:`p_c&gt;1` increases the recall, :math:`p_c&lt;1` increases the precision.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Type of reduction to be applied to loss. The optional values are &#39;mean&#39;, &#39;sum&#39;, and &#39;none&#39;,</span>
<span class="sd">             not case sensitive. If &#39;none&#39;, do not perform reduction. Default: &#39;mean&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits. Data type must be float16 or float32.</span>
<span class="sd">          Tensor of shape :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **label** (Tensor) - Ground truth label, has the same shape as `logits`.</span>
<span class="sd">          Data type must be float16 or float32.</span>
<span class="sd">        - **weight** (Tensor) - A rescaling weight applied to the loss of each batch element. It can be</span>
<span class="sd">          broadcast to a tensor with shape of `logits`. Data type must be float16 or float32.</span>
<span class="sd">        - **pos_weight** (Tensor) - A weight of positive examples. Must be a vector with length equal to the</span>
<span class="sd">          number of classes. It can be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">          Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, it&#39;s a tensor with the same shape and type as input `logits`.</span>
<span class="sd">        Otherwise, the output is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If any input is not Tensor.</span>
<span class="sd">        TypeError: If data type of any input is neither float16 nor float32.</span>
<span class="sd">        TypeError: If data type of `reduction` is not string.</span>
<span class="sd">        ValueError: If `weight` or `pos_weight` can not be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[-0.8, 1.2, 0.7], [-0.1, -0.4, 0.7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; label = Tensor(np.array([[0.3, 0.8, 1.2], [-0.6, 0.1, 2.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pos_weight = Tensor(np.array([1.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.BCEWithLogitsLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, label, weight, pos_weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.3463612</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BCEWithLogitsLoss&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;BCEWithLogitsLoss&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Pad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Pad.html#mindspore.ops.Pad">[文档]</a><span class="k">class</span> <span class="nc">Pad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings.</span>

<span class="sd">    Refer to :func:`mindspore.ops.pad` for more detail. Use :func:`mindspore.ops.pad` instead if `paddings` has</span>
<span class="sd">    negative values.</span>

<span class="sd">    Args:</span>
<span class="sd">        paddings (tuple): The shape of parameter `paddings` is (N, 2). N is the rank of input data. All elements of</span>
<span class="sd">            paddings are int type. For the input in `D` th dimension, paddings[D, 0] indicates how many sizes to be</span>
<span class="sd">            extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1] indicates how many sizes to</span>
<span class="sd">            be extended behind the input tensor in the `D` th dimension.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `paddings` is not a tuple.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of `paddings` is not :math:`(N, 2)`.</span>
<span class="sd">        ValueError: If paddings.size is not equal to 2 * len(input_x).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pad_op = ops.Pad(((1, 2), (2, 1)))</span>
<span class="sd">        &gt;&gt;&gt; output = pad_op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.   0.   0.   0.   0.   0. ]</span>
<span class="sd">         [ 0.   0.  -0.1  0.3  3.6  0. ]</span>
<span class="sd">         [ 0.   0.   0.4  0.5 -3.2  0. ]</span>
<span class="sd">         [ 0.   0.   0.   0.   0.   0. ]</span>
<span class="sd">         [ 0.   0.   0.   0.   0.   0. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;paddings&quot;</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paddings</span> <span class="o">=</span> <span class="n">paddings</span></div>


<span class="k">class</span> <span class="nc">PadV3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings, mode and paddings_contiguous.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (str): An optional string, Defaults to &quot;constant&quot;, indicates padding mode,</span>
<span class="sd">            support &quot;constant&quot;, &quot;reflect&quot;, &quot;edge&quot;, Defaults to &quot;constant&quot;.</span>
<span class="sd">        paddings_contiguous (bool): An optional bool value, Defaults to True.</span>
<span class="sd">            If true, paddings is arranged as [begin0, end0, begin1, end1, ...]</span>
<span class="sd">            If false, paddings is arranged as [begin0, begin1, ..., end1, end2, ...]</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions.</span>
<span class="sd">        - **paddings** (Tensor) - Only constant value is allowed. A 1D tensor of type int32 or int64.</span>
<span class="sd">        - **constant_value** (Tensor, optional) - A tensor with the same type as `x`, padding value in &#39;constant&#39; mode.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `paddings` is not a Tensor.</span>
<span class="sd">        TypeError: If `padding_contiguous` is not a bool.</span>
<span class="sd">        ValueError: If `mode` is not a str or not in support modes.</span>
<span class="sd">        ValueError: If `mode` is constant, the element&#39;s number of paddings not be even.</span>
<span class="sd">        ValueError: If `mode` is constant, the element&#39;s number of paddings large than input dim * 2.</span>
<span class="sd">        ValueError: If `mode` is edge or reflect, the element&#39;s number of paddings is not 2, 4 or 6.</span>
<span class="sd">        ValueError: If `mode` is edge or reflect, x dims equal 3, the element&#39;s number of paddings is 2.</span>
<span class="sd">        ValueError: If `mode` is edge or reflect, x dims equal 4, the element&#39;s number of paddings is 4.</span>
<span class="sd">        ValueError: If `mode` is edge or reflect, x dims smaller than 3.</span>
<span class="sd">        ValueError: If `mode` is edge, x dims bigger than 5.</span>
<span class="sd">        ValueError: If `mode` is reflect, x dims bigger than 4.</span>
<span class="sd">        ValueError: If `mode` is reflect, padding size bigger than the corresponding x dimension.</span>
<span class="sd">        ValueError: After padding, output&#39;s shape number must be greater than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case1: mode=&quot;reflect&quot;, paddings_contiguous=True</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...    def __init__(self, mode, paddings_contiguous):</span>
<span class="sd">        ...        super(Net, self).__init__()</span>
<span class="sd">        ...        self.pad = ops.PadV3(mode=mode, paddings_contiguous=paddings_contiguous)</span>
<span class="sd">        ...        self.paddings = Tensor([1, 1])</span>
<span class="sd">        ...    def construct(self, x):</span>
<span class="sd">        ...        return self.pad(x, self.paddings)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[0., 1.]]])</span>
<span class="sd">        &gt;&gt;&gt; pad = Net(mode=&quot;reflect&quot;, paddings_contiguous=True)</span>
<span class="sd">        &gt;&gt;&gt; output = pad(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1., 0., 1., 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case2: mode=&quot;constant&quot;, padding_contigous=False</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...    def __init__(self, mode, paddings_contiguous):</span>
<span class="sd">        ...        super(Net, self).__init__()</span>
<span class="sd">        ...        self.pad = ops.PadV3(mode=mode, paddings_contiguous=paddings_contiguous)</span>
<span class="sd">        ...        self.paddings = Tensor([1, 0, 1, 0])</span>
<span class="sd">        ...        self.value = Tensor(1.5)</span>
<span class="sd">        ...    def construct(self, x):</span>
<span class="sd">        ...        return self.pad(x, self.paddings, self.value)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[0., 1., 2.]])</span>
<span class="sd">        &gt;&gt;&gt; pad = Net(mode=&quot;constant&quot;, paddings_contiguous=False)</span>
<span class="sd">        &gt;&gt;&gt; output = pad(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.5, 0., 1., 2., 1.5]]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">paddings_contiguous</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize PadV3&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;paddings&#39;</span><span class="p">,</span> <span class="s1">&#39;constant_value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="s1">&#39;reflect&#39;</span><span class="p">,</span> <span class="s1">&#39;edge&#39;</span><span class="p">],</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">paddings_contiguous</span><span class="p">,</span> <span class="s2">&quot;paddings_contiguous&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_const_input_indexes</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paddings_contiguous</span> <span class="o">=</span> <span class="n">paddings_contiguous</span>


<div class="viewcode-block" id="MirrorPad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MirrorPad.html#mindspore.ops.MirrorPad">[文档]</a><span class="k">class</span> <span class="nc">MirrorPad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings and mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (str): Specifies the padding mode. The optional values are &quot;REFLECT&quot; and &quot;SYMMETRIC&quot;.</span>
<span class="sd">            Default: &quot;REFLECT&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions.</span>
<span class="sd">        - **paddings** (Tensor) - Paddings requires constant tensor. The value of `paddings` is a</span>
<span class="sd">          matrix(list), and its shape is (N, 2). N is the rank of input data. All elements of paddings</span>
<span class="sd">          are int type. For the input in the `D` th dimension, paddings[D, 0] indicates how many sizes</span>
<span class="sd">          to be extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1]</span>
<span class="sd">          indicates how many sizes to be extended behind the input tensor in the `D` th dimension. Both</span>
<span class="sd">          paddings[D, 0] and paddings[D, 1] must be no greater than input_x.dim_size(D)</span>
<span class="sd">          (or input_x.dim_size(D) - 1) if mode is SYMMETRIC (if REFLECT, respectively).</span>


<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">        - If `mode` is &quot;REFLECT&quot;, it uses a way of symmetrical copying through the axis of symmetry to fill in.</span>
<span class="sd">          If the `input_x` is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the</span>
<span class="sd">          `Outputs` is [[6,5,4,5,6,5,4], [3,2,1,2,3,2,1], [6,5,4,5,6,5,4], [9,8,7,8,9,8,7], [6,5,4,5,6,5,4]].</span>
<span class="sd">          For a more intuitive understanding, please see the example below.</span>
<span class="sd">        - If `mode` is &quot;SYMMETRIC&quot;, the filling method is similar to the &quot;REFLECT&quot;. It is also copied</span>
<span class="sd">          according to the symmetry axis, except that it includes the symmetry axis. If the `input_x`</span>
<span class="sd">          is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the `Outputs` is</span>
<span class="sd">          [[2,1,1,2,3,3,2], [2,1,1,2,3,3,2], [5,4,4,5,6,6,5], [8,7,7,8,9,9,8], [8,7,7,8,9,9,8]].</span>
<span class="sd">          For a more intuitive understanding, please see the example below.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `paddings` is not a Tensor.</span>
<span class="sd">        TypeError: If `mode` is not a str.</span>
<span class="sd">        ValueError: If paddings.size is not equal to 2 * rank of input_x.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops</span>
<span class="sd">        &gt;&gt;&gt; # case1: mode=&quot;REFLECT&quot;</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...    def __init__(self, mode):</span>
<span class="sd">        ...        super(Net, self).__init__()</span>
<span class="sd">        ...        self.pad = ops.MirrorPad(mode=mode)</span>
<span class="sd">        ...        self.paddings = Tensor([[1, 1], [2, 2]])</span>
<span class="sd">        ...    def construct(self, input_x):</span>
<span class="sd">        ...        return self.pad(input_x, self.paddings)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[1,2,3], [4,5,6], [7,8,9]])</span>
<span class="sd">        &gt;&gt;&gt; pad = Net(&quot;REFLECT&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = pad(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[6 5 4 5 6 5 4]</span>
<span class="sd">         [3 2 1 2 3 2 1]</span>
<span class="sd">         [6 5 4 5 6 5 4]</span>
<span class="sd">         [9 8 7 8 9 8 7]</span>
<span class="sd">         [6 5 4 5 6 5 4]]</span>
<span class="sd">        &gt;&gt;&gt; # case2: mode=&quot;SYMMETRIC&quot;</span>
<span class="sd">        &gt;&gt;&gt; pad = Net(&quot;SYMMETRIC&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = pad(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2 1 1 2 3 3 2]</span>
<span class="sd">         [2 1 1 2 3 3 2]</span>
<span class="sd">         [5 4 4 5 6 6 5]</span>
<span class="sd">         [8 7 7 8 9 9 8]</span>
<span class="sd">         [8 7 7 8 9 9 8]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;REFLECT&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;paddings&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;REFLECT&#39;</span><span class="p">,</span> <span class="s1">&#39;SYMMETRIC&#39;</span><span class="p">],</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_const_input_indexes</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span></div>


<div class="viewcode-block" id="ComputeAccidentalHits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ComputeAccidentalHits.html#mindspore.ops.ComputeAccidentalHits">[文档]</a><span class="k">class</span> <span class="nc">ComputeAccidentalHits</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute accidental hits of sampled classes which match target classes.</span>

<span class="sd">    When a target class matches the sample class, we call it &quot;accidental hit&quot;.</span>
<span class="sd">    The result of calculating accidental hits contain three parts (index, id, weight),</span>
<span class="sd">    where index represents the row number in true_classes, and id represents the position in sampled_candidates,</span>
<span class="sd">    the weight is FLOAT_MAX. FLOAT_MAX indicates the max value in the type of Float</span>

<span class="sd">    Args:</span>
<span class="sd">        num_true (int): The number of target classes per training example. Default: 1.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **true_classes** (Tensor) - The target classes. With data type of int32 or int64</span>
<span class="sd">          and shape :math:`(batch\_size, num\_true)`.</span>
<span class="sd">        - **sampled_candidates** (Tensor) - The Candidate sampling results of operators, types of training samples,</span>
<span class="sd">          with data type of int32 or int64 and shape :math:`(num\_sampled, )`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors.</span>

<span class="sd">        - **indices** (Tensor) - A Tensor with shape :math:`(num\_accidental\_hits, )`,</span>
<span class="sd">          with the same type as `true_classes`.</span>
<span class="sd">        - **ids** (Tensor) - A Tensor with shape :math:`(num\_accidental\_hits, )`,</span>
<span class="sd">          with the same type as `true_classes`.</span>
<span class="sd">        - **weights** (Tensor) - A Tensor with shape :math:`(num\_accidental\_hits, )`, with the type float32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `num_true` is not int.</span>
<span class="sd">        TypeError: If `true_classes` or `sampled_candidates` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `true_classes` or `sampled_candidates` is neither int32 nor int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; true_classes = np.array([[1, 2], [0, 4], [3, 3]])</span>
<span class="sd">        &gt;&gt;&gt; sampled_candidates = np.array([0, 1, 2, 3, 4])</span>
<span class="sd">        &gt;&gt;&gt; sampler = ops.ComputeAccidentalHits(2)</span>
<span class="sd">        &gt;&gt;&gt; indices, ids, weights = sampler(Tensor(true_classes), Tensor(sampled_candidates))</span>
<span class="sd">        &gt;&gt;&gt; print(indices, ids, weights)</span>
<span class="sd">        [0 0 1 1 2 2]</span>
<span class="sd">        [1 2 0 4 3 3]</span>
<span class="sd">        [-3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38]</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ComputeAccidentalHits&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;true_classes&#39;</span><span class="p">,</span> <span class="s1">&#39;sampled_candidates&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;ids&#39;</span><span class="p">,</span> <span class="s1">&#39;weights&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;num_true&quot;</span><span class="p">,</span> <span class="n">num_true</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;num_true&quot;</span><span class="p">,</span> <span class="n">num_true</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_true</span> <span class="o">=</span> <span class="n">num_true</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">true_classes_shape</span><span class="p">,</span> <span class="n">sampled_candidates_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">true_classes_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;dim of true_classes&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sampled_candidates_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;dim of sampled_candidates&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;true_classes shape[1]&quot;</span><span class="p">,</span> <span class="n">true_classes_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;num_true&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_true</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">indices_len</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">indices_len</span><span class="p">,),</span> <span class="p">(</span><span class="n">indices_len</span><span class="p">,),</span> <span class="p">(</span><span class="n">indices_len</span><span class="p">,)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">true_classes_type</span><span class="p">,</span> <span class="n">sampled_candidates_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;true_classes_type&quot;</span><span class="p">,</span> <span class="n">true_classes_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;sampled_candidates_type&quot;</span><span class="p">,</span> <span class="n">sampled_candidates_type</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_types</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;true_classes_type&quot;</span><span class="p">,</span> <span class="n">true_classes_type</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;sampled_candidates_type&quot;</span><span class="p">,</span> <span class="n">sampled_candidates_type</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">weights_type</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
        <span class="k">return</span> <span class="n">true_classes_type</span><span class="p">,</span> <span class="n">true_classes_type</span><span class="p">,</span> <span class="n">weights_type</span></div>


<div class="viewcode-block" id="ROIAlign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ROIAlign.html#mindspore.ops.ROIAlign">[文档]</a><span class="k">class</span> <span class="nc">ROIAlign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Region of Interest (RoI) Align operator.</span>

<span class="sd">    The operator computes the value of each sampling point by bilinear interpolation from the nearby grid points on the</span>
<span class="sd">    feature map. No quantization is performed on any coordinates involved in the RoI, its bins, or the sampling</span>
<span class="sd">    points. The details of (RoI) Align operator are described in `Mask R-CNN &lt;https://arxiv.org/abs/1703.06870&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        pooled_height (int): The output features height.</span>
<span class="sd">        pooled_width (int): The output features width.</span>
<span class="sd">        spatial_scale (float): A scaling factor that maps the raw image coordinates to the input</span>
<span class="sd">            feature map coordinates. Suppose the height of a RoI is `ori_h` in the raw image and `fea_h` in the</span>
<span class="sd">            input feature map, the `spatial_scale` must be `fea_h / ori_h`.</span>
<span class="sd">        sample_num (int): Number of sampling points. Default: 2.</span>
<span class="sd">        roi_end_mode (int): Number must be 0 or 1. If roi_end_mode=0, use the legacy implementation.</span>
<span class="sd">            If roi_end_mode=1, end pixel of the roi_box will be shifted by +1*spatial_scale. Default: 1.</span>


<span class="sd">    Inputs:</span>
<span class="sd">        - **features** (Tensor) - The input features, whose shape must be :math:`(N, C, H, W)`.</span>
<span class="sd">        - **rois** (Tensor) - The shape is :math:`(rois\_n, 5)`. With data type of float16 or float32.</span>
<span class="sd">          `rois_n` represents the number of RoI. The size of the second dimension must be `5` and the `5` colunms</span>
<span class="sd">          are :math:`(image\_index, top\_left\_x, top\_left\_y, bottom\_right\_x, bottom\_right\_y)`.</span>
<span class="sd">          `image_index` represents the index of image. `top_left_x` and `top_left_y` represent the `x, y`</span>
<span class="sd">          coordinates of the top left corner of corresponding RoI, respectively. `bottom_right_x` and `bottom_right_y`</span>
<span class="sd">          represent the `x, y` coordinates of the bottom right corner of corresponding RoI, respectively.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is :math:`(rois\_n, C, pooled\_height, pooled\_width)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `pooled_height`, `pooled_width`, `sample_num` or `roi_end_mode` is not an int.</span>
<span class="sd">        TypeError: If `spatial_scale` is not a float.</span>
<span class="sd">        TypeError: If `features` or `rois` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; features = Tensor(np.array([[[[1., 2.], [3., 4.]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rois = Tensor(np.array([[0, 0.2, 0.3, 0.2, 0.3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; roi_align = ops.ROIAlign(2, 2, 0.5, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = roi_align(features, rois)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.775 2.025]</span>
<span class="sd">           [2.275 2.525]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="n">sample_num</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">roi_end_mode</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ROIAlign&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooled_height&quot;</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooled_width&quot;</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;spatial_scale&quot;</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sample_num&quot;</span><span class="p">,</span> <span class="n">sample_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;roi_end_mode&quot;</span><span class="p">,</span> <span class="n">roi_end_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">roi_end_mode</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;roi_end_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span> <span class="o">=</span> <span class="n">pooled_height</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span> <span class="o">=</span> <span class="n">pooled_width</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_scale</span> <span class="o">=</span> <span class="n">spatial_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_num</span> <span class="o">=</span> <span class="n">sample_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">roi_end_mode</span> <span class="o">=</span> <span class="n">roi_end_mode</span></div>


<div class="viewcode-block" id="Adam"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Adam.html#mindspore.ops.Adam">[文档]</a><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates gradients by the Adaptive Moment Estimation (Adam) algorithm.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    For more details, please refer to :class:`mindspore.nn.Adam`.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t(\beta_1^{t})` and :math:`beta_2^t(\beta_2^{t})`</span>
<span class="sd">    represent `beta1_power` and `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`,</span>
<span class="sd">    :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If false, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Tensor) - Weights to be updated. The shape is :math:`(N, *)` where :math:`*` means,</span>
<span class="sd">          any number of additional dimensions. The data type can be float16 or float32.</span>
<span class="sd">        - **m** (Tensor) - The 1st moment vector in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`.</span>
<span class="sd">        - **v** (Tensor) - the 2nd moment vector in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`. Mean square gradients with the same type as `var`.</span>
<span class="sd">        - **beta1_power** (float) - :math:`beta_1^t(\beta_1^{t})` in the updating formula,</span>
<span class="sd">          the data type value should be the same as `var`.</span>
<span class="sd">        - **beta2_power** (float) - :math:`beta_2^t(\beta_2^{t})` in the updating formula,</span>
<span class="sd">          the data type value should be the same as `var`.</span>
<span class="sd">        - **lr** (float) - :math:`l` in the updating formula. The paper suggested value is :math:`10^{-8}`,</span>
<span class="sd">          the data type value should be the same as `var`.</span>
<span class="sd">        - **beta1** (float) - The exponential decay rate for the 1st moment estimations,</span>
<span class="sd">          the data type value should be the same as `var`. The paper suggested value is :math:`0.9`.</span>
<span class="sd">        - **beta2** (float) - The exponential decay rate for the 2nd moment estimations,</span>
<span class="sd">          the data type value should be the same as `var`. The paper suggested value is :math:`0.999`.</span>
<span class="sd">        - **epsilon** (float) - Term added to the denominator to improve numerical stability.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same shape and data type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as Inputs `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as Inputs `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as Inputs `v`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_nesterov` is a bool.</span>
<span class="sd">        TypeError: If `var`, `m` or `v` is not a Tensor.</span>
<span class="sd">        TypeError: If `beta1_power`, `beta2_power1`, `lr`, `beta1`, `beta2`, `epsilon` or `gradient` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adam = ops.Adam()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        ...         out = self.apply_adam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2,</span>
<span class="sd">        ...                               epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(0.9, 0.999, 0.001, 0.9, 0.999, 1e-8, gradient)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.9996838 0.9996838]</span>
<span class="sd">         [0.9996838 0.9996838]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Adam.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="AdamWeightDecay"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AdamWeightDecay.html#mindspore.ops.AdamWeightDecay">[文档]</a><span class="k">class</span> <span class="nc">AdamWeightDecay</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates gradients by the Adaptive Moment Estimation algorithm with weight decay (AdamWeightDecay).</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>
<span class="sd">    The AdamWeightDecay variant was proposed in `Decoupled Weight Decay Regularization</span>
<span class="sd">    &lt;https://arxiv.org/abs/1711.05101&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            update = \frac{m}{\sqrt{v} + \epsilon} \\</span>
<span class="sd">            update =</span>
<span class="sd">            \begin{cases}</span>
<span class="sd">                update + weight\_decay * w</span>
<span class="sd">                    &amp; \text{ if } weight\_decay &gt; 0 \\</span>
<span class="sd">                update</span>
<span class="sd">                    &amp; \text{ otherwise }</span>
<span class="sd">            \end{cases} \\</span>
<span class="sd">            w  = w - lr * update</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`lr` represents `learning_rate`, :math:`w` represents `var`, :math:`decay` represents `weight_decay`,</span>
<span class="sd">    :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated. The shape is :math:`(N, *)` where :math:`*` means,</span>
<span class="sd">          any number of additional dimensions. The data type can be float16 or float32.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula,</span>
<span class="sd">          it should have the the shape as `var`. The data type can be float16 or float32.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula,</span>
<span class="sd">          it should have the same shape and dtype as `m`.</span>
<span class="sd">        - **lr** (float) - :math:`l` in the updating formula. The paper suggested value is :math:`10^{-8}`,</span>
<span class="sd">          the data type should be float32.</span>
<span class="sd">        - **beta1** (float) - The exponential decay rate for the 1st moment estimations,</span>
<span class="sd">          the data type should be float32. The paper suggested value is :math:`0.9`</span>
<span class="sd">        - **beta2** (float) - The exponential decay rate for the 2nd moment estimations,</span>
<span class="sd">          the data type should be float32. The paper suggested value is :math:`0.999`</span>
<span class="sd">        - **epsilon** (float) - Term added to the denominator to improve numerical stability,</span>
<span class="sd">          the data type should be float32.</span>
<span class="sd">        - **decay** (float) - The weight decay value, must be a scalar tensor with float32 data type.</span>
<span class="sd">          Default: 0.0.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same shape and data type as `var`.</span>
<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter, ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.adam_weight_decay = ops.AdamWeightDecay()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, lr, beta1, beta2, epsilon, decay, grad):</span>
<span class="sd">        ...         out = self.adam_weight_decay(self.var, self.m, self.v, lr, beta1, beta2,</span>
<span class="sd">        ...                               epsilon, decay, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(0.001, 0.9, 0.999, 1e-8, 0.0, gradient)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.999 0.999]</span>
<span class="sd">         [0.999 0.999]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;decay&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AdamWeightDecay.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span>
                    <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">decay_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;var_shape&quot;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span>
                    <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">decay_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">,</span>
                <span class="s2">&quot;decay&quot;</span><span class="p">:</span> <span class="n">decay_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span></div>


<span class="k">class</span> <span class="nc">AdamNoUpdateParam</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates gradients by the Adaptive Moment Estimation (Adam) algorithm. This operator do not update the parameter, but</span>
<span class="sd">    calculate the value that should be added to the parameter instead.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            \Delta{w} = - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t(\beta_1^{t})` and :math:`beta_2^t(\beta_2^{t})`</span>
<span class="sd">    represent `beta1_power` and `beta2_power`, :math:`\alpha` represents `learning_rate`,</span>
<span class="sd">    :math:`w` represents the parameter to be updated, :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If false, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **m** (Tensor) - The 1st moment vector in the updating formula. The shape is :math:`(N, *)`</span>
<span class="sd">          where :math:`*` means, any number of additional dimensions. The data type must be float32.</span>
<span class="sd">        - **v** (Tensor) - the 2nd moment vector in the updating formula. The shape must be the same as `m`.</span>
<span class="sd">          The data type must be float32.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t(\beta_1^{t})` in the updating formula.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t(\beta_2^{t})` in the updating formula.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, the shape must be the same as `m`, the data type must be float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose shape and data type are the same with Inputs `gradient`, is a value that should be added to the</span>
<span class="sd">        parameter to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_nesterov` is a bool.</span>
<span class="sd">        TypeError: If `m`,  `v`, `beta1_power`, `beta2_power1`, `lr`, `beta1`, `beta2`, `epsilon` or `gradient`</span>
<span class="sd">                   is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.adam = ops.AdamNoUpdateParam()</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2]]).astype(np.float32)),</span>
<span class="sd">        ...                            name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.array([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2]]).astype(np.float32)),</span>
<span class="sd">        ...                            name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        ...         out = self.adam(self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; result = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[-0.00010004 -0.00010004 -0.00010004]</span>
<span class="sd">        [-0.00013441 -0.00013441 -0.00013441]]</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AdamNoUpdateParam.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">beta1_power_shape</span><span class="p">,</span> <span class="n">beta2_power_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span>
                    <span class="n">beta1_shape</span><span class="p">,</span> <span class="n">beta2_shape</span><span class="p">,</span> <span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s2">&quot;m_shape&quot;</span><span class="p">,</span> <span class="n">m_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;grad_shape&quot;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="s2">&quot;v_shape&quot;</span><span class="p">,</span> <span class="n">v_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span>
                    <span class="n">beta1_dtype</span><span class="p">,</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="n">epsilon_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;m&quot;</span><span class="p">:</span> <span class="n">m_dtype</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">:</span> <span class="n">v_dtype</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1_power&quot;</span><span class="p">:</span> <span class="n">beta1_power_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2_power&quot;</span><span class="p">:</span> <span class="n">beta2_power_dtype</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">,</span>
                <span class="s2">&quot;beta1&quot;</span><span class="p">:</span> <span class="n">beta1_dtype</span><span class="p">,</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span> <span class="n">beta2_dtype</span><span class="p">,</span> <span class="s2">&quot;epsilon&quot;</span><span class="p">:</span> <span class="n">epsilon_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">grad_dtype</span>


<span class="k">class</span> <span class="nc">FusedSparseAdam</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates parameters by the Adaptive Moment Estimation (Adam)</span>
<span class="sd">    algorithm. This operator is used when the gradient is sparse.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`\beta_1^t` and :math:`\beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If false, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Parameters to be updated with float32 data type. The shape is :math:`(N, *)`</span>
<span class="sd">          where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same shape and data type as `var`.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula, has the same shape and data type as `var`.</span>
<span class="sd">          Mean square gradients, has the same type as `var` with float32 data type.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t` in the updating formula with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t` in the updating formula with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula. With float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same data type as `var` and</span>
<span class="sd">          gradient.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - Gradient indices with int32 data type and indices.shape[0] = gradient.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape :math:`(N, *)`.</span>
<span class="sd">        - **m** (Tensor) - A Tensor with shape :math:`(1, )`.</span>
<span class="sd">        - **v** (Tensor) - A Tensor with shape :math:`(1, )`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_neserov` is a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `beta1_power`, `beta2_power`, `lr`, `beta1`, `beta2`, `epsilon`,</span>
<span class="sd">                   `gradient` or `indices` is not float32.</span>
<span class="sd">        RuntimeError: If the data type of all inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_adam = ops.FusedSparseAdam()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_adam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2,</span>
<span class="sd">        ...                                      epsilon, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([[[0.1, 0.1]], [[0.1, 0.1]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[[0.9997121  0.9997121 ]]</span>
<span class="sd">         [[0.9997121  0.9997121 ]]</span>
<span class="sd">         [[0.99971527 0.99971527]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FusedSparseAdam.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FusedSparseLazyAdam</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates parameters by the Adaptive Moment Estimation (Adam)</span>
<span class="sd">    algorithm. This operator is used when the gradient is sparse. The behavior is not equivalent to the</span>
<span class="sd">    original Adam algorithm, as only the current indices parameters will be updated.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`\beta_1^t` and :math:`\beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If true, updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If false, the result is unpredictable. Default: False.</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If true, update the gradients using NAG.</span>
<span class="sd">            If false, update the gradients without using NAG. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Parameters to be updated with float32 data type. The shape is :math:`(N, *)`</span>
<span class="sd">          where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same shape and data type as `var`.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula, has the same shape and data type as `var`.</span>
<span class="sd">          Mean square gradients, has the same type as `var` with float32 data type.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t` in the updating formula with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t` in the updating formula with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient value with float32 data type and</span>
<span class="sd">          gradient.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - Gradient indices with int32 data type and indices.shape[0] = gradient.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape :math:`(N, *)`.</span>
<span class="sd">        - **m** (Tensor) - A Tensor with shape :math:`(1, )`.</span>
<span class="sd">        - **v** (Tensor) - A Tensor with shape :math:`(1, )`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_nestrov` is a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `beta1_power`, `beta2_power`, `lr`, `beta1`, `beta2`, `epsilon` or</span>
<span class="sd">                   gradient is not float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        RuntimeError: If the data type of all inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_lazyadam = ops.FusedSparseLazyAdam()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_lazyadam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1,</span>
<span class="sd">        ...                                          beta2, epsilon, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([[[0.1, 0.1]], [[0.1, 0.1]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[[0.9997121  0.9997121 ]]</span>
<span class="sd">         [[0.9997121  0.9997121 ]]</span>
<span class="sd">         [[1.         1.        ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FusedSparseLazyAdam.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FusedSparseFtrl</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates relevant entries according to the FTRL-proximal scheme.</span>

<span class="sd">    All inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if true . Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float32. The shape is :math:`(N, *)`</span>
<span class="sd">          where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var` and</span>
<span class="sd">          grad.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The type must be int32 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape :math:`(N, *)`.</span>
<span class="sd">        - **accum** (Tensor) - A Tensor with shape :math:`(1, )`.</span>
<span class="sd">        - **linear** (Tensor) - A Tensor with shape :math:`(1, )`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `lr_power` is not a float.</span>
<span class="sd">        ValueError: If shape of `lr_power` less than or equal to zero.</span>
<span class="sd">        TypeError: If dtype of `var` is not float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        TypeError: If shape of `accum`, `linear` or `grad` is not same as `var`.</span>
<span class="sd">        TypeError: If shape of `indices` is not same as shape of first dimension of `grad`.</span>
<span class="sd">        RuntimeError: If the data type of all of inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(SparseApplyFtrlNet, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_ftrl = ops.FusedSparseFtrl(lr=0.01, l1=0.0, l2=0.0, lr_power=-0.5)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_ftrl(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[[0.1, 0.1]], [[0.1, 0.1]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[[-0.00598256 -0.00598256]]</span>
<span class="sd">         [[-0.00598256 -0.00598256]]</span>
<span class="sd">         [[ 1.          1.        ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FusedSparseFtrl.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FusedSparseProximalAdagrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates relevant entries according to the proximal adagrad</span>
<span class="sd">    algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}} \\</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>
<span class="sd">        \end{array}</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If true, the variable and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Variable tensor to be updated, has the same shape and data type as `var`.</span>
<span class="sd">        - **lr** (Tensor) - The learning rate value. The data type must be float32. The shape is :math:`(1, )`.</span>
<span class="sd">        - **l1** (Tensor) - l1 regularization strength. The data type must be float32. The shape is :math:`(1, )`.</span>
<span class="sd">        - **l2** (Tensor) - l2 regularization strength. The data type must be float32. The shape is :math:`(1, )`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same data type as `var` and</span>
<span class="sd">          grad.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The type must be int32 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape :math:`(N, *)`.</span>
<span class="sd">        - **accum** (Tensor) - A Tensor with shape :math:`(1, )`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr`, `l1`, `l2` or `grad` is not float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        RuntimeError: If the data type of all inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_proximal_adagrad = ops.FusedSparseProximalAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.lr = Tensor(0.01, mindspore.float32)</span>
<span class="sd">        ...         self.l1 = Tensor(0.0, mindspore.float32)</span>
<span class="sd">        ...         self.l2 = Tensor(0.0, mindspore.float32)</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1,</span>
<span class="sd">        ...                                                  self.l2, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[[0.1, 0.1]], [[0.1, 0.1]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[[0.99900496 0.99900496]]</span>
<span class="sd">         [[0.99900496 0.99900496]]</span>
<span class="sd">         [[1.         1.        ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FusedSparseProximalAdagrad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="KLDivLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.KLDivLoss.html#mindspore.ops.KLDivLoss">[文档]</a><span class="k">class</span> <span class="nc">KLDivLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Kullback-Leibler divergence between the logits and the labels.</span>

<span class="sd">    For tensors of the same shape :math:`x` and :math:`target`,</span>
<span class="sd">    the updating formulas of KLDivLoss algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(x, target) = target \cdot (\log target - x)</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, target) = \begin{cases}</span>
<span class="sd">        L(x, target), &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L(x, target)), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L(x, target)) / x.\operatorname{shape}[0], &amp; \text{if reduction} = \text{&#39;batchmean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L(x, target)),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`x` represents `logits`.</span>
<span class="sd">    :math:`target` represents `labels`.</span>
<span class="sd">    :math:`\ell(x, target)` represents `output`.</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend, float64 dtype is not currently supported.</span>
<span class="sd">        The output aligns with the mathematical definition of KL divergence</span>
<span class="sd">        only when `reduction` is set to &#39;batchmean&#39;.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Default: &#39;mean&#39;.</span>

<span class="sd">            - On Ascend, the value of `reduction` must be one of &#39;batchmean&#39;, &#39;none&#39; or &#39;sum&#39;.</span>
<span class="sd">            - On GPU, the value of `reduction` must be one of &#39;mean&#39;, &#39;none&#39; or &#39;sum&#39;.</span>
<span class="sd">            - On CPU, the value of `reduction` must be one of &#39;mean&#39;, &#39;batchmean&#39;, &#39;none&#39; or &#39;sum&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input Tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">        - **labels** (Tensor) - The label Tensor which has the same shape and data type as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is &#39;none&#39;, then output is a tensor and has the same shape as `logits`.</span>
<span class="sd">        Otherwise it is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `reduction` is not a str.</span>
<span class="sd">        TypeError: If neither `logits` nor `labels` is a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is not currently supported.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>
<span class="sd">        RuntimeError: If `logits` or `labels` is a scalar when `reduction` is &#39;batchmean&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.kldiv_loss = ops.KLDivLoss(reduction=&#39;sum&#39;)</span>
<span class="sd">        ...     def construct(self, logits, labels):</span>
<span class="sd">        ...         result = self.kldiv_loss(logits, labels)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        -0.7</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize KLDivLoss.&quot;&quot;&quot;</span>
        <span class="n">device_target</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device_target</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
            <span class="n">support_mode</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;batchmean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">device_target</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
            <span class="n">support_mode</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">device_target</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
            <span class="n">support_mode</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;batchmean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; unknown device target: &#39;</span><span class="si">{</span><span class="n">device_target</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="n">support_mode</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="BinaryCrossEntropy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BinaryCrossEntropy.html#mindspore.ops.BinaryCrossEntropy">[文档]</a><span class="k">class</span> <span class="nc">BinaryCrossEntropy</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the binary cross entropy between the logits and the labels.</span>

<span class="sd">    Sets logits as :math:`x`, labels as :math:`y`, output as :math:`\ell(x, y)`.</span>
<span class="sd">    Let,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]</span>

<span class="sd">    In which, :math:`L` indicates the loss of all batch_sizes, :math:`l` indicates the loss of one batch_size,</span>
<span class="sd">    and n indicates one batch_size in the 1-N range. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The value of &quot;x&quot; must range from 0 to 1.</span>
<span class="sd">        - The value of &quot;y&quot; must be &quot;0&quot; or &quot;1&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Its value must be one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;. Default: &#39;mean&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input Tensor. The data type must be float16 or float32,</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **labels** (Tensor) - The label Tensor which has the same shape and data type as `logits`.</span>
<span class="sd">        - **weight** (Tensor, optional) - A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">          And it must have the same shape and data type as `logits`. Default: None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `logits`. if `reduction` is &#39;none&#39;, then it has the same shape as `logits`.</span>
<span class="sd">        Otherwise, it is a scalar Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits`, `labels` or `weight` (if given) is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39; or &#39;sum&#39;.</span>
<span class="sd">        ValueError: If shape of `labels` is not the same as `logits` or `weight` (if given).</span>
<span class="sd">        TypeError: If `logits`, `labels` or `weight` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.binary_cross_entropy = ops.BinaryCrossEntropy()</span>
<span class="sd">        ...     def construct(self, logits, labels, weight):</span>
<span class="sd">        ...         result = self.binary_cross_entropy(logits, labels, weight)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(logits, labels, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.38240486</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BinaryCrossEntropy.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAdaMax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdaMax.html#mindspore.ops.ApplyAdaMax">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdaMax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adamax scheme.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t+1} = \beta_1 * m_{t} + (1 - \beta_1) * g \\</span>
<span class="sd">            v_{t+1} = \max(\beta_2 * v_{t}, \left| g \right|) \\</span>
<span class="sd">            var = var - \frac{l}{1 - \beta_1^{t+1}} * \frac{m_{t+1}}{v_{t+1} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t}`</span>
<span class="sd">    is the last moment of :math:`m_{t+1}`, :math:`v` represents the 2nd moment vector, :math:`v_{t}`</span>
<span class="sd">    is the last moment of :math:`v_{t+1}`, :math:`l` represents scaling factor `lr`,</span>
<span class="sd">    :math:`g` represents `grad`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`\beta_1^{t+1}` represents `beta1_power`, :math:`var` represents the variable to be updated,</span>
<span class="sd">    :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Inputs of `var`, `m`, `v` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float32 or float16 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same shape and type as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula. Mean square gradients</span>
<span class="sd">          with the same shape and type as `var`. With float32 or float16 data type.</span>
<span class="sd">        - **beta1_power** (Union[Number, Tensor]) - :math:`beta_1^t` in the updating formula, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Learning rate, :math:`l` in the updating formula, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **beta1** (Union[Number, Tensor]) - The exponential decay rate for the 1st moment estimations,</span>
<span class="sd">          must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **beta2** (Union[Number, Tensor]) - The exponential decay rate for the 2nd moment estimations,</span>
<span class="sd">          must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **epsilon** (Union[Number, Tensor]) - A small value added for numerical stability, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient, has the same shape and type as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `beta_power`, `lr`, `beta1`, `beta2`, `epsilon` or `grad` is neither</span>
<span class="sd">                   float16 nor float32.</span>
<span class="sd">        TypeError: If `beta_power`, `lr`, `beta1`, `beta2` or `epsilon` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `m`, `v` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_ada_max = ops.ApplyAdaMax()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                             [0.2, 0.6]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.array([[0.9, 0.1],</span>
<span class="sd">        ...                                             [0.7, 0.8]]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        ...         out = self.apply_ada_max(self.var, self.m, self.v, beta1_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power =Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.99, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-10, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(beta1_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.93602717e-01,  3.92571449e-01],</span>
<span class="sd">         [ 9.72582996e-02,  4.92249995e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.69999993e-01,  5.19999981e-01],</span>
<span class="sd">         [ 1.89999998e-01,  6.20000005e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 8.90999973e-01,  6.99999988e-01],</span>
<span class="sd">         [ 6.93000019e-01,  8.00000012e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T5</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdaMax&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAdadelta"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdadelta.html#mindspore.ops.ApplyAdadelta">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdadelta</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adadelta scheme.</span>

<span class="sd">    The Adadelta algorithm is proposed in</span>
<span class="sd">    `ADADELTA: AN ADAPTIVE LEARNING RATE METHOD &lt;https://arxiv.org/abs/1212.5701&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            \text{accum} = \rho * \text{accum} + (1 - \rho) * \text{grad}^2 \\</span>
<span class="sd">            \text{update} = \sqrt{\text{accum_update} +</span>
<span class="sd">              \epsilon} * \frac{\text{grad}}{\sqrt{\text{accum} + \epsilon}} \\</span>
<span class="sd">            \text{accum_update} = \rho * \text{accum_update} + (1 - \rho) * \text{update}^2 \\</span>
<span class="sd">            \text{var} = \text{var} - \text{lr} * \text{update}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`\rho` represents `rho`, :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Inputs of `var`, `accum`, `accum_update` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated. With float32 or float16 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated, has the same shape and data type as `var`.</span>
<span class="sd">        - **accum_update** (Parameter) - Accum_update to be updated, has the same shape and data type as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Learning rate, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **rho** (Union[Number, Tensor]) - Decay rate, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **epsilon** (Union[Number, Tensor]) - A small value added for numerical stability, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - Gradients, has the same shape and data type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>
<span class="sd">        - **accum_update** (Tensor) - The same shape and data type as `accum_update`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `accum_update`, `lr`, `rho`, `epsilon` or `grad` is neither float16 nor</span>
<span class="sd">                   float32.</span>
<span class="sd">        TypeError: If `accum_update`, `lr`, `rho` or `epsilon` is neither a Number nor a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum`, `accum_update` and `grad` conversion of Parameter</span>
<span class="sd">                      is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import nn, Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adadelta = ops.ApplyAdadelta()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.accum_update = Parameter(Tensor(np.array([[0.9, 0.1],</span>
<span class="sd">        ...                                                        [0.7, 0.8]]).astype(np.float32)),</span>
<span class="sd">        ...                                                             name=&quot;accum_update&quot;)</span>
<span class="sd">        ...     def construct(self, lr, rho, epsilon, grad):</span>
<span class="sd">        ...         out = self.apply_adadelta(self.var, self.accum, self.accum_update, lr, rho, epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rho = Tensor(0.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-6, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, rho, epsilon, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99051356e-01,  3.99683774e-01],</span>
<span class="sd">         [ 9.91633832e-02,  4.99105573e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 9.00000036e-02,  4.89999980e-01],</span>
<span class="sd">         [ 1.00000007e-02,  6.40000045e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 8.99990857e-01,  1.00000791e-01],</span>
<span class="sd">         [ 6.99930906e-01,  7.99999774e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum_update&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdadelta&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAdagrad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdagrad.html#mindspore.ops.ApplyAdagrad">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdagrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagrad scheme.</span>
<span class="sd">    The Adagrad algorithm was proposed in</span>
<span class="sd">    `Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</span>
<span class="sd">    &lt;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&gt;`_.</span>
<span class="sd">    This module can adaptively assign different learning rates for each parameter in view of the uneven number</span>
<span class="sd">    of samples for different parameters.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum}}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad`  comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        update_slots (bool): If `True`, `accum` will be updated. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float32 or float16 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and data type must be the same as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. The shape and data type must be the same as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr` is neither a Number nor a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adagrad = ops.ApplyAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...     def construct(self, lr, grad):</span>
<span class="sd">        ...         out = self.apply_adagrad(self.var, self.accum, lr, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99638879e-01,  3.99296492e-01],</span>
<span class="sd">         [ 9.97817814e-02,  4.99281585e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.90000057e-01,  9.90000010e-01],</span>
<span class="sd">         [ 2.10000008e-01,  1.24000001e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdagrad.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAdagradV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdagradV2.html#mindspore.ops.ApplyAdagradV2">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdagradV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagradv2 scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Note:</span>
<span class="sd">        The difference is that `ApplyAdagradV2` has one more small constant value than `ApplyAdagrad`.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): A small value added for numerical stability.</span>
<span class="sd">        update_slots (bool): If `True`, `accum` will be updated. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float16 or float32 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and data type must be the same as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. The shape and data type must be the same as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr` is neither a Number nor a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adagrad_v2 = ops.ApplyAdagradV2(epsilon=1e-6)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...     def construct(self, lr, grad):</span>
<span class="sd">        ...         out = self.apply_adagrad_v2(self.var, self.accum, lr, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99638879e-01,  3.99296492e-01],</span>
<span class="sd">         [ 9.97817814e-02,  4.99281585e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.90000057e-01,  9.90000010e-01],</span>
<span class="sd">         [ 2.10000008e-01,  1.24000001e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdagradV2.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseApplyAdagrad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseApplyAdagrad.html#mindspore.ops.SparseApplyAdagrad">[文档]</a><span class="k">class</span> <span class="nc">SparseApplyAdagrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.9&quot;</span><span class="p">,</span> <span class="s2">&quot;SparseApplyAdagrad&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyAdagrad.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseApplyAdagradV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseApplyAdagradV2.html#mindspore.ops.SparseApplyAdagradV2">[文档]</a><span class="k">class</span> <span class="nc">SparseApplyAdagradV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagrad scheme, one more epsilon attribute than SparseApplyAdagrad.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): Learning rate.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability.</span>
<span class="sd">        use_locking (bool): If `True`, the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>
<span class="sd">        update_slots (bool): If `True`, the computation logic will be different to `False`. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape and data type must be the same as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradients has the same data type as `var` and</span>
<span class="sd">          grad.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The type must be int32 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `lr` nor `epsilon` is a float.</span>
<span class="sd">        TypeError: If neither `update_slots` nor `use_locking` is a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_adagrad_v2 = ops.SparseApplyAdagradV2(lr=1e-8, epsilon=1e-6)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.1]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_adagrad_v2(self.var, self.accum, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[ 1.99999988e-01]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[ 5.89999974e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyAdagradV2.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_slots</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyProximalAdagrad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyProximalAdagrad.html#mindspore.ops.ApplyProximalAdagrad">[文档]</a><span class="k">class</span> <span class="nc">ApplyProximalAdagrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the proximal adagrad algorithm.</span>
<span class="sd">    The proximal adagrad algorithm was proposed in `Efficient Learning using Forward-Backward Splitting</span>
<span class="sd">    &lt;http://papers.nips.cc//paper/3793-efficient-learning-using-forward-backward-splitting.pdf&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}} \\</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If true, the var and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated, must have the same shape and dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be a scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be a scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **grad** (Tensor) - Gradient with the same shape and dtype as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_blocking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `lr`, `l1` or `l2` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr`, `l1` or `l2` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_proximal_adagrad = ops.ApplyProximalAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.lr = 0.01</span>
<span class="sd">        ...         self.l1 = 0.0</span>
<span class="sd">        ...         self.l2 = 0.0</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1, self.l2, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.96388459e-01,  3.92964751e-01],</span>
<span class="sd">         [ 9.78178233e-02,  4.92815793e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.90000057e-01,  9.90000010e-01],</span>
<span class="sd">         [ 2.10000008e-01,  1.24000001e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyProximalAdagrad.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseApplyProximalAdagrad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseApplyProximalAdagrad.html#mindspore.ops.SparseApplyProximalAdagrad">[文档]</a><span class="k">class</span> <span class="nc">SparseApplyProximalAdagrad</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the proximal adagrad algorithm. Compared with ApplyProximalAdagrad,</span>
<span class="sd">    an additional index tensor is input.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}} \\</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If true, the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Variable tensor to be updated, has the same shape and dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var` and</span>
<span class="sd">          grad.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr`, `l1`, `l2` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_proximal_adagrad = ops.SparseApplyProximalAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[4.1, 7.2], [1.1, 3.0]], np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0, 0], [0, 0]], np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.lr = 1.0</span>
<span class="sd">        ...         self.l1 = 1.0</span>
<span class="sd">        ...         self.l2 = 0.0</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1,</span>
<span class="sd">        ...                                                  self.l2, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[1, 1], [1, 1]], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1], np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 2.09999990e+00,  5.19999981e+00],</span>
<span class="sd">         [ 0.00000000e+00,  1.00000000e+00]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.00000000e+00,  1.00000000e+00],</span>
<span class="sd">         [ 1.00000000e+00,  1.00000000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyProximalAdagrad.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">lr_shape</span><span class="p">,</span> <span class="n">l1_shape</span><span class="p">,</span> <span class="n">l2_shape</span><span class="p">,</span>
                    <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">lr_dtype</span><span class="p">,</span> <span class="n">l1_dtype</span><span class="p">,</span> <span class="n">l2_dtype</span><span class="p">,</span>
                    <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;var&#39;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="n">l1_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_scalar_or_tensor_types_same</span><span class="p">({</span><span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="n">l2_dtype</span><span class="p">},</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAddSign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAddSign.html#mindspore.ops.ApplyAddSign">[文档]</a><span class="k">class</span> <span class="nc">ApplyAddSign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the AddSign algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t+1} = \beta * m_{t} + (1 - \beta) * g \\</span>
<span class="sd">            \text{update} = (\alpha + \text{sign_decay} * sign(g) * sign(m)) * g \\</span>
<span class="sd">            var = var - lr_{t+1} * \text{update}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t}`</span>
<span class="sd">    is the last moment of :math:`m_{t+1}`, :math:`lr` represents scaling factor `lr`, :math:`g` represents `grad`,</span>
<span class="sd">    :math:`\alpha` represents `alpha`, :math:`\beta` represents `beta`.</span>

<span class="sd">    Inputs of `var`, `accum` and `grad`  comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>
<span class="sd">    The data type of inputs must be float16 or float32 on Ascend and float16, float32 or float64 on CPU and GPU.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float16, float32 or float64 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **m** (Parameter) - Variable tensor to be updated, has the same shape and data type as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a scalar.</span>
<span class="sd">          With float16, float32 or float64 data type.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Must be a scalar. With float16, float32 or float64 data type.</span>
<span class="sd">        - **sign_decay** (Union[Number, Tensor]) - Must be a scalar. With float16, float32 or float64 data type.</span>
<span class="sd">        - **beta** (Union[Number, Tensor]) - The exponential decay rate, must be a scalar.</span>
<span class="sd">          With float16, float32 or float64 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same shape and data type as `var`, for the gradient.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `lr`, `alpha`, `sign_decay` or `beta` is not float16, float32 or float64.</span>
<span class="sd">        TypeError: If `lr`, `alpha` or `sign_decay` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_add_sign = ops.ApplyAddSign()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                             [0.2, 0.6]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.lr = 0.001</span>
<span class="sd">        ...         self.alpha = 1.0</span>
<span class="sd">        ...         self.sign_decay = 0.99</span>
<span class="sd">        ...         self.beta = 0.9</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_add_sign(self.var, self.m, self.lr, self.alpha, self.sign_decay, self.beta, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99403024e-01,  3.98607016e-01],</span>
<span class="sd">         [ 9.98010039e-02,  4.98407990e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.70000052e-01,  5.19999981e-01],</span>
<span class="sd">         [ 1.89999998e-01,  6.20000064e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sign_decay&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAddSign.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyPowerSign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyPowerSign.html#mindspore.ops.ApplyPowerSign">[文档]</a><span class="k">class</span> <span class="nc">ApplyPowerSign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the AddSign algorithm.</span>

<span class="sd">    The AddSign algorithm was proposed in `Neural Optimizer Search with Reinforcement Learning</span>
<span class="sd">    &lt;https://arxiv.org/abs/1709.07417&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t+1} = \beta * m_{t} + (1 - \beta) * g \\</span>
<span class="sd">            \text{update} = \exp(\text{logbase} * \text{sign_decay} * sign(g) * sign(m)) * g \\</span>
<span class="sd">            var = var - lr_{t+1} * \text{update}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t}`</span>
<span class="sd">    is the last moment of :math:`m_{t+1}`, :math:`lr` represents scaling factor `lr`, :math:`g` represents `grad`,</span>
<span class="sd">    :math:`\beta` represents `beta`.</span>

<span class="sd">    All of inputs comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If `lr`, `logbase`, `sign_decay` or `beta` is a number, the number is automatically converted to Tensor,</span>
<span class="sd">    and the data type is consistent with the Tensor data type involved in the operation.</span>
<span class="sd">    If inputs are tensors and have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend, input data type of float64 is currently not supported.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float64, float32 or float16 data type.</span>
<span class="sd">          If data type of `var` is float16, all inputs must have the same data type as `var`.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **m** (Parameter) - Variable tensor to be updated, has the same shape and data type as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, should be a scalar or Tensor</span>
<span class="sd">          with float64, float32 or float16 data type.</span>
<span class="sd">        - **logbase** (Union[Number, Tensor]) - Should be a scalar or Tensor with float64, float32 or float16 data type.</span>
<span class="sd">        - **sign_decay** (Union[Number, Tensor]) - Should be a scalar or Tensor with float64, float32 or</span>
<span class="sd">          float16 data type.</span>
<span class="sd">        - **beta** (Union[Number, Tensor]) - The exponential decay rate, should be a scalar or Tensor</span>
<span class="sd">          with float64, float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same shape and data type as `var`, for the gradient.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `lr`, `logbase`, `sign_decay`, `beta` or `grad` is not one of float16,</span>
<span class="sd">        float32 or float64.</span>
<span class="sd">        TypeError: If `lr`, `logbase`, `sign_decay` or `beta` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `lr`, `logbase`, `sign_decay` and `grad` conversion of Parameter</span>
<span class="sd">                      is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_power_sign = ops.ApplyPowerSign()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                             [0.2, 0.6]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.lr = 0.001</span>
<span class="sd">        ...         self.logbase = np.e</span>
<span class="sd">        ...         self.sign_decay = 0.99</span>
<span class="sd">        ...         self.beta = 0.9</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_power_sign(self.var, self.m, self.lr, self.logbase,</span>
<span class="sd">        ...                                        self.sign_decay, self.beta, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.95575690e-01,  3.89676481e-01],</span>
<span class="sd">         [ 9.85252112e-02,  4.88201708e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.70000052e-01,  5.19999981e-01],</span>
<span class="sd">         [ 1.89999998e-01,  6.20000064e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;logbase&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sign_decay&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyPowerSign.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyGradientDescent"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyGradientDescent.html#mindspore.ops.ApplyGradientDescent">[文档]</a><span class="k">class</span> <span class="nc">ApplyGradientDescent</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates `var` by subtracting `alpha` * `delta` from it.</span>

<span class="sd">    .. math::</span>
<span class="sd">        var = var - \alpha * \delta</span>

<span class="sd">    where :math:`\alpha` represents `alpha`, :math:`\delta` represents `delta`.</span>

<span class="sd">    Inputs of `var` and `delta` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Scaling factor, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **delta** (Tensor) - A tensor for the change, has the same shape and data type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the updated `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var` or `alpha` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `delta` is not a Tensor.</span>
<span class="sd">        TypeError: If `alpha` is neither a Number nor a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `var` and `delta` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_gradient_descent = ops.ApplyGradientDescent()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.alpha = 0.001</span>
<span class="sd">        ...     def construct(self, delta):</span>
<span class="sd">        ...         out = self.apply_gradient_descent(self.var, self.alpha, delta)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(np.array([[0.1, 0.1], [0.1, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(delta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.9999 0.9999]</span>
<span class="sd">         [0.9999 0.9999]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;delta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyGradientDescent.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyProximalGradientDescent"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyProximalGradientDescent.html#mindspore.ops.ApplyProximalGradientDescent">[文档]</a><span class="k">class</span> <span class="nc">ApplyProximalGradientDescent</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FOBOS(Forward Backward Splitting) algorithm.</span>
<span class="sd">    Refer to the paper `Efficient Learning using Forward-Backward Splitting</span>
<span class="sd">    &lt;http://papers.nips.cc//paper/3793-efficient-learning-using-forward-backward-splitting.pdf&gt;`_ for more detail.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            \text{prox_v} = var - \alpha * \delta \\</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + \alpha * l2} * \max(\left| \text{prox_v} \right| - \alpha * l1, 0)</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`\alpha` represents `alpha`, :math:`\delta` represents `delta`.</span>

<span class="sd">    Inputs of `var` and `delta` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Scaling factor, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **delta** (Tensor) - A tensor for the change.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the updated `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `alpha`, `l1` or `l2` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `alpha`, `l1` or `l2` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `delta` is not a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `var`, and `delta` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_proximal_gradient_descent = ops.ApplyProximalGradientDescent()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.alpha = 0.001</span>
<span class="sd">        ...         self.l1 = 0.1</span>
<span class="sd">        ...         self.l2 = 0.1</span>
<span class="sd">        ...     def construct(self, delta):</span>
<span class="sd">        ...         out = self.apply_proximal_gradient_descent(self.var, self.alpha, self.l1, self.l2, delta)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(np.array([[0.1, 0.1], [0.1, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(delta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.99969995 0.99969995]</span>
<span class="sd">         [0.99969995 0.99969995]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;delta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyGradientDescent.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="LARSUpdate"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LARSUpdate.html#mindspore.ops.LARSUpdate">[文档]</a><span class="k">class</span> <span class="nc">LARSUpdate</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Conducts LARS (layer-wise adaptive rate scaling) update on the sum of squares of gradient.</span>

<span class="sd">    For more details, please refer to :class:`mindspore.nn.LARS`.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): Term added to the denominator to improve numerical stability. Default: 1e-05.</span>
<span class="sd">        hyperpara (float): Trust coefficient for calculating the local learning rate. Default: 0.001.</span>
<span class="sd">        use_clip (bool): Whether to use clip operation for calculating the local learning rate. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **weight** (Tensor) - A tensor, representing the weight.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **gradient** (Tensor) - The gradient of weight, which has the same shape and dtype with weight.</span>
<span class="sd">        - **norm_weight** (Tensor) - A scalar tensor, representing the sum of squares of weight.</span>
<span class="sd">        - **norm_gradient** (Tensor) - A scalar tensor, representing the sum of squares of gradient.</span>
<span class="sd">        - **weight_decay** (Union[Number, Tensor]) - Weight decay. It must be a scalar tensor or number.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. It must be a scalar tensor or number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the new gradient.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `epsilon` nor `hyperpara` is a float.</span>
<span class="sd">        TypeError: If `use_clip` is not a bool.</span>
<span class="sd">        TypeError: If `weight`, `gradient`, `norm_weight` or `norm_gradient` is not a Tensor.</span>
<span class="sd">        TypeError: If `weight_decay` or `learning_rate` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If shape of `gradient` is not the same as `weight`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.lars = ops.LARSUpdate()</span>
<span class="sd">        ...         self.reduce = ops.ReduceSum()</span>
<span class="sd">        ...         self.square = ops.Square()</span>
<span class="sd">        ...     def construct(self, weight, gradient):</span>
<span class="sd">        ...         w_square_sum = self.reduce(self.square(weight))</span>
<span class="sd">        ...         grad_square_sum = self.reduce(self.square(gradient))</span>
<span class="sd">        ...         grad_t = self.lars(weight, gradient, w_square_sum, grad_square_sum, 0.0, 1.0)</span>
<span class="sd">        ...         return grad_t</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([[0.5, 0.8, 0.2], [0.6, 0.4, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([[0.4, 0.4, 0.5], [0.2, 0.4, 0.3]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(Tensor(weight), Tensor(gradient))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.0005265  0.0005265 0.00065813]</span>
<span class="sd">         [0.00026325 0.0005265 0.00039488]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">hyperpara</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">use_clip</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LARSUpdate.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;hyperpara&quot;</span><span class="p">,</span> <span class="n">hyperpara</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_clip&quot;</span><span class="p">,</span> <span class="n">use_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyFtrl"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyFtrl.html#mindspore.ops.ApplyFtrl">[文档]</a><span class="k">class</span> <span class="nc">ApplyFtrl</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL scheme.</span>

<span class="sd">    For more details, please refer to :class:`mindspore.nn.FTRL`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if true . Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same shape and data type as `var`.</span>
<span class="sd">        - **linear** (Parameter) - The linear coefficient to be updated, must be same shape and data type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient. The data type must be float16 or float32.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be positive. Default: 0.001.</span>
<span class="sd">          It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">          Default: 0.0. It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">          Default: 0.0. It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **lr_power** (Union[Number, Tensor]) - Learning rate power controls how the learning rate decreases</span>
<span class="sd">          during training, must be less than or equal to zero. Use fixed learning rate if lr_power is zero.</span>
<span class="sd">          Default: -0.5. It must be a float number or a scalar tensor with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Represents the updated `var`. As the input parameters has been updated in-place, this</span>
<span class="sd">          value is always zero when the platform is GPU.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `grad`, `lr`, `l1`, `l2` or `lr_power` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `lr_power` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class ApplyFtrlNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(ApplyFtrlNet, self).__init__()</span>
<span class="sd">        ...         self.apply_ftrl = ops.ApplyFtrl()</span>
<span class="sd">        ...         self.lr = 0.001</span>
<span class="sd">        ...         self.l1 = 0.0</span>
<span class="sd">        ...         self.l2 = 0.0</span>
<span class="sd">        ...         self.lr_power = -0.5</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.array([[0.9, 0.1],</span>
<span class="sd">        ...                                                  [0.7, 0.8]]).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_ftrl(self.var, self.accum, self.linear, grad, self.lr, self.l1, self.l2,</span>
<span class="sd">        ...                               self.lr_power)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[ 0.0390525  0.11492836]</span>
<span class="sd">         [ 0.00066425 0.15075898]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyFtrl.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;lr_power&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseApplyFtrl"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseApplyFtrl.html#mindspore.ops.SparseApplyFtrl">[文档]</a><span class="k">class</span> <span class="nc">SparseApplyFtrl</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL-proximal scheme</span>
<span class="sd">    For more details, please refer to :class:`mindspore.nn.FTRL`.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if true . Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same data type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - The linear coefficient to be updated, must be the same data type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var` and grad.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined.</span>
<span class="sd">          The type must be int32 or int64 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - Tensor, has the same shape and data type as `accum`.</span>
<span class="sd">        - **linear** (Tensor) - Tensor, has the same shape and data type as `linear`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `lr_power` is not a float.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `linear` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        RuntimeError: If the data type of all of inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(SparseApplyFtrlNet, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_ftrl = ops.SparseApplyFtrl(lr=0.01, l1=0.0, l2=0.0, lr_power=-0.5)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.1]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.array([[0.6]]).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_ftrl(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.ones([1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[2.00000003e-01]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[1.00000001e-01]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[6.00000024e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyFtrl.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseApplyFtrlV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseApplyFtrlV2.html#mindspore.ops.SparseApplyFtrlV2">[文档]</a><span class="k">class</span> <span class="nc">SparseApplyFtrlV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL-proximal scheme. This class has one more attribute, named</span>
<span class="sd">    l2_shrinkage, than class SparseApplyFtrl.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>


<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2_shrinkage (float): L2 shrinkage regularization.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool): If `True`, the var and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same data type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be same data type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var` and grad.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          The type must be int32 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - Tensor, has the same shape and data type as `accum`.</span>
<span class="sd">        - **linear** (Tensor) - Tensor, has the same shape and data type as `linear`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2`, `lr_power` or `use_locking` is not a float.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `linear` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        RuntimeError: If the data type of all of inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlV2Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(SparseApplyFtrlV2Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_ftrl_v2 = ops.SparseApplyFtrlV2(lr=0.01, l1=0.0, l2=0.0,</span>
<span class="sd">        ...                                                         l2_shrinkage=0.0, lr_power=-0.5)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2, 0.3]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.5, 0.9]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.array([[0.7, 0.5]]).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_ftrl_v2(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlV2Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.8, 0.5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.ones([1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 2.00000003e-01,  3.00000012e-01]]), Tensor(shape=[1, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.00000000e-01,  8.99999976e-01]]), Tensor(shape=[1, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.99999988e-01,  5.00000000e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l2_shrinkage</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyFtrlV2.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2_shrinkage</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2_shrinkage&quot;</span><span class="p">,</span> <span class="n">l2_shrinkage</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;linear shape&#39;</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var_dtype&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;accum_dtype&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span>
                <span class="s2">&quot;linear_dtype&quot;</span><span class="p">:</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="s2">&quot;grad_dtype&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indicese&quot;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span></div>


<div class="viewcode-block" id="Dropout"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Dropout.html#mindspore.ops.Dropout">[文档]</a><span class="k">class</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the elements of the input tensor</span>
<span class="sd">    with probability 1-`keep_prob` from a Bernoulli distribution.</span>

<span class="sd">    Refer to :func:`mindspore.ops.dropout` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">Seed0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Dropout.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed0</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Seed0&quot;</span><span class="p">,</span> <span class="n">Seed0</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;Seed1&quot;</span><span class="p">,</span> <span class="n">Seed1</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Dropout2D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Dropout2D.html#mindspore.ops.Dropout2D">[文档]</a><span class="k">class</span> <span class="nc">Dropout2D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor with probability 1-`keep_prob`</span>
<span class="sd">    from a Bernoulli distribution(For a 4-dimensional tensor with a shape of NCHW, the channel feature map refers</span>
<span class="sd">    to a 2-dimensional feature map with the shape of HW).</span>

<span class="sd">    Dropout2D can improve the independence between channel feature maps.</span>

<span class="sd">    Note:</span>
<span class="sd">        The keep probability :math:`keep\_prob` is equal to :math:`1 - p` in :func:`mindspore.ops.dropout2d`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.dropout2d` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations.nn_ops import Dropout2D</span>
<span class="sd">        &gt;&gt;&gt; dropout = Dropout2D(keep_prob=0.5)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([2, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1, 2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Dropout2D.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;Dropout2D&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Dropout3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Dropout3D.html#mindspore.ops.Dropout3D">[文档]</a><span class="k">class</span> <span class="nc">Dropout3D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor</span>
<span class="sd">    with probability 1-`keep_prob` from a Bernoulli distribution(For a 5-dimensional tensor with a shape of NCDHW,</span>
<span class="sd">    the channel feature map refers to a 3-dimensional feature map with a shape of DHW).</span>

<span class="sd">    Dropout3D can improve the independence between channel feature maps.</span>


<span class="sd">    Note:</span>
<span class="sd">        The keep probability :math:`keep\_prob` is equal to :math:`1 - p` in :func:`mindspore.ops.dropout2d`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.dropout3d` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dropout = ops.Dropout3D(keep_prob=0.5)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([2, 1, 2, 1, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1, 2, 1, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Dropout3D.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;Dropout3D&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="CTCLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.CTCLoss.html#mindspore.ops.CTCLoss">[文档]</a><span class="k">class</span> <span class="nc">CTCLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the CTC (Connectionist Temporal Classification) loss and the gradient.</span>

<span class="sd">    The bottom layer of this interface calls the implementation of the third-party baidu-research::warp-ctc.</span>
<span class="sd">    The CTC algorithm is proposed in `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_.</span>

<span class="sd">    CTCLoss calculates loss between a continuous time series and a target sequence.</span>
<span class="sd">    CTCLoss sums over the probability of input to target, producing a loss value which is differentiable with</span>
<span class="sd">    respect to each input node. The alignment of input to target is assumed to be “many-to-one”,</span>
<span class="sd">    such that the length of target series must be less than or equal to the length of input.</span>

<span class="sd">    Args:</span>
<span class="sd">        preprocess_collapse_repeated (bool): If true, repeated labels will be collapsed prior to the CTC calculation.</span>
<span class="sd">                                             Default: False.</span>
<span class="sd">        ctc_merge_repeated (bool): If false, during CTC calculation, repeated non-blank labels will not be merged</span>
<span class="sd">                                   and these labels will be interpreted as individual ones. This is a simplified</span>
<span class="sd">                                   version of CTC. Default: True.</span>
<span class="sd">        ignore_longer_outputs_than_inputs (bool): If true, sequences with longer outputs than inputs will be ignored.</span>
<span class="sd">                                                  Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor must be a `3-D` tensor whose shape is</span>
<span class="sd">          :math:`(max\_time, batch\_size, num\_classes)`. `num_classes` must be `num_labels + 1` classes, `num_labels`</span>
<span class="sd">          indicates the number of actual labels. Blank labels are reserved. Default blank label is `num_classes - 1`.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>
<span class="sd">        - **labels_indices** (Tensor) - The indices of labels. `labels_indices[i, :] = [b, t]` means</span>
<span class="sd">          `labels_values[i]` stores the id for `(batch b, time t)`. The type must be int64 and rank must be 2.</span>
<span class="sd">        - **labels_values** (Tensor) - A `1-D` input tensor. The values are associated with the given batch and time.</span>
<span class="sd">          The type must be int32. `labels_values[i]` must be in the range of `[0, num_classes)`.</span>
<span class="sd">        - **sequence_length** (Tensor) - A tensor containing sequence lengths with the shape of :math:`(batch\_size, )`.</span>
<span class="sd">          The type must be int32. Each value in the tensor must not be greater than `max_time`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **loss** (Tensor) - A tensor containing log-probabilities, the shape is :math:`(batch\_size, )`.</span>
<span class="sd">          The tensor has the same data type as `x`.</span>
<span class="sd">        - **gradient** (Tensor) - The gradient of `loss`, has the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `preprocess_collapse_repeated`, `ctc_merge_repeated` or `ignore_longer_outputs_than_inputs`</span>
<span class="sd">                   is not a bool.</span>
<span class="sd">        TypeError: If `x`, `labels_indices`, `labels_values` or `sequence_length` is not a Tensor.</span>
<span class="sd">        ValueError: If rank of `labels_indices` is not equal to 2.</span>
<span class="sd">        TypeError: If dtype of `x` is not one of the following: float16, float32 nor float64.</span>
<span class="sd">        TypeError: If dtype of `labels_indices` is not int64.</span>
<span class="sd">        TypeError: If dtype of `labels_values` or `sequence_length` is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[0.3, 0.6, 0.6],</span>
<span class="sd">        ...                       [0.4, 0.3, 0.9]],</span>
<span class="sd">        ...</span>
<span class="sd">        ...                      [[0.9, 0.4, 0.2],</span>
<span class="sd">        ...                       [0.9, 0.9, 0.1]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; labels_indices = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; labels_values = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sequence_length = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = ops.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss, gradient = ctc_loss(x, labels_indices, labels_values, sequence_length)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        [ 0.79628  0.5995158 ]</span>
<span class="sd">        &gt;&gt;&gt; print(gradient)</span>
<span class="sd">        [[[ 0.27029088  0.36485454  -0.6351454  ]</span>
<span class="sd">          [ 0.28140804  0.25462854  -0.5360366 ]]</span>
<span class="sd">         [[ 0.47548494  0.2883962    0.04510255 ]</span>
<span class="sd">          [ 0.4082751   0.4082751    0.02843709 ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preprocess_collapse_repeated</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ctc_merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">ignore_longer_outputs_than_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CTCLoss.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;labels_indices&quot;</span><span class="p">,</span> <span class="s2">&quot;labels_values&quot;</span><span class="p">,</span> <span class="s2">&quot;sequence_length&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="s2">&quot;gradient&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;preprocess_collapse_repeated&quot;</span><span class="p">,</span> <span class="n">preprocess_collapse_repeated</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_collapse_repeated_</span> <span class="o">=</span> <span class="n">preprocess_collapse_repeated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ctc_merge_repeated_</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ctc_merge_repeated&quot;</span><span class="p">,</span> <span class="n">ctc_merge_repeated</span><span class="p">,</span>
                                                              <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ignore_longer_outputs_than_inputs&quot;</span><span class="p">,</span>
                                   <span class="n">ignore_longer_outputs_than_inputs</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_longer_outputs_than_inputs_</span> <span class="o">=</span> <span class="n">ignore_longer_outputs_than_inputs</span></div>


<div class="viewcode-block" id="CTCGreedyDecoder"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.CTCGreedyDecoder.html#mindspore.ops.CTCGreedyDecoder">[文档]</a><span class="k">class</span> <span class="nc">CTCGreedyDecoder</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs greedy decoding on the logits given in inputs.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ctc_greedy_decoder` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CTCGreedyDecoder.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">merge_repeated</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;merge_repeated&quot;</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">BasicLSTMCell</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It&#39;s similar to operator :class:`mindspore.ops.DynamicRNN`. BasicLSTMCell will be deprecated in the future.</span>
<span class="sd">    Please use DynamicRNN instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BasicLSTMCell.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_is_tuple</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;state_is_tuple&quot;</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;c rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;w rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;b rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape[0]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape[0]&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape[1]&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;h_shape[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;4*h_shape[1]&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[0]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_shape[1]+h_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[0]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;4*h_shape[1]&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">ct_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ht_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">it_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">jt_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ft_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ot_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">tanhct_shape</span> <span class="o">=</span> <span class="n">c_shape</span>

        <span class="k">return</span> <span class="n">ct_shape</span><span class="p">,</span> <span class="n">ht_shape</span><span class="p">,</span> <span class="n">it_shape</span><span class="p">,</span> <span class="n">jt_shape</span><span class="p">,</span> <span class="n">ft_shape</span><span class="p">,</span> <span class="n">ot_shape</span><span class="p">,</span> <span class="n">tanhct_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">):</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span>
                          <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;h_dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;w_dtype&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">)))</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;c_dtype&quot;</span><span class="p">:</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="s2">&quot;b_dtype&quot;</span><span class="p">:</span> <span class="n">b_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span>


<div class="viewcode-block" id="DynamicRNN"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.DynamicRNN.html#mindspore.ops.DynamicRNN">[文档]</a><span class="k">class</span> <span class="nc">DynamicRNN</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a recurrent neural network to the input.</span>
<span class="sd">    Only long short-term memory (LSTM) is supported currently.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            i_{t+1} = \sigma(W_{ix} x_{t+1} + b_{ix} + W_{ih} h_{(t)} + b_{ih}) \\</span>
<span class="sd">            f_{t+1} = \sigma(W_{fx} x_{t+1} + b_{fx} + W_{fh} h_{(t)} + b_{fh}) \\</span>
<span class="sd">            \tilde{c}_{t+1} = \tanh(W_{cx} x_{t+1} + b_{cx} + W_{ch} h_{(t)} + b_{ch}) \\</span>
<span class="sd">            o_{t+1} = \sigma(W_{ox} x_{t+1} + b_{ox} + W_{oh} h_{(t)} + b_{oh}) \\</span>
<span class="sd">            c_{t+1} = f_{t+1} * c_{(t)} + i_t * \tilde{c}_{t+1} \\</span>
<span class="sd">            h_{t+1} = o_{t+1} * \tanh(c_{t+1}) \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`h_{t+1}` is the hidden state at time `t+1`, :math:`x_{t+1}` is the input</span>
<span class="sd">    at time `t+1`, :math:`h_{t}` is the hidden state of the layer</span>
<span class="sd">    at time `t` or the initial hidden state at time `0`,</span>
<span class="sd">    :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product. :math:`W, b`</span>
<span class="sd">    are learnable weights between the output and the input in the formula. For instance,</span>
<span class="sd">    :math:`W_{ix}, b_{ix}` are the weight and bias used to transform from input :math:`x` to :math:`i`.</span>

<span class="sd">    Args:</span>
<span class="sd">        cell_type (str): A string identifying the cell type in the operator. Default: &#39;LSTM&#39;.</span>
<span class="sd">            Only &#39;LSTM&#39; is currently supported.</span>
<span class="sd">        direction (str): A string identifying the direction in the operator. Default: &#39;UNIDIRECTIONAL&#39;.</span>
<span class="sd">            Only &#39;UNIDIRECTIONAL&#39; is currently supported.</span>
<span class="sd">        cell_depth (int): An integer identifying the cell depth in the operator. Default: 1.</span>
<span class="sd">        use_peephole (bool): A bool identifying if use peephole in the operator. Default: False.</span>
<span class="sd">        keep_prob (float): A float identifying the keep prob in the operator. Default: 1.0.</span>
<span class="sd">        cell_clip (float): A float identifying the cell clip in the operator. Default: -1.0.</span>
<span class="sd">        num_proj (int): An integer identifying the number projection in the operator. Default: 0.</span>
<span class="sd">        time_major (bool): A bool identifying the time major in the operator. Default: True.</span>
<span class="sd">            Only `True` is currently supported.</span>
<span class="sd">        activation (str): A string identifying the type of activation function in the operator. Default: &#39;tanh&#39;.</span>
<span class="sd">            Only &#39;tanh&#39; is currently supported.</span>
<span class="sd">        forget_bias (float): A float identifying the forget bias in the operator. Default: 0.0.</span>
<span class="sd">        is_training (bool): A bool identifying is training in the operator. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Current words. Tensor of shape :math:`(num\_step, batch\_size, input\_size)`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **w** (Tensor) - Weight. Tensor of shape :math:`(input\_size + hidden\_size, 4 * hidden\_size)`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **b** (Tensor) - Bias. Tensor of shape :math:`(4 * hidden\_size)`.</span>
<span class="sd">          The data type must be float16 or float32.</span>
<span class="sd">        - **seq_length** (Tensor) - The length of each batch. Tensor of shape :math:`(batch\_size, )`.</span>
<span class="sd">          Only `None` is currently supported.</span>
<span class="sd">        - **init_h** (Tensor) - Hidden state of initial time. Tensor of shape :math:`(1, batch\_size, hidden\_size)`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **init_c** (Tensor) - Cell state of initial time. Tensor of shape :math:`(1, batch\_size, hidden\_size)`.</span>
<span class="sd">          The data type must be float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **output_h** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          With data type of float16.</span>
<span class="sd">        - **output_c** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **i** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **j** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **f** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **o** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **tanhct** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `cell_type`, `direction` or `activation` is not a str.</span>
<span class="sd">        TypeError: If `cell_depth` or `num_proj` is not an int.</span>
<span class="sd">        TypeError: If `keep_prob`, `cell_clip` or `forget_bias` is not a float.</span>
<span class="sd">        TypeError: If `use_peehpole`, `time_major` or `is_training` is not a bool.</span>
<span class="sd">        TypeError: If `x`, `w`, `b`, `seq_length`, `init_h` or `init_c` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x`, `w`, `init_h` or `init_c` is not float16.</span>
<span class="sd">        TypeError: If dtype of `b` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(2, 16, 64).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; w = Tensor(np.random.rand(96, 128).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.random.rand(128).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_h = Tensor(np.random.rand(1, 16, 32).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_c = Tensor(np.random.rand(1, 16, 32).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; dynamic_rnn = ops.DynamicRNN()</span>
<span class="sd">        &gt;&gt;&gt; output = dynamic_rnn(x, w, b, None, init_h, init_c)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].shape)</span>
<span class="sd">        (2, 16, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">cell_type</span><span class="o">=</span><span class="s1">&#39;LSTM&#39;</span><span class="p">,</span>
                 <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">,</span>
                 <span class="n">cell_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">use_peephole</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">time_major</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DynamicRNN.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_depth</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_depth&quot;</span><span class="p">,</span> <span class="n">cell_depth</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_clip</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_clip&quot;</span><span class="p">,</span> <span class="n">cell_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">num_proj</span><span class="p">,</span> <span class="s2">&quot;num_proj&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_peephole</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_peephole&quot;</span><span class="p">,</span> <span class="n">use_peephole</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_major</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;time_major&quot;</span><span class="p">,</span> <span class="n">time_major</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_type&quot;</span><span class="p">,</span> <span class="n">cell_type</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">cell_type</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;LSTM&#39;</span><span class="p">],</span> <span class="s2">&quot;cell_type&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;direction&quot;</span><span class="p">,</span> <span class="n">direction</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">direction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">direction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">],</span> <span class="s2">&quot;direction&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">,</span> <span class="n">seq_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;x_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;w rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;b rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;c_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">seq_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;seq_length&#39; must be None.&quot;</span><span class="p">)</span>

        <span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">w_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">4</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[-1]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;w_shape[-1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">w_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the last dimension of &#39;w&#39; must be a multiple of 4, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">w_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[0]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;input_size + hidden_size&quot;</span><span class="p">,</span>
                        <span class="n">input_size</span> <span class="o">+</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[0]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;h_shape[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;h_shape[2]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="s2">&quot;h_shape&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;placeholder_index&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_index</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;input_size&quot;</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">y_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">seq_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">):</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">)))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span></div>


<div class="viewcode-block" id="DynamicGRUV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.DynamicGRUV2.html#mindspore.ops.DynamicGRUV2">[文档]</a><span class="k">class</span> <span class="nc">DynamicGRUV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a single-layer gated recurrent unit (GRU) to an input sequence.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll}</span>
<span class="sd">            r_{t+1} = \sigma(W_{ir} x_{t+1} + b_{ir} + W_{hr} h_{(t)} + b_{hr}) \\</span>
<span class="sd">            z_{t+1} = \sigma(W_{iz} x_{t+1} + b_{iz} + W_{hz} h_{(t)} + b_{hz}) \\</span>
<span class="sd">            n_{t+1} = \tanh(W_{in} x_{t+1} + b_{in} + r_{t+1} * (W_{hn} h_{(t)}+ b_{hn})) \\</span>
<span class="sd">            h_{t+1} = (1 - z_{t+1}) * n_{t+1} + z_{t+1} * h_{(t)}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`h_{t+1}` is the hidden state at time `t+1`, :math:`x_{t+1}` is the input</span>
<span class="sd">    at time `t+1`, :math:`h_{t}` is the hidden state of the layer</span>
<span class="sd">    at time `t` or the initial hidden state at time `0`, and :math:`r_{t+1}`,</span>
<span class="sd">    :math:`z_{t+1}`, :math:`n_{t+1}` are the reset, update, and new gates, respectively.</span>
<span class="sd">    :math:`W`, :math:`b` are the weight parameter and the deviation parameter respectively.</span>
<span class="sd">    :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.</span>

<span class="sd">    Args:</span>
<span class="sd">        direction (str): A string identifying the direction in the operator. Default: &#39;UNIDIRECTIONAL&#39;.</span>
<span class="sd">            Only &#39;UNIDIRECTIONAL&#39; is currently supported.</span>
<span class="sd">        cell_depth (int): An integer identifying the cell depth in the operator. Default: 1.</span>
<span class="sd">        keep_prob (float): A float identifying the keep prob in the operator. Default: 1.0.</span>
<span class="sd">        cell_clip (float): A float identifying the cell clip in the operator. Default: -1.0.</span>
<span class="sd">        num_proj (int): An integer identifying the number projection in the operator. Default: 0.</span>
<span class="sd">        time_major (bool): A bool identifying the time major in the operator. Default: True.</span>
<span class="sd">        activation (str) : A string identifying the type of activation function in the operator. Default: &#39;tanh&#39;.</span>
<span class="sd">            Only &#39;tanh&#39; is currently supported.</span>
<span class="sd">        gate_order (str): A string identifying the gate order in weight and bias. Default: &#39;rzh&#39;.</span>
<span class="sd">            &#39;zrh&#39; is another option. Here, &#39;rzh&#39; means the gate order is: reset gate, update gate, hidden gate.</span>
<span class="sd">            &#39;zrh&#39; means the gate order is: update gate, reset gate, hidden gate.</span>
<span class="sd">        reset_after (bool): A bool identifying whether to apply reset gate after matrix multiplication. Default: True.</span>
<span class="sd">        is_training (bool): A bool identifying is training in the operator. Default: True.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Current words.</span>
<span class="sd">          Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{input_size})`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **weight_input** (Tensor) - Input-hidden weight :math:`W_{\{ir,iz,in\}}`.</span>
<span class="sd">          Tensor of shape :math:`(\text{input_size}, 3 \times \text{hidden_size})`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **weight_hidden** (Tensor) - Hidden-hidden weight :math:`W_{\{hr,hz,hn\}}`.</span>
<span class="sd">          Tensor of shape :math:`(\text{hidden_size}, 3 \times \text{hidden_size})`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **bias_input** (Tensor) - Input-hidden bias :math:`b_{\{ir,iz,in\}}`.</span>
<span class="sd">          Tensor of shape :math:`(3 \times \text{hidden_size})`, or None.</span>
<span class="sd">          Has the same data type with input `init_h`.</span>
<span class="sd">        - **bias_hidden** (Tensor) - Hidden-hidden bias :math:`b_{\{hr,hz,hn\}}`.</span>
<span class="sd">          Tensor of shape :math:`(3 \times \text{hidden_size})`,</span>
<span class="sd">          or None. Has the same data type with input `init_h`.</span>
<span class="sd">        - **seq_length** (Tensor) - The length of each batch. Tensor of shape :math:`(\text{batch_size})`.</span>
<span class="sd">          Only `None` is currently supported.</span>
<span class="sd">        - **init_h** (Tensor) - Hidden state of initial time.</span>
<span class="sd">          Tensor of shape :math:`(\text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A Tensor of shape:</span>

<span class="sd">          - y_shape = :math:`(num\_step, batch\_size, min(hidden\_size, num\_proj))`: `If num_proj &gt; 0`,</span>
<span class="sd">          - y_shape = :math:`(num\_step, batch\_size, hidden\_size)`: `If num_proj = 0`.</span>

<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **output_h** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **update** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **reset** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **new** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **hidden_new** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>

<span class="sd">        A note about the bias_type:</span>

<span class="sd">        - If `bias_input` and `bias_hidden` both are `None`, `bias_type` is the data type of `init_h`.</span>
<span class="sd">        - If `bias_input` is not `None`, `bias_type` is the data type of `bias_input`.</span>
<span class="sd">        - If `bias_input` is `None` and `bias_hidden` is not `None`, `bias_type` is the data type of `bias_hidden`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `direction`, `activation` or `gate_order` is not a str.</span>
<span class="sd">        TypeError: If `cell_depth` or `num_proj` is not an int.</span>
<span class="sd">        TypeError: If `keep_prob` or `cell_clip` is not a float.</span>
<span class="sd">        TypeError: If `time_major`, `reset_after` or `is_training` is not a bool.</span>
<span class="sd">        TypeError: If `x`, `weight_input`, `weight_hidden`, `bias_input`, `bias_hidden`, `seq_length` or `ini_h` is not</span>
<span class="sd">                   a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x`, `weight_input` or `weight_hidden` is not float16.</span>
<span class="sd">        TypeError: If dtype of `init_h` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(2, 8, 64).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; weight_i = Tensor(np.random.rand(64, 48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; weight_h = Tensor(np.random.rand(16, 48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; bias_i = Tensor(np.random.rand(48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; bias_h = Tensor(np.random.rand(48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_h = Tensor(np.random.rand(8, 16).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; dynamic_gru_v2 = ops.DynamicGRUV2()</span>
<span class="sd">        &gt;&gt;&gt; output = dynamic_gru_v2(x, weight_i, weight_h, bias_i, bias_h, None, init_h)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].shape)</span>
<span class="sd">        (2, 8, 16)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">,</span>
                 <span class="n">cell_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">time_major</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
                 <span class="n">gate_order</span><span class="o">=</span><span class="s2">&quot;rzh&quot;</span><span class="p">,</span>
                 <span class="n">reset_after</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DynamicGRUV2.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_depth</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_depth&quot;</span><span class="p">,</span> <span class="n">cell_depth</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_clip</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_clip&quot;</span><span class="p">,</span> <span class="n">cell_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">num_proj</span><span class="p">,</span> <span class="s2">&quot;num_proj&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_major</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;time_major&quot;</span><span class="p">,</span> <span class="n">time_major</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">direction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">direction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">],</span> <span class="s2">&quot;direction&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_order</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">gate_order</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;zrh&#39;</span><span class="p">,</span> <span class="s1">&#39;rzh&#39;</span><span class="p">],</span> <span class="s2">&quot;gate_order&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_after</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reset_after&quot;</span><span class="p">,</span> <span class="n">reset_after</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">winput_shape</span><span class="p">,</span> <span class="n">whidden_shape</span><span class="p">,</span> <span class="n">binput_shape</span><span class="p">,</span> <span class="n">bhidden_shape</span><span class="p">,</span> <span class="n">seq_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;x shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">winput_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;weight input shape rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">whidden_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;weight hidden shape rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="n">x_shape</span>
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">winput_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">3</span>
        <span class="k">if</span> <span class="n">winput_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">3</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the last dimension of &#39;w&#39; must be a multiple of 3, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">winput_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">binput_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">binput_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;bias input shape rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;bias_input_shape&quot;</span><span class="p">,</span> <span class="n">binput_shape</span><span class="p">,</span> <span class="s2">&quot;3 * hidden_shape&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_index</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bhidden_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bhidden_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;bias hidden shape rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;bias_hidden_shape&quot;</span><span class="p">,</span> <span class="n">bhidden_shape</span><span class="p">,</span>
                            <span class="s2">&quot;3 * hidden_shape&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_index</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">seq_shape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the dimension of &#39;seq_length&#39; must be None, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">seq_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;init_h shape rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;init_h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;init_h_shape[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;weight_input_shape[-1]&quot;</span><span class="p">,</span> <span class="n">winput_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;weight_hidden_shape[-1]&quot;</span><span class="p">,</span>
                        <span class="n">whidden_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;weight_input_shape[0]&quot;</span><span class="p">,</span> <span class="n">winput_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;input_size&quot;</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;weight_hidden_shape[0]&quot;</span><span class="p">,</span> <span class="n">whidden_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_step</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;placeholder_index&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">winput_dtype</span><span class="p">,</span> <span class="n">whidden_dtype</span><span class="p">,</span> <span class="n">binput_dtype</span><span class="p">,</span> <span class="n">bhidden_dtype</span><span class="p">,</span> <span class="n">seq_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;x dtype&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;weight input dtype&quot;</span><span class="p">,</span> <span class="n">winput_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;weight hidden dtype&quot;</span><span class="p">,</span> <span class="n">whidden_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;init_h dtype&quot;</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">b_dtype</span> <span class="o">=</span> <span class="n">h_dtype</span>
        <span class="k">if</span> <span class="n">binput_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;init_h&#39;</span><span class="p">:</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="s1">&#39;bias_input&#39;</span><span class="p">:</span> <span class="n">binput_dtype</span><span class="p">}</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">b_dtype</span> <span class="o">=</span> <span class="n">binput_dtype</span>
        <span class="k">if</span> <span class="n">bhidden_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;init_h&#39;</span><span class="p">:</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="s1">&#39;bias_hidden&#39;</span><span class="p">:</span> <span class="n">bhidden_dtype</span><span class="p">}</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">valid_dtypes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">b_dtype</span> <span class="o">=</span> <span class="n">bhidden_dtype</span>

        <span class="k">return</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">,</span> <span class="n">b_dtype</span></div>


<div class="viewcode-block" id="InTopK"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.InTopK.html#mindspore.ops.InTopK">[文档]</a><span class="k">class</span> <span class="nc">InTopK</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether the targets are in the top `k` predictions.</span>

<span class="sd">    Refer to :func:`mindspore.ops.intopk` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1, 8, 5, 2, 7], [4, 9, 1, 3, 5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([1, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; in_top_k = ops.InTopK(3)</span>
<span class="sd">        &gt;&gt;&gt; output = in_top_k(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InTopK&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="LRN"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LRN.html#mindspore.ops.LRN">[文档]</a><span class="k">class</span> <span class="nc">LRN</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Local Response Normalization.</span>

<span class="sd">    .. math::</span>

<span class="sd">        b_{c} = a_{c}\left(k + \frac{\alpha}{n}</span>
<span class="sd">        \sum_{c&#39;=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c&#39;}^2\right)^{-\beta}</span>

<span class="sd">    where the :math:`a_{c}` indicates the specific value of the pixel corresponding to c in feature map;</span>
<span class="sd">    where the :math:`n/2` indicates the `depth_radius`; where the :math:`k` indicates the `bias`;</span>
<span class="sd">    where the :math:`\alpha` indicates the `alpha`; where the :math:`\beta` indicates the `beta`.</span>

<span class="sd">    Args:</span>
<span class="sd">        depth_radius (int): Half-width of the 1-D normalization window with the shape of 0-D. Default: 5.</span>
<span class="sd">        bias (float): An offset (usually positive to avoid dividing by 0). Default: 1.0.</span>
<span class="sd">        alpha (float): A scale factor, usually positive. Default: 1.0.</span>
<span class="sd">        beta (float): An exponent. Default: 0.5.</span>
<span class="sd">        norm_region (str): Specifies normalization region. Options: &quot;ACROSS_CHANNELS&quot;. Default: &quot;ACROSS_CHANNELS&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 4-D Tensor with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `depth_radius` is not an int.</span>
<span class="sd">        TypeError: If `bias`, `alpha` or `beta` is not a float.</span>
<span class="sd">        TypeError: If `norm_region` is not a str.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[[0.1], [0.2]],</span>
<span class="sd">        ...                       [[0.3], [0.4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lrn = ops.LRN()</span>
<span class="sd">        &gt;&gt;&gt; output = lrn(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[0.09534626]</span>
<span class="sd">           [0.1825742 ]]</span>
<span class="sd">          [[0.2860388 ]</span>
<span class="sd">           [0.3651484 ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">depth_radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">norm_region</span><span class="o">=</span><span class="s2">&quot;ACROSS_CHANNELS&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LRN&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;LRN&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;depth_radius&quot;</span><span class="p">,</span> <span class="n">depth_radius</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;norm_region&quot;</span><span class="p">,</span> <span class="n">norm_region</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">norm_region</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;ACROSS_CHANNELS&#39;</span><span class="p">],</span> <span class="s1">&#39;norm_region&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">depth_radius</span><span class="p">,</span> <span class="s2">&quot;depth_radius&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="AvgPool3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AvgPool3D.html#mindspore.ops.AvgPool3D">[文档]</a><span class="k">class</span> <span class="nc">AvgPool3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    3D Average pooling operation.</span>

<span class="sd">    Applies a 3D average pooling over an input Tensor which can be regarded as a composition of 3D input planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`, AvgPool3D outputs</span>
<span class="sd">    regional average in the :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1, s_2)`, the operation is as follows.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        &quot;kernel_size&quot; is in the range [1, 255]. &quot;strides&quot; is in the range [1, 63].</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \frac{1}{d_{ker} * h_{ker} * w_{ker}} \sum_{l=0}^{d_{ker}-1} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the average value,</span>
<span class="sd">            is an int number that represents depth, height and width are both kernel_size, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &quot;same&quot;, &quot;valid&quot;, &quot;pad&quot;.</span>
<span class="sd">            Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be the same as</span>
<span class="sd">              the input. The total number of padding will be calculated in depth, horizontal and vertical</span>
<span class="sd">              directions and evenly distributed to head and tail, top and bottom, left and right if possible.</span>
<span class="sd">              Otherwise, the last extra padding will be done from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `pad`</span>
<span class="sd">              must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input in depth, height, width. The number of `pad` will</span>
<span class="sd">              be padded to the input Tensor borders. `pad` must be greater than or equal to 0.</span>
<span class="sd">        pad (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">                    head, tail, top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of six</span>
<span class="sd">                    integers, the padding of head, tail, top, bottom, left and right equal to pad[0], pad[1], pad[2],</span>
<span class="sd">                    pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        ceil_mode (bool): If True, ceil instead of floor to compute the output shape. Default: False.</span>
<span class="sd">        count_include_pad (bool): If True, averaging calculation will include the zero-padding. Default: True.</span>
<span class="sd">        divisor_override (int): If specified, it will be used as divisor in the averaging calculation,</span>
<span class="sd">            otherwise kernel_size will be used. Default: 0.</span>
<span class="sd">        data_format (str) : The optional value for data format. Currently only support &#39;NCDHW&#39;. Default: &#39;NCDHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Currently support float16 and float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})`. Has the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size`, `strides` or `pad` is neither an int not a tuple.</span>
<span class="sd">        TypeError: If `ceil_mode` or `count_include_pad` is not a bool.</span>
<span class="sd">        TypeError: If `pad_mode` or `data_format` is not a string.</span>
<span class="sd">        TypeError: If `divisor_override` is not an int.</span>
<span class="sd">        ValueError: If numbers in `kernel_size` or `strides` are not positive.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is a tuple whose length is not equal to 3.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If element of `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to 0 or (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 2 * 2 * 2 * 3).reshape((1, 2, 2, 2, 3)), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; avg_pool3d = ops.AvgPool3D(kernel_size=2, strides=1, pad_mode=&quot;valid&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = avg_pool3d(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[ 5.  6.]]]</span>
<span class="sd">          [[[17. 18.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">divisor_override</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AvgPool3D&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;pad&#39; must be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;six positive int numbers, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="s1">&#39;PAD&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;PAD&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero or (0, 0, 0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">PAD</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;PAD&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad or item of pad&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count_include_pad</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;count_include_pad&#39;</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">divisor_override</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">divisor_override</span><span class="p">,</span> <span class="s1">&#39;divisor_override&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Conv3D.html#mindspore.ops.Conv3D">[文档]</a><span class="k">class</span> <span class="nc">Conv3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    3D convolution layer.</span>

<span class="sd">    Applies a 3D convolution over an input tensor which is typically of shape</span>
<span class="sd">    :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})` and output shape</span>
<span class="sd">    :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`. Where :math:`N` is batch size, :math:`C` is channel number,</span>
<span class="sd">    :math:`D` is depth, :math:`H` is height, :math:`W` is width.</span>
<span class="sd">    the formula is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \operatorname{out}\left(N_{i}, C_{\text {out}_j}\right)=\operatorname{bias}\left(C_{\text {out}_j}\right)+</span>
<span class="sd">        \sum_{k=0}^{C_{in}-1} ccor(\text {weight}\left(C_{\text {out}_j}, k\right),</span>
<span class="sd">        \operatorname{input}\left(N_{i}, k\right))</span>

<span class="sd">    where :math:`k` is kernel, :math:`ccor` is the cross-correlation operator.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;valid&quot;, the output depth, height and width will be</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{D_{in} + 2 \times \text{padding} - \text{ks_d} -</span>
<span class="sd">    (\text{ks_d} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` and</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{H_{in} + 2 \times \text{padding} - \text{ks_h} -</span>
<span class="sd">    (\text{ks_h} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` and</span>
<span class="sd">    :math:`\left \lfloor{1 + \frac{W_{in} + 2 \times \text{padding} - \text{ks_w} -</span>
<span class="sd">    (\text{ks_w} - 1) \times (\text{dilation} - 1) }{\text{stride}}} \right \rfloor` respectively. Where</span>
<span class="sd">    :math:`dilation` is Spacing between kernel elements, :math:`stride` is The step length of each step,</span>
<span class="sd">    :math:`padding` is zero-padding added to both sides of the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The number of output channel :math:`C_{out}`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The data type is int or a tuple of 3 integers. Specifies the depth, height</span>
<span class="sd">            and width of the 3D convolution window. Single int means the value is for the depth, height and width</span>
<span class="sd">            of the kernel. A tuple of 3 ints means the first value is for the depth, height and the other is for the</span>
<span class="sd">            width of the kernel.</span>
<span class="sd">        mode (int): Modes for different convolutions. It is currently not used. Default: 1.</span>
<span class="sd">        stride (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot; and &quot;pad&quot;. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in head and tail, top and bottom,</span>
<span class="sd">              left and right directions possiblily.</span>
<span class="sd">              Otherwise, the last extra padding will be calculated from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `pad`</span>
<span class="sd">              must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input in depth, height and width. The number of `pad` will</span>
<span class="sd">              be padded to the input Tensor borders. `pad` must be greater than or equal to 0.</span>

<span class="sd">        pad (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">                    head, tail, top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of six</span>
<span class="sd">                    integers, the padding of head, tail, top, bottom, left and right equal to pad[0], pad[1], pad[2],</span>
<span class="sd">                    pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): The data type is int or a tuple of 3 integers</span>
<span class="sd">                                      :math:`(dilation_d, dilation_h, dilation_w)`.</span>
<span class="sd">                                      Currently, dilation on depth only supports the case of 1.</span>
<span class="sd">                                      Specifies the dilation rate to use for dilated convolution.</span>
<span class="sd">                                      If set :math:`k &gt; 1`, there will be :math:`k - 1` pixels skipped</span>
<span class="sd">                                      for each sampling location. Its value must be greater than or equal to 1 and</span>
<span class="sd">                                      bounded by the height and width of the input. Default: 1.</span>
<span class="sd">        group (int): Splits filter into groups, `in_channels` and `out_channels` must be</span>
<span class="sd">            divisible by the number of groups. Default: 1. Only 1 is currently supported.</span>
<span class="sd">        data_format (str): The optional value for data format. Currently only support &quot;NCDHW&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Currently input data type only support float16 and float32.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(k_d, K_h, K_w)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}/groups, k_d, K_h, K_w)`.</span>
<span class="sd">          Currently weight data type only support float16 and float32.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`C_{in}`. Currently, only support none.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied 3D convolution. The shape is :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If `out_channel`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([16, 3, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 3, 4, 3, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; conv3d = ops.Conv3D(out_channel=32, kernel_size=(4, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 7, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3D&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                               <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">third_one</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;pad&#39; must be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;six positive int numbers, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero or (0, 0, 0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;offset_x&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Conv3DBackpropInput</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradients of convolution 3D with respect to the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimension of the output.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The kernel size of the 3D convolution.</span>
<span class="sd">        mode (int): Modes for different convolutions. Not currently used.</span>
<span class="sd">        pad_mode (str): Modes to fill padding. It could be &quot;valid&quot;, &quot;same&quot;, or &quot;pad&quot;. Default: &quot;valid&quot;.</span>
<span class="sd">        pad (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">                    head, tail, top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of four</span>
<span class="sd">                    integers, the padding of head, tail, top, bottom, left and right equal to pad[0], pad[1], pad[2],</span>
<span class="sd">                    pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int])): The stride to be applied to the convolution filter. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int])): Specifies the space to use between kernel elements. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1.</span>
<span class="sd">        data_format (str): The optional value for data format. Currently only support &#39;NCDHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(D_in, K_h, K_w)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}, D_{in}, K_h, K_w)`. Currently weight data type only support float16 and float32.</span>
<span class="sd">        - **dout** (Tensor) - the gradients with respect to the output of the convolution.</span>
<span class="sd">          The shape conforms to the default.</span>
<span class="sd">          data_format :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`. Currently dout data type only support float16</span>
<span class="sd">          and float32.</span>
<span class="sd">        - **input_size** (tuple(int)) - A tuple describes the shape of the input which conforms to the format</span>
<span class="sd">          :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients with respect to the input of convolution 3D. It has the same shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If `out_channel`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([16, 32, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 4, 6, 2]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([16, 32, 13, 37, 33]))</span>
<span class="sd">        &gt;&gt;&gt; conv3d_backprop_input = ops.Conv3DBackpropInput(out_channel=4, kernel_size=(4, 6, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d_backprop_input(dout, weight, ops.shape(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 13, 37, 33)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3DBackpropInput&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="s1">&#39;input_size&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be (0, 0, 0, 0, 0, 0) &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;when &#39;pad_mode&#39; is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_deconv_output_length</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride_size</span><span class="p">,</span> <span class="n">dilation_size</span><span class="p">):</span>
    <span class="n">filter_size</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">filter_size</span> <span class="o">-</span> <span class="n">stride_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">input_length</span> <span class="o">*</span> <span class="n">stride_size</span> <span class="o">+</span> <span class="n">filter_size</span> <span class="o">-</span> <span class="n">stride_size</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">input_length</span> <span class="o">*</span> <span class="n">stride_size</span>
    <span class="k">return</span> <span class="n">length</span>


<span class="k">class</span> <span class="nc">SparseApplyAdadelta</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adadelta scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                accum = \rho * accum + (1 - \rho) * grad^2 \\</span>
<span class="sd">                \text{update} = \sqrt{\text{accum_update} + \epsilon} * \frac{grad}{\sqrt{accum + \epsilon}} \\</span>
<span class="sd">                var = var -  update * lr \\</span>
<span class="sd">                \text{accum_update} = \rho * \text{accum_update} + (1 - \rho) * update^2 \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">    Inputs of &#39;var&#39;, &#39;accum&#39;, &#39;accum_update&#39; and &#39;grad&#39; comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent. Besides, inputs of &#39;lr&#39; and &#39;rho&#39; also support implicit type conversion.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Note:</span>
<span class="sd">        If there are negative values or values greater than or equal to var.shape[0] in `indices`,</span>
<span class="sd">        the behavior is undefined. Besides, this operator doesn&#39;t support duplicates in `indices`.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Its value must be greater or equal to 0.</span>
<span class="sd">        use_locking (bool): If `True`, the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. Mush have the same shape and dtype as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **accum_update** (Parameter) - Accum_update to be updated. Must have the same shape and dtype as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **lr** (Union[float, Tensor]) - Learning rate, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **rho** (Union[float, Tensor]) - Decay rate, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          Must be one of the following types: int32, int64 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>
<span class="sd">        - **accum_update** (Tensor) - The same shape and data type as `accum_update`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `epsilon` is not a float.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `var`, &#39;accum&#39;, &#39;accum_update&#39; is not a Parameter.</span>
<span class="sd">        TypeError: If dtype of `accum`, `accum_updata`, `grad` is not same as `var`.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `accum_update`, `lr`, `rho` or `grad` is neither float16 nor</span>
<span class="sd">                   float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `epsilon` is less than 0.</span>
<span class="sd">        ValueError: If the shape of `accum`, `accum_updata`, `grad` is not same as `var`.</span>
<span class="sd">        ValueError: If the rank of `indices` is not equal to 1.</span>
<span class="sd">        ValueError: If shape of `indices` is not same as shape of first dimension of `grad`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self,epsilon,use_locking = False):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_adadelta = P.SparseApplyAdadelta(epsilon,use_locking)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[1.0,2.0],[2.0,3.0]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[1.5,2.5],[3.5,4.5]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.accum_update = Parameter(Tensor(np.array([[1.2,2.4],[1.8,0.6]]).astype(np.float32)),</span>
<span class="sd">        ...                name=&quot;accum_update&quot;)</span>
<span class="sd">        ...     def construct(self, lr, rho, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_adadelta(self.var, self.accum, self.accum_update, lr, rho, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; epsilon = 1e-6</span>
<span class="sd">        &gt;&gt;&gt; net = Net(epsilon)</span>
<span class="sd">        &gt;&gt;&gt; lr = 0.01</span>
<span class="sd">        &gt;&gt;&gt; rho = 0.2</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, rho, grad, Tensor(np.array([0,1],dtype=np.int32)))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 9.94611859e-01,  1.98851788e+00],</span>
<span class="sd">         [ 1.99840558e+00,  2.99478507e+00]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 3.72000009e-01,  8.91999960e-01],</span>
<span class="sd">         [ 7.08000004e-01,  1.41200006e+00]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 4.72257614e-01,  1.53470778e+00],</span>
<span class="sd">         [ 3.80338937e-01,  3.37563992e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum_updata&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyAdadelta&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CTCLossV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the CTC (Connectionist Temporal Classification) loss and the gradient.</span>

<span class="sd">    The CTC algorithm is proposed in `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank (int): The blank label. Default: 0.</span>
<span class="sd">        reduction (string): Apply specific reduction method to the output. Currently only support &#39;none&#39;,</span>
<span class="sd">            not case sensitive. Default: &quot;none&quot;.</span>
<span class="sd">        zero_infinity (bool): Whether to set infinite loss and correlation gradient to zero. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **log_probs** (Tensor) - A tensor of shape (T, N, C), where T is input length, N is batch size and C is number</span>
<span class="sd">          of classes (including blank).</span>
<span class="sd">        - **targets** (Tensor) - A tensor of shape (N, S), where S is max target length, means the target sequences.</span>
<span class="sd">        - **input_lengths** (Union(Tuple, Tensor)) - A tuple or Tensor of shape(N). It means the lengths of the input.</span>
<span class="sd">        - **target_lengths** (Union(Tuple, Tensor)) - A tuple or Tensor of shape(N). It means the lengths of the target.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **neg_log_likelihood** (Tensor) - A loss value which is differentiable with respect to each input node.</span>
<span class="sd">        - **log_alpha** (Tensor) - The probability of possible trace of input to target.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `zero_infinity` is not a bool, reduction is not string.</span>
<span class="sd">        TypeError: If the dtype of `log_probs` is not float or double.</span>
<span class="sd">        TypeError: If the dtype of `targets`, `input_lengths` or `target_lengths` is not int32 or int64.</span>
<span class="sd">        RuntimeError: If the rank of `log_probs` is not 3.</span>
<span class="sd">        RuntimeError: If the rank of `targets` is not 2.</span>
<span class="sd">        RuntimeError: If the shape of `input_lengths` does not match {batch_size|N}.</span>
<span class="sd">        RuntimeError: If the shape of `target_lengths` does not match {batch_size|N}.</span>
<span class="sd">        RuntimeError: If the types of `targets`, `input_lengths` or `target_lengths` are different.</span>
<span class="sd">        RuntimeError: If the value of `blank` is not in range [0, num_labels|C).</span>
<span class="sd">        RuntimeError: If any value of `input_lengths` is larger than (time_series|T).</span>
<span class="sd">        RuntimeError: If any target_lengths[i] is not in range [0, input_length[i]].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; log_probs = Tensor(np.array([[[0.3, 0.6, 0.6]],</span>
<span class="sd">                                         [[0.9, 0.4, 0.2]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; targets = Tensor(np.array([[0, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = Tensor(np.array([2]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = Tensor(np.array([1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; CTCLossV2 = op.CTCLossV2(blank=0, reduction=&#39;none&#39;, zero_infinity=False)</span>
<span class="sd">        &gt;&gt;&gt; neg_log_hood, log_alpha = CTCLossV2(</span>
<span class="sd">        ...     log_probs, targets, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; print(neg_log_hood)</span>
<span class="sd">        [-2.2986124]</span>
<span class="sd">        &gt;&gt;&gt; print(log_alpha)</span>
<span class="sd">        [[[0.3       0.3            -inf      -inf      -inf]</span>
<span class="sd">          [1.2       1.8931472 1.2            -inf      -inf]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CTCLossV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;log_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;targets&quot;</span><span class="p">,</span> <span class="s2">&quot;input_lengths&quot;</span><span class="p">,</span> <span class="s2">&quot;target_lengths&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;neg_log_likelihood&quot;</span><span class="p">,</span> <span class="s2">&quot;log_alpha&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="n">blank</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;zero_infinity&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;zero_infinity&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CTCLossV2Grad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the gradient of CTC (Connectionist Temporal Classification) loss.</span>

<span class="sd">    The CTC algorithm is proposed in `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank (int): The blank label. Default: 0.</span>
<span class="sd">        reduction (string): Apply specific reduction method to the output. Currently only support &#39;none&#39;.</span>
<span class="sd">            Default: &quot;none&quot;.</span>
<span class="sd">        zero_infinity (bool): Whether to set infinite loss and correlation gradient to zero. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **grad_out** (Tenosr) - Gradient renewal codfficient, A tensor for shape (N), where N is batch size.</span>
<span class="sd">        - **log_probs** (Tensor) - A tensor of shape (T, N, C), where T is input length, N is batch size and C is number</span>
<span class="sd">          of classes (including blank).</span>
<span class="sd">        - **targets** (Tensor) - A tensor of shape (N, S), where S is max target length, means the target sequences.</span>
<span class="sd">        - **input_lengths** (Union(tuple, Tensor)) - A tuple or Tensor of shape(N). It means the lengths of the input.</span>
<span class="sd">        - **target_lengths** (Union(tuple, Tensor)) - A tuple or Tensor of shape(N). It means the lengths of the target.</span>
<span class="sd">        - **log_alpha** (Tensor) - The probability of possible trace of input to target.</span>
<span class="sd">        - **neg_log_likelihood** (Tensor) - A loss value which is differentiable with respect to each input node.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **grad** (Tensor) - The grad of Connectionist Temporal Classification Loss.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `zero_infinity` is not a bool, reduction is not string.</span>
<span class="sd">        TypeError: If the dtype of `log_probs` or `grad_out` is not float or double.</span>
<span class="sd">        TypeError: If the dtype of `targets`, `input_lengths` or `target_lengths` is not int32 or int64.</span>
<span class="sd">        RuntimeError: If the rank of `log_probs` is not 3.</span>
<span class="sd">        RuntimeError: If the rank of `targets` is not 2.</span>
<span class="sd">        RuntimeError: If the shape of `input_lengths` does not match {batch_size|N}.</span>
<span class="sd">        RuntimeError: If the shape of `target_lengths` does not match {batch_size|N}.</span>
<span class="sd">        RuntimeError: If the types of `targets`, `input_lengths`, `grad_out` or `target_lengths` are different.</span>
<span class="sd">        RuntimeError: If the value of `blank` is not in range [0, num_labels|C).</span>
<span class="sd">        RuntimeError: If any value of `input_lengths` is larger than (num_labels|C).</span>
<span class="sd">        RuntimeError: If any target_lengths[i] is not in range [0, input_length[i]].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CTCLossV2Grad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;grad_out&quot;</span><span class="p">,</span> <span class="s2">&quot;log_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;targets&quot;</span><span class="p">,</span> <span class="s2">&quot;input_lengths&quot;</span><span class="p">,</span> <span class="s2">&quot;target_lengths&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;neg_log_likelihood&quot;</span><span class="p">,</span> <span class="s2">&quot;log_alpha&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;grad&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="n">blank</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;zero_infinity&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;zero_infinity&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">)</span>


<div class="viewcode-block" id="Conv3DTranspose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Conv3DTranspose.html#mindspore.ops.Conv3DTranspose">[文档]</a><span class="k">class</span> <span class="nc">Conv3DTranspose</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a 3D transposed convolution, which is also known as a deconvolution</span>
<span class="sd">    (although it is not an actual deconvolution).</span>

<span class="sd">    Input is typically of shape :math:`(N, C, D, H, W)`, where :math:`N` is batch size, :math:`C` is channel number,</span>
<span class="sd">    :math:`D` is depth, :math:`H` is height, :math:`W` is width.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;pad&quot;, the depth, height and width of output are defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{pad}[0] + \text{dilation}[0]</span>
<span class="sd">        \times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1</span>

<span class="sd">        H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{pad}[1] + \text{dilation}[1]</span>
<span class="sd">        \times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1</span>

<span class="sd">        W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{pad}[2] + \text{dilation}[2]</span>
<span class="sd">        \times (\text{kernel_size}[2] - 1) + \text{output_padding}[2] + 1</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channel (int): The channel of the input x.</span>
<span class="sd">        out_channel (int): The channel of the weight x.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The data type is int or a tuple of 3 integers.</span>
<span class="sd">            Specifies the depth, height and width of the 3D convolution window.</span>
<span class="sd">            Single int means the value is for the depth, height and width of the kernel.</span>
<span class="sd">            A tuple of 3 ints means the first value is for the depth, the second value is for the height and the</span>
<span class="sd">            other is for the width of the kernel.</span>
<span class="sd">        mode (int): Modes for different convolutions. Default is 1. It is currently not used.</span>
<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot;, &quot;pad&quot;. Default: &quot;valid&quot;.</span>

<span class="sd">            - same: Adopts the way of completion. The depth, height and width of the output will be equal to</span>
<span class="sd">              the input `x` divided by stride. The padding will be evenly calculated in head and tail, top and bottom,</span>
<span class="sd">              left and right directions possiblily.</span>
<span class="sd">              Otherwise, the last extra padding will be calculated from the tail, bottom and the right side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest depth, height and width of output</span>
<span class="sd">              will be returned without padding. Extra pixels will be discarded. If this mode is set, `pad`</span>
<span class="sd">              and `output_padding` must be 0.</span>

<span class="sd">            - pad: Implicit paddings on both sides of the input in depth, height and width. The number of `pad` will</span>
<span class="sd">              be padded to the input Tensor borders. `pad` must be greater than or equal to 0.</span>

<span class="sd">        pad (Union(int, tuple[int])): The pad value to be filled. Default: 0. If `pad` is an integer, the paddings of</span>
<span class="sd">             head, tail, top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of six integers,</span>
<span class="sd">             the padding of head, tail, top, bottom, left and right equal to pad[0], pad[1], pad[2], pad[3], pad[4]</span>
<span class="sd">             and pad[5] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int])): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: 1.</span>
<span class="sd">        dilation (Union(int, tuple[int])): Specifies the space to use between kernel elements. Default: 1.</span>
<span class="sd">        group (int): Splits input into groups. Default: 1. Only 1 is currently supported.</span>
<span class="sd">        output_padding (Union(int, tuple[int])): Add extra size to each dimension of the output. Default: 0.</span>
<span class="sd">        data_format (str): The optional value for data format. Currently only &#39;NCDHW&#39; is supported.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **dout** (Tensor) - The gradients with respect to the output of the convolution.</span>
<span class="sd">          The shape conforms to the default.</span>
<span class="sd">          data_format :math:`(N, C_{in}, D_{out}, H_{out}, W_{out})`. Currently dout data type only supports float16</span>
<span class="sd">          and float32.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(K_d, K_h, K_w)`, then the shape is</span>
<span class="sd">          :math:`(C_{in}, C_{out}//group, K_d, K_h, K_w)`. Where :math:`group` is the Args parameter,</span>
<span class="sd">          :math:`//` is the symbol for integer division.</span>
<span class="sd">          Currently weight data type only supports float16 and float32.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`C_{out}`. Currently, only support none. Default: None.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients with respect to the input of convolution 3D.</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}//group, D_{out}, H_{out}, W_{out})`,</span>
<span class="sd">        where :math:`group` is the Args parameter.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `in_channel`, `out_channel` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` , `dilation` or `output_padding` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If `in_channel`, `out_channel`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; nor &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>
<span class="sd">        TypeError: If data type of dout and weight is not float16.</span>
<span class="sd">        ValueError: If bias is not none. The rank of dout and weight is not 5.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([32, 16, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([16, 3, 4, 6, 2]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; conv3d_transpose = ops.Conv3DTranspose(in_channel=16, out_channel=3, kernel_size=(4, 6, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d_transpose(dout, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (32, 3, 13, 37, 33)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channel</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3DTranspose&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;filter&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">in_channel</span><span class="p">,</span> <span class="s1">&#39;in_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;in_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channel</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                             <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                               <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">third_one</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;pad&#39; must be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;six positive int numbers, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero or (0, 0, 0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;output_padding&#39;</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                     <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">greater_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">output_padding_</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">output_padding_</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;output_padding&#39; must be zero or (0, 0, 0) &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;when &#39;pad_mode&#39; is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;output_padding&#39; is &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output_padding</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_padding&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">343</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span>
                                  <span class="s1">&#39;The product of height, width and depth of kernel_size belonging [1, 343]&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">343</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span>
                                  <span class="s1">&#39;The product of height, width and depth of stride belonging [1, 343]&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span>
                                  <span class="s1">&#39;The product of height, width and depth of stride belonging [1, 256]&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;output_padding_d belonging [0, max(stride_d, dilation_d))&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;output_padding_h belonging [0, max(stride_h,dilation_h))&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;output_padding_w belonging [0, max(stride_w,dilation_w))&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Dilation2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the grayscale dilation of 4-D input and 3-D filters tensors.</span>

<span class="sd">    Applies a 2D dilation over an input tensor which is typically of shape :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size, :math:`H` is height, :math:`W` is width, :math:`C` is channel number.</span>
<span class="sd">    Given kernel size :math:`ks = (h_{ker}, w_{ker})`, stride :math:`s = (s_0, s_1)` and</span>
<span class="sd">    dilation :math:`d = (d_0, d_1)`, the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + d_0 \times m, s_1 \times w + d_1 \times n) + \text{filter}(C_j, m, n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This operator is an experimental operator, which has some accuracy problems for some inputs.</span>
<span class="sd">        If the input data type is float32, this operator is still executed in float16 mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        stride (Union(int, tuple[int])): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively, or a tuple of four int numbers when</span>
<span class="sd">            data_format is &#39;NCHW&#39; represents [1, 1, stride_height, stride_width].</span>

<span class="sd">        dilation (Union(int, tuple[int])): The data type is int or a tuple of 2 integers or a tuple of 4 integers.</span>
<span class="sd">                                      Specifies the dilation rate to use for dilated convolution.</span>
<span class="sd">                                      If set to be :math:`k &gt; 1`, there will be :math:`k - 1` pixels skipped for</span>
<span class="sd">                                      each sampling location. Its value must be greater or equal to 1 and bounded by</span>
<span class="sd">                                      the height and width of the input `x`.</span>

<span class="sd">        pad_mode (str): Specifies padding mode. The optional values are</span>
<span class="sd">            &quot;same&quot;, &quot;valid&quot;. Default: &quot;same&quot;. Both upper and lower case are supported.</span>

<span class="sd">            - same: Adopts the way of completion. The height and width of the output will be the same as</span>
<span class="sd">              the input `x`.</span>

<span class="sd">            - valid: Adopts the way of discarding. The possible largest height and width of output will be returned</span>
<span class="sd">              without padding. Extra pixels will be discarded.</span>
<span class="sd">        data_format (str): The value for data format, only &#39;NCHW&#39; is supported at present. Default: &quot;NCHW&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input data. A four dimension tensor with float16 or float32 data type. The shape must be</span>
<span class="sd">          :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **filter** (Tensor) - A three dimension tensor with the same type as input. The shape must be</span>
<span class="sd">          :math:`(C_{in}, H_{filter}, W_{filter})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied 2D dilation. The shape is :math:`(N, C_{out}, H_{out}, W_{out})` which</span>
<span class="sd">        is not necessarily the same as the input x, the type is the same as the input x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If type of `x` or `filter` is not the tpye in [uint8, uint16, uint32, uint64, int8, int16,</span>
<span class="sd">                                  int32, int64, float16, float32, float64].</span>
<span class="sd">        TypeError: If `stride` or `dilation` is not an int number or a tuple of two or four int numbers.</span>
<span class="sd">        ValueError: If the length of `stride` or `dilation` is neither two nor four when they are tuple.</span>
<span class="sd">        ValueError: If `stride` or `dilation` shape is not (1, 1, height, width) when it is a tuple of four int numbers.</span>
<span class="sd">        ValueError: If `stride` is not in the range of [1, 255].</span>
<span class="sd">        ValueError: If `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not a str of &#39;same&#39;, &#39;valid&#39;, &#39;SAME&#39; or &#39;VALID&#39;.</span>
<span class="sd">        ValueError: If `data_format` is not the str of &#39;NCHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 5, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; filter = Tensor(np.ones([5, 3, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; dilation2d = ops.Dilation2D(stride=1, dilation=1, pad_mode=&#39;VALID&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = dilation2d(x, filter)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 5, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Dilation2D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;filter&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

        <span class="k">def</span> <span class="nf">_check_format_stride_or_dilation</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">,</span> <span class="n">data_format</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="n">prim_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">ret_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">ret_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span> <span class="k">else</span> \
                    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; should be [1, </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_height, </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_weigth, 1]&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;when data_format is &#39;NHWC&#39;, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; should be [1, 1, </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_height, </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_weigth]&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;when data_format is &#39;NCHW&#39;, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">ret_value</span> <span class="o">=</span> <span class="n">arg_value</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; should be an positive int number or a tuple of two &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;or four positive int numbers, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ret_value</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span> <span class="ow">and</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; should be an positive int number or a tuple of two &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;or four positive int numbers, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">ret_value</span>

        <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, NHWC format is not supported at present.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_format_stride_or_dilation</span><span class="p">(</span><span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">255</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">255</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For Dilation2D, size of stride is not supported, &#39;</span>
                             <span class="sa">f</span><span class="s1">&#39;stride should be in the range of [1, 255], &#39;</span>
                             <span class="sa">f</span><span class="s1">&#39;but got stride_h: `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s1">`, stride_w: `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s1">`.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_format_stride_or_dilation</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>


<div class="viewcode-block" id="SoftShrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SoftShrink.html#mindspore.ops.SoftShrink">[文档]</a><span class="k">class</span> <span class="nc">SoftShrink</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the SoftShrink function element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.soft_shrink` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[ 0.5297,  0.7871,  1.1754], [ 0.7836,  0.6218, -1.1542]]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; softshrink = ops.SoftShrink()</span>
<span class="sd">        &gt;&gt;&gt; output = softshrink(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.02979  0.287    0.676  ]</span>
<span class="sd">         [ 0.2837   0.1216  -0.6543 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SoftShrink&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lambd&quot;</span><span class="p">,</span> <span class="n">lambd</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lambd&quot;</span><span class="p">,</span> <span class="n">lambd</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="HShrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.HShrink.html#mindspore.ops.HShrink">[文档]</a><span class="k">class</span> <span class="nc">HShrink</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard Shrink activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.hardshrink` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0.5,  1,  2.0], [0.0533, 0.0776, -2.1233]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; hshrink = ops.HShrink()</span>
<span class="sd">        &gt;&gt;&gt; output = hshrink(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.      1.      2.    ]</span>
<span class="sd">        [ 0.      0.     -2.1233]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize HShrink&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;lambd&#39;</span><span class="p">,</span> <span class="n">lambd</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lambd</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">lambd</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;lambd&#39;</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAdagradDA"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdagradDA.html#mindspore.ops.ApplyAdagradDA">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdagradDA</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update `var` according to the proximal adagrad scheme.</span>
<span class="sd">    The Adagrad algorithm was proposed in</span>
<span class="sd">    `Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</span>
<span class="sd">    &lt;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            grad\_accum += grad \\</span>
<span class="sd">            grad\_squared\_accum += grad * grad \\</span>
<span class="sd">            tmp\_val=</span>
<span class="sd">                \begin{cases}</span>
<span class="sd">                     sign(grad\_accum) * max\left \{|grad\_accum|-l1*global\_step, 0\right \} &amp; \text{ if } l1&gt;0 \\</span>
<span class="sd">                     grad\_accum &amp; \text{ otherwise } \\</span>
<span class="sd">                 \end{cases} \\</span>
<span class="sd">            x\_value = -1 * lr * tmp\_val \\</span>
<span class="sd">            y\_value = l2 * global\_step * lr + \sqrt{grad\_squared\_accum} \\</span>
<span class="sd">            var = \frac{ x\_value }{ y\_value }</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `gradient_accumulator`, `gradient_squared_accumulator` and `grad`</span>
<span class="sd">    comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If `True`, updating of the `var` and `accum` tensors will be protected by a lock.</span>
<span class="sd">                            Otherwise the behavior is undefined, but may exhibit less contention. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **gradient_accumulator** (Parameter) - The dict of mutable tensor gradient_accumulator. Must have the same</span>
<span class="sd">          shape and dtype as `var`.</span>
<span class="sd">        - **gradient_squared_accumulator** (Parameter) - The dict of mutable tensor gradient_squared_accumulator.</span>
<span class="sd">          Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **lr** ([Number, Tensor]) - Scaling factor. Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **l1** ([Number, Tensor]) -  L1 regularization. Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **l2** ([Number, Tensor]) -  L2 regularization. Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **global_step** ([Number, Tensor]) - Training step number. Must be a scalar. With int32 or int64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **gradient_accumulator** (Tensor) - The same shape and data type as `gradient_accumulator`.</span>
<span class="sd">        - **gradient_squared_accumulator** (Tensor) - The same shape and data type as `gradient_squared_accumulator`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `gradient_accumulator` or `gradient_squared_accumulator` is not a Parameter.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `global_step` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If use_locking is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `gradient_accumulator`, `gradient_squared_accumulator`, `grad`,</span>
<span class="sd">                   `lr`, `l1` or `l2` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `gradient_accumulator`, `gradient_squared_accumulator` or `grad` is not same as `var`.</span>
<span class="sd">        TypeError: If dtype of `global_step` is not int32 nor int64.</span>
<span class="sd">        ValueError: If the shape size of `lr`, `l1`, `l2` and `global_step` is not 0.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `gradient_accumulator`, `gradient_squared_accumulator` and `grad`</span>
<span class="sd">                      conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class ApplyAdagradDANet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, use_locking=False):</span>
<span class="sd">        ...         super(ApplyAdagradDANet, self).__init__()</span>
<span class="sd">        ...         self.apply_adagrad_d_a = P.ApplyAdagradDA(use_locking)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4], [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.gradient_accumulator = Parameter(Tensor(np.array([[0.1, 0.3],</span>
<span class="sd">        ...                                                                [0.1, 0.5]]).astype(np.float32)),</span>
<span class="sd">        ...                                               name=&quot;gradient_accumulator&quot;)</span>
<span class="sd">        ...         self.gradient_squared_accumulator = Parameter(Tensor(np.array([[0.2, 0.1],</span>
<span class="sd">        ...                                                                        [0.1, 0.2]]).astype(np.float32)),</span>
<span class="sd">        ...                                                       name=&quot;gradient_squared_accumulator&quot;)</span>
<span class="sd">        ...         self.gradient_accumulator = Parameter(Tensor(np.array([[0.1, 0.3],</span>
<span class="sd">        ...                                                                [0.1, 0.5]]).astype(np.float32)),</span>
<span class="sd">        ...                                               name=&quot;gradient_accumulator&quot;)</span>
<span class="sd">        ...     def construct(self, grad, lr, l1, l2, global_step):</span>
<span class="sd">        ...         out = self.apply_adagrad_d_a(self.var, self.gradient_accumulator,</span>
<span class="sd">        ...                                      self.gradient_squared_accumulator, grad, lr, l1, l2, global_step)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyAdagradDANet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.4], [0.1, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l1 = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l2 = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; global_step = Tensor(2, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, lr, l1, l2, global_step)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[-7.39064650e-04, -1.36888528e-03],</span>
<span class="sd">         [-5.96988888e-04, -1.42478070e-03]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 4.00000006e-01,  7.00000048e-01],</span>
<span class="sd">         [ 2.00000003e-01,  6.99999988e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 2.90000021e-01,  2.60000020e-01],</span>
<span class="sd">         [ 1.09999999e-01,  2.40000010e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient_accumulator&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient_squared_accumulator&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;global_step&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdagradDA&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">SparseApplyRMSProp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update relevant entries according to the rmsprop algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            ms = rho * ms_{t-1} + (1 - rho) * grad * grad \\</span>
<span class="sd">            mom = momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon) \\</span>
<span class="sd">            var = var - mom</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `ms`, `mom` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        rho (float): Decay rate. The value should between 0 and 1, otherwise the behavior is undefined.</span>
<span class="sd">        momentum (float): Momentum. The value should be greater or equal to 0, otherwise the behavior is undefined.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. The value should be greater than 0,</span>
<span class="sd">                         otherwise the behavior is undefined.</span>
<span class="sd">        use_locking (bool): If `True`, updating of the var, ms, and mom tensors is protected by a lock;</span>
<span class="sd">                            otherwise the behavior is undefined, but may exhibit less contention. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **ms** (Parameter) - The dict of mutable tensor ms. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **mom** (Parameter) - The dict of mutable tensor mom. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **lr** ([Number, Tensor]) - Learning rate. Must be a scalar. With float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var`, `ms` and `mom`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64 and indices.shape[0] = var.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) -  The same shape and data type as `var`.</span>
<span class="sd">        - **ms** (Tensor) - The same shape and data type as `ms`.</span>
<span class="sd">        - **mom** (Tensor) - The same shape and data type as `mom`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `ms` or `mom` is not a Parameter.</span>
<span class="sd">        TypeError: If `grad` or `indices` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `var`, `ms`, `mom`, `lr`, `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        TypeError: If `lr` is neither a Number or a Tensor.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `epsilon`, `rho`, `momentum` is not a float.</span>
<span class="sd">        ValueError: If shape of `ms`, `mom`, `grad` is not same as `var`.</span>
<span class="sd">        ValueError: If the shape size of `lr` is not 0.</span>
<span class="sd">        ValueError: If shape of `indices` is not same as shape of first dimension of `var`.</span>
<span class="sd">        ValueError: If `epsilon` is less than or equal to 0.</span>
<span class="sd">        ValueError: If `momentum` is less than 0.</span>
<span class="sd">        ValueError: If `rho` is less than 0 or greater than 1.</span>
<span class="sd">        ValueError: If dimension of `var` is less than 1.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `ms`, `mom` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyRMSPropNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, rho, momentum, epsilon, use_locking=False):</span>
<span class="sd">        ...         super(SparseApplyRMSPropNet, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_r_m_s_prop = P.SparseApplyRMSProp(rho, momentum, epsilon, use_locking)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.3], [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.ms = Parameter(Tensor(np.array([[0.2, 0.4], [0.1, 0.3]]).astype(np.float32)), name=&quot;ms&quot;)</span>
<span class="sd">        ...         self.mom = Parameter(Tensor(np.array([[0.3, 0.1], [0.3, 0.6]]).astype(np.float32)), name=&quot;mom&quot;)</span>
<span class="sd">        ...     def construct(self, lr, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_r_m_s_prop(self.var, self.ms, self.mom, lr, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; rho = 0.2</span>
<span class="sd">        &gt;&gt;&gt; momentum = 0.01</span>
<span class="sd">        &gt;&gt;&gt; epsilon = 1e-6</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyRMSPropNet(rho, momentum, epsilon)</span>
<span class="sd">        &gt;&gt;&gt; lr = 0.01</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1], dtype=np.int32))</span>
<span class="sd">        &gt;&gt;&gt; out = net(lr, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.88035822e-01,  2.88811117e-01],</span>
<span class="sd">         [ 9.10239667e-02,  4.83422279e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.12000003e-01,  4.72000003e-01],</span>
<span class="sd">         [ 2.80000009e-02,  5.72000027e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.19641740e-02,  1.11888833e-02],</span>
<span class="sd">         [ 8.97603668e-03,  1.65777095e-02]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;ms&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mom&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;Initialize SparseApplyRMSProp&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;rho&quot;</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;momentum&quot;</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;momentum&quot;</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">rho</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;rho&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SparseApplyCenteredRMSProp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update `var` according to the centered RMSProp algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{l}</span>
<span class="sd">            \text { mean_square }=\text { decay } * \text { mean_square }+(1-\text { decay }) *</span>
<span class="sd">            \text { gradient }^{2} \\</span>
<span class="sd">            \text { mean_grad }=\text { decay } * \text { mean_grad }+(1-\text { decay }) *</span>
<span class="sd">            \text { gradient } \\</span>
<span class="sd">            \text { Delta }=l r * \frac{\text { gradient }}{\sqrt{\text { mean_square }+</span>
<span class="sd">            \text { epsilon-mean_grad }^{2}}} \\</span>
<span class="sd">            \text { ms }&lt;-\text { rho } * \text { ms }_{t-1}+(1-\text { rho }) * \text { grad } * \text { grad } \\</span>
<span class="sd">            \text { mom }&lt;-\text { momentum } * \text { mom }_{t-1}+\operatorname{lr} *</span>
<span class="sd">            \frac{\text { grad }}{\sqrt{\text { ms+epsilon }}} \\</span>
<span class="sd">            \text { var }&lt;-\text { var }-\text { mom }</span>
<span class="sd">        \end{array}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        In dense implementation of this algorithm, `mean_gradient`, `mean_square`, and `moment` will update</span>
<span class="sd">        even if the `grad` is zero. But in this sparse implementation, `mean_gradient`, `mean_square`, and `moment`</span>
<span class="sd">        will not update in iterations during which the `grad` is zero.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If `True`, updating of the `var`, `mg`, `ms`, and `mom` tensors will be protected by a lock.</span>
<span class="sd">                            Otherwise the behavior is undefined, but may exhibit less contention. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be int8, int16, int32, int64,</span>
<span class="sd">          uint8, uint16, uint32, uint64, float16, float32 or float64.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **mg** (Parameter) - Mean gradients. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **ms** (Parameter) - Mean square gradients. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **mom** (Parameter) - Delta of `var`. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Learning rate. Must be a float number or a scalar tensor.</span>
<span class="sd">          Must have the same type as `var`.</span>
<span class="sd">        - **rho** (Union[Number, Tensor]) - Decay rate. Must be a float number or a scalar tensor.</span>
<span class="sd">          Must have the same type as `var`.</span>
<span class="sd">        - **momentum** (Union[Number, Tensor]) - Momentum. Must be a float number or a scalar tensor.</span>
<span class="sd">          Must have the same type as `var`.</span>
<span class="sd">        - **epsilon** (Union[Number, Tensor]) - Ridge term. Must be a float number or a scalar tensor.</span>
<span class="sd">          Must have the same type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var` and grad.shape[1:] = var.shape[1:] if rank(var) &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - Gradient indices. Must be one of the following types: int32, int64.</span>
<span class="sd">          and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and data type as `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `var`, `mg`, `ms`, `mom`, `grad`, `indices` is not a Tensor.</span>
<span class="sd">        TypeError: If `lr`, `rho`, `momentum` or `epsilon` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `var`, `mg`, `ms`, `mom`, `lr`, `rho`, `momentum`, `epsilon` or `grad`</span>
<span class="sd">                   is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `mg`, `ms`, `mom`, `grad` is not same as `var`.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32 or int64.</span>
<span class="sd">        ValueError: If shape of `mg`, `ms` or `mom` is not same as `var`.</span>
<span class="sd">        ValueError: If the rank of `indices` is not equal to 1.</span>
<span class="sd">        ValueError: If dimension of `grad` is not equal or greater than 1.</span>
<span class="sd">        ValueError: If shape of `indices` is not same as shape of first dimension of `grad`.</span>
<span class="sd">        ValueError: If shape of `grad` is not same as shape of `var` except first dimension.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops.operations.nn_ops as nn_ops</span>
<span class="sd">        &gt;&gt;&gt; var = Tensor(np.array([[0.6, 0.4], [0.1, 0.5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; mg = Tensor(np.array([[0.1, 0.3], [0.1, 0.5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; ms = Tensor(np.array([[0.2, 0.1], [0.1, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; mom = Tensor(np.array([[0.2, 0.1], [0.1, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; rho = Tensor(1e-10, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; momentum = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(0.01, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.4], [0.1, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; sparse_apply_centered_rms_prop = nn_ops.SparseApplyCenteredRMSProp()</span>
<span class="sd">        &gt;&gt;&gt; output = sparse_apply_centered_rms_prop(var, mg, ms, mom, lr, rho, momentum, epsilon, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.5968 0.3959]</span>
<span class="sd">         [0.0989 0.4978]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mg&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;ms&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mom&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyCenteredRMSProp.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;mg&#39;</span><span class="p">,</span> <span class="s1">&#39;ms&#39;</span><span class="p">,</span> <span class="s1">&#39;mom&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ApplyKerasMomentum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update `var` according to the momentum scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum = accum * momentum - grad * lr \\</span>
<span class="sd">            var =</span>
<span class="sd">            \begin{cases}</span>
<span class="sd">                var + accum * momentum - grad * lr, &amp;\text{if use_nesterov} \\</span>
<span class="sd">                var + accum, &amp;\text{else}</span>
<span class="sd">            \end{cases}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Refer to the paper `On the importance of initialization and momentum in deep</span>
<span class="sd">    learning &lt;https://dl.acm.org/doi/10.5555/3042817.3043064&gt;`_  for more details.</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If `True`, updating of the `var` and `accum` tensors will be protected by a lock;</span>
<span class="sd">                            Otherwise the behavior is undefined, but may exhibit less contention. Default: False.</span>
<span class="sd">        use_nesterov (bool): If `True`, the tensor passed to compute grad will be var + momentum * accum,</span>
<span class="sd">                            so in the end, the var you get is actually var + momentum * accum. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **accum** (Parameter) - Must have the same shape and type as `var`. With float16 or float32 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Scaling factor. Must be a scalar. With float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - The gradient. Must have the same shape and type as `var`.</span>
<span class="sd">          With float16 or float32 data type.</span>
<span class="sd">        - **momentum** (Union[Number, Tensor]) - Momentum. Must be a scalar. With float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the use_locking or use_nesterov is not a bool.</span>
<span class="sd">        TypeError: If `var` or `accum` is not a Parameter.</span>
<span class="sd">        TypeError: If `lr` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If `momentum` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr`, `grad`, `momentum` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `accum` or `grad` doesn&#39;t have the same shape as `var`.</span>
<span class="sd">        ValueError: If the shape size of `lr`, `momentum` is not 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class ApplyKerasMomentumNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, use_locking=False, use_nesterov=False):</span>
<span class="sd">        ...         super(ApplyKerasMomentumNet, self).__init__()</span>
<span class="sd">        ...         self.apply_keras_momentum = P.ApplyKerasMomentum(use_locking, use_nesterov)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2, 0.3], [0.1, 0.4]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.2, 0.3], [0.1, 0.4]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...     def construct(self, lr, grad, momentum):</span>
<span class="sd">        ...         out = self.apply_keras_momentum(self.var, self.accum, lr, grad, momentum)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyKerasMomentumNet()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.2], [0.4, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; momentum = Tensor(0.99, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, grad, momentum)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 3.97700012e-01,  5.96800029e-01],</span>
<span class="sd">        [ 1.98599994e-01,  7.95899987e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.97699994e-01,  2.96800017e-01],</span>
<span class="sd">        [ 9.86000001e-02,  3.95900011e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyKerasMomentum&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MultilabelMarginLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MultilabelMarginLoss operation.</span>

<span class="sd">    Creates a criterion that optimizes a multi-class multi-classification</span>
<span class="sd">    hinge loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`)</span>
<span class="sd">    and output :math:`y` (which is a 2D `Tensor` of target class indices).</span>
<span class="sd">    For each sample in the mini-batch:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`0 \leq y[j] \leq \text{x.size}(0)-1`, \</span>
<span class="sd">    and :math:`i \neq y[j]` for all :math:`i` and :math:`j`.</span>

<span class="sd">    :math:`y` and :math:`x` must have the same size.</span>

<span class="sd">    The criterion only considers a contiguous block of non-negative targets that</span>
<span class="sd">    starts at the front.</span>

<span class="sd">    This allows for different samples to have variable amounts of target classes.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;. Default: &quot;mean&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Predict data. Tensor of shape :math:`(C)` or :math:`(N, C)`, where :math:`N`</span>
<span class="sd">          is the batch size and :math:`C` is the number of classes. Data type must be float16 or float32.</span>
<span class="sd">        - **target** (Tensor) - Ground truth data, with the same shape as `x`, data type must be int32 and</span>
<span class="sd">          label targets padded by -1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Union[Tensor, Scalar]) - The loss of MultilabelMarginLoss. If `reduction` is &quot;none&quot;, its shape</span>
<span class="sd">          is :math:`(N)`. Otherwise, a scalar value will be returned.</span>
<span class="sd">        - **is_target** (Tensor) - Output tensor for backward input, with the same shape as `target`,</span>
<span class="sd">          data type must be int32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `target` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `target` is not int32.</span>
<span class="sd">        ValueError: If length of shape of `x` is neither 1 nor 2.</span>
<span class="sd">        ValueError: If shape of `x` is not the same as `target`.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">       &gt;&gt;&gt; loss = ops.MultilabelMarginLoss()</span>
<span class="sd">       &gt;&gt;&gt; x = Tensor(np.array([[0.1, 0.2, 0.4, 0.8], [0.2, 0.3, 0.5, 0.7]]), mindspore.float32)</span>
<span class="sd">       &gt;&gt;&gt; target = Tensor(np.array([[1, 2, 0, 3], [2, 3, -1, 1]]), mindspore.int32)</span>
<span class="sd">       &gt;&gt;&gt; output = loss(x, target)</span>
<span class="sd">       &gt;&gt;&gt; print(output)</span>
<span class="sd">       (Tensor(shape=[], dtype=Float32, value= 0.325), Tensor(shape=[2, 4], dtype=Int32, value=</span>
<span class="sd">       [[1, 1, 1, 1], [0, 0, 1, 1]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MultilabelMarginLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;is_target&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ApplyAdamWithAmsgrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update var according to the Adam algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{l1} \\</span>
<span class="sd">            lr_t:=learning\_rate*\sqrt{1-\beta_2^t}/(1-\beta_1^t) \\</span>
<span class="sd">            m_t:=\beta_1*m_{t-1}+(1-\beta_1)*g \\</span>
<span class="sd">            v_t:=\beta_2*v_{t-1}+(1-\beta_2)*g*g \\</span>
<span class="sd">            \hat v_t:=max(\hat v_{t-1}, v_t) \\</span>
<span class="sd">            var:=var-lr_t*m_t/(\sqrt{\hat v_t}+\epsilon) \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        beta1 (float): A Tensor. Must have the same type as beta1_power. Momentum factor. Must be a scalar.</span>
<span class="sd">        beta2 (float): A Tensor. Must have the same type as beta1_power. Momentum factor. Must be a scalar.</span>
<span class="sd">        epsilon (float): A Tensor. Must have the same type as beta1_power. Ridge term. Must be a scalar.</span>
<span class="sd">        use_locking (bool): use_locking: If True , updating of the `var`, `m`, and `v` tensors will</span>
<span class="sd">          be protected by a lock; Otherwise the behavior is undefined, but may exhibit less contention.</span>
<span class="sd">          Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type can be float16 or float32.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`.</span>
<span class="sd">        - **v** (Parameter) - the 2nd moment vector in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`.</span>
<span class="sd">        - **vhat** (Parameter) - :math:`\hat v_t` in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`.</span>
<span class="sd">        - **beta1_power** (Union[float, Tensor]) - :math:`beta_1^t(\beta_1^{t})` in the updating formula,</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **beta2_power** (Union[float, Tensor]) - :math:`beta_2^t(\beta_2^{t})` in the updating formula,</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **lr** (Union[float, Tensor]) - Scaling factor, a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - The gradient, has the same shape and data type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 4 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>
<span class="sd">        - **vhat** (Tensor) - The same shape and data type as `vhat`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `m`, `v`, `vhat` is not a Parameter.</span>
<span class="sd">        TypeError: If `beta1_power`, `beta2_power`, `lr` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `vhat`, `beta1_power`, `beta2_power`,</span>
<span class="sd">          `lr`, `grad`, `momentum` is not float32 or float16.</span>
<span class="sd">        ValueError: If `m` or `v` or `vhat` or `grad` doesn&#39;t have the same shape of `var`.</span>
<span class="sd">        ValueError: If the shape of `beta1_power`, `beta2_power`, `lr` is not 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class ApplyAdamWithAmsgradNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, beta1=0.9, beta2=0.999, epsilon=1e-8, use_locking=False):</span>
<span class="sd">        ...         super(ApplyAdamWithAmsgradNet, self).__init__()</span>
<span class="sd">        ...         self.apply_adam_with_amsgrad = P.ApplyAdamWithAmsgrad(beta1, beta2, epsilon, use_locking)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2, 0.2], [0.2, 0.2]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.1, 0.2], [0.4, 0.3]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.array([[0.2, 0.1], [0.3, 0.4]]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...         self.vhat = Parameter(Tensor(np.array([[0.1, 0.2], [0.6, 0.2]]).astype(np.float32)), name=&quot;vhat&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, grad):</span>
<span class="sd">        ...         out = self.apply_adam_with_amsgrad(self.var, self.m, self.v, self.vhat,</span>
<span class="sd">        ...                                            beta1_power, beta2_power, lr, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyAdamWithAmsgradNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.4, 0.2], [0.2, 0.3]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(Tensor(0.9, mstype.float32), Tensor(0.999, mstype.float32), Tensor(0.01, mstype.float32), grad)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.19908068 0.1985858 ]</span>
<span class="sd">        [0.19844866 0.19849943]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;vhat&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdamWithAmsgrad&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;beta1&quot;</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;beta2&quot;</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GridSampler3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given an `input_x` and a flow-field `grid`, computes the `output` using `input_x` values and pixel locations from</span>
<span class="sd">    `grid`. Only volumetric (5-D) `input_x` is supported.</span>

<span class="sd">    For `input_x` with shape :math:`(N, C, D_{in}, H_{in}, W_{in})` and `grid` with shape :math:`(N, D_{out}, H_{out},</span>
<span class="sd">    W_{out}, 3)`, the `output` will have shape :math:`(N, C, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    For each output location `output[n, :, d, h, w]`, the size-3 vector `grid[n, d, h, w]` specifies `input_x` pixel</span>
<span class="sd">    locations x, y, z, which are used to interpolate the output value `output[n, :, d, h, w]`. And `interpolation_mode`</span>
<span class="sd">    argument specifies &quot;nearest&quot; or &quot;bilinear&quot; interpolation method to sample the input pixels.</span>

<span class="sd">    `grid` specifies the sampling pixel locations normalized by the `input_x` spatial dimensions. Therefore, it should</span>
<span class="sd">    have most values in the range of :math:`[-1, 1]`.</span>

<span class="sd">    If `grid` has values outside the range of :math:`[-1, 1]`, the corresponding outputs are handled as defined by</span>
<span class="sd">    `padding_mode`. If `padding_mode` is set to be &quot;zeros&quot;, use :math:`0` for out-of-bound grid locations. If</span>
<span class="sd">    `padding_mode` is set to be &quot;border&quot;, use border values for out-of-bound grid locations. If `padding_mode` is set</span>
<span class="sd">    to be &quot;reflection&quot;, use values at locations reflected by the border for out-of-bound grid locations. For location</span>
<span class="sd">    far away from the border, it will keep being reflected until becoming in bound.</span>

<span class="sd">    Args:</span>
<span class="sd">        interpolation_mode (str): An optional string specifying the interpolation method. The optional values are</span>
<span class="sd">            &quot;bilinear&quot; or &quot;nearest&quot;. Default: &quot;bilinear&quot;.</span>
<span class="sd">        padding_mode (str): An optional string specifying the pad method. The optional values are &quot;zeros&quot;, &quot;border&quot; or</span>
<span class="sd">            &quot;reflection&quot;. Default: &quot;zeros&quot;.</span>
<span class="sd">        align_corners (bool): An optional bool. If set to `True`, the extrema (-1 and 1) are considered as referring to</span>
<span class="sd">            the center points of the input’s corner pixels. If set to `False`, they are instead considered as referring</span>
<span class="sd">            to the corner points of the input’s corner pixels, making the sampling more resolution agnostic. Default:</span>
<span class="sd">            `False`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A 5-D tensor with dtype of float32 or float64 and shape of :math:`(N, C, D_{in},</span>
<span class="sd">          H_{in}, W_{in})`.</span>
<span class="sd">        - **grid** (Tensor) - A 5-D tensor whose dtype is the same as `input_x` and whose shape is :math:`(N, D_{out},</span>
<span class="sd">          H_{out}, W_{out}, 3)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 5-D Tensor whose dtype is the same as `input_x` and whose shape is :math:`(N, C, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `grid` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtypes of `input_x` and `grid` are inconsistent.</span>
<span class="sd">        TypeError: If the dtype of `input_x` or `grid` is not a valid type.</span>
<span class="sd">        TypeError: If `align_corners` is not a boolean value.</span>
<span class="sd">        ValueError: If the rank of `input_x` or `grid` is not equal to 5.</span>
<span class="sd">        ValueError: If the first dimension of `input_x` is not equal to that of `grid`.</span>
<span class="sd">        ValueError: If the last dimension of `grid` is not equal to 3.</span>
<span class="sd">        ValueError: If `interpolation_mode` is not &quot;bilinear&quot;, &quot;nearest&quot; or a string value.</span>
<span class="sd">        ValueError: If `padding_mode` is not &quot;zeros&quot;, &quot;border&quot;, &quot;reflection&quot; or a string value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; gridsampler = GridSampler3D(interpolation_mode=&#39;bilinear&#39;, padding_mode=&#39;zeros&#39;, align_corners=True)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(32).reshape((2, 2, 2, 2, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grid = Tensor(np.arange(-0.2, 1, 0.1).reshape((2, 2, 1, 1, 3)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = gridsampler(input_x, grid)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[ 3.3     ]]</span>
<span class="sd">           [[ 4.35    ]]]</span>
<span class="sd">          [[[11.300001]]</span>
<span class="sd">           [[12.349999]]]]</span>
<span class="sd">         [[[[21.4     ]]</span>
<span class="sd">           [[22.449999]]]</span>
<span class="sd">          [[[29.4     ]]</span>
<span class="sd">           [[30.449999]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GridSampler3D.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="s1">&#39;nearest&#39;</span><span class="p">],</span> <span class="s1">&#39;interpolation_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">padding_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="s1">&#39;border&#39;</span><span class="p">,</span> <span class="s1">&#39;reflection&#39;</span><span class="p">],</span> <span class="s1">&#39;padding_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">align_corners</span><span class="p">,</span> <span class="s1">&#39;align_corners&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;grid&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;interpolation_mode&#39;</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;padding_mode&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;align_corners&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FractionalMaxPool</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs fractional max pooling on the input.</span>

<span class="sd">    Fractional max pooling is similar to regular max pooling, In regular max pooling, you downsize an</span>
<span class="sd">    input set by taking the maximum value of smaller N x N subsections of the set (often 2x2), and try</span>
<span class="sd">    to reduce the set by a factor of N, where N is an integer. Fractional max pooling, means that the</span>
<span class="sd">    overall reduction ratio N does not have to be an integer.</span>
<span class="sd">    The sizes of the pooling regions are generated randomly but are fairly uniform.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        &quot;pooling_ratio&quot;, currently only supports row and col dimension and should be &gt;= 1.0, the first</span>
<span class="sd">        and last elements must be 1.0 because we don&#39;t allow pooling on batch and channels dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        pooling_ratio (list(float)): Decide the shape of output, is a list of floats that has length &gt;= 4.</span>
<span class="sd">            Pooling ratio for each dimension of value should be &gt;=0, currently only support for row and col</span>
<span class="sd">            dimension. The first and last elements must be 1.0 because we don&#39;t allow pooling on batch and</span>
<span class="sd">            channels dimensions.</span>
<span class="sd">        pseudo_random(bool): An optional bool. Defaults to False. When set to True, generates the pooling</span>
<span class="sd">            sequence in a pseudo random fashion, otherwise, in a random fashion.</span>
<span class="sd">            Check paper Benjamin Graham, Fractional Max-Pooling for difference between pseudo_random and</span>
<span class="sd">            random.</span>
<span class="sd">        overlapping(bool): An optional bool. Defaults to False. When set to True, it means when pooling,</span>
<span class="sd">            the values at the boundary of adjacent pooling cells are used by both cells.</span>
<span class="sd">        deterministic(bool): An optional bool. Defaults to False. When set to True, a fixed pooling region</span>
<span class="sd">            will be used when iterating over a FractionalMaxPool node in the computation graph. Mainly</span>
<span class="sd">            used in unit test to make FractionalMaxPool deterministic.</span>
<span class="sd">        seed(int): An optional int. Defaults to 0. If either seed or seed2 are set to be non-zero, the</span>
<span class="sd">            random number generator is seeded by the given seed. Otherwise, it is seeded by a random seed.</span>
<span class="sd">        seed2(int): An optional int. Defaults to 0. An second seed to avoid seed collision.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) -The data type must be one of the following types: float32, float64, int32, int64.</span>
<span class="sd">          Tensor of shape :math:`(N, H_{in}, W_{in}, C_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - the output of FractionalMaxPool, has the same data type with `x`.</span>
<span class="sd">          Tensor of shape :math:`(N, H_{out}, W_{out}, C_{out})`.</span>

<span class="sd">        - **row_pooling_sequence** (Tensor) - A tensor of type int64, the result list of pool boundary rows.</span>

<span class="sd">        - **col_pooling_sequence** (Tensor) - A tensor of type int64, the result list of pool boundary cols.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` is not float32, float64, int32, int64.</span>
<span class="sd">        TypeError: If `x` is not a 4D tensor.</span>
<span class="sd">        ValueError: If element of `x` equals 0 or is less than 0.</span>
<span class="sd">        ValueError: If `pooling_ratio` is a list whose length is not equal to 4.</span>
<span class="sd">        ValueError: If the first and last element of `pooling_ratio` is not equal to 1.0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]).reshape([1,4,4,1]).astype(np.int64)</span>
<span class="sd">        &gt;&gt;&gt; pooling_ratio=[1.0,1.5,1.5,1.0]</span>
<span class="sd">        &gt;&gt;&gt; fractionalmaxpool_op = ops.FractionalMaxPool(pooling_ratio=pooling_ratio)</span>
<span class="sd">        &gt;&gt;&gt; output = fractionalmaxpool_op(Tensor(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2, 2, 1], dtype=Int64, value=</span>
<span class="sd">        [[[[ 6],</span>
<span class="sd">           [ 8]],</span>
<span class="sd">          [[14],</span>
<span class="sd">           [16]]]]), Tensor(shape=[3], dtype=Int64, value= [0, 2, 4]), Tensor(shape=[3], dtype=Int64, value= [0, 2, 4]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooling_ratio</span><span class="p">,</span> <span class="n">pseudo_random</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">overlapping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed2</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FractionalMaxPool.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;row_pooling_sequence&quot;</span><span class="p">,</span> <span class="s2">&quot;col_pooling_sequence&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pooling_ratio&#39;</span><span class="p">,</span> <span class="n">pooling_ratio</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pooling_ratio</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooling_ratio_item&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pseudo_random&quot;</span><span class="p">,</span> <span class="n">pseudo_random</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;overlapping&quot;</span><span class="p">,</span> <span class="n">overlapping</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;deterministic&quot;</span><span class="p">,</span> <span class="n">deterministic</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;seed2&quot;</span><span class="p">,</span> <span class="n">seed2</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FractionalMaxPool3DWithFixedKsize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    3D fractional max pooling operation.</span>

<span class="sd">    This operator applies a 3D fractional max pooling over an input signal composed of several input planes.</span>
<span class="sd">    The max-pooling operation is applied in kD x kH x kW regions by a stochastic step size determined</span>
<span class="sd">    by the target output size.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Refer to the paper `Fractional MaxPooling by Ben Graham &lt;https://arxiv.org/abs/1412.6071&gt;`_  for more details.</span>

<span class="sd">    The input and output data format can be &quot;NCDHW&quot; and &quot;NDHWC&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    D the feature depth, H is the feature height, and W is the feature width.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[float, tuple]): The target ksize is D x H x W.</span>
<span class="sd">            ksize can be a tuple, or a single K for K x K x K.</span>
<span class="sd">            specifying the window size (D, H, W) of the input tensor.</span>

<span class="sd">        output_shape (Union[int, tuple]): The target output_shape is D x H x W.</span>
<span class="sd">            output_shape can be a tuple, or a single H for H x H x H.</span>
<span class="sd">            specifying the size (D, H, W) of the output tensor.</span>

<span class="sd">        data_format (str) : The optional value for data format.</span>
<span class="sd">            Currently support &#39;NCDHW&#39; and &#39;NHDWC&#39;. Default: &#39;NCDHW&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input of FractionalMaxPool3DWithFixedKsize, which is a 4D or 5D tensor.</span>
<span class="sd">          Tensor of data type : float16, float32, double, int32, int64.</span>
<span class="sd">          Supported shape :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(N, D_{in}, H_{in}, W_{in}, C)`.</span>

<span class="sd">        - **random_samples** (Tensor) - The random step of FractionalMaxPool3DWithFixedKsize, which is a 3D tensor.</span>
<span class="sd">          Tensor of data type : float16, float32, double, and value is between (0, 1).</span>
<span class="sd">          Supported shape :math:`(N, C, 3)`</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Outputs:</span>
<span class="sd">        - **y** (Tensor) - A tensor, the output of FractionalMaxPool3DWithFixedKsize.</span>
<span class="sd">        Has the same data type with `x`.</span>
<span class="sd">        Tensor of shape :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(N, D_{out}, H_{out}, W_{out}, C)`.</span>

<span class="sd">        - **argmax** (Tensor) - A tensor, the indices along with the outputs.</span>
<span class="sd">        Has the same shape as the `y` and int32 or int64 data type.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a 4D or 5D tensor.</span>
<span class="sd">        TypeError: If `random_samples` is not a 3D tensor.</span>
<span class="sd">        TypeError: If data type of `x` is not float16, float32, double, int32, int64.</span>
<span class="sd">        TypeError: If dtype of `random_samples` is not float16, float32, double.</span>
<span class="sd">        TypeError: If dtype of `argmax` is not int32, int64.</span>
<span class="sd">        ValueError: If `output_shape` is a tuple and if `output_shape` length is not 3.</span>
<span class="sd">        ValueError: If `ksize` is a tuple and if `ksize` length is not 3.</span>
<span class="sd">        ValueError: If numbers in `output_shape` or `ksize` is not positive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCDHW&#39; nor &#39;NDHWC&#39;.</span>
<span class="sd">        ValueError: If the first dimension size of `input_x` and `random_samples` is not equal.</span>
<span class="sd">        ValueError: If the second dimension size of `input_x` and `random_samples` is not equal.</span>
<span class="sd">        ValueError: If the third dimension size of `random_samples` is not 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])</span>
<span class="sd">        ...       .reshape([1, 1, 2, 2, 4]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; random_samples = Tensor(np.array([0.7, 0.7, 0.7]).reshape([1, 1, 3]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; ksize = (1.0, 1.0, 1.0)</span>
<span class="sd">        &gt;&gt;&gt; output_shape = (1, 1, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = ops.FractionalMaxPool3DWithFixedKsize(ksize = ksize, output_shape = output_shape)</span>
<span class="sd">        &gt;&gt;&gt; output, argmax = net(x, random_samples)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[[13. 16.]]]]]</span>
<span class="sd">        [[[[[12 15]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FractionalMaxPool3DWithFixedKsize.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;random_samples&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;argmax&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">ksize</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="p">(</span><span class="n">ksize</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">ksize</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;ksize&#39; must be an positive float number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;three float numbers, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span><span class="si">}</span><span class="s2"> numbers.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;ksize item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NDHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span>
                                                   <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FractionalAvgPool</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs fractional avg pooling on the input.</span>

<span class="sd">    Fractional avg pooling is similar to regular avg pooling, In regular avg pooling, you downsize an</span>
<span class="sd">    input set by taking the avgrage value of smaller N x N subsections of the set (often 2x2), and try</span>
<span class="sd">    to reduce the set by a factor of N, where N is an integer. Fractional avg pooling, means that the</span>
<span class="sd">    overall reduction ratio N does not have to be an integer. In each pooling region, a mean operation</span>
<span class="sd">    is performed.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        &quot;pooling_ratio&quot;, currently only supports row and col dimension and should be &gt;= 1.0, the first</span>
<span class="sd">        and last elements must be 1.0 because we don&#39;t allow pooling on batch and channels dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        pooling_ratio (list(float)): Decide the shape of output, is a list of floats that has length &gt;= 4.</span>
<span class="sd">            Pooling ratio for each dimension of value should be &gt;=0, currently only support for row and col</span>
<span class="sd">            dimension. The first and last elements must be 1.0 because we don&#39;t allow pooling on batch and</span>
<span class="sd">            channels dimensions.</span>
<span class="sd">        pseudo_random(bool): An optional bool. Defaults to False. When set to True, generates the pooling</span>
<span class="sd">            sequence in a pseudorandom fashion, otherwise, in a random fashion.</span>
<span class="sd">            Check paper Benjamin Graham, Fractional Max-Pooling for difference between pseudo_random and</span>
<span class="sd">            random.</span>
<span class="sd">        overlapping(bool): An optional bool. Defaults to False. When set to True, it means when pooling,</span>
<span class="sd">            the values at the boundary of adjacent pooling cells are used by both cells.</span>
<span class="sd">        deterministic(bool): An optional bool. Defaults to False. When set to True, a fixed pooling region</span>
<span class="sd">            will be used when iterating over a FractionalAvgPool node in the computation graph. Mainly</span>
<span class="sd">            used in unit test to make FractionalAvgPool deterministic.</span>
<span class="sd">        seed(int): An optional int. Defaults to 0. If either seed or seed2 are set to be non-zero, the</span>
<span class="sd">            random number generator is seeded by the given seed. Otherwise, it is seeded by a random seed.</span>
<span class="sd">        seed2(int): An optional int. Defaults to 0. An second seed to avoid seed collision.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) -The data type must be one of the following types: float32, float64, int32, int64.</span>
<span class="sd">          Tensor of shape :math:`(N, H_{in}, W_{in}, C_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A tensor, the output of FractionalAvgPool, has the same data type with `x`.</span>
<span class="sd">          Tensor of shape :math:`(N, H_{out}, W_{out}, C_{out})`.</span>

<span class="sd">        - **row_pooling_sequence** (Tensor) - A tensor of type int64, the result list of pool boundary rows.</span>

<span class="sd">        - **col_pooling_sequence** (Tensor) - A tensor of type int64, the result list of pool boundary cols.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` is not float32, float64, int32, int64.</span>
<span class="sd">        TypeError: If `x` is not a 4D tensor.</span>
<span class="sd">        ValueError: If element of `x` equals 0 or is less than 0.</span>
<span class="sd">        ValueError: If `pooling_ratio` is a list whose length is not equal to 4.</span>
<span class="sd">        ValueError: If the first and last element of `pooling_ratio` is not equal to 1.0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]).reshape([1,4,4,1]).astype(np.int64)</span>
<span class="sd">        &gt;&gt;&gt; pooling_ratio=[1.0,1.5,1.5,1.0]</span>
<span class="sd">        &gt;&gt;&gt; fractionalavgpool_op = ops.FractionalAvgPool(pooling_ratio=pooling_ratio)</span>
<span class="sd">        &gt;&gt;&gt; output = fractionalavgpool_op(Tensor(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2, 2, 1], dtype=Int64, value=</span>
<span class="sd">        [[[[ 3],</span>
<span class="sd">           [ 5]],</span>
<span class="sd">          [[11],</span>
<span class="sd">           [13]]]]), Tensor(shape=[3], dtype=Int64, value= [0, 2, 4]), Tensor(shape=[3], dtype=Int64, value= [0, 2, 4]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooling_ratio</span><span class="p">,</span> <span class="n">pseudo_random</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">overlapping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed2</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FractionalAvgPool.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;row_pooling_sequence&quot;</span><span class="p">,</span> <span class="s2">&quot;col_pooling_sequence&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pooling_ratio&#39;</span><span class="p">,</span> <span class="n">pooling_ratio</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pooling_ratio</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooling_ratio_item&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pseudo_random&quot;</span><span class="p">,</span> <span class="n">pseudo_random</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;overlapping&quot;</span><span class="p">,</span> <span class="n">overlapping</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;deterministic&quot;</span><span class="p">,</span> <span class="n">deterministic</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;seed2&quot;</span><span class="p">,</span> <span class="n">seed2</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NthElement</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds values of the n-th order statistic for the last dimension.</span>
<span class="sd">    If the input is a vector (rank-1), finds the entries which is the nth-smallest value in</span>
<span class="sd">    the vector and outputs their values as scalar tensor.</span>
<span class="sd">    For matrices (resp. higher rank input), computes the entries which is the nth-smallest value in</span>
<span class="sd">    each row (resp. vector along the last dimension). Thus, values.shape = input.shape[:-1].</span>

<span class="sd">    Args:</span>
<span class="sd">        reverse (bool): An optional bool. Defaults to False. When set to True, find the nth-largest value</span>
<span class="sd">          in the vector and vice versa.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - A Tensor. 1-D or higher with last dimension at least n+1.</span>
<span class="sd">        - **n** (int or Tensor) -  If the n is a tensor, it should be a 0-D tensor, dtype is int32.</span>
<span class="sd">          Valid range of n is [0, input.shape[-1]).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, values.shape = input.shape[:-1]. The dtype is same to the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the type  of input is out of the valid list.</span>
<span class="sd">        TypeError: If the n is not int32 or not a Tensor.</span>
<span class="sd">        ValueError: If n is out of [0, input.shape[-1]).</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">         ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[1,2,3],[4,5,6]]) , mstype.int8)</span>
<span class="sd">        &gt;&gt;&gt; n = 1</span>
<span class="sd">        &gt;&gt;&gt; net = P.NthElement()</span>
<span class="sd">        &gt;&gt;&gt; out = net(input, n)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [2 5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NthElement.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reverse</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">PSROIPooling</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Position Sensitive ROI-Pooling</span>

<span class="sd">    Args:</span>
<span class="sd">        spatial_scale (float): a scaling factor that maps the box coordinates to the input coordinates.</span>
<span class="sd">                               For example, if your boxes are defined on the scale of a 224x224 image and</span>
<span class="sd">                               your input is a 112x112 feature map (resulting from a 0.5x scaling of the original</span>
<span class="sd">                               image), you’ll want to set this to 0.5.</span>
<span class="sd">        group_size (int): the size of the output (in pixels) after the pooling is performed, as (height, width).</span>
<span class="sd">        output_dim (int): the dim of the output after the pooling is performed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **features** (Tensor) - The input features, whose shape must be :math:`(N, C, H, W)`. With data type is</span>
<span class="sd">          float16 or float32. This formula should hold: :math:`(C == output_dim * group_size * group_size)`.</span>
<span class="sd">        - **rois** (Tensor) - The shape is `(batch, 5, rois_n)`. With data type of float16 or float32.</span>
<span class="sd">          The size of first dimension `batch` is batch_size. The size of the second dimension must be `5`.</span>
<span class="sd">          The size of third dimension `rois_n` is the number of rois. The value of `rois` like:</span>
<span class="sd">          (index, x1, y1, x2, y2). The first element of `rois_n` is the index of the `rois`. And the box coordinates</span>
<span class="sd">          in (x1, y1, x2, y2) format where the regions will be taken from. The coordinate must satisfy</span>
<span class="sd">          0 &lt;= x1 &lt; x2 and 0 &lt;= y1 &lt; y2.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - out (rois.shape[0] * rois.shape[2], output_dim, group_size, group_size), the result after pooling.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import nn_ops</span>
<span class="sd">        &gt;&gt;&gt; features = np.random.randn(4, 3 * 7 * 7, 80, 48)</span>
<span class="sd">        &gt;&gt;&gt; features = Tensor.from_numpy(features).astype(mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rois = Tensor.from_numpy(</span>
<span class="sd">        ...     np.array([[[0.0000],</span>
<span class="sd">        ...                [150.3563],</span>
<span class="sd">        ...                [200.1320],</span>
<span class="sd">        ...                [579.3563],</span>
<span class="sd">        ...                [602.3452]],</span>
<span class="sd">        ...               [[1.0000],</span>
<span class="sd">        ...                [657.1263],</span>
<span class="sd">        ...                [302.8564],</span>
<span class="sd">        ...                [762.4214],</span>
<span class="sd">        ...                [567.9854]],</span>
<span class="sd">        ...               [[2.0000],</span>
<span class="sd">        ...                [321.3122],</span>
<span class="sd">        ...                [232.2410],</span>
<span class="sd">        ...                [679.0281],</span>
<span class="sd">        ...                [587.6346]],</span>
<span class="sd">        ...               [[3.0000],</span>
<span class="sd">        ...                [664.1630],</span>
<span class="sd">        ...                [387.4919],</span>
<span class="sd">        ...                [778.7322],</span>
<span class="sd">        ...                [562.7321]]])).astype(mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; psROIPooling = nn_ops.PSROIPooling(spatial_scale=1.0/16, output_dim=3,</span>
<span class="sd">        ...                                       group_size=7)</span>
<span class="sd">        &gt;&gt;&gt; out = psROIPooling(features, rois)</span>
<span class="sd">        &gt;&gt;&gt; print(out.shape)</span>
<span class="sd">            (4, 3, 7, 7)</span>
<span class="sd">        &gt;&gt;&gt; print(out.dtype)</span>
<span class="sd">            Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize PSROIPooling&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">spatial_scale</span><span class="p">,</span> <span class="s2">&quot;spatial_scale&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group_size</span><span class="p">,</span> <span class="s2">&quot;group_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_scale</span> <span class="o">=</span> <span class="n">spatial_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="o">=</span> <span class="n">group_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;spatial_scale&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_scale</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;group_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_dim&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TripletMarginLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TripletMarginLoss operation.</span>

<span class="sd">    Creates a criterion that measures the triplet loss given an input</span>
<span class="sd">    tensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.</span>
<span class="sd">    This is used for measuring a relative similarity between samples. A triplet</span>
<span class="sd">    is composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative</span>
<span class="sd">    examples` respectively). The shapes of all input tensors should be</span>
<span class="sd">    :math:`(N, D)`.</span>

<span class="sd">    The distance swap is described in detail in the paper `Learning shallow</span>
<span class="sd">    convolutional feature descriptors with triplet losses` by</span>
<span class="sd">    V. Balntas, E. Riba et al.</span>

<span class="sd">    The loss function for each sample in the mini-batch is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</span>

<span class="sd">    Args:</span>
<span class="sd">        p (int): The norm degree for pairwise distance. Default: 2.</span>
<span class="sd">        eps (float): Default: 1e-06.</span>
<span class="sd">        swap (bool): The distance swap is described in detail in the paper</span>
<span class="sd">            `Learning shallow convolutional feature descriptors with triplet losses` by</span>
<span class="sd">            V. Balntas, E. Riba et al. Default: &quot;False&quot;.</span>
<span class="sd">        reduction (str): Apply specific reduction method to the output: &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;. Default: &quot;mean&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A sample randomly selected from the training set. Data type must be BasicType.</span>
<span class="sd">        - **positive** (Tensor) - A sample belonging to the same category as x, with the same type and shape as `x`.</span>
<span class="sd">        - **negative** (Tensor) - A sample belonging to the different class from x, with the same type and shape as `x`.</span>
<span class="sd">        - **margin** (Tensor) - Make a margin between the positive pair and the negative pair.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Union[Tensor, Scalar], if `reduction` is &quot;none&quot;, its shape is :math:`(N)`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `positive` or &#39;negative&#39; or &#39;margin&#39; is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` or `positive` or `negative` is not BasicType.</span>
<span class="sd">        TypeError: If dtype of `x`, `positive` and `negative` is not the same.</span>
<span class="sd">        TypeError: If `margin` is not float32.</span>
<span class="sd">        TypeError: If `p` is not an int.</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        TypeError: If `swap` is not a bool.</span>
<span class="sd">        ValueError: If dimensions of input `x`, `positive` and `negative` are less than or equal to 1 at the same time.</span>
<span class="sd">        ValueError: If the dimension of input `x` or `positive` or `negative` is bigger than or equal to 8.</span>
<span class="sd">        ValueError: If length of shape of `margin` is not 0.</span>
<span class="sd">        ValueError: If shape of `x`, `positive` and `negative` cannot broadcast.</span>
<span class="sd">        ValueError: If `reduction` is not one of &#39;none&#39;, &#39;mean&#39;, &#39;sum&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.TripletMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0.3, 0.7], [0.5, 0.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; positive = Tensor(np.array([[0.4, 0.6], [0.4, 0.6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; negative = Tensor(np.array([[0.2, 0.9], [0.3, 0.7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; margin = Tensor(1.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(x, positive, negative, margin)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.8881968</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TripletMarginLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;positive&#39;</span><span class="p">,</span> <span class="s1">&#39;negative&#39;</span><span class="p">,</span> <span class="s1">&#39;margin&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;swap&quot;</span><span class="p">,</span> <span class="n">swap</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">DeformableOffsets</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the deformed convolution output with the expected input.</span>

<span class="sd">    Refer to :func:`mindspore.ops.deformable_conv2d` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">strides</span><span class="p">,</span>
                 <span class="n">pads</span><span class="p">,</span>
                 <span class="n">ksize</span><span class="p">,</span>
                 <span class="n">dilations</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">deformable_groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">modulated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DeformableOffsets&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;offsets&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">pos_c</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="n">pos_c</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_size_and_element_type_of_tuple</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">strides</span><span class="p">[</span><span class="n">pos_c</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, The N and C dimensions of &#39;strides&#39; must be set to 1.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_size_and_element_type_of_tuple</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="n">pads</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_size_and_element_type_of_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_size_and_element_type_of_tuple</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="n">dilations</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dilations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">dilations</span><span class="p">[</span><span class="n">pos_c</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, The N and C dimensions of &#39;dilations&#39; must be set to 1.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="n">dilations</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">deformable_groups</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">deformable_groups</span><span class="p">,</span> <span class="s1">&#39;deformable_groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;deformable_groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deformable_groups</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">modulated</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">modulated</span><span class="p">,</span> <span class="s1">&#39;modulated&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">modulated</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, The modulated must be set to True.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;modulated&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">modulated</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GridSampler2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operation samples 2d input_x by using interpolation based on flow field grid, which is usually gennerated by</span>
<span class="sd">    affine_grid.</span>

<span class="sd">    Args:</span>
<span class="sd">        interpolation_mode (str): An optional string specifying the interpolation method. The optional values are</span>
<span class="sd">            &quot;bilinear&quot; or &quot;nearest&quot;. Default: &quot;bilinear&quot;.</span>
<span class="sd">        padding_mode (str): An optional string specifying the pad method. The optional values are &quot;zeros&quot;, &quot;border&quot; or</span>
<span class="sd">            &quot;reflection&quot;. Default: &quot;zeros&quot;.</span>
<span class="sd">        align_corners (bool): An optional bool. If &quot;true&quot;, the centers of the corner pixels of the input and output</span>
<span class="sd">            tensors are aligned. Defaults to &quot;false&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A 4-D tensor with dtype of float16 or float32 and shape of :math:`(N, C,</span>
<span class="sd">          H_{in}, W_{in})`.</span>
<span class="sd">        - **grid** (Tensor) - A 4-D tensor whose dtype is the same as `input_x` and whose shape is :math:`(N,</span>
<span class="sd">          H_{out}, W_{out}, 2)`. Used to specify the sampling pixel locations normalized by the input spatial</span>
<span class="sd">          dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">       A 4-D Tensor whose dtype is the same as `input_x` and whose shape is :math:`(N, C, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `grid` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtypes of `input_x` and `grid` are inconsistent.</span>
<span class="sd">        TypeError: If the dtype of `input_x` or `grid` is not a valid type.</span>
<span class="sd">        TypeError: If `align_corners` is not a boolean value.</span>
<span class="sd">        ValueError: If the rank of `input_x` or `grid` is not equal to 4.</span>
<span class="sd">        ValueError: If the first dimension of `input_x` is not equal to that of `grid`.</span>
<span class="sd">        ValueError: If the forth dimension of `grid` is not equal to 2.</span>
<span class="sd">        ValueError: If `interpolation_mode` is not &quot;bilinear&quot;, &quot;nearest&quot; or a string value.</span>
<span class="sd">        ValueError: If `padding_mode` is not &quot;zeros&quot;, &quot;border&quot;, &quot;reflection&quot; or a string value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; gridsampler = GridSampler2D(interpolation_mode=&#39;bilinear&#39;, padding_mode=&#39;zeros&#39;, align_corners=True)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(16).reshape((2, 2, 2, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grid = Tensor(np.arange(-9, 9, 0.5).reshape((2, 3, 3, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = gridsampler(input_x, grid)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     0.     0.5  ]]</span>
<span class="sd">          [[ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     1.5    4.5  ]]]</span>
<span class="sd">         [[[10.     8.25   1.375]</span>
<span class="sd">           [ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     0.     0.   ]]</span>
<span class="sd">          [[14.    11.25   1.875]</span>
<span class="sd">           [ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     0.     0.   ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GridSampler2D.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="s1">&#39;nearest&#39;</span><span class="p">],</span> <span class="s1">&#39;interpolation_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">padding_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="s1">&#39;border&#39;</span><span class="p">,</span> <span class="s1">&#39;reflection&#39;</span><span class="p">],</span> <span class="s1">&#39;padding_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">align_corners</span><span class="p">,</span> <span class="s1">&#39;align_corners&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;grid&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;interpolation_mode&#39;</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;padding_mode&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;align_corners&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Pdist</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the p-norm distance between each pair of row vectors in the input.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y[n] = \sqrt[p]{{\mid x_{i} - x_{j} \mid}^p},</span>

<span class="sd">    where :math:`x_{i}, x_{j}` are two different row vectors in the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (float): p value for the p norm distance to calculate between each vector pair ∈[0,∞]. Default: 2.0.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input tensor with dtype of float16 or float32 and shape of :math:`(N, M)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `x`, whose shape is :math:`(N * (N - 1) / 2)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>
<span class="sd">        TypeError: If `p` is not a float.</span>
<span class="sd">        ValueError: If `p` is a negative float.</span>
<span class="sd">        ValueError: If dimension of `x` is not 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations.nn_ops import Pdist</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = Pdist(p=2.0)</span>
<span class="sd">        &gt;&gt;&gt; y = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1.4142135 2.828427  1.4142135]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pdist&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Pdist p must be a non-negative value, but got `</span><span class="si">{}</span><span class="s1">`.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">UpsampleNearest3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs nearest neighbor upsampling operation.</span>

<span class="sd">    This operator scale up the volumetric input with specified `output_size` or `scales` factors, using nearest</span>
<span class="sd">    neighbor algorithm.</span>

<span class="sd">    One of `output_size` or `scales` must be given, and cannot specify both.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size (Union[tuple[int], list[int]]): A tuple or list of int specifying the output volumetric size.</span>
<span class="sd">            Default: None.</span>
<span class="sd">        scales (Union[tuple[float], list[float]]): A tuple or list of float specifying the upsampling factors.</span>
<span class="sd">            Default: None.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - 5D tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`. Must be one of the</span>
<span class="sd">          following types: [float16, float32, float64].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Upsampled output with the same data type as `x`.</span>
<span class="sd">          Tensor of shape :math:`(N, C, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: When `output_size` is not none and `output_size` is not list[int] or tuple[int].</span>
<span class="sd">        TypeError: When `scales` is not none and `scales` is not list[float] or tuple[float].</span>
<span class="sd">        TypeError: If dtype of `x` is not int [float16, float32, float64].</span>
<span class="sd">        ValueError: If any value of `output_size` is negative or zero when `output_size` is not empty.</span>
<span class="sd">        ValueError: If any value of `scales` is negative or zero when `scales` is not empty.</span>
<span class="sd">        ValueError: If shape of `x` is not 5D.</span>
<span class="sd">        ValueError: If none of `scales` and `output_size` is specified or both specified.</span>
<span class="sd">        ValueError: If size of `scales` is not equal 3 when `scales` is specified.</span>
<span class="sd">        ValueError: If size of `output_size` is not equal 3 when `output_size` is specified.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])</span>
<span class="sd">        ...       .reshape([1, 1, 2, 2, 4]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_size = [3, 4, 5]</span>
<span class="sd">        &gt;&gt;&gt; net = ops.UpsampleNearest3D(output_size = output_size)</span>
<span class="sd">        &gt;&gt;&gt; output = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[ 1.  1.  2.  3.  4.]</span>
<span class="sd">            [ 1.  1.  2.  3.  4.]</span>
<span class="sd">            [ 5.  5.  6.  7.  8.]</span>
<span class="sd">            [ 5.  5.  6.  7.  8.]]</span>
<span class="sd">           [[ 1.  1.  2.  3.  4.]</span>
<span class="sd">            [ 1.  1.  2.  3.  4.]</span>
<span class="sd">            [ 5.  5.  6.  7.  8.]</span>
<span class="sd">            [ 5.  5.  6.  7.  8.]]</span>
<span class="sd">           [[ 9.  9. 10. 11. 12.]</span>
<span class="sd">            [ 9.  9. 10. 11. 12.]</span>
<span class="sd">            [13. 13. 14. 15. 16.]</span>
<span class="sd">            [13. 13. 14. 15. 16.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize UpsampleNearest3D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">scales</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scales</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">output_size</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="s1">&#39;output_size_item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;scales&#39;</span><span class="p">,</span> <span class="n">scales</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">scales</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_float</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="s1">&#39;scales_item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;scales&#39;</span><span class="p">,</span> <span class="n">scales</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SparseApplyAdagradDA</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update `var` according to the proximal adagrad scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            grad_accum += grad \\</span>
<span class="sd">            grad_squared_accum += grad * grad \\</span>
<span class="sd">            tmp_val=sign(grad_accum) * max\left \{|grad_accum|-l1*global_step, 0\right \}</span>
<span class="sd">                    if l1&gt;0 else grad_accum \\</span>
<span class="sd">            x_value = -1 * lr * tmp_val \\</span>
<span class="sd">            y_value = l2 * global_step * lr + \sqrt{grad_squared_accum} \\</span>
<span class="sd">            var = x_value / y_value</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `grad_accum`, `grad_square_accum` and `grad`</span>
<span class="sd">    comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to the</span>
<span class="sd">    relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If `True`, updating of the `var` and `accum` tensors will be protected by a lock.</span>
<span class="sd">                            Otherwise the behavior is undefined, but may exhibit less contention. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **grad_accum** (Parameter) - The dict of mutable tensor grad_accum. Must have the same</span>
<span class="sd">          shape and dtype as `var`.</span>
<span class="sd">        - **grad_square_accum** (Parameter) - The dict of mutable tensor grad_square_accum.</span>
<span class="sd">          Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var` and grad.shape[1:] = var.shape[1:] if rank(var) &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64 and indices.shape[0] = grad.shape[0].</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Scaling factor. Must be a scalar. Must have the same type as `var`.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) -  L1 regularization. Must be a scalar. Must have the same type as `var`.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) -  L2 regularization. Must be a scalar. Must have the same type as `var`.</span>
<span class="sd">        - **global_step** (Union[Number, Tensor]) - Training step number. Must be a scalar.</span>
<span class="sd">          Must be one of the following types: int32, int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as &#39;var&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `grad_accum`, `grad_square_accum` is not a Parameter.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `global_step` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If use_locking is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `grad_accum`, `grad_square_accum`, `grad_accum`,</span>
<span class="sd">                   `lr`, `l1`, `l2` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `grad_accum`, `grad_square_accum`, `grad_accum`</span>
<span class="sd">                     is not same as `var`.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        TypeError: If shape of `indices` is not same as shape of first dimension of `grad`.</span>
<span class="sd">        TypeError: If dtype of `global_step` is not int64.</span>
<span class="sd">        ValueError: If the shape size of `lr`, `l1`, `l2` and `global_step` is not 0.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `grad_accum`, `grad_square_accum` and `grad`</span>
<span class="sd">                      conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.common.dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops.operations.nn_ops as nn_ops</span>
<span class="sd">        &gt;&gt;&gt; var = Tensor(np.array([[1,2], [1,2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grad_accum = Tensor(np.array([[2,1], [3,1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grad_square_accum = Tensor(np.array([[4,1], [5,1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[5,1], [6,1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1], dtype=np.int32))</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(2, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l1 = Tensor(-1, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l2 = Tensor(1, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; global_step=Tensor(1, mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; sparse_apply_adagrad_da = nn_ops.SparseApplyAdagradDA()</span>
<span class="sd">        &gt;&gt;&gt; output = sparse_apply_adagrad_da(var, grad_accum, grad_square_accum,</span>
<span class="sd">        ...                                  grad, indices, lr, l1, l2, global_step)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.8956923 -1.1715728]</span>
<span class="sd">         [-2.1420605 -1.1715728]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad_accum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad_square_accum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;global_step&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyAdagradDA&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;grad_accum&#39;</span><span class="p">,</span> <span class="s1">&#39;grad_square_accum&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;global_step&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SparseApplyMomentum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update relevant entries in &#39;*var&#39; and &#39;*accum&#39; according to the momentum scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum = accum * momentum + grad \\</span>
<span class="sd">            var -= lr * accum</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If `True`, the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>
<span class="sd">        use_nesterov (bool): If `True`, the tensor passed to compute grad will be var + momentum * accum,</span>
<span class="sd">            so in the end, the var you get is actually var + momentum * accum. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be int8, int16, int32, int64,</span>
<span class="sd">          uint8, uint16, uint32, uint64, float16, float32 or float64.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Variable tensor to be updated, has the same shape and type as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value. Must be a scalar with same type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient, has the same type as `var`,</span>
<span class="sd">          and grad.shape[1:] = var.shape[1:] if rank(var) &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64 and indices.shape[0] = grad.shape[0].</span>
<span class="sd">        - **momentum** (Union[Number, Tensor]) - Momentum. Must be a scalar with same type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and type as &#39;var&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `accum`, `grad` or `indices` is not a Parameter.</span>
<span class="sd">        TypeError: If `lr`, `momentum` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `use_locking` or `use_nesterov` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr`, `grad`, or `momentum` is not one of int8, int16,</span>
<span class="sd">                   int32, int64, uint8, uint16, uint32, uint64, float16, float32, float64.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If the shape of `var`, `accum` or `grad` is rank 0.</span>
<span class="sd">        ValueError: If shape of `accum` or `grad` is not same as `var`.</span>
<span class="sd">        ValueError: If shape of `indices` is not same as the shape of first dimension of `grad`.</span>
<span class="sd">        ValueError: If the shape of `lr` or `momentum` is not rank 0.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum`, `lr`, `grad` and &#39;momentum&#39; conversion of Parameter</span>
<span class="sd">                      is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops.operations.nn_ops as nn_ops</span>
<span class="sd">        &gt;&gt;&gt; var = Tensor(np.array([[4.1, 7.2], [1.1, 3.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; accum = Tensor(np.array([[2.2, 3.0], [3.1, 0.5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.01, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.2], [0.4, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; momentum = Tensor(0.99, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; sparse_apply_momentum = nn_ops.SparseApplyMomentum()</span>
<span class="sd">        &gt;&gt;&gt; output = sparse_apply_momentum(var, accum, lr, grad, indices, momentum)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4.07522   7.1682997]</span>
<span class="sd">         [1.06531   2.99405  ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyMomentum&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SparseApplyProximalGradientDescent</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sparse update &#39;*var&#39; as FOBOS algorithm with fixed learning rate.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            \text{prox_v} = var - alpha \\</span>
<span class="sd">            var = sign(\text{prox_v})/(1 + alpha * l2) * \max(\left| \text{prox_v} \right| - alpha * l1,0)</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var` and `delta` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If `True`, the `var` tensors will be protected from being updated.</span>
<span class="sd">            Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be int8, int16, int32, int64,</span>
<span class="sd">          uint8, uint16, uint32, uint64, float16, float32 or float64.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Scaling factor. Must be a scalar with same type as `var`.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - L1 regularization. Must be a scalar with same type as `var`.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization. Must be a scalar with same type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient, has the same type as `var`,</span>
<span class="sd">          and grad.shape[1:] = var.shape[1:] if rank(var) &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and type as &#39;var&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `grad` or `indices` is not a Parameter..</span>
<span class="sd">        TypeError: If `alpha`, `l1`, `l2` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `alpha`, `l1`, `l2` or `grad` is not one of int8, int16,</span>
<span class="sd">                   int32, int64, uint8, uint16, uint32, uint64, float16, float32, float64.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If the shape of `var` or `grad` is rank 0.</span>
<span class="sd">        ValueError: If shape of `grad` is not same as `var`.</span>
<span class="sd">        ValueError: If the shape of `alpha`, `l1` or `l2` is not rank 0.</span>
<span class="sd">        ValueError: If shape of `indices` is not same as the shape of first dimension of `grad`.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `alpha`, `l1`, `l2`, `grad` conversion of Parameter</span>
<span class="sd">                      is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops.operations.nn_ops as nn_ops</span>
<span class="sd">        &gt;&gt;&gt; var = Tensor(np.array([[4.1, 7.2], [1.1, 3.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; alpha = Tensor(1.0, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l1 = Tensor(1.0, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l2 = Tensor(0.0, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[1, 1], [1, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; sparse_apply_proximal_gradient_descent = nn_ops.SparseApplyProximalGradientDescent()</span>
<span class="sd">        &gt;&gt;&gt; output = sparse_apply_proximal_gradient_descent(var, alpha, l1, l2, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2.1 5.2]</span>
<span class="sd">         [0.  1. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyProximalGradientDescent.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NuclearNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the matrix nuclear norm of a given tensor.</span>

<span class="sd">    Attr `dim` specifies which two dimensions of the input `x` to calculate the nuclear norm across. If `dim` is None,</span>
<span class="sd">    the nuclear norm will be calculated across all dimensions of input. Because the nuclear norm is the sum of the</span>
<span class="sd">    singular values of the matrix, the input at this time should be 2-dimensional. That is, if the input is</span>
<span class="sd">    2-dimensional, we compute the nuclear norm of the input matrix. At this point, `dim` should be None. If you set</span>
<span class="sd">    `dim`, it also needs to be in the proper range, although it doesn&#39;t work. If the input is 3-dimensional and above,</span>
<span class="sd">    the attribute `dim` is required. It specifies which two dimensions of input to calculate the nuclear norm across.</span>

<span class="sd">    According to the `dim` list, the input tensor is reordered by `dim`. The two dimensions pointed to by the attribute</span>
<span class="sd">    `dim` are placed at the end, and the order of the other dimensions is relatively unchanged. Perform the SVD of each</span>
<span class="sd">    slice of the adjusted tensor to obtain the singular value. Sum all of the singular value of each slice/matrix to</span>
<span class="sd">    obtain the nuclear norm.</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (Union[list(int), tuple(int)]): Specifies which two dimensions of `x` to calculate the matrix nuclear norm</span>
<span class="sd">            across. If `dim` is None, the nuclear norm will be calculated across all dimensions of `x`. The length of</span>
<span class="sd">            `dim` should be 2. The value in `dim` should be in this range:[-x_rank, x_rank). x_rank is the dimension of</span>
<span class="sd">            Tensor `x`. The value of `dim[0]` or `dim[1]` can not point to the same dimension. Default: None.</span>
<span class="sd">        keepdim (bool): whether the output tensor have `dim` retained or not. Default: False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input to compute the matrix nuclear norm. The dimension of `x` should be greater than or</span>
<span class="sd">          equal to 2. Data type must be float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, output tensor with dimensions in `dim` reduced to 1 will be returned if `keepdim` is `True`;</span>
<span class="sd">        otherwise a tensor with dimensions in `dim` removed is returned. The data type is same as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float32 nor float64.</span>
<span class="sd">        TypeError: If dtype of `dim` is neither list(int) nor tuple(int).</span>
<span class="sd">        TypeError: If dtype of `keepdim` is not bool.</span>
<span class="sd">        ValueError: If dimension of Tensor `x` is less than 2.</span>
<span class="sd">        ValueError: If the length of `dim` is not 2 when `dim` is set.</span>
<span class="sd">        ValueError: If the dimension of tensor `x` is not 2 when `dim` is not set.</span>
<span class="sd">        ValueError: If `dim[0]` or `dim[1]` point to the same dimension.</span>
<span class="sd">        ValueError: If `dim[0]` or `dim[1]` is not in this range:[-x_rank, x_rank).</span>
<span class="sd">                    x_rank is the dimension of Tensor `x`.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],</span>
<span class="sd">        ...                           [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; dim = [0, 2]</span>
<span class="sd">        &gt;&gt;&gt; keepdim = True</span>
<span class="sd">        &gt;&gt;&gt; nuclearnorm = nn_ops.NuclearNorm(dim = dim,keepdim = keepdim)</span>
<span class="sd">        &gt;&gt;&gt; output = nuclearnorm(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[15.407588]</span>
<span class="sd">        [21.711605]]]</span>
<span class="sd">        &gt;&gt;&gt; keepdim = False</span>
<span class="sd">        &gt;&gt;&gt; nuclearnorm = nn_ops.NuclearNorm(dim = dim,keepdim = keepdim)</span>
<span class="sd">        &gt;&gt;&gt; output = nuclearnorm(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [15.407588 21.711605]</span>
<span class="sd">        &gt;&gt;&gt; dim = [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; keepdim = True</span>
<span class="sd">        &gt;&gt;&gt; nuclearnorm = nn_ops.NuclearNorm(dim = dim,keepdim = keepdim)</span>
<span class="sd">        &gt;&gt;&gt; output = nuclearnorm(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[14.212674 15.81139  17.492853]]]</span>
<span class="sd">        &gt;&gt;&gt; keepdim = False</span>
<span class="sd">        &gt;&gt;&gt; nuclearnorm = nn_ops.NuclearNorm(dim = dim,keepdim = keepdim)</span>
<span class="sd">        &gt;&gt;&gt; output = nuclearnorm(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [14.212674 15.81139  17.492853]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NuclearNorm.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dim</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Rel</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;length of dim_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;dim[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">dim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;dim[1]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1000</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keepdim&quot;</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FractionalMaxPoolWithFixedKsize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fractional max pooling operation.</span>

<span class="sd">    Applies a 2D fractional max pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    The max-pooling operation is applied in kH × kW regions by a stochastic step size determined by</span>
<span class="sd">    the target output size. For any input size, the size of the specified output is H x W. The number</span>
<span class="sd">    of output features is equal to the number of input planes.</span>

<span class="sd">    Fractional MaxPooling is described in the paper `Fractional Max-Pooling &lt;https://arxiv.org/pdf/1412.6071&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of kernel window used to take the maximum value.</span>
<span class="sd">            The target ksize is H x W. ksize can be a tuple, or a single K for K x K.</span>
<span class="sd">            specifying the window size (H, W) of the input tensor.</span>
<span class="sd">        output_shape (Union[int, tuple[int]]): The target output size is H x W.</span>
<span class="sd">            output_shape can be a tuple, or a single H for H x H.</span>
<span class="sd">            specifying the size (H, W) of the output tensor.</span>
<span class="sd">        data_format (str): The optional value for data format, is &#39;NCHW&#39;.</span>
<span class="sd">            Default: &quot;NCHW&quot;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C, H_{in}, W_{in})`,</span>
<span class="sd">          with float16, float32, float64, int32, int64 data type.</span>
<span class="sd">        - **random_samples** (Tensor) - Tensor of shape :math:`(N, C, 2)`.</span>
<span class="sd">          with float16, float32, float64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Has the same type as the `input_x`.</span>
<span class="sd">          Has the shape :math:`(N, C, output\underline{~}shape{H}, output\underline{~}shape{W})`.</span>
<span class="sd">        - **argmax** (Tensor) -A tensor whose data type must be int64. Has the same shape as the `y`.</span>
<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `input_x` is not one of the following: float16, float32, float64, int32, int64.</span>
<span class="sd">        TypeError: If data type of `random_samples` is not one of the following: float16, float32, float64.</span>
<span class="sd">        ValueError: If `ksize` is not a number and `ksize` is not a tuple of length 2.</span>
<span class="sd">        ValueError: If `output_shape` is not a number and `output_shape` is not a tuple of length 2.</span>
<span class="sd">        ValueError: If the sum of `ksize`,`output_shape` and -1 is larger than the corresponding dimension of `input_x`.</span>
<span class="sd">        ValueError: If the dimension of `random_samples` is not 3.</span>
<span class="sd">        ValueError: If the first dimension size of `input_x` and `random_samples` is not equal.</span>
<span class="sd">        ValueError: If the second dimension size of `input_x` and `random_samples` is not equal.</span>
<span class="sd">        ValueError: If the third dimension size of `random_samples` is not 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # the ksize is an int number and the output_shape is a tuple.</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import nn_ops</span>
<span class="sd">        &gt;&gt;&gt; ksize = 2</span>
<span class="sd">        &gt;&gt;&gt; output_shape = (2,2)</span>
<span class="sd">        &gt;&gt;&gt; data_format = &quot;NCHW&quot;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.3220, 0.9545, 0.7879, 0.0975, 0.3698,</span>
<span class="sd">        ...                            0.5135, 0.5740, 0.3435, 0.1895, 0.8764,</span>
<span class="sd">        ...                            0.9581, 0.4760, 0.9014, 0.8522, 0.3664,</span>
<span class="sd">        ...                            0.4980, 0.9673, 0.9879, 0.6988, 0.9022,</span>
<span class="sd">        ...                            0.9304, 0.1558, 0.0153, 0.1559, 0.9852]).reshape([1, 1, 5, 5]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; random_samples = Tensor(np.array([[[0.8, 0.8]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = nn_ops.FractionalMaxPoolWithFixedKsize(ksize, output_shape, data_format)</span>
<span class="sd">        &gt;&gt;&gt; y, argmax = net(input_x, random_samples)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[[0.9545 0.8764]</span>
<span class="sd">           [0.9673 0.9852]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[ 1  9]</span>
<span class="sd">           [16 24]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FractionalMaxPoolWithFixedKsize.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;output_shape&#39;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;random_samples&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">])</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>