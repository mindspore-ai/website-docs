

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.ops.ApplyAdadelta &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/training.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="mindspore.ops.ApplyAdagrad" href="mindspore.ops.ApplyAdagrad.html" />
    <link rel="prev" title="mindspore.ops.AdaptiveAvgPool2D" href="mindspore.ops.AdaptiveAvgPool2D.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">术语</a></li>
</ul>
<p class="caption"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.9/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#算子原语">算子原语</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#装饰器">装饰器</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.ops.html#神经网络层算子">神经网络层算子</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#神经网络">神经网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#损失函数">损失函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#激活函数">激活函数</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../mindspore.ops.html#优化器">优化器</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Adam.html">mindspore.ops.Adam</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdamWeightDecay.html">mindspore.ops.AdamWeightDecay</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdaptiveAvgPool2D.html">mindspore.ops.AdaptiveAvgPool2D</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">mindspore.ops.ApplyAdadelta</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagrad.html">mindspore.ops.ApplyAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagradDA.html">mindspore.ops.ApplyAdagradDA</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagradV2.html">mindspore.ops.ApplyAdagradV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdaMax.html">mindspore.ops.ApplyAdaMax</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAddSign.html">mindspore.ops.ApplyAddSign</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyCenteredRMSProp.html">mindspore.ops.ApplyCenteredRMSProp</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyFtrl.html">mindspore.ops.ApplyFtrl</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyGradientDescent.html">mindspore.ops.ApplyGradientDescent</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyMomentum.html">mindspore.ops.ApplyMomentum</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyPowerSign.html">mindspore.ops.ApplyPowerSign</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyProximalAdagrad.html">mindspore.ops.ApplyProximalAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyProximalGradientDescent.html">mindspore.ops.ApplyProximalGradientDescent</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyRMSProp.html">mindspore.ops.ApplyRMSProp</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.LARSUpdate.html">mindspore.ops.LARSUpdate</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyAdagrad.html">mindspore.ops.SparseApplyAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyAdagradV2.html">mindspore.ops.SparseApplyAdagradV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyProximalAdagrad.html">mindspore.ops.SparseApplyProximalAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SGD.html">mindspore.ops.SGD</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyFtrl.html">mindspore.ops.SparseApplyFtrl</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyFtrlV2.html">mindspore.ops.SparseApplyFtrlV2</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#距离函数">距离函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#采样算子">采样算子</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#图像处理">图像处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.html#文本处理">文本处理</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#数学运算算子">数学运算算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#tensor操作算子">Tensor操作算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#parameter操作算子">Parameter操作算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#数据操作算子">数据操作算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#通信算子">通信算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#调试算子">调试算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#稀疏算子">稀疏算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#框架算子">框架算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#算子信息注册">算子信息注册</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.html#自定义算子">自定义算子</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.9/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/faq.html">常见问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/typical_api_comparision.html">与PyTorch典型区别</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/use_third_party_op.html">基于自定义算子接口调用第三方算子库</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindspore.ops.html">mindspore.ops</a> &raquo;</li>
        
      <li>mindspore.ops.ApplyAdadelta</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/api_python/ops/mindspore.ops.ApplyAdadelta.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="mindspore-ops-applyadadelta">
<h1>mindspore.ops.ApplyAdadelta<a class="headerlink" href="#mindspore-ops-applyadadelta" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="mindspore.ops.ApplyAdadelta">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.ops.</code><code class="sig-name descname">ApplyAdadelta</code><a class="reference internal" href="../../_modules/mindspore/ops/operations/nn_ops.html#ApplyAdadelta"><span class="viewcode-link">[源代码]</span></a><a class="headerlink" href="#mindspore.ops.ApplyAdadelta" title="Permalink to this definition">¶</a></dt>
<dd><p>根据Adadelta算法更新相关参数。</p>
<p>Adadelta算法，具体细节可参考论文 <a class="reference external" href="https://arxiv.org/abs/1212.5701">ADADELTA: AN ADAPTIVE LEARNING RATE METHOD</a> 。</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    \text{accum} = \rho * \text{accum} + (1 - \rho) * \text{grad}^2 \\
    \text{update} = \sqrt{\text{accum_update} + \epsilon} * \frac{\text{grad}}{\sqrt{\text{accum} + \epsilon}} \\
    \text{accum_update} = \rho * \text{accum_update} + (1 - \rho) * \text{update}^2 \\
    \text{var} = \text{var} - \text{lr} * \text{update}
\end{array}\end{split}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\rho\)</span> 代表 <cite>rho</cite> ， <span class="math notranslate nohighlight">\(\epsilon\)</span> 代表 <cite>epsilon</cite> 。</p>
<p><cite>var</cite> 、 <cite>accum</cite> 、 <cite>accum_update</cite> 和 <cite>grad</cite> 的输入遵循隐式类型转换规则，使数据类型一致。如果它们具有不同的数据类型，则较低精度的数据类型将转换为相对最高精度的数据类型。</p>
<dl class="simple">
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - 待更新的公式参数 var。数据类型为float32或float16。shape： <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <span class="math notranslate nohighlight">\(*\)</span> 表示任意数量的附加维度。</p></li>
<li><p><strong>accum</strong> (Parameter) - 待更新的公式参数 accum，shape和数据类型与 <cite>var</cite> 相同。</p></li>
<li><p><strong>accum_update</strong> (Parameter) - 待更新的公式参数 accum_update，shape和数据类型与 <cite>var</cite> 相同。</p></li>
<li><p><strong>lr</strong> (Union[Number, Tensor]) - 学习率，必须是Scalar。数据类型为float32或float16。</p></li>
<li><p><strong>rho</strong> (Union[Number, Tensor]) - <span class="math notranslate nohighlight">\(\rho\)</span> 衰减率，必须是Scalar。数据类型为float32或float16。</p></li>
<li><p><strong>epsilon</strong> (Union[Number, Tensor]) - <span class="math notranslate nohighlight">\(\epsilon\)</span> 加在分母上的值，以确保数值稳定，必须是Scalar。数据类型为float32或float16。</p></li>
<li><p><strong>grad</strong> (Tensor) - 梯度，shape和数据类型与 <cite>var</cite> 相同。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>3个Tensor的元组，更新后的数据。</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - 与 <cite>var</cite> 相同的shape和数据类型。</p></li>
<li><p><strong>accum</strong> (Tensor)- 与 <cite>accum</cite> 相同的shape和数据类型。</p></li>
<li><p><strong>accum_update</strong> (Tensor) - 与 <cite>accum_update</cite> 相同的shape和数据类型。</p></li>
</ul>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - 如果 <cite>var</cite> 、 <cite>accum</cite> 、 <cite>accum_update</cite> 、 <cite>lr</cite> 、 <cite>rho</cite> 、 <cite>epsilon</cite> 或 <cite>grad</cite> 的数据类型既不是float16也不是float32。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>accum_update</cite> 、 <cite>lr</cite> 、 <cite>rho</cite> 或 <cite>epsilon</cite> 既不是数值型也不是Tensor。</p></li>
<li><p><strong>RuntimeError</strong> - 如果 <cite>var</cite> 、 <cite>accum</cite> 、 <cite>accum_update</cite> 和 <cite>grad</cite> 不支持数据类型转换。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ops</span><span class="p">,</span> <span class="n">Parameter</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_adadelta</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ApplyAdadelta</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span>
<span class="gp">... </span>                                              <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
<span class="gp">... </span>                                                <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">accum_update</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
<span class="gp">... </span>                                                       <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                                            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accum_update&quot;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_adadelta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">accum_update</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rho</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="go">(Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="go">[[ 5.99051356e-01,  3.99683774e-01],</span>
<span class="go">[ 9.91633832e-02,  4.99105573e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="go">[[ 9.00000036e-02,  4.89999980e-01],</span>
<span class="go">[ 1.00000007e-02,  6.40000045e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="go">[[ 8.99990857e-01,  1.00000791e-01],</span>
<span class="go">[ 6.99930906e-01,  7.99999774e-01]]))</span>
</pre></div>
</div>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindspore.ops.ApplyAdagrad.html" class="btn btn-neutral float-right" title="mindspore.ops.ApplyAdagrad" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindspore.ops.AdaptiveAvgPool2D.html" class="btn btn-neutral float-left" title="mindspore.ops.AdaptiveAvgPool2D" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>