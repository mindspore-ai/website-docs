<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>模型分析与准备 &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/training.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="MindSpore网络搭建" href="model_development/model_development.html" />
    <link rel="prev" title="环境准备与资料获取" href="enveriment_preparation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.9/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.audio.html">mindspore.dataset.audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.config.html">mindspore.dataset.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.text.html">mindspore.dataset.text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.dataset.vision.html">mindspore.dataset.vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.ops.function.html">mindspore.ops.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r1.9/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">常见问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="typical_api_comparision.html">与PyTorch典型区别</a></li>
<li class="toctree-l1"><a class="reference internal" href="use_third_party_op.html">基于自定义算子接口调用第三方算子库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>模型分析与准备</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/migration_guide/analysis_and_preparation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="模型分析与准备">
<h1>模型分析与准备<a class="headerlink" href="#模型分析与准备" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.9/docs/mindspore/source_zh_cn/migration_guide/analysis_and_preparation.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.9/resource/_static/logo_source.png"></a></p>
<section id="获取参考代码">
<h2>获取参考代码<a class="headerlink" href="#获取参考代码" title="Permalink to this headline"></a></h2>
<p>我们拿到一篇论文，需要在MindSpore上进行迁移实现时，优先需要找到在其他框架已经实现好的参考代码，原则上这个参考代码需要符合以下要求中的至少一项：</p>
<ol class="arabic simple">
<li><p>论文原作者开源的实现；</p></li>
<li><p>大众普遍认可的实现(star数，fork数较多)；</p></li>
<li><p>比较新的代码，有开发者对代码进行维护；</p></li>
<li><p>优先考虑PyTorch的参考代码。</p></li>
</ol>
<p>如果是全新的论文，无可参考实现，请参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.9/migration_guide/model_development/model_development.html">MindSpore网络搭建</a>进行开发。</p>
</section>
<section id="分析算法及网络结构">
<h2>分析算法及网络结构<a class="headerlink" href="#分析算法及网络结构" title="Permalink to this headline"></a></h2>
<p>在阅读论文及参考代码时，首先需要分析网络结构，用以组织代码编写。比如下面是YOLOX的大致网络结构：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模块</p></th>
<th class="head"><p>实现</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>backbone</p></td>
<td><p>CSPDarknet(s,m,l,x等)</p></td>
</tr>
<tr class="row-odd"><td><p>neck</p></td>
<td><p>FPN</p></td>
</tr>
<tr class="row-even"><td><p>head</p></td>
<td><p>Decoupled Head</p></td>
</tr>
</tbody>
</table>
<p>其次需要分析迁移算法的创新点，记录在训练过程中使用了哪些trick，如数据处理加了哪些数据增强，是否有shuffle，使用了什么优化器，学习率衰减策略，参数初始化方式等。可以整理一个checklist，在分析过程中可以填写相应项来记录。</p>
<p>比如这里记录了YOLOX网络在训练时使用的一些trick：</p>
<table>
    <tr>
        <th>trick</th>
        <th>记录</th>
   </tr>
    <tr>
        <td rowspan="2">数据增强</td>
        <td >mosaic，包含随机缩放，随机剪裁，随机排布 </td>
    </tr>
    <tr>
        <td >MixUp</td>
    </tr>
    <tr>
        <td >学习率衰减策略</td>
        <td >多种衰减方式供选择，默认使用cos学习率衰减</td>
    </tr>
    <tr>
        <td >优化器参数</td>
        <td >带动量SGD momentum=0.9，nesterov=True，无weight decay</td>
    </tr>
    <tr>
        <td >训练参数</td>
        <td >epoch：300；batchsize：8</td>
    </tr>
    <tr>
        <td >网络结构优化点</td>
        <td >Decoupled Head；Anchor Free；SimOTA</td>
    </tr>
    <tr>
        <td >训练流程优化点</td>
        <td >EMA；后15epoch不做数据增强；混合精度</td>
    </tr>
</table>
<p><strong>注意，以复现代码中使用的trick为主，有些论文里提到的不一定有用。</strong></p>
<p>此外，需要判断论文是否能通过在MindSpore已有模型上做少量修改来实现，若是，可以在已有模型的基础上进行开发，这样能极大的减少开发的工作量。比如WGAN-PG可以基于WGAN进行开发。
<a class="reference external" href="https://gitee.com/mindspore/models">MindSpore models</a>是MindSpore的模型仓库，当前已经覆盖了机器视觉、自然语言处理、语音、推荐系统等多个领域的主流模型，可以从中查找是否有需要的模型。</p>
</section>
<section id="复现论文实现">
<h2>复现论文实现<a class="headerlink" href="#复现论文实现" title="Permalink to this headline"></a></h2>
<p>获取到参考代码后，需要复现下参考实现的精度，获取参考实现的性能数据。这样做有几点好处：</p>
<ol class="arabic simple">
<li><p>提前识别一些问题：</p>
<ul class="simple">
<li><p>判断参考代码使用的三方库是否有版本依赖，提前识别版本适配问题；</p></li>
<li><p>判断数据集是否能获取的到，有的数据集是私有的或者原作者在公开数据集上加了自己的部分数据集，在复现参考实现阶段就可以发现这种问题；</p></li>
<li><p>参考实现是否能复现论文精度，有的官方的参考实现也不一定能复现论文的精度，当出现这种情况时要及时发现问题，更换参考实现或者调整精度基线。</p></li>
</ul>
</li>
<li><p>获取一些参考数据作为MindSpore迁移过程的参考：</p>
<ul class="simple">
<li><p>获取loss下降趋势，帮助验证MindSpore上训练收敛趋势是否ok；</p></li>
<li><p>获取参数文件，用于进行转换，进行推理验证，详细过程参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.9/migration_guide/model_development/training_and_evaluation_procession.html">推理及训练流程</a>；</p></li>
<li><p>获取性能基线，在做性能优化时有一个基础目标，如需做性能优化，请参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.9/migration_guide/debug_and_tune.html">调试调优</a>。</p></li>
</ul>
</li>
</ol>
</section>
<section id="分析api满足度">
<h2>分析API满足度<a class="headerlink" href="#分析api满足度" title="Permalink to this headline"></a></h2>
<p>这里分析的API缺失专指网络执行图中的API，包含MindSpore的<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.9/api_python/mindspore.ops.html">算子</a>及高级封装API，不包括数据处理中使用的API。数据处理过程中使用的API建议使用三方的实现代替，如numpy，opencv，pandas，PIL等。</p>
<section id="查询api映射表">
<h3>查询API映射表<a class="headerlink" href="#查询api映射表" title="Permalink to this headline"></a></h3>
<p>以PyTorch的代码迁移为例，拿到参考代码实现后，可以通过过滤<code class="docutils literal notranslate"><span class="pre">torch</span></code>，<code class="docutils literal notranslate"><span class="pre">nn</span></code>，<code class="docutils literal notranslate"><span class="pre">ops</span></code>等关键字获取使用的API接口，如调用了其他库的方法，需要手动分析。然后对照<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.9/note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API 映射</a>
或者<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.9/api_python/mindspore.ops.html">API</a> 查找对应的API实现。</p>
<p>其他框架API的映射可以参考API命名与功能描述。注意，针对相同功能的API，MindSpore的命名可能与其他框架不同，同名API参数与功能也可能与其他框架有区别，均以官方描述为准。</p>
<p>如果没有找到对应的API接口，需要用具体的策略来处理API缺失的问题。</p>
</section>
<section id="缺失api处理策略">
<h3>缺失API处理策略<a class="headerlink" href="#缺失api处理策略" title="Permalink to this headline"></a></h3>
<p>有以下方法来处理缺失API的情况。</p>
<section id="1-等价替换">
<h4>1. 等价替换<a class="headerlink" href="#1-等价替换" title="Permalink to this headline"></a></h4>
<p>在有些场景下API的功能是可以等价替换的，比如：</p>
<ul class="simple">
<li><p>Squeeze，Flatten，ExpandDims等没有实际的计算，只是改变Tensor shape的API均可以用Reshape代替；</p></li>
<li><p>AdaptiveAvgPool，AdaptiveMaxPool在输出的shape是1时，与ReduceMean，ReduceMax在设置keep_dims=True时是等价的；</p></li>
<li><p>MaxPool和MaxPoolWithArgmax在不使用indices的情况是等价的；</p></li>
<li><p>Sort和在全排序场景下的TopK是等价的。</p></li>
</ul>
</section>
<section id="2-使用已有api包装等价功能逻辑">
<h4>2. 使用已有API包装等价功能逻辑<a class="headerlink" href="#2-使用已有api包装等价功能逻辑" title="Permalink to this headline"></a></h4>
<p>对于一些缺失的API，可以基于MindSpore已有的API实现等价功能。下面举一个<code class="docutils literal notranslate"><span class="pre">sigmoid</span> <span class="pre">focal</span> <span class="pre">loss</span></code>的例子：</p>
<p>先来分析一下这个API的算法基础。</p>
<p>Focal Loss[1]是一种用来处理单阶段目标检测器训练过程中出现的正负样本、难易样本不平衡问题的方法。</p>
<p>常用的sigmoid focal loss的API接口是MMDetection的实现，我们来看一下使用PyTorch是怎么实现的：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">def</span> <span class="nf">reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">reduction</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reduce loss as specified.</span>
<span class="sd">    Args:</span>
<span class="sd">        loss (Tensor): Elementwise loss tensor.</span>
<span class="sd">        reduction (str): Options are &quot;none&quot;, &quot;mean&quot; and &quot;sum&quot;.</span>
<span class="sd">    Return:</span>
<span class="sd">        Tensor: Reduced loss tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="c1"># none: 0, elementwise_mean:1, sum: 2</span>
    <span class="k">if</span> <span class="n">reduction_enum</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span>
    <span class="k">elif</span> <span class="n">reduction_enum</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">reduction_enum</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">weight_reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">weight</span>

    <span class="c1"># if avg_factor is not specified, just reduce the loss</span>
    <span class="k">if</span> <span class="n">avg_factor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># if reduction is mean, then average the loss by avg_factor</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">avg_factor</span>
        <span class="c1"># if reduction is &#39;none&#39;, then do nothing, otherwise raise an error</span>
        <span class="k">elif</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;avg_factor can not be used with reduction=&quot;sum&quot;&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>


<span class="k">def</span> <span class="nf">py_sigmoid_focal_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span>
                          <span class="n">target</span><span class="p">,</span>
                          <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                          <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
                          <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                          <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;PyTorch version of `Focal Loss &lt;https://arxiv.org/abs/1708.02002&gt;`_.</span>
<span class="sd">    Args:</span>
<span class="sd">        pred (torch.Tensor): The prediction with shape (N, C), C is the</span>
<span class="sd">            number of classes</span>
<span class="sd">        target (torch.Tensor): The learning label of the prediction.</span>
<span class="sd">        weight (torch.Tensor, optional): Sample-wise loss weight.</span>
<span class="sd">        gamma (float, optional): The gamma for calculating the modulating</span>
<span class="sd">            factor. Defaults to 2.0.</span>
<span class="sd">        alpha (float, optional): A balanced form for Focal Loss.</span>
<span class="sd">            Defaults to 0.25.</span>
<span class="sd">        reduction (str, optional): The method used to reduce the loss into</span>
<span class="sd">            a scalar. Defaults to &#39;mean&#39;.</span>
<span class="sd">        avg_factor (int, optional): Average factor that is used to average</span>
<span class="sd">            the loss. Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pred_sigmoid</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
    <span class="n">pt</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_sigmoid</span><span class="p">)</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="n">pred_sigmoid</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">focal_weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span>
                    <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">))</span> <span class="o">*</span> <span class="n">pt</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span> <span class="o">*</span> <span class="n">focal_weight</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">loss</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
                <span class="c1"># For most cases, weight is of shape (num_priors, ),</span>
                <span class="c1">#  which means it does not have the second axis num_class</span>
                <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Sometimes, weight per anchor per class is also needed. e.g.</span>
                <span class="c1">#  in FSAF. But it may be flattened of shape</span>
                <span class="c1">#  (num_priors x num_class, ), while loss is still of shape</span>
                <span class="c1">#  (num_priors, num_class).</span>
                <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">loss</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">weight</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="n">loss</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">weight_reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">avg_factor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>参考API映射表，可以看到代码中使用的API MindSpore上都有对应实现，没有缺失。</p>
<p>参考上面的PyTorch代码，实现下MindSpore的版本：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">ops</span>

<span class="k">class</span> <span class="nc">SigmoidFoaclLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SigmoidFoaclLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span> <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">avg_factor</span> <span class="o">=</span> <span class="n">avg_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reduce_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reduce loss as specified.</span>
<span class="sd">        Args:</span>
<span class="sd">            loss (Tensor): Elementwise loss tensor.</span>
<span class="sd">        Return:</span>
<span class="sd">            Tensor: Reduced loss tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">weight_reduce_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="c1"># if avg_factor is not specified, just reduce the loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_factor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># if reduction is mean, then average the loss by avg_factor</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">avg_factor</span>
            <span class="c1"># if reduction is &#39;none&#39;, then do nothing, otherwise raise an error</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;avg_factor can not be used with reduction=&quot;sum&quot;&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">pred_sigmoid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">pred</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">pt</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_sigmoid</span><span class="p">)</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="n">pred_sigmoid</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">focal_weight</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">target</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">))</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">pt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="n">focal_weight</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_weight</span><span class="p">:</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="c1"># For most cases, weight is of shape (num_priors, ),</span>
                    <span class="c1">#  which means it does not have the second axis num_class</span>
                    <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="n">loss</span><span class="o">.</span><span class="n">size</span><span class="p">:</span>
                    <span class="c1"># Sometimes, weight per anchor per class is also needed. e.g.</span>
                    <span class="c1">#  in FSAF. But it may be flattened of shape</span>
                    <span class="c1">#  (num_priors x num_class, ), while loss is still of shape</span>
                    <span class="c1">#  (num_priors, num_class).</span>
                    <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="n">loss</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;weight shape </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> is not match to loss shape </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">weight</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_reduce_loss</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<p>然后我们做个测试：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_compare</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">ms_s_focal_loss</span> <span class="o">=</span> <span class="n">SigmoidFoaclLoss</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                                       <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="n">avg_factor</span><span class="p">)</span>
    <span class="n">loss_ms</span> <span class="o">=</span> <span class="n">ms_s_focal_loss</span><span class="p">(</span><span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>
    <span class="n">loss_pt</span> <span class="o">=</span> <span class="n">py_sigmoid_focal_loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">pred</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">weight</span><span class="p">),</span>
                                    <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="n">avg_factor</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">loss_ms</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">-</span> <span class="n">loss_pt</span><span class="o">.</span><span class="n">numpy</span><span class="p">())))</span>

<span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">test_compare</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">test_compare</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">test_compare</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">test_compare</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">avg_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>可以看到最后的误差在1e-5以下，是合理的精度误差：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>6.891787e-08
1.4305115e-06
2.8014183e-06
3.799796e-07
</pre></div>
</div>
</section>
<section id="3-自定义算子">
<h4>3. 自定义算子<a class="headerlink" href="#3-自定义算子" title="Permalink to this headline"></a></h4>
<p>当有些情况无法使用已有的API进行包装，或者用Cell封装的方式性能非常差，这个时候就需要使用自定义算子，详情请参考Custom算子的<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r1.9/operation/op_custom.html">使用指南</a>。</p>
<p>除了可以自己迁移实现API，也可以利用<code class="docutils literal notranslate"><span class="pre">Custom</span></code>算子的<code class="docutils literal notranslate"><span class="pre">aot</span></code>开发方式调用PyTorch Aten的算子进行快速验证，请参考<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.9/migration_guide/use_third_party_op.html">基于自定义算子接口调用第三方算子库</a>。</p>
<p><strong>注意，PyTorch实现的算子迁移到GPU和CPU上比较方便，这里展示的也大多是GPU和CPU的，Ascend的算子由于需要使用TBE进行算子开发，门槛较高，推荐使用官方实现的算子进行包装。</strong></p>
</section>
<section id="4-社区求助">
<h4>4. 社区求助<a class="headerlink" href="#4-社区求助" title="Permalink to this headline"></a></h4>
<p>在MindSpore的<a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">Gitee</a>上提交issue建议开发缺失API。</p>
</section>
</section>
</section>
<section id="分析功能满足度">
<h2>分析功能满足度<a class="headerlink" href="#分析功能满足度" title="Permalink to this headline"></a></h2>
<p>MindSpore在持续交付中，部分功能存在限制，在网络迁移过程中涉及受限功能使用的情况，可采取一些措施来避免功能限制的影响。</p>
<section id="动态shape">
<h3>动态shape<a class="headerlink" href="#动态shape" title="Permalink to this headline"></a></h3>
<p>想要了解动态shape，需要先了解什么是静态shape。
静态shape指在网路执行阶段Tensor的shape没有发生变化。
比如resnet50网络如果保证图片的输入shape一直是<code class="docutils literal notranslate"><span class="pre">224*224</span></code>的，那么在网络训练阶段，四个残差模块的输出Tensor的shape分别是<code class="docutils literal notranslate"><span class="pre">B*64*56*56</span></code>，<code class="docutils literal notranslate"><span class="pre">B*128*28*28</span></code>，<code class="docutils literal notranslate"><span class="pre">B*256*14*14</span></code>，<code class="docutils literal notranslate"><span class="pre">B*512*7*7</span></code>，<code class="docutils literal notranslate"><span class="pre">B</span></code>指<code class="docutils literal notranslate"><span class="pre">BatchSize</span></code>，在训练过程中也是固定的，此时网络中全部是静态的shape，没有动态shape。
如果输入的shape不一定是<code class="docutils literal notranslate"><span class="pre">224*224</span></code>的，那么四个残差模块输出Tensor的shape将会随输入shape变化，此时就不是静态shape，而是动态shape了。一般动态shape引入的原因有：</p>
<section id="输入shape不固定">
<h4>输入shape不固定<a class="headerlink" href="#输入shape不固定" title="Permalink to this headline"></a></h4>
<p>比如输入图片需要有不同的shape，音频的label需要不同长度，这都会引入动态shape；</p>
<p>这种场景可以读代码分析数据处理的输出shape是否固定，也可以直接打印数据处理输出的shape，进行对比：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">batch_idx</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="网络执行过程中有引发shape变化的api">
<h4>网络执行过程中有引发shape变化的API<a class="headerlink" href="#网络执行过程中有引发shape变化的api" title="Permalink to this headline"></a></h4>
<p>在网络执行过程中可能有一些操作会引起Tensor的shape变化。</p>
<p>引起这种场景常见的API有：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>功能描述</p></th>
<th class="head"><p>引发动态shape场景</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>StridedSlice/Slice</p></td>
<td><p>切片，用户编程时也可以使用 [start_idx:end_idx]这种方式</p></td>
<td><p>当切片下标是变量时</p></td>
</tr>
<tr class="row-odd"><td><p>TopK</p></td>
<td><p>取前K大</p></td>
<td><p>当K取值不定时</p></td>
</tr>
<tr class="row-even"><td><p>Gather</p></td>
<td><p>取Tensor在指定 axis 上索引对应的元素组成的切片</p></td>
<td><p>当index长度不定时</p></td>
</tr>
<tr class="row-odd"><td><p>UnsortedSegmentX</p></td>
<td><p>包含UnsortedSegmentSum，UnsortedSegmentMax等沿分段计算输入Tensor的某个计算</p></td>
<td><p>当分段不固定时</p></td>
</tr>
<tr class="row-even"><td><p>Sampler</p></td>
<td><p>取样器相关操作，比如where，random.choice等</p></td>
<td><p>当抽取数量不固定时</p></td>
</tr>
<tr class="row-odd"><td><p>ReduceX</p></td>
<td><p>ReduceSum，ReduceMean等归约操作</p></td>
<td><p>当axis不固定时</p></td>
</tr>
<tr class="row-even"><td><p>Transpose</p></td>
<td><p>根据轴进行变换</p></td>
<td><p>当变化轴不定时</p></td>
</tr>
<tr class="row-odd"><td><p>Unique</p></td>
<td><p>去重</p></td>
<td><p>使用就会引入动态shape</p></td>
</tr>
<tr class="row-even"><td><p>MaskedSelect</p></td>
<td><p>根据bool型的mask取值</p></td>
<td><p>使用就会引入动态shape</p></td>
</tr>
<tr class="row-odd"><td><p>NonZero</p></td>
<td><p>计算非零元素的下标</p></td>
<td><p>使用就会引入动态shape</p></td>
</tr>
</tbody>
</table>
<p>比如：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">ms</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># 6</span>
<span class="c1"># (6,)</span>
</pre></div>
</div>
<p>在网络训练时有个切片的操作<code class="docutils literal notranslate"><span class="pre">x[:k]</span></code>这里的k不是一个常量，会导致<code class="docutils literal notranslate"><span class="pre">x[:k]</span></code>的shape随k的值改变，导致后续所有和<code class="docutils literal notranslate"><span class="pre">x[:k]</span></code>相关的操作的shape不确定。</p>
</section>
<section id="控制流不同分支引入shape上的变化">
<h4>控制流不同分支引入shape上的变化<a class="headerlink" href="#控制流不同分支引入shape上的变化" title="Permalink to this headline"></a></h4>
<p>网络中可能会有一些控制流的输出是不一样的，而当控制流的条件控制项不是固定的时，可能会引发动态shape，比如：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="n">cond</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>

<span class="k">if</span> <span class="n">cond</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># [4.17021990e-01 7.20324516e-01 1.14374816e-04 3.02332580e-01</span>
<span class="c1">#  1.46755889e-01 9.23385918e-02 1.86260208e-01 3.45560730e-01</span>
<span class="c1">#  3.96767467e-01 5.38816750e-01]</span>
<span class="c1"># True</span>
<span class="c1"># [0.7203245  0.53881675]</span>
</pre></div>
</div>
<p>在这个过程其实有两个地方有动态shape，一个是<code class="docutils literal notranslate"><span class="pre">cond=True</span></code>时<code class="docutils literal notranslate"><span class="pre">masked_select</span></code>结果的shape是动态，另外是控制流，由于cond不定，控制流两个分支的shape输出不同也会造成动态shape。</p>
<p>动态shape一般可以从算法、代码层面进行分析，也可以直接打印参考代码相关Tensor进行判断。如果存在动态shape，我们在<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.9/migration_guide/model_development/model_and_loss.html">网络主体和loss搭建</a>篇章有规避策略的介绍。</p>
</section>
<section id="稀疏">
<h4>稀疏<a class="headerlink" href="#稀疏" title="Permalink to this headline"></a></h4>
<p><a class="reference external" href="https://matteding.github.io/2019/04/25/sparse-matrices/">稀疏张量</a> 是一种特殊张量，其中绝大部分元素的值为零。</p>
<p>在某些应用场景中（比如推荐系统、分子动力学、图神经网络等），数据的特征是稀疏的，若使用普通张量表征这些数据会引入大量不必要的计算、存储和通讯开销。在这种时候就可以使用稀疏张量来表征这些数据。</p>
<p>MindSpore现在已经支持最常用的<a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r1.9/beginner/tensor.html#%E7%A8%80%E7%96%8F%E5%BC%A0%E9%87%8F">CSR和COO两种稀疏数据格式</a>。但是由于目前支持稀疏算子有限，大部分稀疏的特性还存在限制，在此情况下，建议优先查找对应的算子是否支持稀疏计算，如不支持的话需要转换成普通算子。
由于转换成稠密算子后使用的显存会增加，可能不能使用参考实现的batch size进行训练，此时可以使用 <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r1.9/others/gradient_accumulation.html">梯度累积</a> 来模拟大batch训练。</p>
</section>
</section>
</section>
<section id="mindspore好用功能和特性推荐">
<h2>MindSpore好用功能和特性推荐<a class="headerlink" href="#mindspore好用功能和特性推荐" title="Permalink to this headline"></a></h2>
<section id="动态图与静态图">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r1.9/advanced/compute_graph.html">动态图与静态图</a><a class="headerlink" href="#动态图与静态图" title="Permalink to this headline"></a></h3>
<p>目前主流的深度学习框架有静态图(Graph)和动态图(PyNative)两种执行模式。</p>
<ul class="simple">
<li><p>静态图模式下，程序在编译执行时，首先生成神经网络的图结构，然后再执行图中涉及的计算操作。因此，在静态图模式下，编译器可以通过使用图优化等技术来获得更好的执行性能，有助于规模部署和跨平台运行。</p></li>
<li><p>动态图模式下，程序按照代码的编写顺序逐行执行，在执行正向过程中根据反向传播的原理，动态生成反向执行图。这种模式下，编译器将神经网络中的各个算子逐一下发到设备进行计算操作，方便用户编写和调试神经网络模型。</p></li>
</ul>
</section>
<section id="调用自定义类">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r1.9/network/ms_class.html">调用自定义类</a><a class="headerlink" href="#调用自定义类" title="Permalink to this headline"></a></h3>
<p>在静态图模式下，通过使用ms_class修饰自定义类，用户可以创建、调用该自定义类的实例，并且可以获取其属性和方法。</p>
<p>ms_class应用于静态图模式，扩充完善静态图编译语法的支持范围。在动态图模式即PyNative模式下，ms_class的使用不影响PyNative模式的执行逻辑。</p>
</section>
<section id="自动微分">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/zh-CN/r1.9/beginner/autograd.html">自动微分</a><a class="headerlink" href="#自动微分" title="Permalink to this headline"></a></h3>
<p>自动微分能够计算可导函数在某点处的导数值，是反向传播算法的一般化。自动微分主要解决的问题是将一个复杂的数学运算分解为一系列简单的基本运算，该功能对用户屏蔽了大量的求导细节和过程，大大降低了框架的使用门槛。</p>
</section>
<section id="混合精度">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r1.9/others/mixed_precision.html">混合精度</a><a class="headerlink" href="#混合精度" title="Permalink to this headline"></a></h3>
<p>通常我们训练神经网络模型的时候，默认使用的数据类型为单精度FP32。近年来，为了加快训练时间、减少网络训练时候所占用的内存，并且保存训练出来的模型精度持平的条件下，业界提出越来越多的混合精度训练的方法。这里的混合精度训练是指在训练的过程中，同时使用单精度（FP32）和半精度（FP16）。</p>
</section>
<section id="自动数据增强">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r1.9/dataset/augment.html">自动数据增强</a><a class="headerlink" href="#自动数据增强" title="Permalink to this headline"></a></h3>
<p>MindSpore除了可以让用户自定义数据增强的使用，还提供了一种自动数据增强方式，可以基于特定策略自动对图像进行数据增强处理。</p>
</section>
<section id="多维度混合并行">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r1.9/parallel/multi_dimensional.html">多维度混合并行</a><a class="headerlink" href="#多维度混合并行" title="Permalink to this headline"></a></h3>
<p>随着深度学习的发展，模型规模越来越大。如NLP领域，短短几年时间，参数量就从BERT的亿级，发展到GPT-3的1700亿，再到盘古alpha 2000亿，以及当前业界甚至提出百万亿级。由此可以看出，近年来参数规模呈指数增长趋势。另一方面，随着大数据、互联网等领域相关技术的发展，可供模型训练的数据集也极速扩增，例如推荐、自然语言处理等场景的数据集可达数TB。</p>
</section>
<section id="梯度累积">
<h3><a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/r1.9/others/gradient_accumulation.html">梯度累积</a><a class="headerlink" href="#梯度累积" title="Permalink to this headline"></a></h3>
<p>梯度累积是一种训练神经网络的数据样本按Batch拆分为几个小Batch的方式，然后按顺序计算。目的是为了解决由于内存不足，导致Batch size过大神经网络无法训练或者网络模型过大无法加载的OOM（Out Of Memory）问题。</p>
</section>
<section id="summary">
<h3><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.9/summary_record.html">Summary</a><a class="headerlink" href="#summary" title="Permalink to this headline"></a></h3>
<p>训练过程中的标量、图像、计算图、训练优化过程以及模型超参等信息记录到文件中，通过可视化界面供用户查看。</p>
</section>
<section id="调试器">
<h3><a class="reference external" href="https://www.mindspore.cn/mindinsight/docs/zh-CN/r1.9/debugger.html">调试器</a><a class="headerlink" href="#调试器" title="Permalink to this headline"></a></h3>
<p>MindSpore调试器是为图模式训练提供的调试工具，可以用来查看并分析计算图节点的中间结果。</p>
</section>
<section id="golden-stick">
<h3><a class="reference external" href="https://www.mindspore.cn/golden_stick/docs/zh-CN/r0.2/index.html">Golden Stick</a><a class="headerlink" href="#golden-stick" title="Permalink to this headline"></a></h3>
<p>MindSpore Golden Stick是华为诺亚团队和华为MindSpore团队联合设计开发的一个模型压缩算法集。包含基本的量化和剪枝方法。</p>
</section>
</section>
<section id="与pytorch典型接口区别">
<h2>与PyTorch典型接口区别<a class="headerlink" href="#与pytorch典型接口区别" title="Permalink to this headline"></a></h2>
<p>在PyTorch往MindSpore进行网络迁移时，需要注意<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r1.9/migration_guide/typical_api_comparision.html">与PyTorch典型接口区别</a>。</p>
<p>[1] Lin, T. Y. , et al. “Focal Loss for Dense Object Detection.” IEEE Transactions on Pattern Analysis &amp; Machine Intelligence PP.99(2017):2999-3007.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="enveriment_preparation.html" class="btn btn-neutral float-left" title="环境准备与资料获取" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="model_development/model_development.html" class="btn btn-neutral float-right" title="MindSpore网络搭建" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>