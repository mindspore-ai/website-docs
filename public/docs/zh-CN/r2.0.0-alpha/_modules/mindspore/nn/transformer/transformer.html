<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.nn.transformer.transformer &mdash; MindSpore master 文档</title>
      <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/js/training.js"></script>
        <script src="../../../../_static/translations.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0.0-alpha/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.transformer.html">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">常见问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/typical_api_comparision.html">与PyTorch典型区别</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/use_third_party_op.html">基于自定义算子接口调用第三方算子库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">模块代码</a> &raquo;</li>
      <li>mindspore.nn.transformer.transformer</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>mindspore.nn.transformer.transformer 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2021-2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Note:</span>
<span class="sd">    Transformer Networks. This is interface that is subject to change or deletion.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span>
<span class="kn">from</span> <span class="nn">mindspore.common.initializer</span> <span class="kn">import</span> <span class="n">initializer</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.cell</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">Validator</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._utils</span> <span class="kn">import</span> <span class="n">_get_parallel_mode</span><span class="p">,</span> <span class="n">_is_sharding_propagation</span>
<span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>
<span class="kn">from</span> <span class="nn">mindspore.log</span> <span class="kn">import</span> <span class="n">_LogActionOnce</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.transformer.layers</span> <span class="kn">import</span> <span class="n">_LayerNorm</span><span class="p">,</span> <span class="n">_Linear</span><span class="p">,</span> <span class="n">_check_input_shape</span><span class="p">,</span> \
    <span class="n">_args_type_validator_check</span><span class="p">,</span> <span class="n">_valid_type_checks</span><span class="p">,</span> <span class="n">_valid_value_checks</span><span class="p">,</span> \
    <span class="n">_check_shape_equal</span><span class="p">,</span> <span class="n">_check_past_none_input_none</span><span class="p">,</span> <span class="n">_check_input_dtype</span><span class="p">,</span> <span class="n">_check_input_shape_value</span><span class="p">,</span> \
    <span class="n">_check_shape_equal_without_batch</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.transformer.op_parallel_config</span> <span class="kn">import</span> <span class="n">default_dpmp_config</span><span class="p">,</span> <span class="n">_PipeLineConfig</span><span class="p">,</span> <span class="n">OpParallelConfig</span><span class="p">,</span> \
    <span class="n">_Config</span><span class="p">,</span> <span class="n">_check_config</span><span class="p">,</span> <span class="n">MoEParallelConfig</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.transformer.moe</span> <span class="kn">import</span> <span class="n">default_moe_config</span><span class="p">,</span> <span class="n">MoE</span><span class="p">,</span> <span class="n">_check_moe_config</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;AttentionMask&quot;</span><span class="p">,</span>
    <span class="s2">&quot;VocabEmbedding&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FeedForward&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Transformer&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerOpParallelConfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;EmbeddingOpParallelConfig&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TransformerRecomputeConfig&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="EmbeddingOpParallelConfig"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.EmbeddingOpParallelConfig">[文档]</a><span class="k">class</span> <span class="nc">EmbeddingOpParallelConfig</span><span class="p">(</span><span class="n">_Config</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The parallel config of :class:`VocabEmbedding`</span>
<span class="sd">        for the setting data parallel or model parallel for the embedding table.</span>

<span class="sd">        Args:</span>
<span class="sd">            data_parallel(int): The data parallel way. The input data will be sliced into n parts for embedding layer</span>
<span class="sd">                according to this value. Default: 1.</span>
<span class="sd">            model_parallel(int): The model parallel way. The embedding table parameters</span>
<span class="sd">                will be sliced at 0-th axis according to the model parallel way. Default: 1.</span>
<span class="sd">            vocab_emb_dp(bool): Shard embedding in model parallel or data parallel. If True, the embedding lookup</span>
<span class="sd">                will be a data parallel style training and model_parallel value will be ignored.  If false, the</span>
<span class="sd">                embedding table will be sharded into n parts at the 0-th dimension row slice of the embedding table,</span>
<span class="sd">                where the n is the model parallel way determined by this parameter. Default: True</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import EmbeddingOpParallelConfig</span>
<span class="sd">            &gt;&gt;&gt; config=EmbeddingOpParallelConfig(data_parallel=1, model_parallel=1, vocab_emb_dp=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span> <span class="o">=</span> <span class="n">OpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="n">model_parallel</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">vocab_emb_dp</span><span class="p">,</span> <span class="s2">&quot;vocab_emb_dp&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_emb_dp</span> <span class="o">=</span> <span class="n">vocab_emb_dp</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">data_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span><span class="o">.</span><span class="n">data_parallel</span>

    <span class="nd">@data_parallel</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">data_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span><span class="o">.</span><span class="n">data_parallel</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">model_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span><span class="o">.</span><span class="n">model_parallel</span>

    <span class="nd">@model_parallel</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">model_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vocab_emb_dp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_emb_dp</span>

    <span class="nd">@vocab_emb_dp</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">vocab_emb_dp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;vocab_emb_dp&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocab_emb_dp</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dp_mp_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dp_mp_config</span></div>


<div class="viewcode-block" id="TransformerRecomputeConfig"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.TransformerRecomputeConfig">[文档]</a><span class="k">class</span> <span class="nc">TransformerRecomputeConfig</span><span class="p">(</span><span class="n">_Config</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        TransformerRecomputeConfig for the setting recompute attributes for encoder/decoder layers.</span>

<span class="sd">        Args:</span>
<span class="sd">            recompute (bool): Enable recomputation of the transformer block or not. Default: False.</span>
<span class="sd">            parallel_optimizer_comm_recompute (bool): Specifies whether the communication operator allgathers</span>
<span class="sd">                introduced by optimizer shard are recomputed in auto parallel or semi auto parallel mode.</span>
<span class="sd">                Default: False.</span>
<span class="sd">            mp_comm_recompute (bool): Specifies whether the model parallel communication operators</span>
<span class="sd">                in the cell are recomputed in auto parallel or semi auto parallel mode. Default: True.</span>
<span class="sd">            recompute_slice_activation (bool): Slice the cell output which would remains in memory. Default: False.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import TransformerRecomputeConfig</span>
<span class="sd">            &gt;&gt;&gt; config=TransformerRecomputeConfig(recompute=True, parallel_optimizer_comm_recompute=True, \</span>
<span class="sd">            ...                                   mp_comm_recompute=True, recompute_slice_activation=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">recompute</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">parallel_optimizer_comm_recompute</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">mp_comm_recompute</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">recompute_slice_activation</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">recompute</span><span class="p">,</span> <span class="s2">&quot;recompute&quot;</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">parallel_optimizer_comm_recompute</span><span class="p">,</span> <span class="s2">&quot;parallel_optimizer_comm_recompute&quot;</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">mp_comm_recompute</span><span class="p">,</span> <span class="s2">&quot;mp_comm_recompute&quot;</span><span class="p">)</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">recompute_slice_activation</span><span class="p">,</span> <span class="s2">&quot;recompute_slice_activation&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_recompute</span> <span class="o">=</span> <span class="n">recompute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parallel_optimizer_comm_recompute</span> <span class="o">=</span> <span class="n">parallel_optimizer_comm_recompute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mp_comm_recompute</span> <span class="o">=</span> <span class="n">mp_comm_recompute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_recompute_slice_activation</span> <span class="o">=</span> <span class="n">recompute_slice_activation</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">recompute</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recompute</span>

    <span class="nd">@recompute</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">recompute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;recompute&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_recompute</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">parallel_optimizer_comm_recompute</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parallel_optimizer_comm_recompute</span>

    <span class="nd">@parallel_optimizer_comm_recompute</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">parallel_optimizer_comm_recompute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;parallel_optimizer_comm_recompute&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_parallel_optimizer_comm_recompute</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">mp_comm_recompute</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mp_comm_recompute</span>

    <span class="nd">@mp_comm_recompute</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">mp_comm_recompute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;mp_comm_recompute&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mp_comm_recompute</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">recompute_slice_activation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recompute_slice_activation</span>

    <span class="nd">@recompute_slice_activation</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">recompute_slice_activation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;recompute_slice_activation&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_recompute_slice_activation</span> <span class="o">=</span> <span class="n">value</span></div>


<span class="n">default_transformer_recompute_config</span> <span class="o">=</span> <span class="n">TransformerRecomputeConfig</span><span class="p">()</span>


<div class="viewcode-block" id="TransformerOpParallelConfig"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.TransformerOpParallelConfig">[文档]</a><span class="k">class</span> <span class="nc">TransformerOpParallelConfig</span><span class="p">(</span><span class="n">_Config</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        TransformerOpParallelConfig for setting parallel configuration, such as the data parallel and model parallel.</span>

<span class="sd">        Note:</span>
<span class="sd">            Except the recompute argument, other arguments will **not** be effective when the user doesn&#39;t set</span>
<span class="sd">            auto_parallel_context to `SEMI_AUTO_PARALLEL` or `AUTO_PARALLEL`.</span>
<span class="sd">            The micro_batch_num must be greater than or equal to pipeline_stage when training.</span>
<span class="sd">            The data_parallel\*model_parallel \*pipeline_stage must be equal or less equal to the device. When setting</span>
<span class="sd">            the pipeline stage and optimizer_shard, the config will overwrite the auto_parallel_context. When given the</span>
<span class="sd">            8 devices and the data_parallel is 1 and model_parallel is 1, the calculation will be repeated on each</span>
<span class="sd">            device.</span>

<span class="sd">        Args:</span>
<span class="sd">            data_parallel (int): The data parallel way. The input data will be sliced into n parts for each layer</span>
<span class="sd">                according to the data parallel way. Default: 1.</span>
<span class="sd">            model_parallel (int): The model parallel way. The parameters of dense layers in MultiheadAttention and</span>
<span class="sd">                FeedForward layer will be sliced according to the model parallel way. Default: 1.</span>
<span class="sd">            expert_parallel (int): The expert parallel way. This is effective only when MoE (Mixture of Experts)</span>
<span class="sd">                is applied. This value specifies the number of partitions to split the experts into.</span>
<span class="sd">            pipeline_stage (int): The number of the pipeline stage. Should be a positive value. Default: 1.</span>
<span class="sd">            micro_batch_num (int): The micro size of the batches for the pipeline training. Default: 1.</span>
<span class="sd">            optimizer_shard (bool): Whether to enable optimizer shard. Default False.</span>
<span class="sd">            gradient_aggregation_group (int): The fusion group size of the optimizer state sharding. Default: 4.</span>
<span class="sd">            recompute (Union[TransformerRecomputeConfig, bool]): The configuration of recomputation for</span>
<span class="sd">                the transformer block. Default: An instance of TransformerRecomputeConfig with default values.</span>
<span class="sd">            vocab_emb_dp (bool): Shard embedding in model parallel or data parallel. Default: True.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import TransformerRecomputeConfig</span>
<span class="sd">            &gt;&gt;&gt; recompute_config=TransformerRecomputeConfig(recompute=True, parallel_optimizer_comm_recompute=True, \</span>
<span class="sd">            ...                                             mp_comm_recompute=True, recompute_slice_activation=True)</span>
<span class="sd">            &gt;&gt;&gt; config=TransformerOpParallelConfig(data_parallel=1, model_parallel=1, recompute=recompute_config)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">expert_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pipeline_stage</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">micro_batch_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">recompute</span><span class="o">=</span><span class="n">default_transformer_recompute_config</span><span class="p">,</span>
                 <span class="n">optimizer_shard</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">gradient_aggregation_group</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recompute</span> <span class="o">=</span> <span class="n">recompute</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_shard</span> <span class="o">=</span> <span class="n">optimizer_shard</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_aggregation_group</span> <span class="o">=</span> <span class="n">gradient_aggregation_group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span> <span class="o">=</span> <span class="n">EmbeddingOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="n">model_parallel</span><span class="p">,</span>
                                                             <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="n">vocab_emb_dp</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pp_config</span> <span class="o">=</span> <span class="n">_PipeLineConfig</span><span class="p">(</span><span class="n">pipeline_stage</span><span class="o">=</span><span class="n">pipeline_stage</span><span class="p">,</span> <span class="n">micro_batch_num</span><span class="o">=</span><span class="n">micro_batch_num</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_moe_config</span> <span class="o">=</span> <span class="n">MoEParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="n">model_parallel</span><span class="p">,</span>
                                             <span class="n">expert_parallel</span><span class="o">=</span><span class="n">expert_parallel</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">recompute</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recompute</span>

    <span class="nd">@recompute</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">recompute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">TransformerRecomputeConfig</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;recompute must be a TransformerRecomputeConfig/bool, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;TransformerRecomputeConfig is recommended as the recompute configuration type.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_recompute</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">vocab_emb_dp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">vocab_emb_dp</span>

    <span class="nd">@vocab_emb_dp</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">vocab_emb_dp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">vocab_emb_dp</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">gradient_aggregation_group</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_aggregation_group</span>

    <span class="nd">@gradient_aggregation_group</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">gradient_aggregation_group</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;gradient_aggregation_group&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gradient_aggregation_group</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">micro_batch_num</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pp_config</span><span class="o">.</span><span class="n">micro_batch_num</span>

    <span class="nd">@micro_batch_num</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">micro_batch_num</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pp_config</span><span class="o">.</span><span class="n">micro_batch_num</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">model_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">model_parallel</span>

    <span class="nd">@model_parallel</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">model_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_moe_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">data_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">data_parallel</span>

    <span class="nd">@data_parallel</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">data_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">data_parallel</span> <span class="o">=</span> <span class="n">value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_moe_config</span><span class="o">.</span><span class="n">data_parallel</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">expert_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_moe_config</span><span class="o">.</span><span class="n">expert_parallel</span>

    <span class="nd">@expert_parallel</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">expert_parallel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_moe_config</span><span class="o">.</span><span class="n">expert_parallel</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">pipeline_stage</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pp_config</span><span class="o">.</span><span class="n">pipeline_stage</span>

    <span class="nd">@pipeline_stage</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">pipeline_stage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_pp_config</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">optimizer_shard</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_shard</span>

    <span class="nd">@optimizer_shard</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">optimizer_shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s2">&quot;optimizer_shard&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer_shard</span> <span class="o">=</span> <span class="n">value</span>
        <span class="n">context</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">embedding_dp_mp_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">dp_mp_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_embed_dp_mp_config</span><span class="o">.</span><span class="n">dp_mp_config</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">moe_parallel_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_moe_config</span></div>


<span class="n">default_transformer_config</span> <span class="o">=</span> <span class="n">TransformerOpParallelConfig</span><span class="p">()</span>
<span class="n">default_embedding_parallel_config</span> <span class="o">=</span> <span class="n">EmbeddingOpParallelConfig</span><span class="p">()</span>


<div class="viewcode-block" id="FeedForward"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.FeedForward">[文档]</a><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The multilayer perceptron with two linear layers with dropout applied at final output. The first linear</span>
<span class="sd">        will project the input dimension from hidden_size to ffn_hidden_size. The second linear will project the</span>
<span class="sd">        dimension from ffn_hidden_size to hidden_size. The first linear is sharded on the relative dimension,</span>
<span class="sd">        and the second linear is sharded on the output dimension. The overview process can be:</span>

<span class="sd">        .. math::</span>
<span class="sd">            Dropout((xW_1+b_1)W_2 + b_2)</span>

<span class="sd">        where the :math:`W_1, W_2, b_1` and :math:`b_2` are trainable parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            hidden_size (int): The dimension of the inputs.</span>
<span class="sd">            ffn_hidden_size (int): The intermediate hidden size.</span>
<span class="sd">            dropout_rate (float): The dropout rate for the second linear&#39;s output.</span>
<span class="sd">            hidden_act (str, nn.Cell): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. User can provide custom activition to the argument.</span>
<span class="sd">                If user wants to run the net in the parallel mode, the custom activation must also provide</span>
<span class="sd">                the `activation_shard` function. Please see examples. Default: gelu.</span>
<span class="sd">            expert_num (int): The number of experts used in Linear. For the case expert_num &gt; 1, BatchMatMul is used</span>
<span class="sd">                and the first dimension in BatchMatMul indicate expert_num. Default: 1.</span>
<span class="sd">            expert_group_size (int): The number of tokens in each data parallel group. Default: None. This parameter is</span>
<span class="sd">                effective only when in AUTO_PARALLEL mode, and NOT SHARDING_PROPAGATION.</span>
<span class="sd">            param_init_type (dtype.Number): The parameter initialization type. Should be mstype.float32 or</span>
<span class="sd">                mstype.float16. Default: mstype.float32.</span>
<span class="sd">            parallel_config (OpParallelConfig, MoEParallelConfig): The config of parallel setting, see</span>
<span class="sd">                `OpParallelConfig` or `MoEParallelConfig`. When MoE is applied, MoEParallelConfig is effective,</span>
<span class="sd">                otherwise OpParallelConfig is effective. Default `default_dpmp_config`,</span>
<span class="sd">                an instance of `OpParallelConfig` with default args.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **x** (Tensor) - should be `[batch, seq_length, hidden_size] or [batch * seq_length, hidden_size]`.</span>
<span class="sd">              Float tensor.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tensor, the output of this layer after mapping. The shape is `[batch, seq_length, hidden_size] or</span>
<span class="sd">            [batch * seq_length, hidden_size]`.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `hidden_act` is not a string or nn.Cell.</span>
<span class="sd">            TypeError: `parallel_config` is not a subclass of OpParallelConfig.</span>
<span class="sd">            ValueError: `ffn_hidden_size` is not a multiple of the model parallel way.</span>
<span class="sd">            ValueError: `hidden_size` is not a multiple of the model parallel way.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import FeedForward</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">            &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">            &gt;&gt;&gt; model = FeedForward(hidden_size=15, ffn_hidden_size=30, dropout_rate=0.1)</span>
<span class="sd">            &gt;&gt;&gt; tensor = Tensor(np.ones((2, 20, 15)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = model(tensor)</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (2, 20, 15)</span>
<span class="sd">            &gt;&gt;&gt; # Example 2 using custom hidden activation</span>
<span class="sd">            &gt;&gt;&gt; class MyActivationNoShard(nn.Cell):</span>
<span class="sd">            &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">            &gt;&gt;&gt;         super(MyActivationNoShard, self).__init__()</span>
<span class="sd">            &gt;&gt;&gt;         self.add = ops.Add()</span>
<span class="sd">            &gt;&gt;&gt;     def construct(self, x):</span>
<span class="sd">            &gt;&gt;&gt;         return self.add(x, 0.1)</span>
<span class="sd">            &gt;&gt;&gt; model = FeedForward(hidden_size=15, ffn_hidden_size=30, dropout_rate=0.1,</span>
<span class="sd">            &gt;&gt;&gt;                     hidden_act=MyActivationNoShard)</span>
<span class="sd">            &gt;&gt;&gt; tensor = Tensor(np.ones((2, 20, 15)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = model(tensor)</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (2, 20, 15)</span>
<span class="sd">            &gt;&gt;&gt; # Example 3 using custom hidden activation with activation_shard</span>
<span class="sd">            &gt;&gt;&gt; # If user wantss to run on the SEMI/AUTO parallel mode, the custom activation must provide</span>
<span class="sd">            &gt;&gt;&gt; # a class function named activation_shard. It accepts the argument parallel_config (OpParallelConfig,</span>
<span class="sd">            &gt;&gt;&gt; # MoEParallelConfig) and set the shard for the primitives used in the construct.</span>
<span class="sd">            &gt;&gt;&gt; class MyActivationWithShard(nn.Cell):</span>
<span class="sd">            &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">            &gt;&gt;&gt;         super(MyActivationWithShard, self).__init__()</span>
<span class="sd">            &gt;&gt;&gt;         self.add = ops.Add()</span>
<span class="sd">            &gt;&gt;&gt;     def construct(self, x):</span>
<span class="sd">            &gt;&gt;&gt;         return self.add(x, 0.1)</span>
<span class="sd">            &gt;&gt;&gt;     def activation_shard(self, parallel_config):</span>
<span class="sd">            &gt;&gt;&gt;         self.add.shard(((parallel_config.data_parallel, parallel_config.model_parallel), ()))</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; model = FeedForward(hidden_size=15, ffn_hidden_size=30, dropout_rate=0.1,</span>
<span class="sd">            &gt;&gt;&gt;                     hidden_act=MyActivationWithShard)</span>
<span class="sd">            &gt;&gt;&gt; tensor = Tensor(np.ones((2, 20, 15)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; output = model(tensor)</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (2, 20, 15)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_LogActionOnce</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;FeedForward&#39;</span><span class="p">,</span>
                    <span class="n">no_warning</span><span class="o">=</span><span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">,))</span>
    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;FeedForward&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">,</span> <span class="n">MoEParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;FeedForward&quot;</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">dropout_rate</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">expert_num</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">expert_group_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FeedForward</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">hidden_act</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">hidden_act</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">hidden_act</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For FeedForward cell, the hidden_act should str type or nn.Cell type, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">hidden_act</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">_is_sharding_propagation</span><span class="p">():</span>
            <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="n">mp</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span>
            <span class="k">if</span> <span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">ep</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">expert_parallel</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ep</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># ffn use less dp than other ops when use_moe, due to there are ops use dp and ep.</span>
            <span class="n">dp</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span> <span class="o">//</span> <span class="n">ep</span>
            <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">mp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;FeedForward&#39;, the class variable &#39;ffn_hidden_size&#39; must be a multiple of the&quot;</span>
                                 <span class="s2">&quot;num of model parallel, but got the ffn_hidden_size is </span><span class="si">{}</span><span class="s2"> and the num of model &quot;</span>
                                 <span class="s2">&quot;parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ffn_hidden_size</span><span class="p">,</span> <span class="n">mp</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">mp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;FeedForward&#39;, the class variable &#39;hidden_size&#39; must be a multiple of the num of &quot;</span>
                                 <span class="s2">&quot;model parallel, but got the hidden_size is </span><span class="si">{}</span><span class="s2"> and the num of model parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                                 <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">mp</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;FeedForward&#39;, the class variable &#39;dropout_rate&#39; must be in the range [0, 1.0), &quot;</span>
                                 <span class="s2">&quot;but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">))</span>
            <span class="n">input_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="n">ffn_hidden_size</span>

            <span class="c1"># Project to ffn_hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
                                   <span class="n">out_channels</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                                   <span class="n">activation</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                   <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                   <span class="n">expert_num</span><span class="o">=</span><span class="n">expert_num</span><span class="p">,</span>
                                   <span class="n">expert_group_size</span><span class="o">=</span><span class="n">expert_group_size</span><span class="p">,</span>
                                   <span class="n">outer_batch</span><span class="o">=</span><span class="n">dp</span><span class="p">,</span>
                                   <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>

            <span class="c1"># Project back to hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                                      <span class="n">out_channels</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
                                      <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                      <span class="n">expert_num</span><span class="o">=</span><span class="n">expert_num</span><span class="p">,</span>
                                      <span class="n">expert_group_size</span><span class="o">=</span><span class="n">expert_group_size</span><span class="p">,</span>
                                      <span class="n">outer_batch</span><span class="o">=</span><span class="n">dp</span><span class="p">,</span>
                                      <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">parallel_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout_4d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="n">mp</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span>
            <span class="k">if</span> <span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">ep</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">expert_parallel</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ep</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># ffn use less dp than other ops when use_moe, due to there are ops use dp and ep.</span>
            <span class="n">dp</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span> <span class="o">//</span> <span class="n">ep</span>
            <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">mp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;FeedForward&#39;, the class variable &#39;ffn_hidden_size&#39; must be a multiple of the&quot;</span>
                                 <span class="s2">&quot;num of model parallel, but got the ffn_hidden_size is </span><span class="si">{}</span><span class="s2"> and the num of model &quot;</span>
                                 <span class="s2">&quot;parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ffn_hidden_size</span><span class="p">,</span> <span class="n">mp</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">mp</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;FeedForward&#39;, the class variable &#39;hidden_size&#39; must be a multiple of the num of &quot;</span>
                                 <span class="s2">&quot;model parallel, but got the hidden_size is </span><span class="si">{}</span><span class="s2"> and the num of model parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                                 <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">mp</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;FeedForward&#39;, the class variable &#39;dropout_rate&#39; must be in the range [0, 1.0), &quot;</span>
                                 <span class="s2">&quot;but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">))</span>
            <span class="n">input_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="n">ffn_hidden_size</span>

            <span class="c1"># Project to ffn_hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
                                   <span class="n">out_channels</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                                   <span class="n">activation</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                   <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                   <span class="n">expert_num</span><span class="o">=</span><span class="n">expert_num</span><span class="p">,</span>
                                   <span class="n">expert_group_size</span><span class="o">=</span><span class="n">expert_group_size</span><span class="p">,</span>
                                   <span class="n">outer_batch</span><span class="o">=</span><span class="n">dp</span><span class="p">,</span>
                                   <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">)),</span>
                                   <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">)),</span>
                                   <span class="n">strategy_activation</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">),))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">)),</span>
                                   <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">mp</span><span class="p">,)),</span>
                                   <span class="n">strategy_activation</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">),))</span>
            <span class="c1"># Project back to hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
                                      <span class="n">out_channels</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
                                      <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                      <span class="n">expert_num</span><span class="o">=</span><span class="n">expert_num</span><span class="p">,</span>
                                      <span class="n">expert_group_size</span><span class="o">=</span><span class="n">expert_group_size</span><span class="p">,</span>
                                      <span class="n">outer_batch</span><span class="o">=</span><span class="n">dp</span><span class="p">,</span>
                                      <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">ep</span><span class="p">,</span> <span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                      <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="n">mp</span><span class="p">),</span> <span class="p">(</span><span class="n">mp</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                      <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">parallel_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3d</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout_4d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout_4d</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="n">ep</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">_check_input_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="c1"># returned shape is [bs, seq_length, ffn_hidden_size] or [bs * seq_length, ffn_hidden_size]</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapping</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="c1"># returned shape is [bs, seq_length, ffn_hidden_size] or [bs * seq_length, ffn_hidden_size]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">output</span><span class="p">))</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_3d</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">output</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_4d</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="AttentionMask"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.AttentionMask">[文档]</a><span class="k">class</span> <span class="nc">AttentionMask</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the Lower triangular matrix from the input mask. The input mask is a 2D tensor (batch_size, seq_length)</span>
<span class="sd">        with 1 and 0, where 1 indicates the current position is a valid token, otherwise not.</span>

<span class="sd">        Args:</span>
<span class="sd">            seq_length(int): The sequence length of the input tensor.</span>
<span class="sd">            parallel_config(OpParallelConfig): The parallel configure. Default `default_dpmp_config`,</span>
<span class="sd">                                               an instance of `OpParallelConfig` with default args.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **input_mask** (Tensor) - The mask indicating whether each position is a valid input with</span>
<span class="sd">              (batch_size, seq_length).</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tensor. The attention mask matrix with shape (batch_size, seq_length, seq_length).</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `seq_length` is not an integer.</span>
<span class="sd">            ValueError: `seq_length` is not a positive value.</span>
<span class="sd">            TypeError: `parallel_config` is not a subclass of OpParallelConfig.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import AttentionMask</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; mask = AttentionMask(seq_length=4)</span>
<span class="sd">            &gt;&gt;&gt; mask_array = np.array([[1, 1, 1, 0]], np.float32)</span>
<span class="sd">            &gt;&gt;&gt; inputs = Tensor(mask_array)</span>
<span class="sd">            &gt;&gt;&gt; res = mask(inputs)</span>
<span class="sd">            &gt;&gt;&gt; print(res)</span>
<span class="sd">            [[[1. 0. 0. 0]</span>
<span class="sd">              [1. 1. 0. 0]</span>
<span class="sd">              [1. 1. 1. 0]</span>
<span class="sd">              [0. 0. 0. 0]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_LogActionOnce</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;AttentionMask&#39;</span><span class="p">,</span>
                    <span class="n">no_warning</span><span class="o">=</span><span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">,))</span>
    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">],</span> <span class="s2">&quot;AttentionMask&quot;</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionMask</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
            <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dim</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">))</span>
        <span class="c1"># Default lower triangle mask matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lower_triangle_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">ones</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multiply</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">):</span>
        <span class="n">_check_input_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_shape_value</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">)</span>
        <span class="n">input_mask</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">input_mask</span><span class="p">)</span>
        <span class="n">shape_right</span> <span class="o">=</span> <span class="p">(</span><span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">shape_left</span> <span class="o">=</span> <span class="n">input_shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
        <span class="c1"># Mask the padded inputs</span>
        <span class="n">mask_left</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="n">shape_left</span><span class="p">)</span>
        <span class="n">mask_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">,</span> <span class="n">shape_right</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">mask_left</span><span class="p">,</span> <span class="n">mask_right</span><span class="p">)</span>
        <span class="n">lower_traiangle</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dim</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lower_triangle_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># the returned shape is [bs, seq_length, seq_length]</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span>
            <span class="n">attention_mask</span><span class="p">,</span> <span class="n">lower_traiangle</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_mask</span></div>


<div class="viewcode-block" id="VocabEmbedding"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.VocabEmbedding">[文档]</a><span class="k">class</span> <span class="nc">VocabEmbedding</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The embedding lookup table from the 0-th dim of the parameter table. When the parallel_config.vocab_emb_dp is</span>
<span class="sd">        True and in the `AUTO_PARALLEL` mode, the embedding lookup will be trained by the data parallel way, as the</span>
<span class="sd">        parameters will be repeated on each device. If false, the embedding table will be sharded into n parts at</span>
<span class="sd">        the 0-th dimension of the embedding table, where the n is the model parallel way determined by</span>
<span class="sd">        `parallel_config.model_parallel` (EmbeddingOpParallelConfig).</span>

<span class="sd">        Note:</span>
<span class="sd">            When `AUTO_PARALLEL` or `SEMI_AUTO_PARALLEL` mode is enabled, this layer support only 2-d dimension inputs,</span>
<span class="sd">            as the shard is designed for 2d inputs.</span>

<span class="sd">        Args:</span>
<span class="sd">            vocab_size (int): Size of the dictionary of embeddings.</span>
<span class="sd">            embedding_size (int): The size of each embedding vector.</span>
<span class="sd">            parallel_config (EmbeddingOpParallelConfig): The parallel config of network. Default</span>
<span class="sd">                `default_embedding_parallel_config`, an instance of `EmbeddingOpParallelConfig` with default args.</span>
<span class="sd">            param_init (Union[Tensor, str, Initializer, numbers.Number]): Initializer for the embedding_table.</span>
<span class="sd">                Refer to class `initializer` for the values of string when a string</span>
<span class="sd">                is specified. Default: &#39;normal&#39;.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **input_ids** (Tensor) - The tokenized inputs with datatype int32 with shape (batch_size, seq_length)</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tuple, a tuple contains (`output`, `embedding_table`)</span>

<span class="sd">            - **output** (Tensor) - The embedding vector for the input with shape (batch_size,</span>
<span class="sd">              seq_length, embedding_size).</span>
<span class="sd">            - **embedding_table** (Tensor) - The embedding table with shape (vocab_size, embedding_size).</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If the parallel_config.vocab_emb_dp is True, the vocab size is not a multiple of</span>
<span class="sd">                parallel_config.model_parallel</span>
<span class="sd">            ValueError: `vocab_size` is not a positive value.</span>
<span class="sd">            ValueError: `embedding_size` is not a positive value.</span>
<span class="sd">            TypeError: `parallel_config` is not a subclass of OpParallelConfig.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import VocabEmbedding</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; model = VocabEmbedding(vocab_size=30, embedding_size=30)</span>
<span class="sd">            &gt;&gt;&gt; tensor = Tensor(np.ones((20, 15)), mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; output, table = model(tensor)</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (20, 15, 30)</span>
<span class="sd">            &gt;&gt;&gt; print(table.shape)</span>
<span class="sd">            (30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_LogActionOnce</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;VocabEmbedding&#39;</span><span class="p">,</span>
                    <span class="n">no_warning</span><span class="o">=</span><span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">,))</span>
    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">embedding_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">EmbeddingOpParallelConfig</span><span class="p">],</span> <span class="s2">&quot;VocabEmbedding&quot;</span><span class="p">))</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_embedding_parallel_config</span><span class="p">,</span>
                 <span class="n">param_init</span><span class="o">=</span><span class="s1">&#39;normal&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VocabEmbedding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="n">param_init</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">]),</span>
                                         <span class="n">name</span><span class="o">=</span><span class="s1">&#39;embedding_table&#39;</span><span class="p">,</span> <span class="n">parallel_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">vocab_emb_dp</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gather</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="si">}</span><span class="s2"> data parallel for the embedding lookup.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The vocab size of the embedding </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2"> must be a &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;multiple of parallel_config.model_parallel </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gather</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="si">}</span><span class="s2"> data parallel and </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="si">}</span><span class="s2"> &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;model parallel for the embedding lookup.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
        <span class="n">_check_input_shape</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_ids</span><span class="p">),</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">input_ids</span><span class="p">),</span> <span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table</span><span class="o">.</span><span class="n">value</span><span class="p">()</span></div>


<div class="viewcode-block" id="MultiHeadAttention"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.MultiHeadAttention">[文档]</a><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This is an implementation of multihead attention in the paper `Attention is all you need</span>
<span class="sd">        &lt;https://arxiv.org/pdf/1706.03762v5.pdf&gt;`_. Given the query vector with source length, and the</span>
<span class="sd">        key and value vector with target length, the attention will be performed as the following</span>

<span class="sd">        .. math::</span>
<span class="sd">               MultiHeadAttention(query, key, vector) = Concat(head_1, \dots, head_h)W^O</span>

<span class="sd">        where :math:`head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)`. The default is with a bias.</span>

<span class="sd">        if query, key and value tensor is same, then it will be self attention.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_size(int): The batch size of the input tensor when do increnmental prediction. Should be a positive</span>
<span class="sd">                value. When do training or prediction, the argument will not work and the user can just pass None to</span>
<span class="sd">                the argument.</span>
<span class="sd">            src_seq_length(int): The sequence length of the query vector.</span>
<span class="sd">            tgt_seq_length(int): The sequence length of the key and value vector.</span>
<span class="sd">            hidden_size(int): The hidden size of the input.</span>
<span class="sd">            num_heads(int): The number of the heads.</span>
<span class="sd">            hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1.</span>
<span class="sd">            attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1.</span>
<span class="sd">            compute_dtype(dtype.Number): The computation type of dense. Default mstype.float16.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16.</span>
<span class="sd">            softmax_compute_type(dtype.Number): The type of softmax computation module. Default mstype.float32.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16.</span>
<span class="sd">            param_init_type(dtype.Number): The parameter initialization type of the module. Default mstype.float32.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16.</span>
<span class="sd">            use_past(bool): Use the past state to compute, used for incremental prediction. For example, if we have two</span>
<span class="sd">                words and want to generate the ten more words. We just need to compute the two words&#39; state only once,</span>
<span class="sd">                and generate the next word one by one. When use_past is True, there are two steps to run the prediction.</span>
<span class="sd">                In the first step, set the is_first_iteration to be True by</span>
<span class="sd">                `model.add_flags_recursive(is_first_iteration=True)`, and pass the full inputs. Then, set the</span>
<span class="sd">                is_first_iteration to be False by `model.add_flags_recursive(is_first_iteration=False)`. At this moment,</span>
<span class="sd">                pass the single step&#39;s input tensor, and loop it. Default False.</span>
<span class="sd">            parallel_config(OpParallelConfig): The parallel configure. Default `default_dpmp_config`,</span>
<span class="sd">                an instance of `OpParallelConfig` with default args.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **query_tensor** (Tensor) - The query vector with shape (batch_size, src_seq_length, hidden_size) or</span>
<span class="sd">              (batch_size * src_seq_length, hidden_size), if the use_past is False or is_first_iteration=True.</span>
<span class="sd">              Otherwise, must be (batch_size, 1, hidden_size)</span>
<span class="sd">            - **key_tensor** (Tensor) - The key vector with shape (batch_size, tgt_seq_length, hidden_size) or</span>
<span class="sd">              (batch_size * tgt_seq_length, hidden_size), if the use_past is False or is_first_iteration=True.</span>
<span class="sd">              Otherwise, must be (batch_size, 1, hidden_size)</span>
<span class="sd">            - **value_tensor** (Tensor) - The value vector with shape (batch_size, tgt_seq_length, hidden_size) or</span>
<span class="sd">              (batch_size * tgt_seq_length, hidden_size), if the use_past is False or is_first_iteration=True.</span>
<span class="sd">              Otherwise, must be (batch_size, 1, hidden_size)</span>
<span class="sd">            - **attention_mask** (Tensor) - If the use_past is False or is_first_iteration=True, the attention mask</span>
<span class="sd">              matrix should ba (batch_size, src_seq_length, tgt_seq_length), or None. None means there will be no mask</span>
<span class="sd">              in softmax computation. Otherwise, the mask must be (batch_size, 1, tgt_seq_length)</span>
<span class="sd">            - **key_past** (Tensor) - Float16 tensor with shape (batch_size, num_heads, size_per_head, tgt_seq_length).</span>
<span class="sd">              The past calculated key vector. Used for incremental prediction when the use_past is True.</span>
<span class="sd">              Default None.</span>
<span class="sd">            - **value_past** (Tensor) - Float16 tensor with shape</span>
<span class="sd">              (batch_size, num_heads, tgt_seq_length, size_per_head).</span>
<span class="sd">              The past calculated value vector. Used for incremental prediction when the use_past is True.</span>
<span class="sd">              Default None.</span>
<span class="sd">            - **batch_valid_length** (Tensor) - Int32 tensor with shape (batch_size,) the past calculated the index.</span>
<span class="sd">              Used for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tuple, a tuple contains(`output`, `layer_present`)</span>

<span class="sd">            - **output** (Tensor) - Tensor, the float tensor of the output of the layer with</span>
<span class="sd">              shape (batch_size, src_seq_length, hidden_size) or (batch_size * src_seq_length, hidden_size),</span>
<span class="sd">              if the use_past is False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size).</span>

<span class="sd">            - **layer_present** (Tuple) - A tuple of the Tensor of the projected key and value vector with</span>
<span class="sd">              ((batch_size, num_heads, size_per_head, tgt_seq_length),</span>
<span class="sd">              (batch_size, num_heads, tgt_seq_length, size_per_head)).</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import MultiHeadAttention</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; model = MultiHeadAttention(batch_size=None, hidden_size=15, src_seq_length=20, tgt_seq_length=20,</span>
<span class="sd">            ...                            num_heads=3)</span>
<span class="sd">            &gt;&gt;&gt; from_tensor = Tensor(np.ones((2, 20, 15)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; to_tensor = Tensor(np.ones((2, 20, 15)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; attention_mask = Tensor(np.ones((2, 20, 20)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; attn_out, past = model(from_tensor, to_tensor, to_tensor, attention_mask)</span>
<span class="sd">            &gt;&gt;&gt; print(attn_out.shape)</span>
<span class="sd">            (2, 20, 15)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">            (2, 3, 5, 20)</span>
<span class="sd">            &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">            (2, 3, 20, 5)</span>
<span class="sd">            &gt;&gt;&gt; # When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="sd">            &gt;&gt;&gt; # Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="sd">            &gt;&gt;&gt; # We need to prepare the memory parameters for saving key and value states firstly.</span>
<span class="sd">            &gt;&gt;&gt; model = MultiHeadAttention(batch_size=2, hidden_size=15, src_seq_length=20, tgt_seq_length=20,</span>
<span class="sd">            ...                            num_heads=3, use_past=True)</span>
<span class="sd">            &gt;&gt;&gt; key_past = Tensor(np.zeros(shape=(2, 3, 5, 20)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; value_past = Tensor(np.zeros(shape=(2, 3, 20, 5)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; batch_valid_length = Tensor(np.ones((2,)), mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; # Set is_first_iteration=True to generate the full memory states</span>
<span class="sd">            &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=True)</span>
<span class="sd">            &gt;&gt;&gt; attn_out, past = model(from_tensor, to_tensor, to_tensor, attention_mask, key_past, value_past,</span>
<span class="sd">            ...                        batch_valid_length)</span>
<span class="sd">            &gt;&gt;&gt; print(attn_out.shape)</span>
<span class="sd">            (2, 20, 15)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">            (2, 3, 5, 20)</span>
<span class="sd">            &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">            (2, 3, 20, 5)</span>
<span class="sd">            &gt;&gt;&gt; from_tensor = Tensor(np.ones((2, 1, 15)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; to_tensor = Tensor(np.ones((2, 1, 15)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; attention_mask = Tensor(np.ones((2, 1, 20)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; # Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than the</span>
<span class="sd">            &gt;&gt;&gt; # full sequence.</span>
<span class="sd">            &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=False)</span>
<span class="sd">            &gt;&gt;&gt; attn_out, past = model(from_tensor, to_tensor, to_tensor, attention_mask, key_past, value_past,</span>
<span class="sd">            ...                        batch_valid_length)</span>
<span class="sd">            &gt;&gt;&gt; print(attn_out.shape)</span>
<span class="sd">            (2, 1, 15)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">            (2, 3, 5, 20)</span>
<span class="sd">            &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">            (2, 3, 20, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_LogActionOnce</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;MultiHeadAttention&#39;</span><span class="p">,</span>
                    <span class="n">no_warning</span><span class="o">=</span><span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">,))</span>
    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">compute_dtype</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                  <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;MultiHeadAttention&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">compute_dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_ascend</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Ascend&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dp</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_parallel_mode</span> <span class="o">=</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">ParallelMode</span><span class="o">.</span><span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">_is_sharding_propagation</span><span class="p">():</span>
            <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="k">if</span> <span class="n">hidden_dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">hidden_dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;MultiHeadAttention&#39;, the class variable &#39;hidden_dropout_rate&#39; must be &quot;</span>
                                 <span class="s2">&quot;in range [0, 1.0), but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_dropout_rate</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">attention_dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">attention_dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;MultiHeadAttention&#39;, the class variable &#39;attention_dropout_rate&#39; must be &quot;</span>
                                 <span class="s2">&quot;in range [0, 1.0), but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attention_dropout_rate</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;MultiHeadAttention&#39;, the class variable &#39;hidden_size&#39; must be a multiple &quot;</span>
                                 <span class="s2">&quot;of &#39;num_heads&#39;, but got the hidden_size is </span><span class="si">{}</span><span class="s2"> and the num_heads is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                                 <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;MultiHeadAttention&#39;, the class variable &#39;num_heads&#39; must be a multiple of &quot;</span>
                                 <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the num_heads is </span><span class="si">{}</span><span class="s2"> &quot;</span>
                                 <span class="s2">&quot;and the parallel_config.model_parallel  is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                                 <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># Output layer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                      <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                      <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                      <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                                      <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span>
                                  <span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                                   <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">parallel_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">merger_head_transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">num_heads</span>
            <span class="c1"># embedding size per head</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">concat_k</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">concat_v</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">multiply_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span>
                <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span>
            <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
            <span class="c1"># Normalize factor for attention, sqrt(dk) as widely used</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hidden_dropout_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_dropout_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">softmax_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_3d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">softmax_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>

            <span class="c1"># Query</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
            <span class="c1"># Key</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
            <span class="c1"># Value</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">compute_dtype</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_dtype</span> <span class="o">=</span> <span class="n">softmax_compute_type</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
                <span class="c1"># operators used for state reuse</span>
                <span class="n">seq_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">src_seq_length</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">range</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">seq_range</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">))),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tensor_le</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,),</span> <span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">less</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="k">if</span> <span class="n">hidden_dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">hidden_dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;MultiHeadAttention&#39;, the class variable &#39;hidden_dropout_rate&#39; must be &quot;</span>
                                 <span class="s2">&quot;in range [0, 1.0), but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_dropout_rate</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">attention_dropout_rate</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">attention_dropout_rate</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;MultiHeadAttention&#39;, the class variable &#39;attention_dropout_rate&#39; must be &quot;</span>
                                 <span class="s2">&quot;in range [0, 1.0), but got the value : </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attention_dropout_rate</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;MultiHeadAttention&#39;, the class variable &#39;hidden_size&#39; must be a multiple &quot;</span>
                                 <span class="s2">&quot;of &#39;num_heads&#39;, but got the hidden_size is </span><span class="si">{}</span><span class="s2"> and the num_heads is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                                 <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;MultiHeadAttention&#39;, the class variable &#39;num_heads&#39; must be a multiple of &quot;</span>
                                 <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the num_heads is </span><span class="si">{}</span><span class="s2"> &quot;</span>
                                 <span class="s2">&quot;and the parallel_config.model_parallel  is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                                 <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># Output layer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                      <span class="n">out_channels</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                      <span class="n">transpose_b</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                      <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                                      <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span>
                                  <span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                                   <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">parallel_optimizer</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
                <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">merger_head_transpose</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
                <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">num_heads</span>
            <span class="c1"># embedding size per head</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">concat_k</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">concat_v</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">multiply_data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span>
                <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span>
            <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">BatchMatMul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
                <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                 <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">RealDiv</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
                <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sub</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
                <span class="p">((</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
                <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
                <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                 <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="c1"># Normalize factor for attention, sqrt(dk) as widely used</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">hidden_dropout_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_dropout_rate</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prob_dropout</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span>
                <span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">softmax_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_3d</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">()</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">softmax_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_3d</span><span class="o">.</span><span class="n">softmax</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

            <span class="c1"># Query</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                              <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                             <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,)))</span>
            <span class="c1"># Key</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                              <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                             <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,)))</span>

            <span class="c1"># Value</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">compute_dtype</span><span class="o">=</span><span class="n">compute_dtype</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span><span class="o">.</span><span class="n">shard</span><span class="p">(</span><span class="n">strategy_matmul</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                              <span class="n">strategy_bias</span><span class="o">=</span><span class="p">((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span>
                                             <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">,)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">compute_dtype</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_dtype</span> <span class="o">=</span> <span class="n">softmax_compute_type</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
                <span class="c1"># operators used for state reuse</span>
                <span class="n">seq_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">src_seq_length</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">range</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">seq_range</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">))),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tensor_le</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Sub</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,),</span> <span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">less</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">value_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_inputs</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="p">,</span>
                           <span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="n">ori_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">)</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_batch_size_from_query</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">)</span>
        <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_convert_to_2d_tensor</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span>
                                                                            <span class="n">key_tensor</span><span class="p">,</span>
                                                                            <span class="n">value_tensor</span><span class="p">,</span>
                                                                            <span class="n">attention_mask</span><span class="p">)</span>
        <span class="n">ori_dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">)</span>
        <span class="n">query_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">key_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># multi head attention: query, key, value are derived from the same inputs</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense1</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense2</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense3</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">)</span>
        <span class="c1"># the returned shape is [bs, num_heads, seq_length, size_per_head]</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">query</span><span class="p">,</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_seq_length_under_incremental</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">),</span>
                 <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="c1"># the returned shape is [bs, size_per_head, seq_length, num_heads]</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_seq_length_under_incremental</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">),</span>
                      <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># the returned shape is [bs, num_heads, seq_length, size_per_head]</span>
        <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span>
            <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="n">value</span><span class="p">,</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_seq_length_under_incremental</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">),</span>
                 <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">)),</span>
            <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="c1"># support input shape is [bs, seq, seq] or [bs, heads, seq, seq]</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">))</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># expand attention mask from [bs, seq, seq] -&gt; [bs, 1, seq, seq]</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># key and value for current token(s)</span>
        <span class="n">key_present</span> <span class="o">=</span> <span class="n">key</span>
        <span class="n">value_present</span> <span class="o">=</span> <span class="n">value</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># The first graph with the input size of (bs, seq_length)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">:</span>
                <span class="c1"># Get the valid input length without padding</span>
                <span class="n">valid_length_vector</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">less</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">range</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="c1"># Cover the key and value numbers corresponding to the padding position</span>
                <span class="n">key_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
                <span class="n">value_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
            <span class="c1"># The second graph with the inpus size of (bs, 1)</span>
            <span class="c1"># the shape of query is (bs, num_heads, 1, size_per_head)</span>
            <span class="c1"># the shape of key is   (bs, num_heads, size_per_head, 1)</span>
            <span class="c1"># the shape of value is (bs, num_heads, 1, size_per_head)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Get the current token position index</span>
                <span class="n">valid_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">key_past</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                                                                               <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                                                                <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">),</span>
                                                                               <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                                                    <span class="mi">0</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
                <span class="n">valid_length</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">valid_length</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="n">valid_length_vector</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">valid_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">range</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="c1"># Pad the key and value to seq_length with only the position index not zero</span>
                <span class="n">current_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">)),</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
                <span class="n">current_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                          <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">valid_length_vector</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
                <span class="c1"># Concat the previous saved state and current state</span>
                <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">key_past</span><span class="p">,</span> <span class="n">current_key</span><span class="p">)</span>
                <span class="n">value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">value_past</span><span class="p">,</span> <span class="n">current_value</span><span class="p">)</span>
                <span class="c1"># Update key_present and value_present for state update</span>
                <span class="n">key_present</span> <span class="o">=</span> <span class="n">key</span>
                <span class="n">value_present</span> <span class="o">=</span> <span class="n">value</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">layer_present</span> <span class="o">=</span> <span class="p">(</span><span class="n">key_present</span><span class="p">,</span> <span class="n">value_present</span><span class="p">)</span>
        <span class="c1"># multi head attention considering attention mask</span>
        <span class="c1"># the return shape is [bs * seq_length, hidden_size]</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
        <span class="c1"># Output</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">ori_shape</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">ori_dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span>

    <span class="k">def</span> <span class="nf">_get_batch_size_from_query</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Get the batch size from query tensor&quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># For the incremental prediction, the seq length for the input is 1.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">batch_size</span>

    <span class="k">def</span> <span class="nf">_get_seq_length_under_incremental</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Return the length of the tensor.</span>
<span class="sd">            For the incremental prediction, the seq length for the input is 1.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">length</span>
        <span class="k">return</span> <span class="mi">1</span>

    <span class="k">def</span> <span class="nf">_check_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">value_past</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check inputs&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">):</span>
            <span class="n">_check_shape_equal_without_batch</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">),</span> <span class="s2">&quot;query_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                             <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
            <span class="n">_check_shape_equal_without_batch</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">),</span> <span class="s2">&quot;key_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                             <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
            <span class="n">_check_shape_equal_without_batch</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">),</span> <span class="s2">&quot;value_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                             <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                   <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">),</span> <span class="s2">&quot;query_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">),</span> <span class="s2">&quot;key_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">),</span> <span class="s2">&quot;value_tensor&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                   <span class="p">[[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]])</span>

        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">),</span> <span class="s2">&quot;query_tensor&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">),</span> <span class="s2">&quot;key_tensor&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">),</span> <span class="s2">&quot;value_tensor&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">key_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key_past</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">value_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value_past</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">batch_valid_length_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">key_is_default</span> <span class="o">=</span> <span class="n">key_past</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">value_is_default</span> <span class="o">=</span> <span class="n">value_past</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">batch_is_default</span> <span class="o">=</span> <span class="n">batch_valid_length</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;key_past&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">key_is_tensor</span><span class="p">,</span>
                                    <span class="n">key_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;value_past&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">value_is_tensor</span><span class="p">,</span>
                                    <span class="n">value_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                    <span class="n">batch_valid_length_is_tensor</span><span class="p">,</span> <span class="n">batch_is_default</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_past</span><span class="p">),</span> <span class="s2">&quot;key_past&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">key_past</span><span class="p">),</span> <span class="s2">&quot;key_past&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">value_past</span><span class="p">),</span> <span class="s2">&quot;value_past&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size_per_head</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">value_past</span><span class="p">),</span> <span class="s2">&quot;value_past&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_convert_to_2d_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;convert a nd tensor to a 2d tensor&quot;&quot;&quot;</span>
        <span class="n">query_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">)</span>
        <span class="n">query_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">query_tensor</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">query_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">key_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">)</span>
        <span class="n">key_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">key_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">value_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">)</span>
        <span class="n">value_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value_tensor</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">value_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="k">return</span> <span class="n">query_tensor</span><span class="p">,</span> <span class="n">key_tensor</span><span class="p">,</span> <span class="n">value_tensor</span>

    <span class="k">def</span> <span class="nf">_merge_heads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        convert a 4d input to a 2d output</span>

<span class="sd">        Inputs:</span>
<span class="sd">            x: input tensor</span>

<span class="sd">        Output:</span>
<span class="sd">            x_merge: the 2d output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">merger_head_transpose</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># bs, seq_length, head, size_per_head</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">new_shape</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">x_merge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_merge</span>

    <span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_scores</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        For the consideration of the performance, do softmax according to different situations</span>
<span class="sd">        :param attention_scores: a 3d tensor before softmax</span>
<span class="sd">        :return: the attention scores.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_ascend</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_ascend</span><span class="p">:</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
            <span class="c1"># attention probs</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_3d</span><span class="p">(</span>
                <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span>
                          <span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
            <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_probs</span>

    <span class="k">def</span> <span class="nf">_attn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the weighted score along the seq_length</span>

<span class="sd">        Inputs:</span>
<span class="sd">            query: the query matrix</span>
<span class="sd">            key: the key matrix</span>
<span class="sd">            value: the value matrix</span>
<span class="sd">            attention_mask: the attention mask matrix with shape (batch_size,</span>
<span class="sd">            1, seq_length, seq_length)</span>
<span class="sd">        Outputs:</span>
<span class="sd">            weighted_values: Tensor, the weighted sum scores</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Normalize query and key before MatMul, default off</span>
        <span class="c1"># Attention score [bs, num_heads, seq_length, seq_length]</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_factor</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">query</span><span class="p">))</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">factor</span><span class="p">)</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_div</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">factor</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

        <span class="n">ori_dtype</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">score</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_dtype</span><span class="p">)</span>

        <span class="c1"># for input size of (bs, 1) namely the second graph,</span>
        <span class="c1"># the shape of attention_mask matrix should be (bs, 1, 1, seq_length)</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">:</span>
                <span class="c1"># Calculate the current total token</span>
                <span class="n">current_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                                                                                <span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">query</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                                                                                 <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">),</span>
                                                                                <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                                                                     <span class="mi">0</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
                <span class="c1"># Get the precise position index</span>
                <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub1</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">current_index</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">index</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
                <span class="c1"># Calculate the attention_mask matrix via the position index</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tensor_le</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">range</span><span class="p">,</span> <span class="n">index</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
                <span class="n">attention_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="c1"># Minus 10000 for the position where masked to exclude them from softmax</span>
            <span class="n">multiplu_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span>
                <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">F</span><span class="o">.</span><span class="n">tuple_to_array</span><span class="p">((</span><span class="mf">1.0</span><span class="p">,)),</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">attention_scores</span><span class="p">)),</span>
                <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">P</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">attention_scores</span><span class="p">)))</span>

            <span class="n">adder</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">multiplu_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">multiply_data</span><span class="p">)</span>
            <span class="n">attention_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">adder</span><span class="p">,</span> <span class="n">attention_scores</span><span class="p">)</span>

        <span class="c1"># attention probs</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">)</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">ori_dtype</span><span class="p">)</span>

        <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob_dropout</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>
        <span class="c1"># Weighted sum output [bs, num_heads, seq_length, size_per_head]</span>
        <span class="n">weighted_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_matmul</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="n">attention_merge</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_merge_heads</span><span class="p">(</span><span class="n">weighted_values</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attention_merge</span></div>


<div class="viewcode-block" id="TransformerEncoderLayer"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.TransformerEncoderLayer">[文档]</a><span class="k">class</span> <span class="nc">TransformerEncoderLayer</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transformer Encoder Layer. This is an implementation of the single layer of the transformer</span>
<span class="sd">        encoder layer, including multihead attention and feedward layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_size(int): The batch size of the input tensor when do increnmental prediction. Should be a positive</span>
<span class="sd">                value. When do training or prediction, the argument will not work and the user can just pass None to</span>
<span class="sd">                the argument.</span>
<span class="sd">            hidden_size(int): The hidden size of the input.</span>
<span class="sd">            ffn_hidden_size(int): The hidden size of bottleneck in the feedforward layer.</span>
<span class="sd">            num_heads(int): The number of the heads.</span>
<span class="sd">            seq_length(int): The input sequence length.</span>
<span class="sd">            attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1.</span>
<span class="sd">            hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1.</span>
<span class="sd">            post_layernorm_residual(bool): Do residuals adds before the layernorm. Default False.</span>
<span class="sd">            layernorm_compute_type(dtype.Number): The computation type of the layernorm.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16. Default mstype.float32.</span>
<span class="sd">            softmax_compute_type(dtype.Number): The computation type of the softmax in the attention.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16. Default mstype.float32.</span>
<span class="sd">            param_init_type(dtype.Number): The parameter initialization type of the module.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16. Default mstype.float32.</span>
<span class="sd">            hidden_act (str, nn.Cell): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. User can provide custom activition to the argument.</span>
<span class="sd">                If user wants to run the net in the parallel mode, the custom activation must also provide</span>
<span class="sd">                the `activation_shard` function. Please see the examples of the</span>
<span class="sd">                class:`mindspore.nn.transformer.FeedForward`. Default: gelu.</span>
<span class="sd">            use_past(bool): Use the past state to compute, used for incremental prediction. For example, if we have two</span>
<span class="sd">                words and want to generate the ten more words. We just need to compute the two words&#39; state only once,</span>
<span class="sd">                and generate the next word one by one. When use_past is True, there are two steps to run the prediction.</span>
<span class="sd">                In the first step, set the is_first_iteration to be True by</span>
<span class="sd">                `model.add_flags_recursive(is_first_iteration=True)`, and pass the full inputs. Then, set the</span>
<span class="sd">                is_first_iteration to be False by `model.add_flags_recursive(is_first_iteration=False)`.</span>
<span class="sd">                At this moment, pass the single step&#39;s input tensor, and loop it. Default False.</span>
<span class="sd">            moe_config(MoEConfig): The configuration of MoE (Mixture of Expert). Default is an instance of MoEConfig</span>
<span class="sd">                with default values. Please see `MoEConfig`.</span>
<span class="sd">            parallel_config(OpParallelConfig, MoEParallelConfig): The parallel configure. When MoE is applied,</span>
<span class="sd">                MoEParallelConfig is effective, otherwise OpParallelConfig is effective. Default `default_dpmp_config`,</span>
<span class="sd">                an instance of `OpParallelConfig` with default args.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **x** (Tensor) - Float Tensor, shape should be [batch_size, seq_length, hidden_size] or</span>
<span class="sd">              [batch_size * seq_length, hidden_size], if the use_past is False or is_first_iteration=True. Otherwise,</span>
<span class="sd">              should be [batch_size, 1, hidden_size]</span>
<span class="sd">            - **input_mask** (Tensor) - Float Tensor, If the use_past is False or is_first_iteration=True,</span>
<span class="sd">              the attention mask matrix should ba [batch_size, seq_length, seq_length], or None. None means there will</span>
<span class="sd">              be no mask in softmax computation. Otherwise, should be [batch_size, 1, hidden_size]</span>
<span class="sd">            - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and</span>
<span class="sd">              past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.</span>
<span class="sd">            - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index.</span>
<span class="sd">              Used for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tuple, a tuple contains(`output`, `layer_present`).</span>

<span class="sd">            - **output** (Tensor) - The float tensor of the output of the layer with</span>
<span class="sd">              shape (batch_size, seq_length, hidden_size) or (batch_size * seq_length, hidden_size), if the use_past is</span>
<span class="sd">              False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size)</span>

<span class="sd">            - **layer_present** (Tuple) - A tuple of the Tensor of the projected key and value vector with</span>
<span class="sd">              ((batch_size, num_heads, size_per_head, seq_length),</span>
<span class="sd">              (batch_size, num_heads, seq_length, size_per_head)).</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import TransformerEncoderLayer</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; model = TransformerEncoderLayer(batch_size=2, hidden_size=8, ffn_hidden_size=64, seq_length=16,</span>
<span class="sd">            ...                                 num_heads=2)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 16, 8)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_mask = Tensor(np.ones((2, 16, 16)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; output, past = model(encoder_input_value, encoder_input_mask)</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (2, 16, 8)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">            (2, 2, 4, 16)</span>
<span class="sd">            &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">            (2, 2, 16, 4)</span>
<span class="sd">            &gt;&gt;&gt; # When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="sd">            &gt;&gt;&gt; # Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="sd">            &gt;&gt;&gt; batch_valid_length = Tensor(np.ones((2,)), mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; init_reset = Tensor([True], mstype.bool_)</span>
<span class="sd">            &gt;&gt;&gt; # Set is_first_iteration=True to generate the full memory states</span>
<span class="sd">            &gt;&gt;&gt; model = TransformerEncoderLayer(batch_size=2, hidden_size=8, ffn_hidden_size=64, seq_length=16,</span>
<span class="sd">            ...                                 num_heads=2, use_past=True)</span>
<span class="sd">            &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=True)</span>
<span class="sd">            &gt;&gt;&gt; hidden, past = model(encoder_input_value, encoder_input_mask, init_reset, batch_valid_length)</span>
<span class="sd">            &gt;&gt;&gt; print(hidden.shape)</span>
<span class="sd">            (2, 16, 8)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">            (2, 2, 4, 16)</span>
<span class="sd">            &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">            (2, 2, 16, 4)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 1, 8)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_mask = Tensor(np.ones((2, 1, 16)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; init_reset = Tensor([False], mstype.bool_)</span>
<span class="sd">            &gt;&gt;&gt; # Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than</span>
<span class="sd">            &gt;&gt;&gt; # the full sequence.</span>
<span class="sd">            &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=False)</span>
<span class="sd">            &gt;&gt;&gt; hidden, past = model(encoder_input_value, encoder_input_mask, init_reset, batch_valid_length)</span>
<span class="sd">            &gt;&gt;&gt; print(hidden.shape)</span>
<span class="sd">            (2, 1, 8)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">            (2, 2, 4, 16)</span>
<span class="sd">            &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">            (2, 2, 16, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_LogActionOnce</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;TransformerEncoderLayer&#39;</span><span class="p">,</span>
                    <span class="n">no_warning</span><span class="o">=</span><span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">,))</span>
    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                           <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">,</span> <span class="n">MoEParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;TransformerEncoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">seq_length</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">or</span> <span class="n">use_past</span><span class="p">:</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">_is_sharding_propagation</span><span class="p">():</span>
            <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;For &#39;TransformerEncoderLayer&#39;, the class variable &#39;num_heads&#39; must be divisibled by the &quot;</span>
                    <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the num_heads is </span><span class="si">{}</span><span class="s2"> and &quot;</span>
                    <span class="s2">&quot;parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;For &#39;TransformerEncoderLayer&#39;, the class variable &#39;hidden_size&#39; must be divisibled by &quot;</span>
                    <span class="s2">&quot;the &#39;parallel_config.model_parallel&#39;, but got the hidden_size is </span><span class="si">{}</span><span class="s2"> and parallel_config.&quot;</span>
                    <span class="s2">&quot; model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;For &#39;TransformerEncoderLayer&#39;, the class variable &#39;ffn_hidden_size&#39; must be divisibled &quot;</span>
                    <span class="s2">&quot;by the &#39;parallel_config.model_parallel&#39;, but got the ffn_hidden_size is </span><span class="si">{}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;and parallel_config. model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ffn_hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>

            <span class="n">attention_parallel_config</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">dpmp</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="k">else</span> <span class="n">parallel_config</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">attention_parallel_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                  <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                  <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                  <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                  <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Feed Forward Network, FFN</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                          <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                          <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                          <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                          <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                          <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span> <span class="o">=</span> <span class="n">post_layernorm_residual</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
                <span class="c1"># operator used for state reuse</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="n">size_per_head</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">)</span>
                <span class="c1"># parameters saving key and value states</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;key_past&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value_past&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">assign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Assign</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">elif</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;For &#39;TransformerEncoderLayer&#39;, the class variable &#39;num_heads&#39; must be divisibled by the &quot;</span>
                    <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the num_heads is </span><span class="si">{}</span><span class="s2"> and &quot;</span>
                    <span class="s2">&quot;parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;For &#39;TransformerEncoderLayer&#39;, the class variable &#39;hidden_size&#39; must be divisibled by &quot;</span>
                    <span class="s2">&quot;the &#39;parallel_config.model_parallel&#39;, but got the hidden_size is </span><span class="si">{}</span><span class="s2"> and parallel_config.&quot;</span>
                    <span class="s2">&quot; model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;For &#39;TransformerEncoderLayer&#39;, the class variable &#39;ffn_hidden_size&#39; must be divisibled &quot;</span>
                    <span class="s2">&quot;by the &#39;parallel_config.model_parallel&#39;, but got the ffn_hidden_size is </span><span class="si">{}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;and parallel_config. model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ffn_hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

            <span class="n">attention_parallel_config</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">dpmp</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="k">else</span> <span class="n">parallel_config</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">attention_parallel_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                  <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                  <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                  <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                  <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Feed Forward Network, FFN</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                          <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                          <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                          <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                          <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                          <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span> <span class="o">=</span> <span class="n">post_layernorm_residual</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
                <span class="c1"># operator used for state reuse</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="n">size_per_head</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">)</span>
                <span class="c1"># parameters saving key and value states</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;key_past&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value_past&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">assign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Assign</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> only support sharding propagation or &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;semi-auto parallel mode now.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">input_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_input</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># indicate whether reset saved states</span>
        <span class="n">key_reset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">value_reset</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># reset states, init_reset True for reuse and False for reset</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="n">key_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="n">value_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="n">attention</span><span class="p">,</span> <span class="n">layer_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="c1"># For post-layernorm the inputs for residual path are output of self-attention and output of layernorm</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>
        <span class="c1"># For pre-layernorm the inputs for residual path are output of self-attention and input of this layer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>

        <span class="n">output_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">aux_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="n">mlp_logit</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>

        <span class="n">value_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">key_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># current key and value</span>
            <span class="n">key_present</span><span class="p">,</span> <span class="n">value_present</span> <span class="o">=</span> <span class="n">layer_present</span>
            <span class="c1"># update key and value calculated this step</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">key_present</span><span class="p">)</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">value_present</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">key_update</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">value_update</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="c1"># add dependency for desired execution order</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">value_update</span><span class="p">)</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">key_update</span><span class="p">)</span>

        <span class="c1"># if shape is 3d, we reshape the inputs of the add</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output_x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">aux_loss</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span>

    <span class="k">def</span> <span class="nf">_check_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check inputs&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">):</span>
            <span class="n">_check_shape_equal_without_batch</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                             <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">input_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                   <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">input_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                   <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span><span class="p">])</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">input_mask</span><span class="p">),</span> <span class="s2">&quot;input_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">init_reset_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">init_reset_is_default</span> <span class="o">=</span> <span class="n">init_reset</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="n">batch_valid_length_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">batch_is_default</span> <span class="o">=</span> <span class="n">batch_valid_length</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">init_reset_is_tensor</span><span class="p">,</span>
                                    <span class="n">init_reset_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                    <span class="n">batch_valid_length_is_tensor</span><span class="p">,</span> <span class="n">batch_is_default</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">init_reset</span><span class="p">),</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">init_reset</span><span class="p">),</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span></div>


<div class="viewcode-block" id="TransformerDecoderLayer"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.TransformerDecoderLayer">[文档]</a><span class="k">class</span> <span class="nc">TransformerDecoderLayer</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transformer Decoder Layer. This is an implementation of the single layer of the transformer</span>
<span class="sd">        decoder layer, including self-attention, cross attention and feedward layer. When the encoder_output is None,</span>
<span class="sd">        the cross attention will not be effective.</span>

<span class="sd">        Args:</span>
<span class="sd">            hidden_size(int): The hidden size of the input.</span>
<span class="sd">            ffn_hidden_size(int): The hidden size of bottleneck in the feedforward layer.</span>
<span class="sd">            num_heads(int): The number of the heads.</span>
<span class="sd">            batch_size(int): The batch size of the input tensor when do increnmental prediction. Should be a positive</span>
<span class="sd">                value. When do training or prediction, the argument will not work and the user can just pass None to</span>
<span class="sd">                the argument.</span>
<span class="sd">            src_seq_length(int): The input source sequence length.</span>
<span class="sd">            tgt_seq_length(int): The input target sequence length.</span>
<span class="sd">            attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1.</span>
<span class="sd">            hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1.</span>
<span class="sd">            post_layernorm_residual(bool): Do residuals adds before the layernorm. Default False.</span>
<span class="sd">            use_past(bool): Use the past state to compute, used for incremental prediction. Default False.</span>
<span class="sd">            layernorm_compute_type(dtype.Number): The computation type of the layernorm.</span>
<span class="sd">                Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">            softmax_compute_type(dtype.Number): The computation type of the softmax in the attention.</span>
<span class="sd">                Should be dtype.float32 or dtype.float16. Default mstype.float32.</span>
<span class="sd">            param_init_type(dtype.Number): The parameter initialization type of the module.</span>
<span class="sd">                Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">            hidden_act (str, nn.Cell): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. User can provide custom activition to the argument.</span>
<span class="sd">                If user wants to run the net in the parallel mode, the custom activation must also provide</span>
<span class="sd">                the `activation_shard` function. Please see the examples of the</span>
<span class="sd">                class:`mindspore.nn.transformer.FeedForward`. Default: gelu.</span>
<span class="sd">            moe_config(MoEConfig): The configuration of MoE (Mixture of Expert). Default is an instance of MoEConfig</span>
<span class="sd">                with default values. Please see `MoEConfig`.</span>
<span class="sd">            parallel_config(OpParallelConfig, MoEParallelConfig): The parallel configure. When MoE is applied,</span>
<span class="sd">                MoEParallelConfig is effective, otherwise OpParallelConfig is effective. Default `default_dpmp_config`,</span>
<span class="sd">                an instance of `OpParallelConfig` with default args.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **hidden_stats** (Tensor) - The input tensor with shape [batch_size, tgt_seq_length, hidden_size] or</span>
<span class="sd">              [batch_size * tgt_seq_length, hidden_size].</span>
<span class="sd">            - **decoder_mask** (Tensor) - The attention mask for decoder with shape [batch_size, src_seq_length,</span>
<span class="sd">              seq_length] or None. None means there will be no mask in softmax computation in self attention.</span>
<span class="sd">            - **encoder_output** (Tensor) - The output of the encoder with shape [batch_size, seq_length, hidden_size]</span>
<span class="sd">              or [batch_size * seq_length, hidden_size].</span>
<span class="sd">              Note this args can not be passed by None when the net is in outermost layer. Default None.</span>
<span class="sd">            - **memory_mask** (Tensor) - The memory mask of the cross attention with shape [batch, tgt_seq_length,</span>
<span class="sd">              src_seq_length] where tgt_seq_length is the length of the decoder. The user can also pass None. None</span>
<span class="sd">              means there will be no mask in softmax computation in cross attention. Default None.</span>
<span class="sd">            - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and</span>
<span class="sd">              past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.</span>
<span class="sd">            - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index.</span>
<span class="sd">              Used for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tuple, a tuple contains(`output`, `layer_present`)</span>

<span class="sd">            - **output** (Tensor) - The output logit of this layer. The shape is [batch, seq_length, hidden_size] or</span>
<span class="sd">              [batch * seq_length, hidden_size].</span>
<span class="sd">            - **layer_present** (Tuple) - A tuple, where each tuple is the tensor of the projected key and value</span>
<span class="sd">              vector in self attention with shape ((batch_size, num_heads, size_per_head, tgt_seq_length),</span>
<span class="sd">              (batch_size, num_heads, tgt_seq_length, size_per_head), and of the projected key and value vector</span>
<span class="sd">              in cross attention with shape  (batch_size, num_heads, size_per_head, src_seq_length),</span>
<span class="sd">              (batch_size, num_heads, src_seq_length, size_per_head)).</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import TransformerDecoderLayer</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; model = TransformerDecoderLayer(batch_size=2, hidden_size=64, ffn_hidden_size=64, num_heads=2,</span>
<span class="sd">            ...                                 src_seq_length=20, tgt_seq_length=10)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 20, 64)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; decoder_input_value = Tensor(np.ones((2, 10, 64)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; decoder_input_mask = Tensor(np.ones((2, 10, 10)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; memory_mask = Tensor(np.ones((2, 10, 20)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; output, past = model(decoder_input_value, decoder_input_mask, encoder_input_value, memory_mask)</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (2, 10, 64)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0].shape)</span>
<span class="sd">            (2, 2, 32, 10)</span>
<span class="sd">            &gt;&gt;&gt; print(past[1].shape)</span>
<span class="sd">            (2, 2, 10, 32)</span>
<span class="sd">            &gt;&gt;&gt; print(past[2].shape)</span>
<span class="sd">            (2, 2, 32, 20)</span>
<span class="sd">            &gt;&gt;&gt; print(past[3].shape)</span>
<span class="sd">            (2, 2, 20, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_LogActionOnce</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;TransformerDecoderLayer&#39;</span><span class="p">,</span>
                    <span class="n">no_warning</span><span class="o">=</span><span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">,))</span>
    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                           <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">OpParallelConfig</span><span class="p">,</span> <span class="n">MoEParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;TransformerDecoderLayer&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_dpmp_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">config_to_attention</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">dpmp</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="k">else</span> <span class="n">parallel_config</span>
        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">or</span> <span class="n">use_past</span><span class="p">:</span>
            <span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">_is_sharding_propagation</span><span class="p">():</span>
            <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;TransformerDecoderLayer&#39;, the class variable &#39;num_heads&#39; must be divisibled by &quot;</span>
                                 <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the num_heads is </span><span class="si">{}</span><span class="s2"> and &quot;</span>
                                 <span class="s2">&quot;parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span>
                                                                                <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;For &#39;TransformerDecoderLayer&#39;, the class variable &#39;hidden_size&#39; must be divisibled by &quot;</span>
                    <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the hidden_size is </span><span class="si">{}</span><span class="s2"> and &quot;</span>
                    <span class="s2">&quot;parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;TransformerDecoderLayer&#39;, the class variable &#39;ffn_hidden_size&#39; must be &quot;</span>
                                 <span class="s2">&quot;divisibled by &#39;parallel_config.model_parallel&#39;, but got the ffn_hidden_size is </span><span class="si">{}</span><span class="s2"> &quot;</span>
                                 <span class="s2">&quot;and parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                                 <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ffn_hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">use_past</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> does not support use_past=True.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_compute_type</span> <span class="o">=</span> <span class="n">softmax_compute_type</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">config_to_attention</span><span class="p">)</span>

            <span class="c1"># Cross attention with the output of encoder as memory tensor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                      <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                      <span class="n">src_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                      <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                                      <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                      <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                      <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                      <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                      <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                      <span class="n">parallel_config</span><span class="o">=</span><span class="n">config_to_attention</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layernorm</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span>
                <span class="n">layernorm_compute_type</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                  <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                  <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                  <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                  <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Feed Forward Network, FFN</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                          <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                          <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                          <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                          <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                          <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span> <span class="o">=</span> <span class="n">post_layernorm_residual</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
                <span class="c1"># operator used for state reuse</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="n">size_per_head</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">)</span>
                <span class="c1"># parameters saving key and value states</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;key_past&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value_past&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">assign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Assign</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">elif</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;TransformerDecoderLayer&#39;, the class variable &#39;num_heads&#39; must be divisibled by &quot;</span>
                                 <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the num_heads is </span><span class="si">{}</span><span class="s2"> and &quot;</span>
                                 <span class="s2">&quot;parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num_heads</span><span class="p">,</span>
                                                                                <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;For &#39;TransformerDecoderLayer&#39;, the class variable &#39;hidden_size&#39; must be divisibled by &quot;</span>
                    <span class="s2">&quot;&#39;parallel_config.model_parallel&#39;, but got the hidden_size is </span><span class="si">{}</span><span class="s2"> and &quot;</span>
                    <span class="s2">&quot;parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">ffn_hidden_size</span> <span class="o">%</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;TransformerDecoderLayer&#39;, the class variable &#39;ffn_hidden_size&#39; must be &quot;</span>
                                 <span class="s2">&quot;divisibled by &#39;parallel_config.model_parallel&#39;, but got the ffn_hidden_size is </span><span class="si">{}</span><span class="s2"> &quot;</span>
                                 <span class="s2">&quot;and parallel_config.model_parallel is </span><span class="si">{}</span><span class="s2">.&quot;</span>
                                 <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ffn_hidden_size</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">use_past</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> does not support use_past=True.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_compute_type</span> <span class="o">=</span> <span class="n">softmax_compute_type</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">layernorm_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">config_to_attention</span><span class="p">)</span>

            <span class="c1"># Cross attention with the output of encoder as memory tensor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                      <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                      <span class="n">src_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                      <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                                      <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                      <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                      <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                      <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                      <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                      <span class="n">parallel_config</span><span class="o">=</span><span class="n">config_to_attention</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layernorm</span> <span class="o">=</span> <span class="n">_LayerNorm</span><span class="p">((</span><span class="n">hidden_size</span><span class="p">,))</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span>
                <span class="n">layernorm_compute_type</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layernorm</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                  <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                  <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                  <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                  <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                  <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Feed Forward Network, FFN</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                          <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                          <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                          <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                          <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                          <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span> <span class="o">=</span> <span class="n">post_layernorm_residual</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
                <span class="c1"># operator used for state reuse</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reducesum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="n">size_per_head</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="o">//</span> <span class="n">num_heads</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="p">,</span> <span class="n">size_per_head</span><span class="p">)</span>
                <span class="c1"># parameters saving key and value states</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">key_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;key_past&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">value_shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;value_past&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tile</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mul</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">assign</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Assign</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> only support sharding propagation or &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;semi-auto parallel mode now.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_stats</span><span class="p">,</span>
                  <span class="n">decoder_mask</span><span class="p">,</span>
                  <span class="n">encoder_output</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_input</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">,</span> <span class="n">decoder_mask</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="c1"># the returned shape is [bs, seq_length, embedding_size] or [bs * seq_length, embedding_size]</span>
        <span class="n">hidden_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">)</span>
        <span class="n">hidden_stats</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># indicate whether reset saved states</span>
        <span class="n">key_reset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">value_reset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># reset states, init_reset True for reuse and False for reset</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="n">key_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
            <span class="n">value_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="n">attention</span><span class="p">,</span> <span class="n">layer_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">decoder_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
        <span class="c1"># For post-layernorm the inputs for residual path are output of self-attention and output of layernorm</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>
        <span class="c1"># For pre-layernorm the inputs for residual path are output of self-attention and input of this layer</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">hidden_stats</span><span class="p">,</span> <span class="n">attention</span><span class="p">)</span>

        <span class="n">middle_output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">encoder_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">middle_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention_layernorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">middle_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">middle_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">encoder_output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">cross_attn_output</span><span class="p">,</span> <span class="n">cross_layer_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span><span class="n">middle_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span>
                                                                          <span class="n">encoder_output</span><span class="p">,</span>
                                                                          <span class="n">memory_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span>
                                                                          <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">layer_present</span> <span class="o">+=</span> <span class="n">cross_layer_present</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">middle_output</span><span class="p">,</span> <span class="n">cross_attn_output</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cross_attn_output</span><span class="p">)</span>

        <span class="n">output_x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">output_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">aux_loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="n">mlp_logit</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">output_x</span><span class="p">)</span>

        <span class="n">value_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">key_update</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="c1"># current key and value</span>
            <span class="n">key_present</span><span class="p">,</span> <span class="n">value_present</span> <span class="o">=</span> <span class="n">layer_present</span>
            <span class="c1"># update key and value calculated this step</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">key_past</span><span class="p">,</span> <span class="n">key_present</span><span class="p">)</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_past</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">value_past</span><span class="p">,</span> <span class="n">value_present</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_past</span>
            <span class="c1"># add dependency for desired execution order</span>
            <span class="n">key_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">key_update</span><span class="p">,</span> <span class="n">key_reset</span><span class="p">)</span>
            <span class="n">value_update</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">value_update</span><span class="p">,</span> <span class="n">value_reset</span><span class="p">)</span>

        <span class="c1"># add dependency for desired execution order</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">value_update</span><span class="p">)</span>
        <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">key_update</span><span class="p">)</span>

        <span class="c1"># if shape is 3d, we reshape the inputs of the add</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">hidden_shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">output_x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>
            <span class="n">mlp_logit</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">mlp_logit</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">x</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_3d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_layernorm_residual</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">output_x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mlp_logit</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">hidden_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span><span class="p">,</span> <span class="n">aux_loss</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">layer_present</span>

    <span class="k">def</span> <span class="nf">_check_input</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check inputs&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_first_iteration</span><span class="p">):</span>
            <span class="n">_check_shape_equal_without_batch</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                             <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                   <span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">])</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                               <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
            <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                   <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">])</span>
        <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="s2">&quot;hidden_states&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">),</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                               <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">encoder_output</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_shape_equal_without_batch</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">),</span> <span class="s2">&quot;encoder_output&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                             <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">),</span> <span class="s2">&quot;encoder_output&quot;</span><span class="p">,</span>
                               <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">memory_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_check_shape_equal_without_batch</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">memory_mask</span><span class="p">),</span> <span class="s2">&quot;memory_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span>
                                             <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">memory_mask</span><span class="p">),</span> <span class="s2">&quot;memory_mask&quot;</span><span class="p">,</span>
                               <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>

        <span class="n">init_reset_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">init_reset</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">init_reset_is_default</span> <span class="o">=</span> <span class="n">init_reset</span> <span class="ow">is</span> <span class="kc">True</span>
        <span class="n">batch_valid_length_is_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
        <span class="n">batch_is_default</span> <span class="o">=</span> <span class="n">batch_valid_length</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">init_reset_is_tensor</span><span class="p">,</span>
                                    <span class="n">init_reset_is_default</span><span class="p">)</span>
        <span class="n">_check_past_none_input_none</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">,</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span>
                                    <span class="n">batch_valid_length_is_tensor</span><span class="p">,</span> <span class="n">batch_is_default</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span><span class="p">:</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">init_reset</span><span class="p">),</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">init_reset</span><span class="p">),</span> <span class="s2">&quot;init_reset&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
            <span class="n">_check_shape_equal</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">])</span>
            <span class="n">_check_input_dtype</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">batch_valid_length</span><span class="p">),</span> <span class="s2">&quot;batch_valid_length&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span></div>


<span class="k">def</span> <span class="nf">_get_lambda_func</span><span class="p">(</span><span class="n">total_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A wrapper function of specifying pipeline stage and gradient aggregation fusion. If the total layer</span>
<span class="sd">    is not None, for example, set in the transformer model, the pipeline stage setting function will be</span>
<span class="sd">    `(layer_id + 0) // (total_layers / parallel_config.pipeline_stage)` for the encoder and,</span>
<span class="sd">    `(layer_id + offset) //</span>
<span class="sd">    (total_layers / parallel_config.pipeline_stage)` for the decoder, where `offset` is the layers in the encoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_set_parallel_configure_for_layer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.</span>

<span class="sd">        Args:</span>
<span class="sd">            network(Cell) - Represents the transformer block</span>
<span class="sd">            layer_id(int) - Means the layer index for the current module, counts from zero.</span>
<span class="sd">            offset(int) - Means the layer_index needs an offset, if there are other modules in the net.</span>
<span class="sd">            layers(int) - The total layers used for the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># override the layers</span>
        <span class="k">if</span> <span class="n">total_layer</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="n">total_layer</span>
        <span class="c1"># Used for the pipeline&#39;s stages setting</span>
        <span class="k">if</span> <span class="n">layers</span> <span class="o">&lt;</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;layers </span><span class="si">{</span><span class="n">layers</span><span class="si">}</span><span class="s2"> must be larger than pipeline stage </span><span class="si">{</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">pp_dis</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">layers</span> <span class="o">//</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># the pipeline stage must be in [0, parallel_config.pipeline_stage - 1]</span>
        <span class="n">pp_id</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">layer_id</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">//</span> <span class="n">pp_dis</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">network</span><span class="o">.</span><span class="n">pipeline_stage</span> <span class="o">=</span> <span class="n">pp_id</span>

        <span class="c1"># Used for optimizer&#39;s fusion tag</span>
        <span class="n">dis</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">layers</span> <span class="o">//</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">gradient_aggregation_group</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">network</span><span class="o">.</span><span class="n">set_comm_fusion</span><span class="p">((</span><span class="n">layer_id</span> <span class="o">+</span> <span class="n">offset</span><span class="p">)</span> <span class="o">//</span> <span class="n">dis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Used for enabling recomputation of the block</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="p">:</span>
                <span class="n">network</span><span class="o">.</span><span class="n">recompute</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="o">.</span><span class="n">recompute</span><span class="p">:</span>
                <span class="n">paralel_op_comm_compute</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="o">.</span><span class="n">parallel_optimizer_comm_recompute</span>
                <span class="n">network</span><span class="o">.</span><span class="n">recompute</span><span class="p">(</span><span class="n">parallel_optimizer_comm_recompute</span><span class="o">=</span><span class="n">paralel_op_comm_compute</span><span class="p">,</span>
                                  <span class="n">mp_comm_recompute</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="o">.</span><span class="n">mp_comm_recompute</span><span class="p">,</span>
                                  <span class="n">recompute_slice_activation</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">recompute</span><span class="o">.</span><span class="n">recompute_slice_activation</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">_set_parallel_configure_for_layer</span>


<div class="viewcode-block" id="TransformerEncoder"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.TransformerEncoder">[文档]</a><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transformer Encoder module with multi-layer stacked of `TransformerEncoderLayer`, including multihead self</span>
<span class="sd">        attention and feedforward layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            batch_size(int): The batch size of the input tensor when do increnmental prediction. Should be a positive</span>
<span class="sd">                value. When do training or prediction, the argument will not work and the user can just pass None to</span>
<span class="sd">                the argument.</span>
<span class="sd">            num_layers(int): The layers of the `TransformerEncoderLayer`</span>
<span class="sd">            hidden_size(int): The hidden size of the input.</span>
<span class="sd">            ffn_hidden_size(int): The hidden size of bottleneck in the feedforward layer.</span>
<span class="sd">            seq_length(int): The seq_length of the input tensor.</span>
<span class="sd">            num_heads(int): The number of the heads.</span>
<span class="sd">            attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1.</span>
<span class="sd">            hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default: 0.1.</span>
<span class="sd">            hidden_act (str, nn.Cell): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. User can provide custom activition to the argument.</span>
<span class="sd">                If user wants to run the net in the parallel mode, the custom activation must also provide</span>
<span class="sd">                the `activation_shard` function. Please see the examples of the</span>
<span class="sd">                class:`mindspore.nn.transformer.FeedForward`. Default: gelu.</span>
<span class="sd">            post_layernorm_residual(bool): Do residuals adds before the layernorm. Default False.</span>
<span class="sd">            layernorm_compute_type(dtype.Number): The computation type of the layernorm.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16. Default mstype.float32.</span>
<span class="sd">            softmax_compute_type(dtype.Number): The computation type of the softmax in the attention.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16. Default: mstype.float32.</span>
<span class="sd">            param_init_type(dtype.Number): The parameter initialization type of the module.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16. Default: mstype.float32.</span>
<span class="sd">            lambda_func(function): A function can determine the fusion index,</span>
<span class="sd">                pipeline stages and recompute attribute. If the</span>
<span class="sd">                user wants to determine the pipeline stage and gradient aggregation fusion, the user can pass a</span>
<span class="sd">                function that accepts `network`, `layer_id`, `offset`, `parallel_config`, `layers`. The `network(Cell)`</span>
<span class="sd">                represents the transformer block, `layer_id(int)` means the layer index for the current module, counts</span>
<span class="sd">                from zero, `offset(int)` means the layer_index needs an offset, if there are other modules in the net.</span>
<span class="sd">                The default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.</span>
<span class="sd">                Default: None.</span>
<span class="sd">            offset(int): The initial layer index for the `encoder`. Used for setting the fusion id and stage id, to not</span>
<span class="sd">                overlap with the encoder layer. Default 0.</span>
<span class="sd">            use_past(bool): Use the past state to compute, used for incremental prediction. For example, if we have two</span>
<span class="sd">                words and want to generate the ten more words. We just need to compute the two words&#39; state only once,</span>
<span class="sd">                and generate the next word one by one. When use_past is True, there are two steps to run the prediction.</span>
<span class="sd">                In the first step, set the is_first_iteration to be True by</span>
<span class="sd">                `model.add_flags_recursive(is_first_iteration=True)`, and pass the full inputs. Then, set the</span>
<span class="sd">                is_first_iteration to be False by `model.add_flags_recursive(is_first_iteration=False)`. At this moment,</span>
<span class="sd">                pass the single step&#39;s input tensor, and loop it. Default: False.</span>
<span class="sd">            moe_config(MoEConfig): The configuration of MoE (Mixture of Expert). Default is an instance of MoEConfig</span>
<span class="sd">                with default values. Please see `MoEConfig`.</span>
<span class="sd">            parallel_config(TransformerOpParallelConfig): The parallel configure. Default `default_transformer_config`,</span>
<span class="sd">                an instance of `TransformerOpParallelConfig` with default args.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **hidden_states** (Tensor) - Tensor, shape should be [batch_size, seq_length, hidden_size] or</span>
<span class="sd">              [batch_size * seq_length, hidden_size], if the use_past is False or is_first_iteration=True. Otherwise,</span>
<span class="sd">              should be [batch_size, 1, hidden_size].</span>
<span class="sd">            - **attention_mask** (Tensor) - Float Tensor, If the use_past is False or is_first_iteration=True,</span>
<span class="sd">              the attention mask matrix should ba [batch_size, seq_length, seq_length], or None. None means there will</span>
<span class="sd">              be no mask in softmax computation. Otherwise, should be [batch_size, 1, hidden_size]</span>
<span class="sd">            - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and</span>
<span class="sd">              past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.</span>
<span class="sd">            - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index.</span>
<span class="sd">              Used for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tuple, a tuple contains(`output`, `layer_present`)</span>

<span class="sd">            - **output** (Tensor) - The float tensor of the output of the layer with</span>
<span class="sd">              shape (batch_size, seq_length, hidden_size) or (batch_size * seq_length, hidden_size), if the use_past is</span>
<span class="sd">              False or is_first_iteration=True. Otherwise, it will be (batch_size, 1, hidden_size).</span>
<span class="sd">            - **layer_present** (Tuple) - A tuple with size of num_layers, where each tuple contains the Tensor the</span>
<span class="sd">              projected key and value vector with shape ((batch_size, num_heads, size_per_head, seq_length),</span>
<span class="sd">              and (batch_size, num_heads, seq_length, size_per_head)).</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import TransformerEncoder</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; model = TransformerEncoder(batch_size=2, num_layers=2, hidden_size=8, ffn_hidden_size=64,</span>
<span class="sd">            ...                            seq_length=16, num_heads=2)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 16, 8)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_mask = Tensor(np.ones((2, 16, 16)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; output, past = model(encoder_input_value, encoder_input_mask)</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (2, 16, 8)</span>
<span class="sd">            &gt;&gt;&gt; print(len(past))</span>
<span class="sd">            2</span>
<span class="sd">            &gt;&gt;&gt; print(past[0][0].shape)</span>
<span class="sd">            (2, 2, 4, 16)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0][1].shape)</span>
<span class="sd">            (2, 2, 16, 4)</span>
<span class="sd">            &gt;&gt;&gt; # When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="sd">            &gt;&gt;&gt; # Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="sd">            &gt;&gt;&gt; batch_valid_length = Tensor(np.ones((2,)), mstype.int32)</span>
<span class="sd">            &gt;&gt;&gt; init_reset = Tensor([True], mstype.bool_)</span>
<span class="sd">            &gt;&gt;&gt; # Set is_first_iteration=True to generate the full memory states</span>
<span class="sd">            &gt;&gt;&gt; model = TransformerEncoder(batch_size=2, hidden_size=8, ffn_hidden_size=64, seq_length=16,</span>
<span class="sd">            ...                            num_heads=2, num_layers=2, use_past=True)</span>
<span class="sd">            &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=True)</span>
<span class="sd">            &gt;&gt;&gt; hidden, past = model(encoder_input_value, encoder_input_mask, init_reset, batch_valid_length)</span>
<span class="sd">            &gt;&gt;&gt; print(hidden.shape)</span>
<span class="sd">            (2, 16, 8)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0][0].shape)</span>
<span class="sd">            (2, 2, 4, 16)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0][1].shape)</span>
<span class="sd">            (2, 2, 16, 4)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 1, 8)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_mask = Tensor(np.ones((2, 1, 16)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; init_reset = Tensor([False], mstype.bool_)</span>
<span class="sd">            &gt;&gt;&gt; # Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than</span>
<span class="sd">            &gt;&gt;&gt; # the full sequence.</span>
<span class="sd">            &gt;&gt;&gt; model.add_flags_recursive(is_first_iteration=False)</span>
<span class="sd">            &gt;&gt;&gt; hidden, past = model(encoder_input_value, encoder_input_mask, init_reset, batch_valid_length)</span>
<span class="sd">            &gt;&gt;&gt; print(hidden.shape)</span>
<span class="sd">            (2, 1, 8)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0][0].shape)</span>
<span class="sd">            (2, 2, 4, 16)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0][1].shape)</span>
<span class="sd">            (2, 2, 16, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_LogActionOnce</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;TransformerEncoder&#39;</span><span class="p">,</span>
                    <span class="n">no_warning</span><span class="o">=</span><span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">,))</span>
    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_layers</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">offset</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                           <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">TransformerOpParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">num_layers</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">seq_length</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">lambda_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_transformer_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">config_to_layer</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">moe_parallel_config</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="k">else</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">dp_mp_config</span>
        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">_is_sharding_propagation</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">block</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                                <span class="n">seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">config_to_layer</span><span class="p">)</span>
                <span class="c1"># If the user doesn&#39;t pass the fusion function, use the default one</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
                    <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">()</span>

                <span class="n">lambda_func</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">layer_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
                            <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;For parallel mode, sharding propagation is recommended, you can use it by setting &quot;</span>
                           <span class="s2">&quot;&#39;set_auto_parallel_context(parallel_mode=ParallelMode.AUTO_PARALLEL, &quot;</span>
                           <span class="s2">&quot;search_mode=</span><span class="se">\&quot;</span><span class="s2">sharding_propagation</span><span class="se">\&quot;</span><span class="s2">)&#39; and &quot;</span>
                           <span class="s2">&quot;&#39;set_algo_parameters(elementwise_op_strategy_follow=False, fully_use_devices=False)&#39;&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">block</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                                <span class="n">seq_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span>
                                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">config_to_layer</span><span class="p">)</span>
                <span class="c1"># If the user doesn&#39;t pass the fusion function, use the default one</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
                    <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">()</span>

                <span class="n">lambda_func</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">layer_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
                            <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> only support sharding propagation or &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;semi-auto parallel mode now.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">present_layer</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                                  <span class="n">attention_mask</span><span class="p">,</span>
                                                                  <span class="n">init_reset</span><span class="p">,</span>
                                                                  <span class="n">batch_valid_length</span><span class="p">)</span>
                <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>
                <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">accum_loss</span><span class="p">,</span> <span class="n">aux_loss</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span><span class="p">,</span> <span class="n">accum_loss</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                    <span class="n">attention_mask</span><span class="p">,</span>
                                                    <span class="n">init_reset</span><span class="p">,</span>
                                                    <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span></div>


<div class="viewcode-block" id="TransformerDecoder"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.TransformerDecoder">[文档]</a><span class="k">class</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transformer Decoder module with multi-layer stacked of `TransformerDecoderLayer`, including multihead self</span>
<span class="sd">        attention, cross attention and feedforward layer.</span>

<span class="sd">        Args:</span>
<span class="sd">            num_layers(int): The layers of the `TransformerDecoderLayer`.</span>
<span class="sd">            batch_size(int): The batch size of the input tensor when do increnmental prediction. Should be a positive</span>
<span class="sd">                value. When do training or prediction, the argument will not work and the user can just pass None to</span>
<span class="sd">                the argument.</span>
<span class="sd">            hidden_size(int): The hidden size of the input.</span>
<span class="sd">            ffn_hidden_size(int): The hidden size of bottleneck in the feedforward layer.</span>
<span class="sd">            src_seq_length(int): The input source sequence length.</span>
<span class="sd">            tgt_seq_length(int): The input target sequence length.</span>
<span class="sd">            num_heads(int): The number of the heads.</span>
<span class="sd">            attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1.</span>
<span class="sd">            hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1.</span>
<span class="sd">            post_layernorm_residual(bool): Do residuals adds before the layernorm. Default False.</span>
<span class="sd">            layernorm_compute_type(dtype.Number): The computation type of the layernorm.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16. Default mstype.float32.</span>
<span class="sd">            softmax_compute_type(dtype.Number): The computation type of the softmax in the attention.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16. Default mstype.float32.</span>
<span class="sd">            param_init_type(dtype.Number): The parameter initialization type of the module.</span>
<span class="sd">                Should be mstype.float32 or mstype.float16. Default mstype.float32.</span>
<span class="sd">            hidden_act (str, nn.Cell): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. User can provide custom activition to the argument.</span>
<span class="sd">                If user wants to run the net in the parallel mode, the custom activation must also provide</span>
<span class="sd">                the `activation_shard` function. Please see the examples of the</span>
<span class="sd">                class:`mindspore.nn.transformer.FeedForward`. Default: gelu.</span>
<span class="sd">            lambda_func(function): A function can determine the fusion index,</span>
<span class="sd">                pipeline stages and recompute attribute. If the</span>
<span class="sd">                user wants to determine the pipeline stage and gradient aggregation fusion, the user can pass a</span>
<span class="sd">                function that accepts `network`, `layer_id`, `offset`, `parallel_config`, `layers`. The `network(Cell)`</span>
<span class="sd">                represents the transformer block, `layer_id(int)` means the layer index for the current module, counts</span>
<span class="sd">                from zero, `offset(int)` means the layer_index needs an offset, if there are other modules in the net.</span>
<span class="sd">                The default setting for the pipeline is: `(layer_id + offset) // (layers / pipeline_stage)`.</span>
<span class="sd">                Default: None.</span>
<span class="sd">            use_past(bool): Use the past state to compute, used for incremental prediction. Default False.</span>
<span class="sd">            offset(int): The initial layer index for the `decoder`. Used for setting the fusion id and stage id, to not</span>
<span class="sd">                overlap with the encoder layer. Default 0.</span>
<span class="sd">            moe_config(MoEConfig): The configuration of MoE (Mixture of Expert). Default is an instance of MoEConfig</span>
<span class="sd">                with default values. Please see `MoEConfig`.</span>
<span class="sd">            parallel_config(TransformerOpParallelConfig): The parallel configure. Default `default_transformer_config`,</span>
<span class="sd">                an instance of `TransformerOpParallelConfig` with default args.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **hidden_stats** (Tensor) - The input tensor with shape [batch_size, seq_length, hidden_size] or</span>
<span class="sd">              [batch_size * seq_length, hidden_size]</span>
<span class="sd">            - **attention_mask** (Tensor) - The attention mask for decoder with shape</span>
<span class="sd">              [batch_size, seq_length, seq_length] or None. None means there will be no mask in softmax</span>
<span class="sd">              computation in self attention.</span>
<span class="sd">            - **encoder_output** (Tensor) - The output of the encoder with shape [batch_size, seq_length, hidden_size]</span>
<span class="sd">              or [batch_size * seq_length, hidden_size]. Note this args can not be passed by None when the net is in</span>
<span class="sd">              outermost layer. Default None.</span>
<span class="sd">            - **memory_mask** (Tensor) - The memory mask of the cross attention with shape [batch, tgt_seq_length,</span>
<span class="sd">              src_seq_length] where tgt_seq_length is the length of the decoder. The user can also pass None. None</span>
<span class="sd">              means there will be no mask in softmax computation in cross attention. Default None.</span>
<span class="sd">            - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and</span>
<span class="sd">              past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.</span>
<span class="sd">            - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index.</span>
<span class="sd">              Used for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tuple, a tuple contains(`output`, `layer_present`)</span>

<span class="sd">            - **output** (Tensor) - The output logit of this layer. The shape is [batch, tgt_seq_length, hidden_size] or</span>
<span class="sd">              [batch * tgt_seq_length, hidden_size]</span>
<span class="sd">            - **layer_present** (Tuple) - A tuple with size of num_layers, where each tuple is the tensor of the</span>
<span class="sd">              projected key and value vector in self attention with shape ((batch_size, num_heads, size_per_head,</span>
<span class="sd">              tgt_seq_length), (batch_size, num_heads, tgt_seq_length, size_per_head), and of the projected key</span>
<span class="sd">              and value vector in cross attention with shape  (batch_size, num_heads, size_per_head, src_seq_length),</span>
<span class="sd">              (batch_size, num_heads, src_seq_length, size_per_head)).</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import TransformerDecoder</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; model = TransformerDecoder(batch_size=2, num_layers=1, hidden_size=64, ffn_hidden_size=64,</span>
<span class="sd">            ...                            num_heads=2, src_seq_length=20, tgt_seq_length=10)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 20, 64)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; decoder_input_value = Tensor(np.ones((2, 10, 64)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; decoder_input_mask = Tensor(np.ones((2, 10, 10)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; memory_mask = Tensor(np.ones((2, 10, 20)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; output, past = model(decoder_input_value, decoder_input_mask, encoder_input_value, memory_mask)</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (2, 10, 64)</span>
<span class="sd">            &gt;&gt;&gt; print(len(past))</span>
<span class="sd">            1</span>
<span class="sd">            &gt;&gt;&gt; print(past[0][0].shape)</span>
<span class="sd">            (2, 2, 32, 10)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0][1].shape)</span>
<span class="sd">            (2, 2, 10, 32)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0][2].shape)</span>
<span class="sd">            (2, 2, 32, 20)</span>
<span class="sd">            &gt;&gt;&gt; print(past[0][3].shape)</span>
<span class="sd">            (2, 2, 20, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_LogActionOnce</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;TransformerDecoder&#39;</span><span class="p">,</span>
                    <span class="n">no_warning</span><span class="o">=</span><span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">,))</span>
    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_layers</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">offset</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                           <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                    <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">TransformerOpParallelConfig</span><span class="p">],</span>
                                                                   <span class="s2">&quot;TransformerDecoder&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_layers</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">lambda_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_transformer_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
        <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">config_to_layer</span> <span class="o">=</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">moe_parallel_config</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="k">else</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">dp_mp_config</span>
        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">_is_sharding_propagation</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">block</span> <span class="o">=</span> <span class="n">TransformerDecoderLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                                <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                                <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">config_to_layer</span><span class="p">)</span>
                <span class="c1"># If the user doesn&#39;t pass the fusion function, use the default one</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
                    <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">()</span>

                <span class="n">lambda_func</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">layer_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
                            <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;For parallel mode, sharding propagation is recommended, you can use it by setting &quot;</span>
                           <span class="s2">&quot;&#39;set_auto_parallel_context(parallel_mode=ParallelMode.AUTO_PARALLEL, &quot;</span>
                           <span class="s2">&quot;search_mode=</span><span class="se">\&quot;</span><span class="s2">sharding_propagation</span><span class="se">\&quot;</span><span class="s2">)&#39; and &quot;</span>
                           <span class="s2">&quot;&#39;set_algo_parameters(elementwise_op_strategy_follow=False, fully_use_devices=False)&#39;&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CellList</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">block</span> <span class="o">=</span> <span class="n">TransformerDecoderLayer</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                                <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                                <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">config_to_layer</span><span class="p">)</span>
                <span class="c1"># If the user doesn&#39;t pass the fusion function, use the default one</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
                    <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">()</span>

                <span class="n">lambda_func</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">layer_id</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
                            <span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> only support sharding propagation or &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;semi-auto parallel mode now.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">encoder_output</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">present_layer</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
                <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span><span class="p">,</span> <span class="n">aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                                  <span class="n">attention_mask</span><span class="p">,</span>
                                                                  <span class="n">encoder_output</span><span class="p">,</span>
                                                                  <span class="n">memory_mask</span><span class="p">,</span>
                                                                  <span class="n">init_reset</span><span class="p">,</span>
                                                                  <span class="n">batch_valid_length</span><span class="p">)</span>
                <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>
                <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">accum_loss</span><span class="p">,</span> <span class="n">aux_loss</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span><span class="p">,</span> <span class="n">accum_loss</span>

        <span class="c1"># Loop through each self-attention layer</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span>
                                                    <span class="n">attention_mask</span><span class="p">,</span>
                                                    <span class="n">encoder_output</span><span class="p">,</span>
                                                    <span class="n">memory_mask</span><span class="p">,</span>
                                                    <span class="n">init_reset</span><span class="p">,</span>
                                                    <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">present_layer</span> <span class="o">=</span> <span class="n">present_layer</span> <span class="o">+</span> <span class="p">(</span><span class="n">present</span><span class="p">,)</span>

        <span class="k">return</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">present_layer</span></div>


<div class="viewcode-block" id="Transformer"><a class="viewcode-back" href="../../../../api_python/mindspore.nn.transformer.html#mindspore.nn.transformer.Transformer">[文档]</a><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transformer module including encoder and decoder. The difference with the original implements is the module use</span>
<span class="sd">        the residual addition before the layer normalization. And the default hidden act is `gelu`.</span>
<span class="sd">        The details can be found in `Attention is all you need &lt;https://arxiv.org/pdf/1706.03762v5.pdf&gt;`_.</span>

<span class="sd">        Note:</span>
<span class="sd">            This is an experimental interface that is subject to change or deletion.</span>

<span class="sd">        Args:</span>
<span class="sd">            hidden_size(int): The hidden size of the input.</span>
<span class="sd">            batch_size(int): The batch size of the input tensor when do increnmental prediction. Should be a positive</span>
<span class="sd">                value. When do training or prediction, the argument will not work and the user can just pass None to</span>
<span class="sd">                the argument.</span>
<span class="sd">            ffn_hidden_size(int): The hidden size of bottleneck in the feedforward layer.</span>
<span class="sd">            src_seq_length(int): The seq_length of the encoder&#39;s input tensor.</span>
<span class="sd">            tgt_seq_length(int): The seq_length of the decoder&#39;s input tensor.</span>
<span class="sd">            encoder_layers(int): The layers of the `TransformerEncoderLayer`. Default 3.</span>
<span class="sd">            decoder_layers(int): The layers of the `TransformerDecoderLayer`. Default 3.</span>
<span class="sd">            num_heads(int): The number of the heads. Default: 2.</span>
<span class="sd">            attention_dropout_rate(float): The dropout rate of the attention scores. Default:0.1.</span>
<span class="sd">            hidden_dropout_rate(float): The dropout rate of the final output of the layer. Default:0.1.</span>
<span class="sd">            hidden_act (str, nn.Cell): The activation of the internal feedforward layer. Supports &#39;relu&#39;,</span>
<span class="sd">                &#39;relu6&#39;, &#39;tanh&#39;, &#39;gelu&#39;, &#39;fast_gelu&#39;, &#39;elu&#39;, &#39;sigmoid&#39;, &#39;prelu&#39;, &#39;leakyrelu&#39;, &#39;hswish&#39;,</span>
<span class="sd">                &#39;hsigmoid&#39;, &#39;logsigmoid&#39; and so on. User can provide custom activition to the argument.</span>
<span class="sd">                If user wants to run the net in the parallel mode, the custom activation must also provide</span>
<span class="sd">                the `activation_shard` function. Please see the examples of the</span>
<span class="sd">                class:`mindspore.nn.transformer.FeedForward`. Default: gelu.</span>
<span class="sd">            post_layernorm_residual(bool): Do residuals adds before the layernorm. Default False.</span>
<span class="sd">            layernorm_compute_type(dtype.Number): The computation type of the layernorm.</span>
<span class="sd">                Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">            softmax_compute_type(dtype.Number): The computation type of the softmax in the attention.</span>
<span class="sd">                Should be dtype.float32 or dtype.float16. Default mstype.float32.</span>
<span class="sd">            param_init_type(dtype.Number): The parameter initialization type of the module.</span>
<span class="sd">                Should be dtype.float32 or dtype.float16. Default dtype.float32.</span>
<span class="sd">            lambda_func: A function can determine the fusion index, pipeline stages and recompute attribute. If the user</span>
<span class="sd">                wants to determine the pipeline stage and gradient aggregation fusion, the user can pass a function</span>
<span class="sd">                that accepts `network`, `layer_id`, `offset`, `parallel_config`, `layers`. The `network(Cell)`</span>
<span class="sd">                represents the transformer block, `layer_id(int)` means the layer index for the current module, counts</span>
<span class="sd">                from zero, `offset(int)` means the layer_index needs an offset, if there are other modules in the net.</span>
<span class="sd">                The default setting for the pipeline is: `(layer_id + offset) // ((encoder_layers + decoder_layers)</span>
<span class="sd">                / pipeline_stage)`. Default None.</span>
<span class="sd">            use_past(bool): Use the past state to compute, used for incremental prediction. Default False.</span>
<span class="sd">            moe_config(MoEConfig): The configuration of MoE (Mixture of Expert). Default is an instance of MoEConfig</span>
<span class="sd">                with default values. Please see `MoEConfig`.</span>
<span class="sd">            parallel_config(TransformerOpParallelConfig): The parallel configure. Default `default_transformer_config`,</span>
<span class="sd">                an instance of `TransformerOpParallelConfig` with default args.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **encoder_inputs** (Tensor) - The input tensor with shape [batch_size, seq_length, hidden_size] or</span>
<span class="sd">              [batch_size * seq_length, hidden_size].</span>
<span class="sd">            - **encoder_masks** (Tensor) - The attention mask for decoder with shape</span>
<span class="sd">              [batch_size, seq_length, seq_length] or None. None means there will be no mask in softmax computation</span>
<span class="sd">              in self attention of the encoder module.</span>
<span class="sd">            - **decoder_inputs** (Tensor) - The output of the encoder with shape [batch_size, seq_length, hidden_size]</span>
<span class="sd">              or [batch_size * seq_length, hidden_size], this should be none if the decoder layer is 0.</span>
<span class="sd">            - **decoder_masks** (Tensor) - The attention mask for decoder with shape</span>
<span class="sd">              [batch_size, seq_length, seq_length] or None. None means there will be no mask in softmax computation</span>
<span class="sd">              in self attention of the decoder module.</span>
<span class="sd">            - **memory_mask** (Tensor) - The memory mask of the cross attention with shape [batch, tgt_seq_length,</span>
<span class="sd">              src_seq_length]</span>
<span class="sd">              where tgt_seq_length is the length of the decoder. The output of the encoder with shape [batch_size,</span>
<span class="sd">              seq_length, hidden_size], this should be none if the decoder layer is 0 or the user wants no mask.</span>
<span class="sd">            - **init_reset** (Tensor) - A bool tensor with shape [1], used to clear the past key parameter and</span>
<span class="sd">              past value parameter used in the incremental prediction. Only valid when use_past is True. Default True.</span>
<span class="sd">            - **batch_valid_length** (Tensor) - Int32 tensor with shape [batch_size] the past calculated the index.</span>
<span class="sd">              Used for incremental prediction when the use_past is True. Default None.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tuple, a tuple contains(`output`, `encoder_layer_present`, `decoder_layer_present`, `accum_loss`)</span>

<span class="sd">            - **output** (Tensor) - If there is only encoder, the output logit of the encoder layer. The shape is</span>
<span class="sd">              [batch, src_seq_length, hidden_size] or [batch * src_seq_length, hidden_size], if there are encoder and</span>
<span class="sd">              decoders, the output is from the decoder layer. The shape is [batch, tgt_seq_length, hidden_size] or</span>
<span class="sd">              [batch * tgt_seq_length, hidden_size].</span>
<span class="sd">            - **encoder_layer_present** (Tuple) - A tuple with size of num_layers, where each tuple is the tensor the</span>
<span class="sd">              projected key and value vector in self attention with shape ((batch_size, num_heads, size_per_head,</span>
<span class="sd">              src_seq_length), (batch_size, num_heads, src_seq_length, size_per_head)).</span>
<span class="sd">            - **decoder_layer_present** (Tuple) - A tuple with size of num_layers, where each tuple is the tensor</span>
<span class="sd">              of the projected key and value vector in self attention with shape ((batch_size, num_heads, size_per_head,</span>
<span class="sd">              tgt_seq_length), (batch_size, num_heads, tgt_seq_length, size_per_head)), and the</span>
<span class="sd">              projected key and value vector in cross attention with shape</span>
<span class="sd">              ((batch_size, num_heads, size_per_head, src_seq_length),</span>
<span class="sd">              (batch_size, num_heads, src_seq_length, size_per_head)). If the decoder is not set, the</span>
<span class="sd">              returned value will be None.</span>
<span class="sd">            - **accum_loss** (Tensor) - A Tensor indicates an auxiliary loss to minimize the mean square of the data</span>
<span class="sd">              part routed to each expert, and only returned if the number of experts is greater than 1.</span>

<span class="sd">        Supported Platforms:</span>
<span class="sd">            ``Ascend`` ``GPU``</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import numpy as np</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">            &gt;&gt;&gt; from mindspore.nn.transformer import Transformer</span>
<span class="sd">            &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">            &gt;&gt;&gt; model = Transformer(batch_size=2, encoder_layers=1, decoder_layers=2, hidden_size=64,</span>
<span class="sd">            ...                     ffn_hidden_size=64, src_seq_length=20, tgt_seq_length=10)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_value = Tensor(np.ones((2, 20, 64)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; encoder_input_mask = Tensor(np.ones((2, 20, 20)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; decoder_input_value = Tensor(np.ones((2, 10, 64)), mstype.float32)</span>
<span class="sd">            &gt;&gt;&gt; decoder_input_mask = Tensor(np.ones((2, 10, 10)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; memory_mask = Tensor(np.ones((2, 10, 20)), mstype.float16)</span>
<span class="sd">            &gt;&gt;&gt; output, en_past, de_past = model(encoder_input_value, encoder_input_mask, decoder_input_value,</span>
<span class="sd">            ...                                  decoder_input_mask, memory_mask)</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            (2, 10, 64)</span>
<span class="sd">            &gt;&gt;&gt; print(len(en_past))</span>
<span class="sd">            1</span>
<span class="sd">            &gt;&gt;&gt; print(len(de_past))</span>
<span class="sd">            2</span>
<span class="sd">            &gt;&gt;&gt; print(en_past[0][0].shape)</span>
<span class="sd">            (2, 2, 32, 20)</span>
<span class="sd">            &gt;&gt;&gt; print(en_past[0][1].shape)</span>
<span class="sd">            (2, 2, 20, 32)</span>
<span class="sd">            &gt;&gt;&gt; print(de_past[0][0].shape)</span>
<span class="sd">            (2, 2, 32, 10)</span>
<span class="sd">            &gt;&gt;&gt; print(de_past[0][1].shape)</span>
<span class="sd">            (2, 2, 10, 32)</span>
<span class="sd">            &gt;&gt;&gt; print(de_past[0][2].shape)</span>
<span class="sd">            (2, 2, 32, 20)</span>
<span class="sd">            &gt;&gt;&gt; print(de_past[0][3].shape)</span>
<span class="sd">            (2, 2, 20, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@_LogActionOnce</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;Transformer&#39;</span><span class="p">,</span>
                    <span class="n">no_warning</span><span class="o">=</span><span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">,))</span>
    <span class="nd">@_args_type_validator_check</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">num_heads</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">src_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">encoder_layers</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">decoder_layers</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">,</span>
                                <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">,</span>
                                <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">,</span>
                                <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">,</span>
                                <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                           <span class="s2">&quot;Transformer&quot;</span><span class="p">),</span>
                                <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span>
                                                                         <span class="s2">&quot;Transformer&quot;</span><span class="p">),</span>
                                <span class="n">param_init_type</span><span class="o">=</span><span class="n">_valid_value_checks</span><span class="p">([</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="s2">&quot;Transformer&quot;</span><span class="p">),</span>
                                <span class="n">parallel_config</span><span class="o">=</span><span class="n">_valid_type_checks</span><span class="p">([</span><span class="n">TransformerOpParallelConfig</span><span class="p">],</span> <span class="s2">&quot;Transformer&quot;</span><span class="p">),</span>
                                <span class="n">use_past</span><span class="o">=</span><span class="n">Validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">)</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">ffn_hidden_size</span><span class="p">,</span>
                 <span class="n">src_seq_length</span><span class="p">,</span>
                 <span class="n">tgt_seq_length</span><span class="p">,</span>
                 <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">decoder_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">hidden_act</span><span class="o">=</span><span class="s1">&#39;gelu&#39;</span><span class="p">,</span>
                 <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">param_init_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">lambda_func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">use_past</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">moe_config</span><span class="o">=</span><span class="n">default_moe_config</span><span class="p">,</span>
                 <span class="n">parallel_config</span><span class="o">=</span><span class="n">default_transformer_config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">_is_sharding_propagation</span><span class="p">():</span>
            <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
            <span class="k">if</span> <span class="n">encoder_layers</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">decoder_layers</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformer doest support encoder layer </span><span class="si">{</span><span class="n">encoder_layers</span><span class="si">}</span><span class="s2"> and decoder&quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;layer </span><span class="si">{</span><span class="n">decoder_layers</span><span class="si">}</span><span class="s2">, please use TransformerDecoder&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">encoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">decoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">use_past</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> with encoder and decoder does not support use_past=True.&quot;</span><span class="p">)</span>
            <span class="c1"># The shard setting of Transformer is set within the TransformerEncoderLayer</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
                <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">(</span><span class="n">total_layer</span><span class="o">=</span><span class="n">encoder_layers</span> <span class="o">+</span> <span class="n">decoder_layers</span><span class="p">)</span>
            <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">encoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="n">encoder_layers</span><span class="p">,</span>
                                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                  <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                  <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                                  <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                  <span class="n">seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                                  <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                  <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                  <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                                  <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                                  <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                  <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                  <span class="n">lambda_func</span><span class="o">=</span><span class="n">lambda_func</span><span class="p">,</span>
                                                  <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                  <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                                  <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Offset is needed as the encoder has consumed some flags.</span>
            <span class="c1"># so the decoder need to increase the flags based on the encoder layer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">decoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TransformerDecoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="n">decoder_layers</span><span class="p">,</span>
                                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                  <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                  <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                                  <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                  <span class="n">src_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                                  <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                  <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                  <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                  <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                                  <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                                  <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                                  <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                  <span class="n">lambda_func</span><span class="o">=</span><span class="n">lambda_func</span><span class="p">,</span>
                                                  <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                  <span class="n">offset</span><span class="o">=</span><span class="n">encoder_layers</span><span class="p">,</span>
                                                  <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                                  <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_get_parallel_mode</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">AUTO_PARALLEL</span><span class="p">,):</span>
            <span class="n">_check_config</span><span class="p">(</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">src_seq_length</span> <span class="o">=</span> <span class="n">src_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tgt_seq_length</span> <span class="o">=</span> <span class="n">tgt_seq_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_past</span> <span class="o">=</span> <span class="n">use_past</span>
            <span class="k">if</span> <span class="n">encoder_layers</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">decoder_layers</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformer doest support encoder layer </span><span class="si">{</span><span class="n">encoder_layers</span><span class="si">}</span><span class="s2"> and decoder&quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;layer </span><span class="si">{</span><span class="n">decoder_layers</span><span class="si">}</span><span class="s2">, please use TransformerDecoder&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">encoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">decoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">use_past</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> with encoder and decoder does not support use_past=True.&quot;</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;For parallel mode, sharding propagation is recommended, you can use it by setting &quot;</span>
                           <span class="s2">&quot;&#39;set_auto_parallel_context(parallel_mode=ParallelMode.AUTO_PARALLEL, &quot;</span>
                           <span class="s2">&quot;search_mode=</span><span class="se">\&quot;</span><span class="s2">sharding_propagation</span><span class="se">\&quot;</span><span class="s2">)&#39; and &quot;</span>
                           <span class="s2">&quot;&#39;set_algo_parameters(elementwise_op_strategy_follow=False, fully_use_devices=False)&#39;&quot;</span><span class="p">)</span>
            <span class="c1"># The shard setting of Transformer is set within the TransformerEncoderLayer</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">lambda_func</span><span class="p">:</span>
                <span class="n">lambda_func</span> <span class="o">=</span> <span class="n">_get_lambda_func</span><span class="p">(</span><span class="n">total_layer</span><span class="o">=</span><span class="n">encoder_layers</span> <span class="o">+</span> <span class="n">decoder_layers</span><span class="p">)</span>
            <span class="n">_check_moe_config</span><span class="p">(</span><span class="n">moe_config</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span> <span class="o">=</span> <span class="p">(</span><span class="n">moe_config</span><span class="o">.</span><span class="n">expert_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((),</span> <span class="p">()))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">encoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="n">encoder_layers</span><span class="p">,</span>
                                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                  <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                  <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                                  <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                  <span class="n">seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                                  <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                  <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                  <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                                  <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                                  <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                  <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                  <span class="n">lambda_func</span><span class="o">=</span><span class="n">lambda_func</span><span class="p">,</span>
                                                  <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                  <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                                  <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Offset is needed as the encoder has consumed some flags.</span>
            <span class="c1"># so the decoder need to increase the flags based on the encoder layer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">decoder_layers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">TransformerDecoder</span><span class="p">(</span><span class="n">num_layers</span><span class="o">=</span><span class="n">decoder_layers</span><span class="p">,</span>
                                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                                  <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                                  <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="n">ffn_hidden_size</span><span class="p">,</span>
                                                  <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                                                  <span class="n">src_seq_length</span><span class="o">=</span><span class="n">src_seq_length</span><span class="p">,</span>
                                                  <span class="n">tgt_seq_length</span><span class="o">=</span><span class="n">tgt_seq_length</span><span class="p">,</span>
                                                  <span class="n">attention_dropout_rate</span><span class="o">=</span><span class="n">attention_dropout_rate</span><span class="p">,</span>
                                                  <span class="n">hidden_dropout_rate</span><span class="o">=</span><span class="n">hidden_dropout_rate</span><span class="p">,</span>
                                                  <span class="n">hidden_act</span><span class="o">=</span><span class="n">hidden_act</span><span class="p">,</span>
                                                  <span class="n">post_layernorm_residual</span><span class="o">=</span><span class="n">post_layernorm_residual</span><span class="p">,</span>
                                                  <span class="n">layernorm_compute_type</span><span class="o">=</span><span class="n">layernorm_compute_type</span><span class="p">,</span>
                                                  <span class="n">softmax_compute_type</span><span class="o">=</span><span class="n">softmax_compute_type</span><span class="p">,</span>
                                                  <span class="n">lambda_func</span><span class="o">=</span><span class="n">lambda_func</span><span class="p">,</span>
                                                  <span class="n">use_past</span><span class="o">=</span><span class="n">use_past</span><span class="p">,</span>
                                                  <span class="n">param_init_type</span><span class="o">=</span><span class="n">param_init_type</span><span class="p">,</span>
                                                  <span class="n">offset</span><span class="o">=</span><span class="n">encoder_layers</span><span class="p">,</span>
                                                  <span class="n">moe_config</span><span class="o">=</span><span class="n">moe_config</span><span class="p">,</span>
                                                  <span class="n">parallel_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cls_name</span><span class="si">}</span><span class="s2"> only support sharding propagation or &quot;</span>
                               <span class="sa">f</span><span class="s2">&quot;semi-auto parallel mode now.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder_inputs</span><span class="p">,</span>
                  <span class="n">encoder_masks</span><span class="p">,</span>
                  <span class="n">decoder_inputs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">decoder_masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">memory_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">init_reset</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                  <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="n">encoder_output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">output</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">encoder_layer_present</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">decoder_layer_present</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aux_loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
                <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_layer_present</span><span class="p">,</span> <span class="n">encoder_aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">encoder_masks</span><span class="p">,</span>
                                                                                       <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
                <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">accum_loss</span><span class="p">,</span> <span class="n">encoder_aux_loss</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_layer_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">encoder_masks</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span>
                                                                     <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">encoder_output</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># decoder mask should be created outside of the model</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
                <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_layer_present</span><span class="p">,</span> <span class="n">decoder_aux_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span> <span class="n">decoder_masks</span><span class="p">,</span>
                                                                                       <span class="n">encoder_output</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">,</span>
                                                                                       <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
                <span class="n">accum_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">accum_loss</span><span class="p">,</span> <span class="n">decoder_aux_loss</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_layer_present</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">,</span>
                                                                     <span class="n">decoder_masks</span><span class="p">,</span>
                                                                     <span class="n">encoder_output</span><span class="p">,</span>
                                                                     <span class="n">memory_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span>
                                                                     <span class="n">batch_valid_length</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">decoder_output</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">encoder_layer_present</span><span class="p">,</span> <span class="n">decoder_layer_present</span><span class="p">,</span> <span class="n">accum_loss</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">encoder_layer_present</span><span class="p">,</span> <span class="n">decoder_layer_present</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2022, MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>