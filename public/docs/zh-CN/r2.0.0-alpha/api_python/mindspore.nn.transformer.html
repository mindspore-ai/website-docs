<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.nn.transformer &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="mindspore.rewrite" href="mindspore.rewrite.html" />
    <link rel="prev" title="mindspore.nn.probability.distribution.Uniform" href="nn_probability/mindspore.nn.probability.distribution.Uniform.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0.0-alpha/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">mindspore.nn.transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/faq.html">常见问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/typical_api_comparision.html">与PyTorch典型区别</a></li>
<li class="toctree-l1"><a class="reference internal" href="../migration_guide/use_third_party_op.html">基于自定义算子接口调用第三方算子库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/distributed_configure.html">分布式配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>mindspore.nn.transformer</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api_python/mindspore.nn.transformer.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindspore-nn-transformer">
<h1>mindspore.nn.transformer<a class="headerlink" href="#mindspore-nn-transformer" title="永久链接至标题"></a></h1>
<div class="admonition note">
<p class="admonition-title">说明</p>
<p>Transformer网络。这些是实验性接口，可能会修改或删除。</p>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.AttentionMask">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">AttentionMask</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_dpmp_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#AttentionMask"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.AttentionMask" title="打开链接"></a></dt>
<dd><p>从输入掩码中获取下三角矩阵。输入掩码是值为1或0的二维Tensor (batch_size, seq_length)，1表示当前位置是一个有效的标记，0则表示当前位置不是一个有效的标记。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>seq_length</strong> (int) - 表示输入Tensor的序列长度。</p></li>
<li><p><strong>parallel_config</strong> (OpParallelConfig) - 表示并行配置。默认值为 <cite>default_dpmp_config</cite> ，表示一个带有默认参数的 <cite>OpParallelConfig</cite> 实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>input_mask</strong> (Tensor) - 掩码矩阵，shape为(batch_size, seq_length)，表示每个位置是否为有效输入。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tensor，表示shape为(batch_size, seq_length, seq_length)的注意力掩码矩阵。</p>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - <cite>seq_length</cite> 不是整数。</p></li>
<li><p><strong>ValueError</strong> - <cite>seq_length</cite> 不是正数。</p></li>
<li><p><strong>TypeError</strong> - <cite>parallel_config</cite> 不是OpParallelConfig的子类。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">AttentionMask</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask</span> <span class="o">=</span> <span class="n">AttentionMask</span><span class="p">(</span><span class="n">seq_length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mask_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">mask_array</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
<span class="go">[[[1. 0. 0. 0]</span>
<span class="go">  [1. 1. 0. 0]</span>
<span class="go">  [1. 1. 1. 0]</span>
<span class="go">  [0. 0. 0. 0]]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.VocabEmbedding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">VocabEmbedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vocab_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_embedding_parallel_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'normal'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#VocabEmbedding"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.VocabEmbedding" title="打开链接"></a></dt>
<dd><p>根据输入的索引查找参数表中的行作为返回值。当设置并行模式为 <cite>AUTO_PARALLEL_MODE</cite> 时，如果parallel_config.vocab_emb_dp为True时，那么embedding lookup表采用数据并行的方式，数据并行度为 <cite>parallel_config.data_parallel</cite> ，如果为False，按 <cite>parallel_config.model_parallel</cite> 对embedding表中的第零维度进行切分。</p>
<div class="admonition note">
<p class="admonition-title">说明</p>
<p>启用 <cite>AUTO_PARALLEL</cite> / <cite>SEMI_AUTO_PARALLEL</cite> 模式时，此层仅支持二维度的输入，因为策略是为2D输入而配置的。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>vocab_size</strong> (int) - 表示查找表的大小。</p></li>
<li><p><strong>embedding_size</strong> (int) - 表示查找表中每个嵌入向量的大小。</p></li>
<li><p><strong>param_init</strong> (Union[Tensor, str, Initializer, numbers.Number]) - 表示embedding_table的Initializer。当指定字符串时，请参见 <cite>initializer</cite> 类了解字符串的值。默认值：’normal’。</p></li>
<li><p><strong>parallel_config</strong> (EmbeddingOpParallelConfig) - 表示网络的并行配置。默认值为 <cite>default_embedding_parallel_config</cite> ，表示带有默认参数的 <cite>EmbeddingOpParallelConfig</cite> 实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>input_ids</strong> (Tensor) - shape为(batch_size, seq_length)的输入，其数据类型为int32。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tuple，表示一个包含(<cite>output</cite>, <cite>embedding_table</cite>)的元组。</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - shape为(batch_size, seq_length, embedding_size)嵌入向量查找结果。</p></li>
<li><p><strong>embedding_table</strong> (Tensor) - shape为(vocab_size, embedding_size)的嵌入表。</p></li>
</ul>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>ValueError</strong> - parallel_config.vocab_emb_dp为True时，词典的大小不是parallel_config.model_parallel的倍数。</p></li>
<li><p><strong>ValueError</strong> - <cite>vocab_size</cite> 不是正值。</p></li>
<li><p><strong>ValueError</strong> - <cite>embedding_size</cite> 不是正值。</p></li>
<li><p><strong>TypeError</strong> - <cite>parallel_config</cite> 不是OpParallelConfig的子类。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">VocabEmbedding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">VocabEmbedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">embedding_size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">table</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(20, 15, 30)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">table</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(30, 30)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.MultiHeadAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">MultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_init_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_dpmp_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#MultiHeadAttention"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.MultiHeadAttention" title="打开链接"></a></dt>
<dd><p>论文 <a class="reference external" href="https://arxiv.org/pdf/1706.03762v5.pdf">Attention Is All You Need</a> 中所述的多头注意力的实现。给定src_seq_length长度的query向量，tgt_seq_length长度的key向量和value，注意力计算流程如下：</p>
<div class="math notranslate nohighlight">
\[MultiHeadAttention(query, key, vector) = Dropout(Concat(head_1, \dots, head_h)W^O)\]</div>
<p>其中， <span class="math notranslate nohighlight">\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\)</span> 。注意：输出层的投影计算中带有偏置参数。</p>
<p>如果query tensor、key tensor和value tensor相同，则上述即为自注意力机制的计算过程。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>batch_size</strong> (int) - 表示增量预测时输入张量的批量大小，应该是正整数。当进行训练或预测时，该参数将不起作用，用户可将None传递给此参数。</p></li>
<li><p><strong>src_seq_length</strong> (int) - 表示query向量的序列长度。</p></li>
<li><p><strong>tgt_seq_length</strong> (int) - 表示key向量和value向量的序列长度。</p></li>
<li><p><strong>hidden_size</strong> (int) - 表示输入的向量大小。</p></li>
<li><p><strong>num_heads</strong> (int) - 表示注意力机制中头的数量。</p></li>
<li><p><strong>hidden_dropout_rate</strong> (float) - 表示最后dense输出的丢弃率。默认值：0.1</p></li>
<li><p><strong>attention_dropout_rate</strong> (float) - 表示注意力score的丢弃率。默认值：0.1</p></li>
<li><p><strong>compute_dtype</strong> (dtype.Number) - 表示dense中矩阵乘法的计算类型。默认值：mstype.float16。其值应为mstype.float32或mstype.float16。</p></li>
<li><p><strong>param_init_type</strong> (dtype.Number) - 表示模块的参数初始化类型。默认值：mstype.float32。其值应为mstype.float32或mstype.float16。</p></li>
<li><p><strong>softmax_compute_type</strong> (dtype.Number) - 表示softmax计算模块的类型。默认值：mstype.float32。  其值应为mstype.float32或mstype.float16。</p></li>
<li><p><strong>use_past</strong> (bool) - 使用过去状态进行计算，用于增量预测。例如，如果我们有两个单词，想生成十个或以上单词。我们只需要计算一次这两个单词的状态，然后逐个生成下一个单词。当use_past为True时，有两个步骤可以执行预测。
第一步是通过 <cite>model.add_flags_recursive(is_first_iteration=True)</cite> 将is_first_iteration设为True，并传递完整的输入。然后，通过 <cite>model.add_flags_recursive(is_first_iteration=False)</cite> 将is_first_iteration设为False。此时，传递step的输入tensor，并对其进行循环。默认值：False</p></li>
<li><p><strong>parallel_config</strong> (OpParallelConfig) - 表示并行配置。默认值为 <cite>default_dpmp_config</cite> ，表示一个带有参数的 <cite>OpParallelConfig</cite> 实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>query_tensor</strong> (Tensor) - use_past为False或is_first_iteration为True时，表示shape为(batch_size, src_seq_length, hidden_size)或(batch_size * src_seq_length, hidden_size)的query向量。否则，shape必须为(batch_size, 1, hidden_size)。</p></li>
<li><p><strong>key_tensor</strong> (Tensor) - use_past为False或is_first_iteration为True时，表示shape为(batch_size, tgt_seq_length, hidden_size)或(batch_size * tgt_seq_length, hidden_size)的key向量。否则，shape必须为(batch_size, 1, hidden_size)。</p></li>
<li><p><strong>value_tensor</strong> (Tensor) - use_past为False或is_first_iteration为True时，表示shape为(batch_size, tgt_seq_length, hidden_size)或(batch_size * tgt_seq_length, hidden_size)的value向量。否则，shape必须为(batch_size, 1, hidden_size)。</p></li>
<li><p><strong>attention_mask</strong> (Tensor) - use_past为False或is_first_iteration为True时，表示shape为(batch_size, src_seq_length, tgt_seq_length)的注意力掩码矩阵, 或者为None，None表示在Softmax计算中将不会进行掩码。否则，shape必须为(batch_size, 1, tgt_seq_length)。</p></li>
<li><p><strong>key_past</strong> (Tensor) - shape为(batch_size, num_heads, size_per_head, tgt_seq_length)的Float16 tensor，表示过去所计算的key向量。
当use_past为True时，需要传入非None值用于增量预测。默认值为None。</p></li>
<li><p><strong>value_past</strong> (Tensor) - shape为(batch_size, num_heads, tgt_seq_length, size_per_head)的Float16 tensor，表示过去所计算的value向量。
当use_past为True时，需要传入非None值用于增量预测。默认值为None。</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - shape为(batch_size,)的Int32 tensor，表示已经计算的token索引。
当use_past为True时，需要传入非None值用于增量预测。默认值为None。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tuple，表示一个包含(<cite>output</cite>, <cite>layer_present</cite>)的元组。</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - Tensor。use_past为False或is_first_iteration为True时，表示shape为(batch_size, src_seq_length, hidden_size)或(batch_size * src_seq_length, hidden_size)的层输出的float tensor。否则，shape将为(batch_size, 1, hidden_size)。</p></li>
<li><p><strong>layer_present</strong> (Tuple) - 表示shape为((batch_size, num_heads, size_per_head, tgt_seq_length)或(batch_size, num_heads, tgt_seq_length, size_per_head))的投影key向量和value向量的Tensor的元组。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">MultiHeadAttention</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">from_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">to_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_out</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">from_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">attn_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 20, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 5, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 20, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We need to prepare the memory parameters for saving key and value states firstly.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">use_past</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">key_past</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">value_past</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_valid_length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set is_first_iteration=True to generate the full memory states</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_out</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">from_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="p">,</span> <span class="n">value_past</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">attn_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 20, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 5, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 20, 5)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">from_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">to_tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than the</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># full sequence.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attn_out</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">from_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">,</span> <span class="n">key_past</span><span class="p">,</span> <span class="n">value_past</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">attn_out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 1, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 5, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 3, 20, 5)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.FeedForward">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">FeedForward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_rate</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expert_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expert_group_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_init_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_dpmp_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#FeedForward"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.FeedForward" title="打开链接"></a></dt>
<dd><p>具有两层线性层的多层感知器，并在最终输出上使用Dropout。第一个线性层将输入维度从hidden_size投影到ffn_hidden_size，并在中间应用激活层。第二个线性层将该维度从ffn_hidden_size投影到hidden_size。配置parallel_config之后，
第一个线性层的权重将在输入维度上被分片，第二个线性层在输出维度上进行切分。总体过程如下：</p>
<div class="math notranslate nohighlight">
\[Dropout((xW_1+b_1)W_2 + b_2)\]</div>
<p>其中 <span class="math notranslate nohighlight">\(W_1, W_2, b_1\)</span> 和 <span class="math notranslate nohighlight">\(b_2\)</span> 为可训练参数。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>hidden_size</strong> (int) - 表示输入的维度。</p></li>
<li><p><strong>ffn_hidden_size</strong> (int) - 表示中间隐藏大小。</p></li>
<li><p><strong>dropout_rate</strong> (float) - 表示第二个线性层输出的丢弃率。</p></li>
<li><p><strong>hidden_act</strong> (str, nn.Cell) - 表示前馈层的激活行为。其值可为’relu’、’relu6’、’tanh’、’gelu’、’fast_gelu’、’elu’、’sigmoid’、’prelu’、’leakyrelu’、’hswish’、’hsigmoid’、’logsigmoid’等等。用户可以传入自定义的激活函数。如果用户要想在并行模式下运行此网络，自定义的激活函数必须提供 <cite>activation_shard</cite> 类方法。请查看类 <cite>mindspore.nn.transformer.FeedForward</cite> 的示例。默认值：gelu。</p></li>
<li><p><strong>expert_num</strong> (int) - 表示线性层中使用的专家数量。对于expert_num &gt; 1用例，使用BatchMatMul。BatchMatMul中的第一个维度表示expert_num。默认值：1。</p></li>
<li><p><strong>expert_group_size</strong> (int) - 表示每个数据并行组收到的词语（token）数量。默认值：None。该参数只在自动并行且非策略传播模式下起作用。</p></li>
<li><p><strong>param_init_type</strong> (dtype.Number) - 表示参数初始化类型。其值应为mstype.float32或mstype.float16。默认值：mstype.float32。</p></li>
<li><p><strong>parallel_config</strong> (OpParallelConfig, MoEParallelConfig) - 表示配置该网络的并行度的并行配置。更多详情，请参见 <a class="reference internal" href="#mindspore.nn.transformer.OpParallelConfig" title="mindspore.nn.transformer.OpParallelConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">mindspore.nn.transformer.OpParallelConfig</span></code></a> 。默认值为 <cite>default_dpmp_config</cite> ，表示一个带有默认参数的 <cite>OpParallelConfig</cite> 实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - 应为 <cite>[batch, seq_length, hidden_size]或[batch * seq_length, hidden_size]</cite> 。表示浮点Tensor。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tensor，表示映射后该层的输出。shape为 <cite>[batch, seq_length, hidden_size]</cite> 或 <cite>[batch * seq_length, hidden_size]</cite> 。</p>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - <cite>hidden_act</cite> 不是字符串或者nn.Cell。</p></li>
<li><p><strong>TypeError</strong> - <cite>parallel_config</cite> 不是OpParallelConfig的子类。</p></li>
<li><p><strong>ValueError</strong> - <cite>ffn_hidden_size</cite> 不是parallel_config中model_parallel的倍数。</p></li>
<li><p><strong>ValueError</strong> - <cite>hidden_size</cite> 不是parallel_config中model_parallel的倍数。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">FeedForward</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 20, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Example 2 using custom hidden activation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyActivationNoShard</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">MyActivationNoShard</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                    <span class="n">hidden_act</span><span class="o">=</span><span class="n">MyActivationNoShard</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 20, 15)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Example 3 using custom hidden activation with activation_shard</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If user wantss to run on the SEMI/AUTO parallel mode, the custom activation must provide</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># a class function named activation_shard. It accepts the argument parallel_config (OpParallelConfig,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># MoEParallelConfig) and set the shard for the primitives used in the construct.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MyActivationWithShard</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">super</span><span class="p">(</span><span class="n">MyActivationWithShard</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Add</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">activation_shard</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parallel_config</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span><span class="p">),</span> <span class="p">()))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                    <span class="n">hidden_act</span><span class="o">=</span><span class="n">MyActivationWithShard</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">15</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 20, 15)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.TransformerEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_layernorm_residual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layernorm_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_init_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_func</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moe_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_moe_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_transformer_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#TransformerEncoder"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.TransformerEncoder" title="打开链接"></a></dt>
<dd><p>Transformer中的编码器模块，具有多层堆叠的 <cite>TransformerEncoderLayer</cite> ，包括多头自注意力层和前馈层。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>batch_size</strong> (int) - 表示增量预测时输入张量的批量大小，应该是正整数。当进行训练或预测时，该参数将不起作用，用户可将None传递给此参数。</p></li>
<li><p><strong>num_layers</strong> (int) - 表示 <cite>TransformerEncoderLayer</cite> 的层。</p></li>
<li><p><strong>hidden_size</strong> (int) - 表示输入的隐藏大小。</p></li>
<li><p><strong>ffn_hidden_size</strong> (int) - 表示前馈层中bottleneck的隐藏大小。</p></li>
<li><p><strong>seq_length</strong> (int) - 表示输入序列长度。</p></li>
<li><p><strong>num_heads</strong> (int) - 表示注意力头的数量。</p></li>
<li><p><strong>hidden_dropout_rate</strong> (float) - 表示作用在隐藏层输出的丢弃率。默认值：0.1</p></li>
<li><p><strong>attention_dropout_rate</strong> (float) - 表示注意力score的丢弃率。默认值：0.1</p></li>
<li><p><strong>post_layernorm_residual</strong> (bool) - 表示是否在LayerNorm之前使用残差，即是否选择残差为Post-LayerNorm或者Pre-LayerNorm。默认值：False</p></li>
<li><p><strong>hidden_act</strong> (str, nn.Cell) - 表示前馈层的激活行为。其值可为’relu’、’relu6’、’tanh’、’gelu’、’fast_gelu’、’elu’、’sigmoid’、’prelu’、’leakyrelu’、’hswish’、’hsigmoid’、’logsigmoid’等等。用户可以传入自定义的激活函数。如果用户要想在并行模式下运行此网络，自定义的激活函数必须提供 <cite>activation_shard</cite> 类方法。请查看类 <cite>mindspore.nn.transformer.FeedForward</cite> 的示例。默认值：gelu。</p></li>
<li><p><strong>layernorm_compute_type</strong> (dtype.Number) - 表示LayerNorm的计算类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>softmax_compute_type</strong> (dtype.Number) - 表示注意力中softmax的计算类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>param_init_type</strong> (dtype.Number) - 表示模块的参数初始化类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>use_past</strong> (bool) - 使用过去状态进行计算，用于增量预测。例如，如果我们有两个单词，想生成十个或以上单词。我们只需要计算一次这两个单词的状态，然后逐个生成下一个单词。当use_past为True时，有两个步骤可以运行预测。第一步是通过 <cite>model.add_flags_recursive(is_first_iteration=True)</cite> 将is_first_iteration设为True，并传递完整的输入。然后，通过 <cite>model.add_flags_recursive(is_first_iteration=False)</cite> 将is_first_iteration设为False。此时，传递step的输入tensor，并对其进行环回。默认值：False。</p></li>
<li><p><strong>lambda_func</strong> (function) - 表示设置融合索引、pipeline阶段和重计算属性的函数。如果用户想确定pipeline阶段和梯度聚合融合，用户可以传递一个接受 <cite>network</cite> 、 <cite>layer_id</cite> 、 <cite>offset</cite> 、 <cite>parallel_config</cite> 和 <cite>layers</cite> 的函数。 <cite>network(Cell)</cite> 表示transformer块， <cite>layer_id(int)</cite> 表示当前模块的层索引，从零开始计数， <cite>offset(int)</cite> 表示如果网络中还有其他模块，则layer_index需要一个偏置。pipeline的默认设置为： <cite>(layer_id + offset) // (layers / pipeline_stage)</cite> 。默认值：None。</p></li>
<li><p><strong>offset</strong> (int) - 表示 <cite>decoder</cite> 的初始层索引。其用于设置梯度聚合的融合值和流水线并行的stage值。默认值：0。</p></li>
<li><p><strong>moe_config</strong> (MoEConfig) - 表示MoE (Mixture of Expert)的配置。默认值为 <cite>default_moe_config</cite> ，表示带有默认参数的 <cite>MoEConfig</cite> 实例。</p></li>
<li><p><strong>parallel_config</strong> (TransformerOpParallelConfig) - 表示并行配置。默认值为 <cite>default_transformer_config</cite> ，表示带有默认参数的 <cite>TransformerOpParallelConfig</cite> 实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>hidden_states</strong> (Tensor) - Tensor。如果use_past为False或者is_first_iteration为True，shape为[batch_size, seq_length, hidden_size]或者[batch_size * seq_length, hidden_size]。否则，shape应为[batch_size, 1, hidden_size]。</p></li>
<li><p><strong>attention_mask</strong> (Tensor) - Tensor。use_past为False或者is_first_iteration为True时，表示shape为[batch_size, seq_length, seq_length]的注意力掩码，或者为None，None表示在Softmax计算中将不会进行掩码。否则，shape应为[batch_size, 1, hidden_size]。</p></li>
<li><p><strong>init_reset</strong> (Tensor) - shape为[1]的bool tensor，用于清除增量预测中使用的past key参数和past value参数。仅当use_past为True时有效。默认值为True。</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - shape为[batch_size]的Int32 tensor，表示过去所计算的索引。当use_past为True时，它用于增量预测。默认值为None。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tuple，表示一个包含(<cite>output</cite>, <cite>layer_present</cite>)的元组。</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - use_past为False或is_first_iteration为True时，表示shape为(batch_size, seq_length, hidden_size)或(batch_size * seq_length, hidden_size)的层输出的float tensor。否则，shape将为(batch_size, 1, hidden_size)。</p></li>
<li><p><strong>layer_present</strong> (Tuple) - 大小为num_layers的元组，其中每个元组都包含shape为((batch_size, num_heads, size_per_head, seq_length)或(batch_size, num_heads, seq_length, size_per_head))的投影key向量和value向量的Tensor的元组。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">TransformerEncoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">seq_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 16, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">past</span><span class="p">))</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_valid_length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_reset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set is_first_iteration=True to generate the full memory states</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">use_past</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 16, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_reset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">False</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the full sequence.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 1, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.TransformerDecoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerDecoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_layernorm_residual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layernorm_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_init_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_func</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moe_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_moe_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_transformer_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#TransformerDecoder"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.TransformerDecoder" title="打开链接"></a></dt>
<dd><p>Transformer中的解码器模块，为多层堆叠的 <cite>TransformerDecoderLayer</cite> ，包括多头自注意力层、交叉注意力层和前馈层。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>batch_size</strong> (int) - 表示增量预测时输入张量的批量大小，应该是正整数。当进行训练或预测时，该参数将不起作用，用户可将None传递给此参数。</p></li>
<li><p><strong>num_layers</strong> (int) - 表示 <cite>TransformerDecoderLayer</cite> 的层数。</p></li>
<li><p><strong>hidden_size</strong> (int) - 表示输入的隐藏大小。</p></li>
<li><p><strong>ffn_hidden_size</strong> (int) - 表示前馈层中bottleneck的隐藏大小。</p></li>
<li><p><strong>src_seq_length</strong> (int) - 表示输入源序列长度。</p></li>
<li><p><strong>tgt_seq_length</strong> (int) - 表示输入目标序列长度。</p></li>
<li><p><strong>num_heads</strong> (int) - 表示注意力头的数量。</p></li>
<li><p><strong>hidden_dropout_rate</strong> (float) - 表示作用在隐藏层输出的丢弃率。默认值：0.1</p></li>
<li><p><strong>attention_dropout_rate</strong> (float) - 表示注意力score的丢弃率。默认值：0.1</p></li>
<li><p><strong>post_layernorm_residual</strong> (bool) - 表示是否在LayerNorm之前使用残差，即是否选择残差为Post-LayerNorm或者Pre-LayerNorm。默认值：False</p></li>
<li><p><strong>hidden_act</strong> (str, nn.Cell) - 表示前馈层的激活行为。其值可为’relu’、’relu6’、’tanh’、’gelu’、’fast_gelu’、’elu’、’sigmoid’、’prelu’、’leakyrelu’、’hswish’、’hsigmoid’、’logsigmoid’等等。用户可以传入自定义的激活函数。如果用户要想在并行模式下运行此网络，自定义的激活函数必须提供 <cite>activation_shard</cite> 类方法。请查看类 <cite>mindspore.nn.transformer.FeedForward</cite> 的示例。默认值：gelu。</p></li>
<li><p><strong>layernorm_compute_type</strong> (dtype.Number) - 表示LayerNorm的计算类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>use_past</strong> (bool) - 表示是否开启增量推理。在推理中会缓存注意力机制计算结果，避免冗余计算。默认值为False。</p></li>
<li><p><strong>softmax_compute_type</strong> (dtype.Number) - 表示注意力中softmax的计算类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>param_init_type</strong> (dtype.Number) - 表示模块的参数初始化类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>offset</strong> (int) - 表示 <cite>decoder</cite> 的初始层索引偏移值。其用于设置梯度聚合的融合值和流水线并行的stage值，使其不与编码器层的相关属性重叠。默认值为0。</p></li>
<li><p><strong>lambda_func</strong> (function) - 表示确定梯度融合索引、pipeline阶段和重计算属性的函数。如果用户想确定pipeline阶段和梯度聚合融合，用户可以传递一个接受 <cite>network</cite> 、 <cite>layer_id</cite> 、 <cite>offset</cite> 、 <cite>parallel_config</cite> 和 <cite>layers</cite> 的函数。 <cite>network(Cell)</cite> 表示transformer块， <cite>layer_id(int)</cite> 表示当前模块的层索引，从零开始计数， <cite>offset(int)</cite> 表示如果网络中还有其他模块，则layer_index需要一个偏置。pipeline的默认设置为： <cite>(layer_id + offset) // (layers / pipeline_stage)</cite> 。默认值：None。</p></li>
<li><p><strong>moe_config</strong> (MoEConfig) - 表示MoE (Mixture of Expert)的配置。默认值为 <cite>default_moe_config</cite> ，表示带有默认参数的 <cite>MoEConfig</cite> 实例。</p></li>
<li><p><strong>parallel_config</strong> (TransformerOpParallelConfig) - 表示并行配置。默认值为 <cite>default_transformer_config</cite> ，表示带有默认参数的 <cite>TransformerOpParallelConfig</cite> 实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>hidden_stats</strong> (Tensor) - shape为[batch_size, seq_length, hidden_size]或[batch_size * seq_length, hidden_size]的输入tensor。</p></li>
<li><p><strong>attention_mask</strong> (Tensor) - shape为[batch_size, seq_length, seq_length]的解码器的注意力掩码。或者为None，None表示将不会在self attention中的Softmax计算中引入掩码计算。</p></li>
<li><p><strong>encoder_output</strong> (Tensor) - shape为[batch_size, seq_length, hidden_size]或[batch_size * seq_length, hidden_size]的编码器的输出。</p>
</li>
<li><p><strong>memory_mask</strong> (Tensor) - shape为[batch, tgt_seq_length, src_seq_length]的交叉注意力的memory掩码，其中tgt_seq_length表示解码器的长度。或者为None，None表示将不会在cross attention中的Softmax计算中引入掩码计算。</p></li>
<li><p><strong>init_reset</strong> (Tensor) - shape为[1]的bool tensor，用于清除增量预测中使用的past key参数和past value参数。仅当use_past为True时有效。默认值为True。</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - shape为[batch_size]的Int32 tensor，表示过去所计算的索引。当use_past为True时，它用于增量预测。默认值为None。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tuple，表示一个包含(<cite>output</cite>, <cite>layer_present</cite>)的元组。</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - 输出的logit。shape为[batch, tgt_seq_length, hidden_size]或[batch * tgt_seq_length, hidden_size]。</p></li>
<li><p><strong>layer_present</strong> (Tuple) - 大小为层数的元组，其中每个元组都是shape为((batch_size, num_heads, size_per_head, tgt_seq_length)或(batch_size, num_heads, tgt_seq_length, size_per_head)的自注意力中的投影key向量和value向量的tensor的元组，或者是shape为(batch_size, num_heads, size_per_head, src_seq_length)或(batch_size, num_heads, src_seq_length, size_per_head))的交叉注意力中的投影key向量和value向量的tensor的元组。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">TransformerDecoder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerDecoder</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">memory_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">decoder_input_value</span><span class="p">,</span> <span class="n">decoder_input_mask</span><span class="p">,</span> <span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 10, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">past</span><span class="p">))</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 10)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 10, 32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 20, 32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.TransformerEncoderLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerEncoderLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_layernorm_residual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layernorm_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_init_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moe_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_moe_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_dpmp_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#TransformerEncoderLayer"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.TransformerEncoderLayer" title="打开链接"></a></dt>
<dd><p>Transformer的编码器层。Transformer的编码器层上的单层的实现，包括多头注意力层和前馈层。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>batch_size</strong> (int) - 表示增量预测时输入张量的批量大小，应该是正整数。当进行训练或预测时，该参数将不起作用，用户可将None传递给此参数。</p></li>
<li><p><strong>hidden_size</strong> (int) - 表示输入的隐藏大小。</p></li>
<li><p><strong>seq_length</strong> (int) - 表示输入序列长度。</p></li>
<li><p><strong>ffn_hidden_size</strong> (int) - 表示前馈层中bottleneck的隐藏大小。</p></li>
<li><p><strong>num_heads</strong> (int) - 表示注意力头的数量。</p></li>
<li><p><strong>hidden_dropout_rate</strong> (float) - 表示作用在隐藏层输出的丢弃率。默认值：0.1</p></li>
<li><p><strong>attention_dropout_rate</strong> (float) - 表示注意力score的丢弃率。默认值：0.1</p></li>
<li><p><strong>post_layernorm_residual</strong> (bool) - 表示是否在LayerNorm之前使用残差，即是否选择残差为Post-LayerNorm或者Pre-LayerNorm。默认值：False</p></li>
<li><p><strong>hidden_act</strong> (str, nn.Cell) - 表示前馈层的激活行为。其值可为’relu’、’relu6’、’tanh’、’gelu’、’fast_gelu’、’elu’、’sigmoid’、’prelu’、’leakyrelu’、’hswish’、’hsigmoid’、’logsigmoid’等等。用户可以传入自定义的激活函数。如果用户要想在并行模式下运行此网络，自定义的激活函数必须提供 <cite>activation_shard</cite> 类方法。请查看类 <cite>mindspore.nn.transformer.FeedForward</cite> 的示例。默认值：gelu。</p></li>
<li><p><strong>layernorm_compute_type</strong> (dtype.Number) - 表示LayerNorm的计算类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>softmax_compute_type</strong> (dtype.Number) - 表示注意力中softmax的计算类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>param_init_type</strong> (dtype.Number) - 表示模块的参数初始化类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>use_past</strong> (bool) - 使用过去状态进行计算，用于增量预测。例如，如果我们有两个单词，想生成十个或以上单词。我们只需要计算一次这两个单词的状态，然后逐个生成下一个单词。当use_past为True时，有两个步骤可以运行预测。第一步是通过 <cite>model.add_flags_recursive(is_first_iteration=True)</cite> 将is_first_iteration设为True，并传递完整的输入。然后，通过 <cite>model.add_flags_recursive(is_first_iteration=False)</cite> 将is_first_iteration设为False。此时，传递step的输入tensor，并对其进行环回。默认值：False</p></li>
<li><p><strong>moe_config</strong> (MoEConfig) - 表示MoE (Mixture of Expert)的配置。默认值为 <cite>default_moe_config</cite> ，表示带有默认参数的 <cite>MoEConfig</cite> 实例。</p></li>
<li><p><strong>parallel_config</strong> (OpParallelConfig, MoEParallelConfig) - 表示并行配置。默认值为 <cite>default_dpmp_config</cite> ，表示一个带有默认参数的 <cite>OpParallelConfig</cite> 实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>x</strong> (Tensor) - Float Tensor。如果use_past为False或者is_first_iteration为True，shape应为[batch_size, seq_length, hidden_size]或者[batch_size * seq_length, hidden_size]。否则，shape应为[batch_size, 1, hidden_size]。</p></li>
<li><p><strong>input_mask</strong> (Tensor) - Float tensor。use_past为False或者is_first_iteration为True时，表示shape为[batch_size, seq_length, seq_length]的注意力掩码，或者为None，None表示在Softmax计算中将不会进行掩码。否则，shape应为[batch_size, 1, hidden_size]。</p></li>
<li><p><strong>init_reset</strong> (Tensor) - shape为[1]的bool tensor，用于清除增量预测中使用的past key参数和past value参数。仅当use_past为True时有效。默认值为True。</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - shape为[batch_size]的Int32 tensor，表示过去所计算的索引。当use_past为True时，它用于增量预测。默认值为None。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tuple，表示一个包含(<cite>output</cite>, <cite>layer_present</cite>)的元组。</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - use_past为False或is_first_iteration为True时，表示shape为(batch_size, seq_length, hidden_size)或(batch_size * seq_length, hidden_size)的层输出的float tensor。否则，shape将为(batch_size, 1, hidden_size)。</p></li>
<li><p><strong>layer_present</strong> (Tuple) - 表示shape为((batch_size, num_heads, size_per_head, seq_length)或(batch_size, num_heads, seq_length, size_per_head))的投影key向量和value向量的Tensor的元组。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">TransformerEncoderLayer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span>                                <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 16, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># When use use_past=True, it includes two steps to implement the incremental prediction.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Step 1: set is_first_iteration=True, and input the full sequence length&#39;s state.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_valid_length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_reset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Set is_first_iteration=True to generate the full memory states</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">seq_length</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
<span class="gp">... </span>                                <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">use_past</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 16, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">16</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">init_reset</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="kc">False</span><span class="p">],</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Step 2: set is_first_iteration=False, and pass the single word to run the prediction rather than</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the full sequence.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">add_flags_recursive</span><span class="p">(</span><span class="n">is_first_iteration</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hidden</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">init_reset</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">hidden</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 1, 8)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 4, 16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 16, 4)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.TransformerDecoderLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerDecoderLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_layernorm_residual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layernorm_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_init_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moe_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_moe_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_dpmp_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#TransformerDecoderLayer"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.TransformerDecoderLayer" title="打开链接"></a></dt>
<dd><p>Transformer的解码器层。Transformer的解码器层上的单层的实现，包括自注意力层、交叉注意力层和前馈层。当encoder_output为None时，交叉注意力将无效。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>batch_size</strong> (int) - 表示增量预测时输入张量的批量大小，应该是正整数。当进行训练或预测时，该参数将不起作用，用户可将None传递给此参数。</p></li>
<li><p><strong>hidden_size</strong> (int) - 表示输入的隐藏大小。</p></li>
<li><p><strong>src_seq_length</strong> (int) - 表示输入源序列长度。</p></li>
<li><p><strong>tgt_seq_length</strong> (int) - 表示输入目标序列长度。</p></li>
<li><p><strong>ffn_hidden_size</strong> (int) - 表示前馈层中bottleneck的隐藏大小。</p></li>
<li><p><strong>num_heads</strong> (int) - 表示注意力头的数量。</p></li>
<li><p><strong>hidden_dropout_rate</strong> (float) - 表示作用在隐藏层输出的丢弃率。默认值：0.1</p></li>
<li><p><strong>attention_dropout_rate</strong> (float) - 表示注意力score的丢弃率。默认值：0.1</p></li>
<li><p><strong>post_layernorm_residual</strong> (bool) - 表示是否在LayerNorm之前使用残差，即是否选择残差为Post-LayerNorm或者Pre-LayerNorm。默认值：False</p></li>
<li><p><strong>hidden_act</strong> (str, nn.Cell) - 表示前馈层的激活行为。其值可为’relu’、’relu6’、’tanh’、’gelu’、’fast_gelu’、’elu’、’sigmoid’、’prelu’、’leakyrelu’、’hswish’、’hsigmoid’、’logsigmoid’等等。用户可以传入自定义的激活函数。如果用户要想在并行模式下运行此网络，自定义的激活函数必须提供 <cite>activation_shard</cite> 类方法。请查看类 <cite>mindspore.nn.transformer.FeedForward</cite> 的示例。默认值：gelu。</p></li>
<li><p><strong>layernorm_compute_type</strong> (dtype.Number) - 表示LayerNorm的计算类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>softmax_compute_type</strong> (dtype.Number) - 表示注意力中softmax的计算类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>param_init_type</strong> (dtype.Number) - 表示模块的参数初始化类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>use_past</strong> (bool) - 使用过去状态进行计算，用于增量预测。默认值：False。</p></li>
<li><p><strong>moe_config</strong> (MoEConfig) - 表示MoE (Mixture of Expert)的配置。默认值为 <cite>default_moe_config</cite> ，表示带有默认参数的 <cite>MoEConfig</cite> 实例。</p></li>
<li><p><strong>parallel_config</strong> (OpParallelConfig, MoEParallelConfig) - 表示并行配置。默认值为 <cite>default_dpmp_config</cite> ，表示一个带有默认参数的 <cite>OpParallelConfig</cite> 实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>hidden_stats</strong> (Tensor) - shape为[batch_size, tgt_seq_length, hidden_size]或[batch_size * tgt_seq_length, hidden_size]的输入tensor。</p></li>
<li><p><strong>decoder_mask</strong> (Tensor) - shape为[batch_size, src_seq_length, seq_length]的解码器的注意力掩码。或者为None，None表示将不会在self attention中的Softmax计算中引入掩码计算。</p></li>
<li><p><strong>encoder_output</strong> (Tensor) - shape为[batch_size, seq_length, hidden_size]或[batch_size * seq_length, hidden_size]的编码器的输出。注：当网络位于最外层时，此参数不能通过None传递。默认值为None。</p></li>
<li><p><strong>memory_mask</strong> (Tensor) - shape为[batch, tgt_seq_length, src_seq_length]的交叉注意力的memory掩码，其中tgt_seq_length表示解码器的长度。或者为None，None表示将不会在cross attention中的Softmax计算中引入掩码计算。</p></li>
<li><p><strong>init_reset</strong> (Tensor) - shape为[1]的bool tensor，用于清除增量预测中使用的past key参数和past value参数。仅当use_past为True时有效。默认值为True。</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - shape为[batch_size]的Int32 tensor，表示过去所计算的索引。当use_past为True时，它用于增量预测。默认值为None。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tuple，表示一个包含(<cite>output</cite>, <cite>layer_present</cite>)的元组。</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - 此层的输出logit。shape为[batch, seq_length, hidden_size]或[batch * seq_length, hidden_size]。</p></li>
<li><p><strong>layer_present</strong> (Tuple) - 元组，其中每个元组都是shape为((batch_size, num_heads, size_per_head, tgt_seq_length)或(batch_size, num_heads, tgt_seq_length, size_per_head)的自注意力中的投影key向量和value向量的tensor的元组，或者是shape为(batch_size, num_heads, size_per_head, src_seq_length)或(batch_size, num_heads, src_seq_length, size_per_head))的交叉注意力中的投影key向量和value向量的tensor的元组。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">TransformerDecoderLayer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TransformerDecoderLayer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                                <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">memory_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">decoder_input_value</span><span class="p">,</span> <span class="n">decoder_input_mask</span><span class="p">,</span> <span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 10, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 10)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 10, 32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">past</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 20, 32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.Transformer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">Transformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_hidden_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">src_seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tgt_seq_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">encoder_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dropout_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">post_layernorm_residual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layernorm_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">softmax_compute_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_init_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">mstype.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_func</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_past</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moe_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_moe_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_transformer_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#Transformer"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.Transformer" title="打开链接"></a></dt>
<dd><p>Transformer模块，包括编码器和解码器。与原始的实现方式的区别在于该模块在实行层归一化之前使用了残差加法。默认的激活层为 <cite>gelu</cite> 。
详细信息可参考 <a class="reference external" href="https://arxiv.org/pdf/1706.03762v5.pdf">Attention Is All You Need</a> 。</p>
<div class="admonition note">
<p class="admonition-title">说明</p>
<p>这是一个实验接口，可能会被更改或者删除。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>batch_size</strong> (int) - 表示增量预测时输入张量的批量大小，应该是正整数。当进行训练或预测时，该参数将不起作用，用户可将None传递给此参数。</p></li>
<li><p><strong>encoder_layers</strong> (int) - 表示 <cite>TransformerEncoderLayer</cite> 的层数。</p></li>
<li><p><strong>decoder_layers</strong> (int) - 表示 <cite>TransformerDecoderLayer</cite> 的层数。</p></li>
<li><p><strong>hidden_size</strong> (int) - 表示输入向量的大小。</p></li>
<li><p><strong>ffn_hidden_size</strong> (int) - 表示前馈层中bottleneck的隐藏大小。</p></li>
<li><p><strong>src_seq_length</strong> (int) - 表示编码器的输入Tensor的seq_length。</p></li>
<li><p><strong>tgt_seq_length</strong> (int) - 表示解码器的输入Tensor的seq_length。</p></li>
<li><p><strong>num_heads</strong> (int) - 表示注意力头的数量。默认值：2</p></li>
<li><p><strong>hidden_dropout_rate</strong> (float) - 表示作用在隐藏层输出的丢弃率。默认值：0.1</p></li>
<li><p><strong>attention_dropout_rate</strong> (float) - 表示注意力score的丢弃率。默认值：0.1</p></li>
<li><p><strong>post_layernorm_residual</strong> (bool) - 表示是否在LayerNorm之前使用残差，即是否选择残差为Post-LayerNorm或者Pre-LayerNorm。默认值：False</p></li>
<li><p><strong>use_past</strong> (bool) - 表示是否开启增量推理。在推理中会缓存注意力机制计算结果，避免冗余计算。默认值为False。</p></li>
<li><p><strong>layernorm_compute_type</strong> (dtype.Number) - 表示LayerNorm的计算类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>softmax_compute_type</strong> (dtype.Number) - 表示注意力机制中softmax的计算类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>param_init_type</strong> (dtype.Number) - 表示模块的参数初始化类型。其值应为mstype.float32或mstype.float16。默认值为mstype.float32。</p></li>
<li><p><strong>hidden_act</strong> (str, nn.Cell) - 表示前馈层的激活行为。其值可为’relu’、’relu6’、’tanh’、’gelu’、’fast_gelu’、’elu’、’sigmoid’、’prelu’、’leakyrelu’、’hswish’、’hsigmoid’、’logsigmoid’等等。用户可以传入自定义的激活函数。如果用户要想在并行模式下运行此网络，自定义的激活函数必须提供 <cite>activation_shard</cite> 类方法。请查看类 <cite>mindspore.nn.transformer.FeedForward</cite> 的示例。默认值：gelu。</p></li>
<li><p><strong>moe_config</strong> (MoEConfig) - 表示MoE (Mixture of Expert)的配置。默认值为 <cite>default_moe_config</cite> ，表示带有默认参数的 <cite>MoEConfig</cite> 实例。</p></li>
<li><p><strong>lambda_func</strong> - 表示设置融合索引、pipeline阶段和重计算属性的函数。如果用户想确定pipeline阶段和梯度融合，用户可以传递一个接受 <cite>network</cite> 、 <cite>layer_id</cite> 、 <cite>offset</cite> 、 <cite>parallel_config</cite> 和 <cite>layers</cite> 的函数。 <cite>network(Cell)</cite> 表示transformer块， <cite>layer_id(int)</cite> 表示当前模块的层索引，从零开始计数， <cite>offset(int)</cite> 表示如果网络中还有其他模块，则layer_id需要一个偏移。pipeline的默认设置为： <cite>(layer_id + offset) // ((encoder_layers + decoder_length) / pipeline_stage)</cite> 。默认值为None。</p></li>
<li><p><strong>parallel_config</strong> (TransformerOpParallelConfig) - 表示并行配置。默认值为 <cite>default_transformer_config</cite> ，表示带有默认参数的 <cite>TransformerOpParallelConfig</cite> 实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>encoder_inputs</strong> (Tensor) - shape为[batch_size, seq_length, hidden_size]或[batch_size * seq_length, hidden_size]的输入Tensor。</p></li>
<li><p><strong>encoder_masks</strong> (Tensor) - shape为[batch_size, seq_length, seq_length]的解码器的注意力掩码。或者为None，None表示在编码器中self attention中的Softmax计算中将不会进行掩码。</p></li>
<li><p><strong>decoder_inputs</strong> (Tensor) - shape为[batch_size, seq_length, hidden_size]或[batch_size * seq_length, hidden_size]的编码器的输出。如果解码器层数为0，则此值应为None。</p></li>
<li><p><strong>decoder_masks</strong> (Tensor) - shape为[batch_size, seq_length, seq_length]的解码器的注意力掩码。或者为None，None表示将不会在解码器中的self attention中的Softmax计算中引入掩码计算。</p></li>
<li><p><strong>memory_mask</strong> (Tensor) - shape为[batch, tgt_seq_length,  src_seq_length]的交叉注意力的memory掩码，其中tgt_seq_length表示解码器的长度。或者为None，None表示将不会在cross attention中的Softmax计算中引入掩码计算。</p></li>
<li><p><strong>init_reset</strong> (Tensor) - shape为[1]的bool tensor，用于清除增量预测中使用的past key参数和past value参数。仅当use_past为True时有效。默认值为True。</p></li>
<li><p><strong>batch_valid_length</strong> (Tensor) - shape为[batch_size]的Int32 tensor，表示过去所计算的索引。当use_past为True时，它用于增量预测。默认值为None。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tuple，表示包含(<cite>output</cite>, <cite>encoder_layer_present</cite>, <cite>encoder_layer_present</cite>, <cite>accum_loss</cite>)的元组。</p>
<ul class="simple">
<li><p><strong>output</strong> (Tensor) - 如果只有编码器，则表示编码器层的输出logit。shape为[batch, src_seq_length, hidden_size] or [batch * src_seq_length, hidden_size]。如果有编码器和解码器，则输出来自于解码器层。shape为[batch, tgt_seq_length, hidden_size]或[batch * tgt_seq_length, hidden_size]。</p></li>
<li><p><strong>encoder_layer_present</strong> (Tuple) - 大小为num_layers的元组，其中每个元组都是shape为((batch_size, num_heads, size_per_head, src_seq_length)或(batch_size, num_heads, src_seq_length, size_per_head))的自注意力中的投影key向量和value向量的tensor的元组。</p></li>
<li><p><strong>decoder_layer_present</strong> (Tuple) - 大小为num_layers的元组，其中每个元组都是shape为((batch_size, num_heads, size_per_head, tgt_seq_length)或(batch_size, num_heads, tgt_seq_length, size_per_head))的自注意力中的投影key向量和value向量的tensor的元组，或者是shape为((batch_size, num_heads, size_per_head, src_seq_length)或(batch_size, num_heads, src_seq_length, size_per_head))的交叉注意力中的投影key向量和value向量的tensor的元组。如果未设置解码器，返回值将为None。</p></li>
<li><p><strong>accum_loss</strong> (Tensor) - 表示一个辅助损失来最小化路由到每个专家的数据部分的均方，且仅仅在专家数大于1时才会返回。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">Transformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">Transformer</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">decoder_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>                    <span class="n">ffn_hidden_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">src_seq_length</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">tgt_seq_length</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoder_input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">memory_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span><span class="p">,</span> <span class="n">en_past</span><span class="p">,</span> <span class="n">de_past</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">encoder_input_value</span><span class="p">,</span> <span class="n">encoder_input_mask</span><span class="p">,</span> <span class="n">decoder_input_value</span><span class="p">,</span>
<span class="gp">... </span>                                 <span class="n">decoder_input_mask</span><span class="p">,</span> <span class="n">memory_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 10, 64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">en_past</span><span class="p">))</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">de_past</span><span class="p">))</span>
<span class="go">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">en_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">en_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 20, 32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">de_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 10)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">de_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 10, 32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">de_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 32, 20)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">de_past</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 2, 20, 32)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.TransformerOpParallelConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerOpParallelConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_parallel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_parallel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expert_parallel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pipeline_stage</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">micro_batch_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_transformer_recompute_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_shard</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_aggregation_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab_emb_dp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#TransformerOpParallelConfig"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.TransformerOpParallelConfig" title="打开链接"></a></dt>
<dd><p>用于设置数据并行、模型并行等等并行配置的TransformerOpParallelConfig。</p>
<div class="admonition note">
<p class="admonition-title">说明</p>
<p>除recompute参数外，当用户未将auto_parallel_context设为 <cite>SEMI_AUTO_PARALLEL</cite> 或 <cite>AUTO_PARALLEL</cite> 时，其他参数将无效。
在训练时，micro_batch_num的值必须大于或等于 pipeline_stage的值。data_parallel*model_parallel  *pipeline_stage的值必须等于或小于总设备的数量。设置pipeline_stage和optimizer_shard时，其配置将覆盖auto_parallel_context的配置。
例如，当给定8张计算卡，并且设置data_parallel为1和model_parallel为1时，模型将会在每张卡上重复计算。</p>
</div>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>data_parallel</strong> (int) - 表示数据并行数。默认值：1。</p></li>
<li><p><strong>model_parallel</strong> (int) - 表示模型并行数。默认值：1。</p></li>
<li><p><strong>expert_parallel</strong> (int) - 表示专家并行数，只有在应用混合专家结构（MoE，Mixture of Experts）时才会生效。默认值：1。</p></li>
<li><p><strong>pipeline_stage</strong> (int) - 表示将Transformer切分成的stage数目。其值应为正数。默认值：1。</p></li>
<li><p><strong>micro_batch_num</strong> (int) - 表示用于pipeline训练的batch的微型大小。默认值：1。</p></li>
<li><p><strong>optimizer_shard</strong> (bool) - 表示是否使能优化器切分。默认值：False。</p></li>
<li><p><strong>gradient_aggregation_group</strong> (int) - 表示优化器切分的融合组大小。默认值：4。</p></li>
<li><p><strong>recompute</strong> (Union[TransformerRecomputeConfig, bool]) - 表示是否启用transformer每层的的重计算。默认值：一个按默认参数初始化的 <cite>TransformerRecomputeConfig</cite> 实例。</p></li>
<li><p><strong>vocab_emb_dp</strong> (bool) - 表示Embedding表是否为数据并行，否则将在查找表中的第零维度根据模型并行度进行切分。默认值：True。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">TransformerRecomputeConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recompute_config</span><span class="o">=</span><span class="n">TransformerRecomputeConfig</span><span class="p">(</span><span class="n">recompute</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parallel_optimizer_comm_recompute</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> \
<span class="gp">... </span>                                            <span class="n">mp_comm_recompute</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">recompute_slice_activation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">=</span><span class="n">TransformerOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">recompute</span><span class="o">=</span><span class="n">recompute_config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.EmbeddingOpParallelConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">EmbeddingOpParallelConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_parallel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_parallel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vocab_emb_dp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#EmbeddingOpParallelConfig"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.EmbeddingOpParallelConfig" title="打开链接"></a></dt>
<dd><p><cite>VocabEmbedding</cite> 类中的并行配置，用来设置对嵌入表进行数据并行或者模型并行。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>data_parallel</strong> (int) - 表示数据并行度。根据这个数值，VocabEmbedding层的的输入数据将会被切分成原来的1/data_parallel。默认值：1。</p></li>
<li><p><strong>model_parallel</strong> (int) - 表示模型并行度。根据这个数值，VocabEmbedding层的的权重将会在第零维度被切分成原来的1/model_parallel。默认值：1。</p></li>
<li><p><strong>vocab_emb_dp</strong> (bool) - 表示将权重进行模型切分或数据并行。如果是True，嵌入表查找的操作将会以数据并行的方式进行，此时model_parallel的值将会被忽略。如果是False,嵌入表将会在第零维度进行切分成model_parallel份数。默认值：True。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">EmbeddingOpParallelConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">=</span><span class="n">EmbeddingOpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vocab_emb_dp</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.TransformerRecomputeConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">TransformerRecomputeConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_optimizer_comm_recompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mp_comm_recompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recompute_slice_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/transformer.html#TransformerRecomputeConfig"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.TransformerRecomputeConfig" title="打开链接"></a></dt>
<dd><p>Transformer的重计算配置接口。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>recompute</strong> (bool) - 是否使能重计算。默认值为False。</p></li>
<li><p><strong>parallel_optimizer_comm_recompute</strong> (bool) - 指定由优化器切分产生的AllGather算子是否进行重计算。默认值为False。</p></li>
<li><p><strong>mp_comm_recompute</strong> (bool) - 指定由模型并行成分产生的通信算子是否进行重计算。默认值为True。</p></li>
<li><p><strong>recompute_slice_activation</strong> (bool) - 指定激活层是否切片保存。默认值为False。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">TransformerRecomputeConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">=</span><span class="n">TransformerRecomputeConfig</span><span class="p">(</span><span class="n">recompute</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">parallel_optimizer_comm_recompute</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> \
<span class="gp">... </span>                                  <span class="n">mp_comm_recompute</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">recompute_slice_activation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.CrossEntropyLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">CrossEntropyLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_dpmp_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/loss.html#CrossEntropyLoss"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.CrossEntropyLoss" title="打开链接"></a></dt>
<dd><p>计算输入和输出之间的交叉熵损失。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>parallel_config</strong> (OpParallelConfig) - 表示并行配置。默认值为 <cite>default_dpmp_config</cite> ，表示一个带有默认参数的 <cite>OpParallelConfig</cite> 实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>logits</strong> (Tensor) - shape为(N, C)的Tensor。表示的输出logits。其中N表示任意大小的维度，C表示类别个数。数据类型必须为float16或float32。</p></li>
<li><p><strong>labels</strong> (Tensor) - shape为(N, )的Tensor。表示样本的真实标签，其中每个元素的取值区间为[0,C)。</p></li>
<li><p><strong>input_mask</strong> (Tensor) - shape为(N, )的Tensor。input_mask表示是否有填充输入。1表示有效，0表示无效，其中元素值为0的位置不会计算进损失值。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tensor，表示对应的交叉熵损失。</p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">logits</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">72</span><span class="p">]]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">input_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">labels_np</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(1,)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.OpParallelConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">OpParallelConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data_parallel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_parallel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/op_parallel_config.html#OpParallelConfig"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.OpParallelConfig" title="打开链接"></a></dt>
<dd><p>用于设置数据并行和模型并行的OpParallelConfig。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>data_parallel</strong> (int) - 表示数据并行度。默认值：1</p></li>
<li><p><strong>model_parallel</strong> (int) - 表示模型并行度。默认值：1</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">OpParallelConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span><span class="o">=</span><span class="n">OpParallelConfig</span><span class="p">(</span><span class="n">data_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.FixedSparseAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">FixedSparseAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size_per_head</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_different_global_patterns</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parallel_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">default_dpmp_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/layers.html#FixedSparseAttention"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.FixedSparseAttention" title="打开链接"></a></dt>
<dd><p>固定稀疏注意力层。</p>
<p>此接口实现了Sparse Transformer中使用的稀疏注意力原语，更多详情，请见论文 <a class="reference external" href="https://arxiv.org/abs/1904.10509">Generating Long Sequences with Sparse Transformers</a> 。</p>
<p>具体来说，它包括以下内容：</p>
<ol class="arabic simple">
<li><p>正常注意力的更快实现（不计算上三角，并且融合了许多操作）。</p></li>
<li><p>如论文Sparse Transformers所述，“分散”和“固定”注意力的实现。</p></li>
</ol>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>batch_size</strong> (int) - 表示输入batch size的数量。</p></li>
<li><p><strong>num_heads</strong> (int) - 表示注意力头数。</p></li>
<li><p><strong>block_size</strong> (int) - 表示用来确定block size的整数。目前稀疏自注意力的实现基于稀疏块矩阵。此参数定义了稀疏矩阵块的大小。目前仅支持64。</p></li>
<li><p><strong>seq_length</strong> (int) - 表示输入序列的长度。目前只支持1024。默认值为1024。</p></li>
<li><p><strong>num_different_global_patterns</strong> (int) - 表示用于确定不同的全局注意力数量。虽然全局注意力由局部的代表性的块决定，
但由于有多个头，所以每个头都可以使用不同的全局代表。目前只支持4。默认值为4。</p></li>
<li><p><strong>size_per_head</strong> (int) - 表示每个注意力头的向量大小。目前仅支持64和128。</p></li>
<li><p><strong>parallel_config</strong> (OpParallelConfig) - 并行设置，内容请参阅 <cite>OpParallelConfig</cite> 的定义。默认值为 <cite>default_dpmp_config</cite> ，一个用默认参数初始化的 <cite>OpParallelConfig</cite> 的实例。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>q</strong> (Tensor) - Tensor query (<code class="xref py py-class docutils literal notranslate"><span class="pre">mstype.fp16</span></code> [batch_size, seq_length, hidden_size])：表示上下文的query向量。</p></li>
<li><p><strong>k</strong> (Tensor) - Tensor key (<code class="xref py py-class docutils literal notranslate"><span class="pre">mstype.fp16</span></code> [batch_size, seq_length, hidden_size])：表示上下文的key向量。</p></li>
<li><p><strong>v</strong> (Tensor) - Tensor value (<code class="xref py py-class docutils literal notranslate"><span class="pre">mstype.fp16</span></code> [批次大小, seq_length, hidden_size])：表示上下文的value向量。</p></li>
<li><p><strong>attention_mask</strong> (Tensor) - Float Tensor the mask of (<code class="xref py py-class docutils literal notranslate"><span class="pre">mstype.fp32</span></code> , <code class="xref py py-class docutils literal notranslate"><span class="pre">mstype.fp16</span></code> [batch_size, seq_length, seq_length])：
表示掩码的下三角形矩阵。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>Tensor，shape为[batch_size, seq_length, hidden_size]。</p>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">FixedSparseAttention</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">FixedSparseAttention</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">size_per_head</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">q</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">8</span><span class="o">*</span><span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">8</span><span class="o">*</span><span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">8</span><span class="o">*</span><span class="mi">64</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(2, 1024, 512)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mindspore.nn.transformer.MoEConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore.nn.transformer.</span></span><span class="sig-name descname"><span class="pre">MoEConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">expert_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">capacity_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">aux_loss_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_experts_chosen</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expert_group_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_wise_a2a</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">comp_comm_parallel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">comp_comm_parallel_degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore/nn/transformer/moe.html#MoEConfig"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.nn.transformer.MoEConfig" title="打开链接"></a></dt>
<dd><p>MoE (Mixture of Expert)的配置。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>expert_num</strong> (int) - 表示使用的专家数量。默认值：1。</p></li>
<li><p><strong>capacity_factor</strong> (float) - 表示专家处理的容量关系，其值大于等于1.0。默认值：1.1。</p></li>
<li><p><strong>aux_loss_factor</strong> (float) - 表示负载均衡损失（由路由器产生）的平衡系数。相乘的结果会加到总损失函数中。此系数的值小于1.0。默认值：0.05。</p></li>
<li><p><strong>num_experts_chosen</strong> (int) - 表示每个标识选择的专家数量，其值小于等于专家数量。默认值：1。</p></li>
<li><p><strong>expert_group_size</strong> (int) - 表示每个数据并行组收到的词语（token）数量。默认值：None。该参数只在自动并行且非策略传播模式下起作用。</p></li>
<li><p><strong>group_wise_a2a</strong> (bool) - 表示否是使能group-wise alltoall通信，group-wise alltoall通信可以把部分节点间通信转化为节点内通信从而减低通信时间。默认值：False。该参数只有在模型并行数大于1且数据并行数等于专家并行数生效。</p></li>
<li><p><strong>comp_comm_parallel</strong> (bool) - 是否使能MoE计算和通信并行，可以通过拆分重叠计算和通信来减少纯通信时间。默认值：False。</p></li>
<li><p><strong>comp_comm_parallel_degree</strong> (int) - 计算和通信的拆分数量。数字越大重叠越多，但会消耗更多的显存。默认值：2。该参数只在comp_comm_parallel为True下生效。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.nn.transformer</span> <span class="kn">import</span> <span class="n">MoEConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">moe_config</span> <span class="o">=</span> <span class="n">MoEConfig</span><span class="p">(</span><span class="n">expert_num</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">capacity_factor</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">aux_loss_factor</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">num_experts_chosen</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">expert_group_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">group_wise_a2a</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">comp_comm_parallel</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">comp_comm_parallel_degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="nn_probability/mindspore.nn.probability.distribution.Uniform.html" class="btn btn-neutral float-left" title="mindspore.nn.probability.distribution.Uniform" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="mindspore.rewrite.html" class="btn btn-neutral float-right" title="mindspore.rewrite" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>