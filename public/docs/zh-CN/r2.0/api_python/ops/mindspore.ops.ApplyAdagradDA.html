

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.ops.ApplyAdagradDA &mdash; MindSpore master 文档</title>
  

  
  <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/training.css" type="text/css" />
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/training.js"></script>
        <script src="../../_static/translations.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="mindspore.ops.ApplyAdagradV2" href="mindspore.ops.ApplyAdagradV2.html" />
    <link rel="prev" title="mindspore.ops.ApplyAdagrad" href="mindspore.ops.ApplyAdagrad.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/programming_paradigm.html">编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/auto_gradient.html">函数式微分编程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/mindir.html">中间表示MindIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/all_scenarios.html">全场景统一</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">术语</a></li>
</ul>
<p class="caption"><span class="caption-text">规格</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/benchmark.html">基准性能</a></li>
<li class="toctree-l1"><a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.0/README_CN.md#目录">网络支持↗</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/operator_list.html">API支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/syntax_list.html">语法支持</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.ops.primitive.html">mindspore.ops.primitive</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#算子原语">算子原语</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#装饰器">装饰器</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.ops.primitive.html#神经网络层算子">神经网络层算子</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#神经网络">神经网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#损失函数">损失函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#激活函数">激活函数</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../mindspore.ops.primitive.html#优化器">优化器</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.Adam.html">mindspore.ops.Adam</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdamWeightDecay.html">mindspore.ops.AdamWeightDecay</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdaptiveAvgPool2D.html">mindspore.ops.AdaptiveAvgPool2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.AdaptiveAvgPool3D.html">mindspore.ops.AdaptiveAvgPool3D</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdadelta.html">mindspore.ops.ApplyAdadelta</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagrad.html">mindspore.ops.ApplyAdagrad</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">mindspore.ops.ApplyAdagradDA</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdagradV2.html">mindspore.ops.ApplyAdagradV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAdaMax.html">mindspore.ops.ApplyAdaMax</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyAddSign.html">mindspore.ops.ApplyAddSign</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyCenteredRMSProp.html">mindspore.ops.ApplyCenteredRMSProp</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyFtrl.html">mindspore.ops.ApplyFtrl</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyGradientDescent.html">mindspore.ops.ApplyGradientDescent</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyMomentum.html">mindspore.ops.ApplyMomentum</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyPowerSign.html">mindspore.ops.ApplyPowerSign</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyProximalAdagrad.html">mindspore.ops.ApplyProximalAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyProximalGradientDescent.html">mindspore.ops.ApplyProximalGradientDescent</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.ApplyRMSProp.html">mindspore.ops.ApplyRMSProp</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.LARSUpdate.html">mindspore.ops.LARSUpdate</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyAdagradV2.html">mindspore.ops.SparseApplyAdagradV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyProximalAdagrad.html">mindspore.ops.SparseApplyProximalAdagrad</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SGD.html">mindspore.ops.SGD</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyFtrl.html">mindspore.ops.SparseApplyFtrl</a></li>
<li class="toctree-l4"><a class="reference internal" href="mindspore.ops.SparseApplyFtrlV2.html">mindspore.ops.SparseApplyFtrlV2</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#距离函数">距离函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#采样算子">采样算子</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#图像处理">图像处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mindspore.ops.primitive.html#文本处理">文本处理</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#数学运算算子">数学运算算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#tensor操作算子">Tensor操作算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#parameter操作算子">Parameter操作算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#数据操作算子">数据操作算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#通信算子">通信算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#调试算子">调试算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#稀疏算子">稀疏算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#框架算子">框架算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#算子信息注册">算子信息注册</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#自定义算子">自定义算子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.ops.primitive.html#光谱算子">光谱算子</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/api_cpp/mindspore.html">C++ API↗</a></li>
</ul>
<p class="caption"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">特性咨询</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindspore.ops.primitive.html">mindspore.ops.primitive</a> &raquo;</li>
        
      <li>mindspore.ops.ApplyAdagradDA</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/api_python/ops/mindspore.ops.ApplyAdagradDA.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mindspore-ops-applyadagradda">
<h1>mindspore.ops.ApplyAdagradDA<a class="headerlink" href="#mindspore-ops-applyadagradda" title="永久链接至标题">¶</a></h1>
<dl class="class">
<dt id="mindspore.ops.ApplyAdagradDA">
<em class="property">class </em><code class="sig-prename descclassname">mindspore.ops.</code><code class="sig-name descname">ApplyAdagradDA</code><span class="sig-paren">(</span><em class="sig-param">use_locking=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/ops/operations/nn_ops.html#ApplyAdagradDA"><span class="viewcode-link">[源代码]</span></a><a class="headerlink" href="#mindspore.ops.ApplyAdagradDA" title="永久链接至目标">¶</a></dt>
<dd><p>根据Adagrad算法更新 <cite>var</cite> 。</p>
<p>Adagrad算法在论文 <a class="reference external" href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a> 中提出。</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ll} \\
    grad\_accum += grad \\
    grad\_squared\_accum += grad * grad \\
    tmp\_val=
        \begin{cases}
             sign(grad\_accum) * max\left \{|grad\_accum|-l1*global\_step, 0\right \} &amp; \text{ if } l1&gt;0 \\
             grad\_accum &amp; \text{ otherwise } \\
         \end{cases} \\
    x\_value = -1 * lr * tmp\_val \\
    y\_value = l2 * global\_step * lr + \sqrt{grad\_squared\_accum} \\
    var = \frac{ x\_value }{ y\_value }
\end{array}\end{split}\]</div>
<p><cite>var</cite> 、 <cite>gradient_accumulator</cite> 、 <cite>gradient_squared_accumulator</cite> 和 <cite>grad</cite> 的输入遵循隐式类型转换规则，使数据类型一致。
如果它们具有不同的数据类型，则较低精度的数据类型将转换为相对最高精度的数据类型。</p>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>use_locking</strong> (bool) - 如果为True， <cite>var</cite> 和 <cite>gradient_accumulator</cite> 的更新将受到锁的保护。否则，行为为未定义，很可能出现较少的冲突。默认值为False。</p></li>
</ul>
</dd>
<dt>输入：</dt><dd><ul class="simple">
<li><p><strong>var</strong> (Parameter) - 要更新的变量。数据类型必须为float16或float32。shape： <span class="math notranslate nohighlight">\((N, *)\)</span> ，其中 <span class="math notranslate nohighlight">\(*\)</span> 表示任意数量的附加维度。</p></li>
<li><p><strong>gradient_accumulator</strong> (Parameter) - 要更新累积的梯度，为公式中的 <span class="math notranslate nohighlight">\(grad\_accum\)</span> 。shape和数据类型必须与 <cite>var</cite> 相同。</p></li>
<li><p><strong>gradient_squared_accumulator</strong> (Parameter) - 要更新的平方累积的梯度， 为公式中的 <span class="math notranslate nohighlight">\(grad\_squared\_accum\)</span> 。shape和数据类型必须与 <cite>var</cite> 相同。</p></li>
<li><p><strong>grad</strong> (Tensor) - 梯度，为一个Tensor。shape和数据类型必须与 <cite>var</cite> 相同。</p></li>
<li><p><strong>lr</strong> ([Number, Tensor]) - 学习率。必须是Scalar。数据类型为float32或float16。</p></li>
<li><p><strong>l1</strong> ([Number, Tensor]) - L1正则化。必须是Scalar。数据类型为float32或float16。</p></li>
<li><p><strong>l2</strong> ([Number, Tensor]) - L2正则化。必须是Scalar。数据类型为float32或float16。</p></li>
<li><p><strong>global_step</strong> ([Number, Tensor]) - 训练步骤的编号。必须是Scalar。数据类型为int32或int64。</p></li>
</ul>
</dd>
<dt>输出：</dt><dd><p>3个Tensor组成的tuple，更新后的参数。</p>
<ul class="simple">
<li><p><strong>var</strong> (Tensor) - shape和数据类型与 <cite>var</cite> 相同。</p></li>
<li><p><strong>gradient_accumulator</strong> (Tensor) - shape和数据类型与 <cite>gradient_accumulator</cite> 相同。</p></li>
<li><p><strong>gradient_squared_accumulator</strong> (Tensor) - shape和数据类型与 <cite>gradient_squared_accumulator</cite> 相同。</p></li>
</ul>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>TypeError</strong> - 如果 <cite>var</cite> 、 <cite>gradient_accumulator</cite> 或 <cite>gradient_squared_accumulator</cite> 不是Parameter。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>grad</cite> 不是 Tensor。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>lr</cite> 、 <cite>l1</cite> 、 <cite>l2</cite> 或者 <cite>global_step</cite> 既不是数值型也不是Tensor。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>use_locking</cite> 不是bool。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>var</cite> 、 <cite>gradient_accumulator</cite> 、 <cite>gradient_squared_accumulator</cite> 、 <cite>grad</cite> 、 <cite>lr</cite> 、 <cite>l1</cite> 或 <cite>l2</cite> 的数据类型既不是float16也不是float32。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>gradient_accumulator</cite> 、 <cite>gradient_squared_accumulator</cite> 、 <cite>grad</cite> 与 <cite>var</cite> 的数据类型不相同。</p></li>
<li><p><strong>TypeError</strong> - 如果 <cite>global_step</cite> 的数据类型不是int32也不是int64。</p></li>
<li><p><strong>ValueError</strong> - 如果 <cite>lr</cite> 、 <cite>l1</cite> 、 <cite>l2</cite> 和 <cite>global_step</cite> 的shape大小不为0。</p></li>
<li><p><strong>RuntimeError</strong> - 如果 <cite>var</cite> 、 <cite>gradient_accumulator</cite> 、 <cite>gradient_squared_accumulator</cite> 和 <cite>grad</cite> 不支持数据类型转换。</p></li>
</ul>
</dd>
<dt>支持平台：</dt><dd><p><code class="docutils literal notranslate"><span class="pre">Ascend</span></code> <code class="docutils literal notranslate"><span class="pre">GPU</span></code> <code class="docutils literal notranslate"><span class="pre">CPU</span></code></p>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ApplyAdagradDANet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">ApplyAdagradDANet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">apply_adagrad_d_a</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ApplyAdagradDA</span><span class="p">(</span><span class="n">use_locking</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">var</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;var&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulator</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="gp">... </span>                                                               <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gradient_accumulator&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_squared_accumulator</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
<span class="gp">... </span>                                                                       <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                                      <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gradient_squared_accumulator&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulator</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="gp">... </span>                                                               <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
<span class="gp">... </span>                                              <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gradient_accumulator&quot;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_adagrad_d_a</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">var</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulator</span><span class="p">,</span>
<span class="gp">... </span>                                     <span class="bp">self</span><span class="o">.</span><span class="n">gradient_squared_accumulator</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">ApplyAdagradDANet</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grad</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l1</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">l2</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">global_step</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">global_step</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="go">(Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="go">[[-7.39064650e-04, -1.36888528e-03],</span>
<span class="go"> [-5.96988888e-04, -1.42478070e-03]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="go">[[ 4.00000006e-01,  7.00000048e-01],</span>
<span class="go"> [ 2.00000003e-01,  6.99999988e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="go">[[ 2.90000021e-01,  2.60000020e-01],</span>
<span class="go"> [ 1.09999999e-01,  2.40000010e-01]]))</span>
</pre></div>
</div>
</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindspore.ops.ApplyAdagradV2.html" class="btn btn-neutral float-right" title="mindspore.ops.ApplyAdagradV2" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindspore.ops.ApplyAdagrad.html" class="btn btn-neutral float-left" title="mindspore.ops.ApplyAdagrad" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>