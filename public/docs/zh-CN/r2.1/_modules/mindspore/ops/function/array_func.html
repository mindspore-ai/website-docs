<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.function.array_func &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../../../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/training.css" type="text/css" /><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script><script src="../../../../_static/js/training.js"></script><script src="../../../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="索引" href="../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
              <div class="version">
                1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">原生分布式并行架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/tensorflow_api_mapping.html">TensorFlow与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">环境准备与资料获取</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/typical_api_comparision.html">与PyTorch典型区别</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">MindSpore网络搭建</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/migrator_with_tools.html">网络迁移工具应用实践指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/operators.html">静态图语法——运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/statements.html">静态图语法——Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/python_builtin_functions.html">静态图语法——Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">模块代码</a> &raquo;</li>
      <li>mindspore.ops.function.array_func</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>mindspore.ops.function.array_func 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators for function.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">import</span> <span class="nn">builtins</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.common.dtype</span> <span class="k">as</span> <span class="nn">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">constexpr</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">_primexpr</span>
<span class="kn">import</span> <span class="nn">mindspore.ops.function</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations._inner_ops</span> <span class="kn">import</span> <span class="n">DynamicBroadcastTo</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations._sequence_ops</span> <span class="kn">import</span> <span class="n">TupleToTensor</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.composite.multitype_ops</span> <span class="kn">import</span> <span class="n">_constexpr_utils</span> <span class="k">as</span> <span class="n">const_utils</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations._sequence_ops</span> <span class="kn">import</span> <span class="n">TensorToList</span>

<span class="kn">from</span> <span class="nn">mindspore.ops.operations.array_ops</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">UniqueConsecutive</span><span class="p">,</span>
    <span class="n">SearchSorted</span><span class="p">,</span>
    <span class="n">NonZero</span><span class="p">,</span>
    <span class="n">MatrixDiagV3</span><span class="p">,</span>
    <span class="n">MatrixDiagPartV3</span><span class="p">,</span>
    <span class="n">MatrixSetDiagV3</span><span class="p">,</span>
    <span class="n">Fills</span><span class="p">,</span>
    <span class="n">Col2Im</span><span class="p">,</span>
    <span class="n">ArgMaxWithValue</span><span class="p">,</span>
    <span class="n">ArgMinWithValue</span><span class="p">,</span>
    <span class="n">ScatterNdMax</span><span class="p">,</span>
    <span class="n">ScatterNdMul</span><span class="p">,</span>
    <span class="n">IndexFill</span><span class="p">,</span>
    <span class="n">AffineGrid</span><span class="p">,</span>
    <span class="n">Im2Col</span><span class="p">,</span>
    <span class="n">Expand</span><span class="p">,</span>
    <span class="n">Lstsq</span><span class="p">,</span>
    <span class="n">Mvlgamma</span><span class="p">,</span>
    <span class="n">Tril</span><span class="p">,</span>
    <span class="n">Argmax</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.array_ops</span> <span class="kn">import</span> <span class="n">TensorScatterElements</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._primitive_cache</span> <span class="kn">import</span> <span class="n">_get_cache_prim</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">_checkparam</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">Tensor</span> <span class="k">as</span> <span class="n">Tensor_</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._utils.utils</span> <span class="kn">import</span> <span class="n">ms_arrange</span>

<span class="n">tuple_to_tensor_</span> <span class="o">=</span> <span class="n">TupleToTensor</span><span class="p">()</span>
<span class="n">eye_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Eye</span><span class="p">()</span>
<span class="n">fills_</span> <span class="o">=</span> <span class="n">Fills</span><span class="p">()</span>
<span class="n">fill_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Fill</span><span class="p">()</span>
<span class="n">fillv2_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FillV2</span><span class="p">()</span>
<span class="n">ones_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ones</span><span class="p">()</span>
<span class="n">ones_like_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>
<span class="n">tile_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span>
<span class="n">unique_with_pad_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UniqueWithPad</span><span class="p">()</span>
<span class="n">size_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Size</span><span class="p">()</span>
<span class="n">shape_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
<span class="n">rank_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Rank</span><span class="p">()</span>
<span class="n">tensor_shape_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">()</span>
<span class="n">reshape_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
<span class="n">tensor_slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()</span>
<span class="n">expand_dims_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
<span class="n">transpose_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">()</span>
<span class="n">scatter_add_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterAdd</span><span class="p">()</span>
<span class="n">scatter_max_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterMax</span><span class="p">()</span>
<span class="n">scatter_min_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterMin</span><span class="p">()</span>
<span class="n">scatter_mul_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterMul</span><span class="p">()</span>
<span class="n">scatter_div_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterDiv</span><span class="p">()</span>
<span class="n">scatter_nd_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScatterNd</span><span class="p">()</span>
<span class="n">gather_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">()</span>
<span class="n">gather_d_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherD</span><span class="p">()</span>
<span class="n">gather_nd_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">GatherNd</span><span class="p">()</span>
<span class="n">nonzero_</span> <span class="o">=</span> <span class="n">NonZero</span><span class="p">()</span>
<span class="n">scalar_cast_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarCast</span><span class="p">()</span>
<span class="n">tensor_scatter_add_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterAdd</span><span class="p">()</span>
<span class="n">tensor_scatter_sub_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterSub</span><span class="p">()</span>
<span class="n">tensor_scatter_mul_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterMul</span><span class="p">()</span>
<span class="n">tensor_scatter_div_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterDiv</span><span class="p">()</span>
<span class="n">tensor_scatter_min_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterMin</span><span class="p">()</span>
<span class="n">tensor_scatter_max_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TensorScatterMax</span><span class="p">()</span>
<span class="n">scalar_to_tensor_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarToTensor</span><span class="p">()</span>
<span class="n">tuple_to_array_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">TupleToArray</span><span class="p">()</span>
<span class="n">masked_select_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">MaskedSelect</span><span class="p">()</span>
<span class="n">matrix_band_part_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">array_ops</span><span class="o">.</span><span class="n">MatrixBandPart</span><span class="p">()</span>
<span class="n">ger_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Ger</span><span class="p">()</span>
<span class="n">diag_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Diag</span><span class="p">()</span>
<span class="n">range_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Range</span><span class="p">()</span>
<span class="n">zeros_like_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ZerosLike</span><span class="p">()</span>
<span class="n">cast_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
<span class="n">tensor_select_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Select</span><span class="p">()</span>
<span class="n">index_fill_</span> <span class="o">=</span> <span class="n">IndexFill</span><span class="p">()</span>
<span class="n">unsorted_segment_sum_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentSum</span><span class="p">()</span>
<span class="n">population_count_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">PopulationCount</span><span class="p">()</span>
<span class="n">reduce_max</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMax</span><span class="p">()</span>
<span class="n">reduce_min</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceMin</span><span class="p">()</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">get_x_shape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">is_sequence_shape_unknown</span><span class="p">(</span><span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,)</span>
    <span class="k">if</span> <span class="n">F</span><span class="o">.</span><span class="n">is_sequence_value_unknown</span><span class="p">(</span><span class="n">x_shape</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x_shape</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s</span> <span class="o">*</span> <span class="n">i</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">s</span><span class="p">,)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_attr_dtype</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">,</span> <span class="n">allow_dtypes</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">)</span>


<span class="n">check_flatten_order_const</span> <span class="o">=</span> <span class="n">constexpr</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_flatten_order</span><span class="p">)</span>


<span class="c1">##############################</span>
<span class="c1"># Tensor Creation Functions.</span>
<span class="c1">##############################</span>


<span class="k">def</span> <span class="nf">_cast_type</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">to_type</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;cast input to the specified type or cast input to tensor&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">to_type</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">scalar_to_tensor_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">to_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">_get_type</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;get the dtype of input&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">typeof</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_max_type</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;get max input type with `level`&quot;&quot;&quot;</span>
    <span class="n">valid_dtypes</span> <span class="o">=</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">]</span>
    <span class="n">arg_map</span> <span class="o">=</span> <span class="p">[</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">]</span>
    <span class="n">arg_type_map</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">_get_type</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">arg_map</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">arg_value</span> <span class="ow">in</span> <span class="n">arg_map</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
                <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">arg_value</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="n">valid_dtypes</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;For arange, the input type must be int or float or a TensorScalar in </span><span class="si">{</span><span class="n">valid_dtypes</span><span class="si">}</span><span class="s2">,&quot;</span>
                <span class="sa">f</span><span class="s2">&quot; but got </span><span class="si">{</span><span class="n">_get_type</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">type_map</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Float64&#39;</span><span class="p">:</span> <span class="s1">&#39;3&#39;</span><span class="p">,</span> <span class="s1">&#39;Float32&#39;</span><span class="p">:</span> <span class="s1">&#39;2&#39;</span><span class="p">,</span> <span class="s2">&quot;&lt;class &#39;float&#39;&gt;&quot;</span><span class="p">:</span> <span class="s1">&#39;2&#39;</span><span class="p">,</span> <span class="s1">&#39;Int64&#39;</span><span class="p">:</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s2">&quot;&lt;class &#39;int&#39;&gt;&quot;</span><span class="p">:</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span>
                <span class="s1">&#39;Int32&#39;</span><span class="p">:</span> <span class="s1">&#39;0&#39;</span><span class="p">}</span>
    <span class="n">type_map_reverse</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;3&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="s1">&#39;2&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="s1">&#39;0&#39;</span><span class="p">:</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">}</span>
    <span class="n">type_level</span> <span class="o">=</span> <span class="p">[</span><span class="n">type_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">arg_type_map</span><span class="p">]</span>
    <span class="n">max_level</span> <span class="o">=</span> <span class="n">builtins</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">type_level</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">type_map_reverse</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">max_level</span><span class="p">)</span>


<div class="viewcode-block" id="arange"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.arange.html#mindspore.ops.arange">[文档]</a><span class="k">def</span> <span class="nf">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a sequence of numbers that begins at `start` and extends by increments of</span>
<span class="sd">    `step` up to but not including `end`.</span>

<span class="sd">    Args:</span>
<span class="sd">        start (Union[float, int, Tensor], optional): The start of the interval.</span>
<span class="sd">            If Tensor, the shape must be :math:`()` . Default: ``0`` .</span>
<span class="sd">        end (Union[float, int, Tensor], optional): The end of the interval, exclusive.</span>
<span class="sd">            If Tensor, the shape must be :math:`()`.</span>
<span class="sd">            Default: ``None`` . If ``None`` , it defaults to the value of `start`, and 0 is used as the starting value.</span>
<span class="sd">        step (Union[float, int, Tensor], optional): Number that increments `start`.</span>
<span class="sd">            If Tensor, the shape must be :math:`()`. Default: ``1`` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (mindspore.dtype, optional): The required data type of returned Tensor. Default: ``None`` .</span>
<span class="sd">            If the value is not specified or is ``None`` , the type with the highest precision in the</span>
<span class="sd">            `start`, `end`, and `step` parameters is inferred.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1-D Tensor, with the same type as the inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `start`, `end` or `step` is not an int or a float or a TensorScalar(Special Tensor with shape ())</span>
<span class="sd">                   in valid dtypes.</span>
<span class="sd">        ValueError: If `step` = 0.</span>
<span class="sd">        ValueError: If `start` &gt;= `end` when `step` &gt; 0.</span>
<span class="sd">        ValueError: If `start` &lt;= `end` when `step` &lt; 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; output = ops.arange(1, 6)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 2 3 4 5]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int64</span>
<span class="sd">        &gt;&gt;&gt; output = ops.arange(0, 3, 1.2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  1.2 2.4]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">        &gt;&gt;&gt; output = ops.arange(7, 1, -2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [7 5 3]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int64</span>
<span class="sd">        &gt;&gt;&gt; output = ops.arange(ms.Tensor(12.0, dtype=ms.float64), 2, ms.Tensor(-1.0, dtype=ms.float32))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [12. 11. 10.  9.  8.  7.  6.  5.  4.  3.]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float64</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">end</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">start</span>
    <span class="n">max_type</span> <span class="o">=</span> <span class="n">_get_max_type</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">_cast_type</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">max_type</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">_cast_type</span><span class="p">(</span><span class="n">end</span><span class="p">,</span> <span class="n">max_type</span><span class="p">)</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">_cast_type</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">max_type</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">start</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">()</span> <span class="ow">or</span> <span class="n">end</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">()</span> <span class="ow">or</span> <span class="n">step</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For arange, the input args must be a TensorScalar,&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot; but got start shape:</span><span class="si">{</span><span class="n">start</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, end shape:</span><span class="si">{</span><span class="n">end</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, step shape:</span><span class="si">{</span><span class="n">step</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Range</span><span class="p">()(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span></div>


<div class="viewcode-block" id="cat"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cat.html#mindspore.ops.cat">[文档]</a><span class="k">def</span> <span class="nf">cat</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Connect input tensors along with the given axis.</span>

<span class="sd">    The input data is a tuple or a list of tensors. These tensors have the same rank :math:`R`.</span>
<span class="sd">    Set the given axis as :math:`m`, and :math:`0 \le m &lt; R`. Set the number of input tensors as :math:`N`.</span>
<span class="sd">    For the :math:`i`-th tensor :math:`t_i`, it has the shape of :math:`(x_1, x_2, ..., x_{mi}, ..., x_R)`.</span>
<span class="sd">    :math:`x_{mi}` is the :math:`m`-th dimension of the :math:`t_i`. Then, the shape of the output tensor is</span>

<span class="sd">    .. math::</span>

<span class="sd">        (x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (Union[tuple, list]): A tuple or a list of input tensors.</span>
<span class="sd">            Suppose there are two tensors in this tuple or list, namely t1 and t2.</span>
<span class="sd">            To perform `concat` in the axis 0 direction, except for the :math:`0`-th axis,</span>
<span class="sd">            all other dimensions should be equal, that is,</span>
<span class="sd">            :math:`t1.shape[1] = t2.shape[1], t1.shape[2] = t2.shape[2], ..., t1.shape[R-1] = t2.shape[R-1]`,</span>
<span class="sd">            where :math:`R` represents the rank of tensor.</span>
<span class="sd">        axis (int): The specified axis, whose value is in range :math:`[-R, R)`. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is :math:`(x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)`.</span>
<span class="sd">            The data type is the same with `tensors`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `tensors` have different dimension of tensor.</span>
<span class="sd">        ValueError: If `axis` not in range :math:`[-R, R)`.</span>
<span class="sd">        RuntimeError: If tensor&#39;s shape in `tensors` except for `axis` are different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x1 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; input_x2 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cat((input_x1, input_x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">         [2. 1.]</span>
<span class="sd">         [0. 1.]</span>
<span class="sd">         [2. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cat((input_x1, input_x2), 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 0. 1.]</span>
<span class="sd">         [2. 1. 2. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_concat</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_concat</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></div>


<div class="viewcode-block" id="eye"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.eye.html#mindspore.ops.eye">[文档]</a><span class="k">def</span> <span class="nf">eye</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor with ones on the diagonal and zeros in the rest.</span>

<span class="sd">    Note:</span>
<span class="sd">        Combines ReverseV2 operator to get an anti-diagonal Tensor,</span>
<span class="sd">        but ReverseV2 only supports Ascend and GPU platforms currently.</span>

<span class="sd">    Args:</span>
<span class="sd">        n (int): The number of rows of returned tensor. Constant value only.</span>
<span class="sd">        m (int): The number of columns of returned tensor. Constant value only.</span>
<span class="sd">            Default: ``None`` , if ``None`` , the number of columns is as the same as n.</span>
<span class="sd">        dtype (mindspore.dtype): MindSpore&#39;s dtype, the data type of the returned tensor.</span>
<span class="sd">            The data type can be bool or Number.</span>
<span class="sd">            Default: ``None`` , the data type of the returned tensor is mindspore.float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a tensor with ones on the diagonal and the rest of elements are zero. The shape of `output` depends on</span>
<span class="sd">        the user&#39;s Inputs `n` and `m`. And the data type depends on Inputs `dtype`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `m` or `n` is not an int.</span>
<span class="sd">        ValueError: If `m` or `n` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; output = ops.eye(2, 2, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 0]</span>
<span class="sd">         [0 1]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int32</span>
<span class="sd">        &gt;&gt;&gt; output = ops.eye(1, 2, mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float64</span>
<span class="sd">        &gt;&gt;&gt; output = ops.eye(2, dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 0]</span>
<span class="sd">         [0 1]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int32</span>
<span class="sd">        &gt;&gt;&gt; output = ops.eye(2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0.]</span>
<span class="sd">         [0. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">m</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">return</span> <span class="n">eye_</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="hamming_window"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hamming_window.html#mindspore.ops.hamming_window">[文档]</a><span class="k">def</span> <span class="nf">hamming_window</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="n">periodic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.54</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.46</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the Hamming window.</span>

<span class="sd">    .. math::</span>

<span class="sd">        w[n]=\alpha − \beta \cos \left( \frac{2 \pi n}{N - 1} \right),</span>

<span class="sd">    where :math:`N` is the full window size.</span>

<span class="sd">    Args:</span>
<span class="sd">        window_length (int): The size of returned window. Must be a non negative integer.</span>
<span class="sd">        periodic (bool, optional): If True, return a periodic window. If False, return a symmetric window.</span>
<span class="sd">            Default: ``True`` .</span>
<span class="sd">        alpha (float, optional): The coefficient α. Default: ``0.54`` .</span>
<span class="sd">        beta (float, optional): The coefficient β. Default: ``0.46`` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (mindspore.dtype, optional): The output window data type. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a 1-D tensor of size (window_length) containing the window.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `window_length` is a negative integer.</span>
<span class="sd">        TypeError: If `periodic` is not bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; print(ops.hamming_window(6, False))</span>
<span class="sd">        [0.08 0.39785218 0.91214782  0.91214782  0.39785218 0.08]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">window_length</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For array function &#39;hamming_window&#39;, &#39;window_length&#39; must be int, but got&quot;</span> \
                        <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">window_length</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">window_length</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For array function &#39;hamming_window&#39;, &#39;window_length&#39; must be non negative number.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">periodic</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For array function &#39;hamming_window&#39;, &#39;periodic&#39; must be bool, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">periodic</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For array function &#39;hamming_window&#39;, &#39;alpha&#39; must be float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For array function &#39;hamming_window&#39;, &#39;beta&#39; must be float, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">window_length</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">window_length</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float_type</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For array function &#39;hamming_window&#39;, &#39;dtype&#39; must be floating point dtypes, but got </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">HammingWindow</span><span class="p">)(</span><span class="n">periodic</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">window_length</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="where"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.where.html#mindspore.ops.where">[文档]</a><span class="k">def</span> <span class="nf">where</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Selects elements from `x` or `y` based on `condition` and returns a tensor.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_i = \begin{cases} x_i,\quad &amp;if\ condition_i \\ y_i,\quad &amp;otherwise \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        condition (Tensor[bool]): If True, yield `x`, otherwise yield `y`.</span>
<span class="sd">        x (Union[Tensor, Scalar]): When `condition` is True, values to select from.</span>
<span class="sd">        y (Union[Tensor, Scalar]): When `condition` is False, values to select from.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, elements are selected from `x` and `y`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `condition` is not a Tensor.</span>
<span class="sd">        TypeError: If both `x` and `y` are scalars.</span>
<span class="sd">        ValueError: If `condition`, `x` and `y` can not broadcast to each other.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.arange(4).reshape((2, 2)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.ones((2, 2)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; condition = a &lt; 3</span>
<span class="sd">        &gt;&gt;&gt; output = ops.where(condition, a, b)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">         [2. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;where&#39;, &#39;condition&#39; must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">condition</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;For &#39;where&#39;, at least one of &#39;x&#39; and &#39;y&#39; should be Tensor, but got x:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">, y:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;For &#39;where&#39;, at least one of &#39;x&#39; and &#39;y&#39; should be Tensor, but got x:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">, y:</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">output_shape</span> <span class="o">=</span> <span class="n">_calc_broadcast_shape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">condition</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">condition</span> <span class="o">=</span> <span class="n">broadcast_to</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">broadcast_to</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">)</span>
    <span class="n">_select</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Select</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">_select</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="reverse"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.reverse.html#mindspore.ops.reverse">[文档]</a><span class="k">def</span> <span class="nf">reverse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses specific dimensions of a tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The value range of &quot;axis&quot; is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of &quot;input_x&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The target tensor.</span>
<span class="sd">            The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        axis (Union[tuple(int), list(int)]): The indices of the dimensions to reverse.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is neither list nor tuple.</span>
<span class="sd">        TypeError: If element of `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse(input_x, axis=[1])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4 3 2 1]</span>
<span class="sd">         [8 7 6 5]]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse(input_x, axis=[1, 0])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[8 7 6 5]</span>
<span class="sd">         [4 3 2 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseV2</span><span class="p">(</span><span class="n">axis</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="ravel"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ravel.html#mindspore.ops.ravel">[文档]</a><span class="k">def</span> <span class="nf">ravel</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Expand the multidimensional Tensor into 1D along the 0 axis direction.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A tensor to be flattened.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a 1-D tensor, containing the same elements of the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `input` is not Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ravel(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0. 1. 2. 1.]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span></div>


<div class="viewcode-block" id="matrix_band_part"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_band_part.html#mindspore.ops.matrix_band_part">[文档]</a><span class="k">def</span> <span class="nf">matrix_band_part</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Copy a tensor setting everything outside a central band in each innermost matrix to zero.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor. :math:`(*, m, n)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        lower (Union[int, Tensor]): Number of subdiagonals to keep. The data type must be int32 or int64.</span>
<span class="sd">            If negative, keep entire lower triangle.</span>
<span class="sd">        upper (Union[int, Tensor]): Number of superdiagonals to keep. The data type must be int32 or int64.</span>
<span class="sd">            If negative, keep entire upper triangle.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not valid.</span>
<span class="sd">        TypeError: If `lower` is neither a number nor a Tensor.</span>
<span class="sd">        TypeError: If `upper` is neither a number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `lower` is neither int32 nor int64.</span>
<span class="sd">        TypeError: If dtype of `upper` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If the shape of `x` is not greater than or equal to 2D.</span>
<span class="sd">        ValueError: If the shape of `lower` is not equal to 0D.</span>
<span class="sd">        ValueError: If the shape of `upper` is not equal to 0D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([2, 4, 4]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_band_part(x, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 0. 0.]</span>
<span class="sd">          [1. 1. 1. 0.]</span>
<span class="sd">          [1. 1. 1. 1.]</span>
<span class="sd">          [0. 1. 1. 1.]]</span>
<span class="sd">         [[1. 1. 0. 0.]</span>
<span class="sd">          [1. 1. 1. 0.]</span>
<span class="sd">          [1. 1. 1. 1.]</span>
<span class="sd">          [0. 1. 1. 1.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">matrix_band_part_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span></div>


<div class="viewcode-block" id="padding"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.padding.html#mindspore.ops.padding">[文档]</a><span class="k">def</span> <span class="nf">padding</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pad_dim_size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extends the last dimension of the input tensor from 1 to pad_dim_size, by filling with 0.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`. The rank of `x` must be at least 2.</span>
<span class="sd">            The last dimension of `x` must be 1. The data type is Number.</span>
<span class="sd">        pad_dim_size (int): The value of the last dimension of `x` to be extended, which must be positive.</span>
<span class="sd">            Default: ``8`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as input shape value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `pad_dim_size` is not an int.</span>
<span class="sd">        ValueError: If `pad_dim_size` is less than 1.</span>
<span class="sd">        ValueError: If last dim of `x` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[8], [10]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pad_dim_size = 4</span>
<span class="sd">        &gt;&gt;&gt; output = ops.padding(x, pad_dim_size)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 8.  0.  0.  0.]</span>
<span class="sd">         [10.  0.  0.  0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">padding_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">array_ops</span><span class="o">.</span><span class="n">Padding</span><span class="p">)(</span><span class="n">pad_dim_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padding_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_axis_type</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">type_int</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">type_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">type_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ops_name</span><span class="o">=</span><span class="s2">&quot;ops&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check axis argument type.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">type_int</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">type_tuple</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">or</span> <span class="p">(</span><span class="n">type_list</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">ops_name</span><span class="si">}</span><span class="s2">, each axis must be integer, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span><span class="si">}</span><span class="s2"> in </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="n">type_str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">if</span> <span class="n">type_int</span><span class="p">:</span>
        <span class="n">type_str</span> <span class="o">+=</span> <span class="s2">&quot;int, &quot;</span>
    <span class="k">if</span> <span class="n">type_tuple</span><span class="p">:</span>
        <span class="n">type_str</span> <span class="o">+=</span> <span class="s2">&quot;tuple, &quot;</span>
    <span class="k">if</span> <span class="n">type_list</span><span class="p">:</span>
        <span class="n">type_str</span> <span class="o">+=</span> <span class="s2">&quot;list, &quot;</span>
    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">ops_name</span><span class="si">}</span><span class="s2">, the axis should be </span><span class="si">{</span><span class="n">type_str</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="one_hot"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.one_hot.html#mindspore.ops.one_hot">[文档]</a><span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a one-hot tensor.</span>

<span class="sd">    The locations represented by indices in `indices` take value `on_value`, while all</span>
<span class="sd">    other locations take value `off_value`.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the input indices is rank `N`, the output will have rank `N+1`. The new axis is created at dimension `axis`.</span>

<span class="sd">    Args:</span>
<span class="sd">        indices(Tensor): A tensor of indices. Tensor of shape :math:`(X_0, \ldots, X_n)`.</span>
<span class="sd">            Data type must be uint8, int32 or int64.</span>
<span class="sd">        depth(int): A scalar defining the depth of the one-hot dimension.</span>
<span class="sd">        on_value(Union[Tensor, int, float]): A value to fill in output when `indices[j] = i`.</span>
<span class="sd">            Support uint8, uint16, uint32, uint64, int8, int16, int32, int64, float16, float32, float64,</span>
<span class="sd">            bool, complex64, complex128.</span>
<span class="sd">        off_value(Union[Tensor, int, float]): A value to fill in output when `indices[j] != i`.</span>
<span class="sd">            Has the same data type as `on_value`.</span>
<span class="sd">        axis(int): Position to insert the value. e.g. If shape of `self` is :math:`(N, C)`, and `axis` is -1,</span>
<span class="sd">            the output shape will be :math:`(N, C, depth)`, If `axis` is 0,</span>
<span class="sd">            the output shape will be :math:`(depth, N, C)`.</span>
<span class="sd">            Default: ``-1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, one-hot tensor. Tensor of shape :math:`(X_0, \ldots, X_{axis}, \text{depth} ,X_{axis+1}, \ldots, X_n)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` or `depth` is not an int.</span>
<span class="sd">        TypeError: If dtype of `indices` is not uint8, int32 or int64.</span>
<span class="sd">        TypeError: If `indices`, `on_value` or `off_value` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is not in range [-1, ndim].</span>
<span class="sd">        ValueError: If `depth` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; depth, on_value, off_value = 3, Tensor(1.0, mindspore.float32), Tensor(0.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.one_hot(indices, depth, on_value, off_value, axis=-1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0. 0.]</span>
<span class="sd">         [0. 1. 0.]</span>
<span class="sd">         [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">on_value</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">on_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">on_value</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">off_value</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">off_value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">off_value</span><span class="p">)</span>
    <span class="n">onehot</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">OneHot</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">onehot</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">)</span></div>


<div class="viewcode-block" id="fill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fill.html#mindspore.ops.fill">[文档]</a><span class="k">def</span> <span class="nf">fill</span><span class="p">(</span><span class="nb">type</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a Tensor of the specified shape and fill it with the specified value.</span>

<span class="sd">    Args:</span>
<span class="sd">        type (mindspore.dtype): The specified type of output tensor. The data type only supports</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ and</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        shape (Union(Tensor, tuple[int])): The specified shape of output tensor.</span>
<span class="sd">        value (Union(Tensor, number.Number, bool)): Value to fill the returned tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is not a tuple or a tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fill(mindspore.float32, (2, 2), 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fill(mindspore.float32, (3, 3), 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fill_</span><span class="p">(</span><span class="nb">type</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="full"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.full.html#mindspore.ops.full">[文档]</a><span class="k">def</span> <span class="nf">full</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span> <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a Tensor of the specified shape and fill it with the specified value.</span>

<span class="sd">    Args:</span>
<span class="sd">        size (Union(tuple[int], list[int])): The specified shape of output tensor.</span>
<span class="sd">        fill_value (number.Number): Value to fill the returned tensor. Complex numbers are not supported for now.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (mindspore.dtype): The specified type of output tensor. `bool_` and `number` are supported, for details,</span>
<span class="sd">            please refer to :class:`mindspore.dtype` . Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `size` is not a tuple or list.</span>
<span class="sd">        ValueError: The element in `size` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; output = ops.full((2, 2), 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.full((3, 3), 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;ops.full&#39;, &#39;size&#39; must be a tuple or list of ints, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mstype</span><span class="o">.</span><span class="n">all_types</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;ops.full&#39;, &#39;dtype&#39; must be mindspore.type, but got </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fill_</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">)</span></div>


<div class="viewcode-block" id="full_like"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.full_like.html#mindspore.ops.full_like">[文档]</a><span class="k">def</span> <span class="nf">full_like</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a Tensor of the same shape as `input` and filled with `fill_value`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input Tensor and the output Tensor have the same shape as `input`.</span>
<span class="sd">        fill_value (Number): Value to fill the returned Tensor. Complex numbers are not supported for now.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (mindspore.dtype, optional): The specified type of output tensor. `bool_` and `number` are supported,</span>
<span class="sd">            for details, please refer to :class:`mindspore.dtype` . Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[0, 1], [2, 1]], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.full_like(input, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[0, 1, 1], [2, 1, 2], [1, 3, 4]], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.full_like(input, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For ops.full_like, the argument &#39;x&#39; must be tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">return</span> <span class="n">full</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">fill_value</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="chunk"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.chunk.html#mindspore.ops.chunk">[文档]</a><span class="k">def</span> <span class="nf">chunk</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">chunks</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cut the input Tensor into `chunks` sub-tensors along the specified axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        This function may return less then the specified number of chunks!</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A Tensor to be cut.</span>
<span class="sd">        chunks (int): Number of sub-tensors to cut.</span>
<span class="sd">        axis (int, optional): Specify the dimensions that you want to split. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `input` is not Tensor.</span>
<span class="sd">        TypeError: The sum of `chunks` is not int.</span>
<span class="sd">        TypeError: If argument `axis` is not int.</span>
<span class="sd">        ValueError: If argument `axis` is out of range of :math:`[-input.ndim, input.ndim)` .</span>
<span class="sd">        ValueError: If argument `chunks` is not positive number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(9).astype(&quot;float32&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.chunk(Tensor(input_x), 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32, value= [ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[3], dtype=Float32, value= [ 3.00000000e+00,  4.00000000e+00,  5.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[3], dtype=Float32, value= [ 6.00000000e+00,  7.00000000e+00,  8.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For ops.chunk parameter `input` must be Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">_check_axis_type</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;ops.chunk&quot;</span><span class="p">)</span>
    <span class="n">arr_axis</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">chunks</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For ops.chunk type of argument `chunks` should be integer, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">chunks</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For ops.chunk parameter &#39;chunks&#39; must be greater than 0, but got </span><span class="si">{</span><span class="n">chunks</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">arr_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">length_along_dim</span> <span class="o">=</span> <span class="n">arr_shape</span><span class="p">[</span><span class="n">arr_axis</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">chunks</span> <span class="o">&gt;</span> <span class="n">length_along_dim</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">arr_axis</span><span class="p">,</span> <span class="n">length_along_dim</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">length_along_dim</span> <span class="o">%</span> <span class="n">chunks</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">arr_axis</span><span class="p">,</span> <span class="n">chunks</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">length_along_dim</span> <span class="o">/</span> <span class="n">chunks</span><span class="p">))</span>
        <span class="n">true_chunks</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">length_along_dim</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">)</span>
        <span class="n">length1</span> <span class="o">=</span> <span class="n">true_chunks</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="n">length2</span> <span class="o">=</span> <span class="n">length_along_dim</span> <span class="o">-</span> <span class="n">length1</span>
        <span class="n">start1</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">rank</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">size1</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">arr_axis</span><span class="p">,</span> <span class="n">length1</span><span class="p">)</span>
        <span class="n">start2</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">start1</span><span class="p">,</span> <span class="n">arr_axis</span><span class="p">,</span> <span class="n">length1</span><span class="p">)</span>
        <span class="n">size2</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">arr_axis</span><span class="p">,</span> <span class="n">length2</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">arr_axis</span><span class="p">,</span> <span class="n">true_chunks</span><span class="p">)(</span><span class="n">tensor_slice</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">start1</span><span class="p">,</span> <span class="n">size1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">length2</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">+=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">arr_axis</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">tensor_slice</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">start2</span><span class="p">,</span> <span class="n">size2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span></div>


<span class="k">def</span> <span class="nf">fills</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `fills` is deprecated, please use `ops.fill` instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">float</span><span class="p">):</span>
        <span class="n">value_</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">value_</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;ops.fills&#39;, if the argument &#39;value&#39; is a tensor, the number of its dimension&quot;</span>
                             <span class="s2">&quot; should be 0, but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">value</span><span class="o">.</span><span class="n">ndim</span><span class="p">))</span>
        <span class="n">value_</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;ops.fills&#39;, the type of argument &#39;value&#39; should be int, float or Tensor,&quot;</span>
                        <span class="s2">&quot; but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">fills_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">value_</span><span class="p">)</span>


<div class="viewcode-block" id="ones"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ones.html#mindspore.ops.ones">[文档]</a><span class="k">def</span> <span class="nf">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor filled with value ones.</span>

<span class="sd">    Creates a tensor with shape described by the first argument and fills it with value ones in type of the second</span>
<span class="sd">    argument.</span>

<span class="sd">    Args:</span>
<span class="sd">        shape (Union[tuple[int], int, Tensor]): The specified shape of output tensor. Only positive integer or</span>
<span class="sd">            tuple or Tensor containing positive integers are allowed. If it is a Tensor,</span>
<span class="sd">            it must be a 0-D or 1-D Tensor with int32 or int64 dtypes.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`): The specified type of output tensor. If `dtype` is ``None`` ,</span>
<span class="sd">            `mindspore.float32` will be used. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as input shape value.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is not tuple, int or Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ones((2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype</span>
    <span class="n">ones_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FillV2</span><span class="p">()</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">shape</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">shape</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">shape</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">ones_op</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="ones_like"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ones_like.html#mindspore.ops.ones_like">[文档]</a><span class="k">def</span> <span class="nf">ones_like</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor with a value of 1 and its shape is the same as the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of any dimension.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The specified dtype of the output tensor. If `dtype` is ``None`` ,</span>
<span class="sd">            the dtype of the input tensor will be used. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `input` but filled with ones.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0, 1], [2, 1]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ones_like(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">         [1 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ones_like_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">OnesLike</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">ones_like_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="zeros"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.zeros.html#mindspore.ops.zeros">[文档]</a><span class="k">def</span> <span class="nf">zeros</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor filled with 0 with shape described by `shape` and fills it with value 0 in type of `dtype`.</span>

<span class="sd">    Args:</span>
<span class="sd">        size (Union[tuple[int], int, Tensor]): The specified shape of output tensor. Only positive integer or</span>
<span class="sd">            tuple or Tensor containing positive integers are allowed. If it is a Tensor,</span>
<span class="sd">            it must be a 0-D or 1-D Tensor with int32 or int64 dtypes.</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The specified type of output tensor. If `dtype` is ``None`` ,</span>
<span class="sd">            mindspore.float32 will be used. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and size as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `size` is not tuple, int or Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; output = ops.zeros((2, 2), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0.]</span>
<span class="sd">         [0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">zero_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FillV2</span><span class="p">()</span>
    <span class="n">_dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">_dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">size</span><span class="p">])</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="n">size</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">size</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">size</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">zero_op</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="zeros_like"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.zeros_like.html#mindspore.ops.zeros_like">[文档]</a><span class="k">def</span> <span class="nf">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor filled with 0, with the same size as x, and the given dtype.</span>

<span class="sd">    If `dtype = None`, the tensor will have the same dtype as input `input`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of any dimension.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        dtype (:class:`mindspore.dtype`, optional): The specified dtype of the output tensor. If `dtype` is ``None`` ,</span>
<span class="sd">            the dtype of the input tensor will be used. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, filled with 0.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype is not a MindSpore dtype.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(4).reshape(2, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.zeros_like(x, dtype=mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0.]</span>
<span class="sd">         [0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype</span>
    <span class="n">zeros_like_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ZerosLike</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">zeros_like_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">_dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="tile"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tile.html#mindspore.ops.tile">[文档]</a><span class="k">def</span> <span class="nf">tile</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">multiples</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replicates an input tensor with given multiples times.</span>

<span class="sd">    Creates a new tensor by replicating `input` `multiples` times. The i&#39;th dimension of</span>
<span class="sd">    output tensor has `input.shape[i] * multiples[i]` elements, and the values of `input`</span>
<span class="sd">    are replicated `multiples[i]` times along the i&#39;th dimension.</span>

<span class="sd">    Note:</span>
<span class="sd">        The length of `multiples` must be greater or equal to the length of dimension in `input`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): 1-D or higher dimensional Tensor. Set the shape of input tensor as</span>
<span class="sd">            :math:`(x_1, x_2, ..., x_S)` .</span>

<span class="sd">        multiples (tuple[int]): The parameter that specifies the number of replications,</span>
<span class="sd">            the parameter type is tuple, and the data type is int, i.e., :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">            The length of `multiples` cannot be smaller than the length of the shape of `input`.</span>
<span class="sd">            Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type as the `input`. Suppose the length of `multiples` is `d`,</span>
<span class="sd">        the dimension of `input` is `input.dim`, and the shape of `input` is :math:`(x_1, x_2, ..., x_S)`.</span>

<span class="sd">        - If `input.dim = d`, then the shape of their corresponding positions can be multiplied, and</span>
<span class="sd">          the shape of Outputs is :math:`(x_1*y_1, x_2*y_2, ..., x_S*y_S)`.</span>
<span class="sd">        - If `input.dim &lt; d`, fill in multiple 1 in the length of the shape of `input` until their</span>
<span class="sd">          lengths are consistent. Such as set the shape of `input` as :math:`(1, ..., x_1, x_2, ..., x_S)`,</span>
<span class="sd">          then the shape of their corresponding positions can be multiplied, and the shape of Outputs is</span>
<span class="sd">          :math:`(1*y_1, ..., x_R*y_R, x_S*y_S)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `multiples` is not a tuple or its elements are not all int.</span>
<span class="sd">        ValueError: If the elements of `multiples` are not all greater than 0.</span>
<span class="sd">        ValueError: If the length of `multiples` are smaller than the length of dimension in `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[1, 2], [3, 4]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; multiples = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tile(input, multiples)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.  2.  1.  2.  1.  2.]</span>
<span class="sd">         [3.  4.  3.  4.  3.  4.]</span>
<span class="sd">         [1.  2.  1.  2.  1.  2.]</span>
<span class="sd">         [3.  4.  3.  4.  3.  4.]]</span>
<span class="sd">        &gt;&gt;&gt; multiples = (2, 3, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tile(input, multiples)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]]</span>
<span class="sd">         [[1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]</span>
<span class="sd">          [1. 2. 1. 2.]</span>
<span class="sd">          [3. 4. 3. 4.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tile_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">multiples</span><span class="p">)</span></div>


<div class="viewcode-block" id="range"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.range.html#mindspore.ops.range">[文档]</a><span class="k">def</span> <span class="nf">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a sequence of numbers that begins at `start` and extends by increments of</span>
<span class="sd">    `limit` up to but not including `end`.</span>

<span class="sd">    The types of all 3 inputs must be the same. The type of the resulting tensor is</span>
<span class="sd">    the same as the type of the inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        start (Tensor): A scalar Tensor. The first number in the sequence. Must have</span>
<span class="sd">          type: int32 ,int64, float32 or float64.</span>
<span class="sd">        end (Tensor): A scalar Tensor. Upper limit of the sequence, exclusive. Must</span>
<span class="sd">          have type: int32 ,int64, float32 or float64.</span>
<span class="sd">        step (Tensor): A scalar Tensor. Number that increments `start`. Must have</span>
<span class="sd">          type: int32 ,int64, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1-D Tensor, with the same type as the inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `start`, `end` or `step` is not scalar Tensor.</span>
<span class="sd">        TypeError: If datatype of `start`, `end` or `step` is not same.</span>
<span class="sd">        TypeError: If datatype of `start`, `end` or `step` is not supported.</span>
<span class="sd">        ValueError: If `step` = 0.</span>
<span class="sd">        ValueError: If `start` &gt;= `end` when `step` &gt; 0.</span>
<span class="sd">        ValueError: If `start` &lt;= `end` when `step` &lt; 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(0, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; end = Tensor(10, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; step = Tensor(4, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.range(start, end, step)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 4 8]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">range_</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span></div>


<span class="c1">##############################</span>
<span class="c1"># Tensor Operation Functions.</span>
<span class="c1">##############################</span>


<div class="viewcode-block" id="unique"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unique.html#mindspore.ops.unique">[文档]</a><span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the unique elements of input tensor and also return a tensor containing the index of each value of input</span>
<span class="sd">    tensor corresponding to the output unique tensor.</span>

<span class="sd">    The output contains Tensor `y` and Tensor `idx`, the format is probably similar to (`y`, `idx`).</span>
<span class="sd">    The shape of Tensor `y` and Tensor `idx` is different in most cases, because Tensor `y` will be deduplicated,</span>
<span class="sd">    and the shape of Tensor `idx` is consistent with the input.</span>

<span class="sd">    To get the same shape between `idx` and `y`, please ref to :class:`mindspore.ops.UniqueWithPad` operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple, containing Tensor objects (`y`, `idx`), `y` is a tensor with the</span>
<span class="sd">        same type as `input`, and contains the unique elements in `input`.</span>
<span class="sd">        `idx` is a tensor containing indices of elements in</span>
<span class="sd">        the input corresponding to the output tensor, have the same shape with `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 5, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unique(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Int32, value= [1, 2, 5]), Tensor(shape=[4], dtype=Int32, value= [0, 1, 2, 1]))</span>
<span class="sd">        &gt;&gt;&gt; y = output[0]</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1 2 5]</span>
<span class="sd">        &gt;&gt;&gt; idx = output[1]</span>
<span class="sd">        &gt;&gt;&gt; print(idx)</span>
<span class="sd">        [0 1 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">unique_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Unique</span><span class="p">)()</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">)()</span>

    <span class="n">shape_x</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">length_x</span> <span class="o">=</span> <span class="n">get_x_shape</span><span class="p">(</span><span class="n">shape_x</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">length_x</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">unique_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">shape_x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">idx</span></div>


<div class="viewcode-block" id="unique_with_pad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unique_with_pad.html#mindspore.ops.unique_with_pad">[文档]</a><span class="k">def</span> <span class="nf">unique_with_pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pad_num</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns unique elements and relative indexes in 1-D tensor, filled with padding num.</span>

<span class="sd">    The basic function is the same as the Unique operator, but the UniqueWithPad operator adds a Pad function.</span>
<span class="sd">    The returned tuple(`y`, `idx`) after the input Tensor `x` is processed by the unique operator,</span>
<span class="sd">    in which the shapes of `y` and `idx` are mostly not equal. Therefore, in order to solve the above situation,</span>
<span class="sd">    the UniqueWithPad operator will fill the `y` Tensor with the `pad_num` specified by the user</span>
<span class="sd">    to make it have the same shape as the Tensor `idx`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The tensor need to be unique. Must be 1-D vector with types: int32, int64.</span>
<span class="sd">        pad_num (int): Pad num. The data type is an int.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple(Tensor), tuple of 2 tensors, `y` and `idx`.</span>

<span class="sd">        - y (Tensor) - The unique elements filled with pad_num, the shape and data type same as `x`.</span>
<span class="sd">        - idx (Tensor) - The index of each value of `x` in the unique output `y`, the shape and data type same as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 2, 3, 5, 5]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unique_with_pad(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[6], dtype=Int32, value= [1, 2, 3, 5, 0, 0]),</span>
<span class="sd">         Tensor(shape=[6], dtype=Int32, value= [0, 1, 1, 2, 3, 3]))</span>
<span class="sd">        &gt;&gt;&gt; y = output[0]</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1 2 3 5 0 0]</span>
<span class="sd">        &gt;&gt;&gt; idx = output[1]</span>
<span class="sd">        &gt;&gt;&gt; print(idx)</span>
<span class="sd">        [0 1 1 2 3 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">unique_with_pad_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pad_num</span><span class="p">)</span></div>


<div class="viewcode-block" id="unique_consecutive"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unique_consecutive.html#mindspore.ops.unique_consecutive">[文档]</a><span class="k">def</span> <span class="nf">unique_consecutive</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">return_idx</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the elements that are unique in each consecutive group of equivalent elements in the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">        return_idx (bool, optional): Whether to return the index of where the element in the original input</span>
<span class="sd">            maps to the position in the output. Default: ``False`` .</span>
<span class="sd">        return_counts (bool, optional): Whether to return the counts of each unique element. Default: ``False`` .</span>
<span class="sd">        axis (int, optional): The dimension to apply unique. If ``None`` , the unique of the flattened input is</span>
<span class="sd">            returned. If specified, it must be int32 or int64. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor or a tuple of tensors containing tensor objects (`output`, `idx`, `counts`). `output` has the</span>
<span class="sd">        same type as `input` and is used to represent the output list of unique scalar elements. If `return_idx` is</span>
<span class="sd">        True, there will be an additional returned tensor, `idx`, which has the same shape as `input` and represents</span>
<span class="sd">        the index of where the element in the original input maps to the position in the output. If `return_counts`</span>
<span class="sd">        is True, there will be an additional returned tensor, `counts`, which represents the number of occurrences</span>
<span class="sd">        for each unique value or tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not supported.</span>
<span class="sd">        TypeError: If `return_idx` is not a bool.</span>
<span class="sd">        TypeError: If `return_counts` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is not in the range of :math:`[-ndim, ndim-1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 1, 2, 2, 3, 1, 1, 2]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output, idx, counts = ops.unique_consecutive(x, True, True, None)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 2 3 1 2]</span>
<span class="sd">        &gt;&gt;&gt; print(idx)</span>
<span class="sd">        [0 0 1 1 2 3 3 4]</span>
<span class="sd">        &gt;&gt;&gt; print(counts)</span>
<span class="sd">        [2 2 1 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;unique_consecutive&#39;, &#39;input&#39; must be Tensor.&quot;</span><span class="p">)</span>
    <span class="n">unique_consecutive_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">UniqueConsecutive</span><span class="p">)(</span><span class="n">return_idx</span><span class="p">,</span> <span class="n">return_counts</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">unique_consecutive_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_idx</span> <span class="ow">and</span> <span class="n">return_counts</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">counts</span>
    <span class="k">if</span> <span class="n">return_idx</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">idx</span>
    <span class="k">if</span> <span class="n">return_counts</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">counts</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="searchsorted"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.searchsorted.html#mindspore.ops.searchsorted">[文档]</a><span class="k">def</span> <span class="nf">searchsorted</span><span class="p">(</span><span class="n">sorted_sequence</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">out_int32</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the position indices such that after inserting the values into the `sorted_sequence`, the order of innermost</span>
<span class="sd">    dimension of the `sorted_sequence` remains unchanged.</span>

<span class="sd">    Args:</span>
<span class="sd">        sorted_sequence (Tensor): The input tensor.</span>
<span class="sd">            It must contain a monotonically increasing sequence on the innermost dimension.</span>
<span class="sd">        values (Tensor): The value that should be inserted.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        out_int32 (bool, optional): Output datatype. If ``True`` , the output datatype will be int32;</span>
<span class="sd">            if ``False`` , the output datatype will be int64. Default: ``False`` .</span>
<span class="sd">        right (bool, optional): Search Strategy. If ``True`` , return the last suitable index found;</span>
<span class="sd">            if ``False`` , return the first such index. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor containing the indices from the innermost dimension of `sorted_sequence` such that,</span>
<span class="sd">        if insert the corresponding value in the `values` tensor, the order of `sorted_sequence` would be preserved,</span>
<span class="sd">        whose datatype is int32 if out_int32 is ``True`` , otherwise int64, and shape is the same as the shape of</span>
<span class="sd">        `values`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the dimension of `sorted_sequence` isn&#39;t 1 and all dimensions except the last dimension of</span>
<span class="sd">            `sorted_sequence` and `values` are different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; sorted_sequence = Tensor(np.array([[0, 1, 3, 5, 7], [2, 4, 6, 8, 10]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; values = Tensor(np.array([[3, 6, 9], [3, 6, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.searchsorted(sorted_sequence, values)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2 4 5]</span>
<span class="sd">         [1 2 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;out_int32&quot;</span><span class="p">,</span> <span class="n">out_int32</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;search_sorted&quot;</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">out_int32</span> <span class="k">else</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span>
    <span class="n">search_sorted_</span> <span class="o">=</span> <span class="n">SearchSorted</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">search_sorted_</span><span class="p">(</span><span class="n">sorted_sequence</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span></div>


<div class="viewcode-block" id="ger"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ger.html#mindspore.ops.ger">[文档]</a><span class="k">def</span> <span class="nf">ger</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">vec2</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Ger product of `input` and `vec2`. Calculate the outer product of two arrays. If `input` is a 1D Tensor of</span>
<span class="sd">    shape :math:`(m,)` and `vec2` is a 1D Tensor of shape :math:`(n,)`, then `output` must be a 2D Tensor of shape</span>
<span class="sd">    :math:`(m, n)`.</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently Ascend does not support float64 data input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input Tensor, with dtype of float16, float32 or float64.</span>
<span class="sd">        vec2 (Tensor): input Tensor, with dtype of float16, float32 or float64, must have the same dtype as `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, output matrix with the same dtype as inputs. With `input` shape :math:`(m,)` and</span>
<span class="sd">        `vec2` shape of :math:`(n,)`, the `output` has shape :math:`(m, n)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `vec2` is not a 1-D Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` and `vec2` is not float16, float32 or float64.</span>
<span class="sd">        TypeError: If the dtype of `input` and `vec2` are not the same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([1., 2., 3., 4.], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; vec2 = Tensor([1., 2., 3.], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ger(input, vec2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  2.  3.]</span>
<span class="sd">         [ 2.  4.  6.]</span>
<span class="sd">         [ 3.  6.  9.]</span>
<span class="sd">         [ 4.  8. 12.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ger_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span></div>


<div class="viewcode-block" id="size"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.size.html#mindspore.ops.size">[文档]</a><span class="k">def</span> <span class="nf">size</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Scalar of type int that represents the size of the input Tensor and the total number of elements in the</span>
<span class="sd">    Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Input parameters, the shape of tensor is :math:`(x_1, x_2, ..., x_R)`. The data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        int. A scalar representing the elements&#39; size of `input_x`, tensor is the number of elements</span>
<span class="sd">        in a tensor, :math:`size=x_1*x_2*...x_R`. The data type is an int.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.size(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        4</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">size_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="shape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.shape.html#mindspore.ops.shape">[文档]</a><span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the shape of the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple[int], the output tuple is constructed by multiple integers,</span>
<span class="sd">        :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.shape(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (3, 2, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">shape_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="dyn_shape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dyn_shape.html#mindspore.ops.dyn_shape">[文档]</a><span class="k">def</span> <span class="nf">dyn_shape</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the shape of the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of `input_x` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.dyn_shape(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_shape_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="rank"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.rank.html#mindspore.ops.rank">[文档]</a><span class="k">def</span> <span class="nf">rank</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the rank of a tensor.</span>

<span class="sd">    Returns a 0-D int32 Tensor representing the rank of input; the rank of a tensor</span>
<span class="sd">    is the number of indices required to uniquely select each element of the tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`. The data type is Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. 0-D int32 Tensor representing the rank of input, i.e., :math:`R`. The data type is an int.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.rank(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        2</span>
<span class="sd">        &gt;&gt;&gt; print(type(output))</span>
<span class="sd">        &lt;class &#39;int&#39;&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">rank_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="reshape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.reshape.html#mindspore.ops.reshape">[文档]</a><span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rearranges the input Tensor based on the given shape.</span>

<span class="sd">    The &#39;shape&#39; can only have one -1 at most, in which case it&#39;s inferred from the remaining dimensions and</span>
<span class="sd">    the number of elements in the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        shape (Union[tuple[int], Tensor[int]]): Constructed by multiple</span>
<span class="sd">            integers, i.e., :math:`(y_1, y_2, ..., y_S)`. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: Given a shape tuple, if it has several -1; or if the product</span>
<span class="sd">            of its elements is less than or equal to 0 or cannot be divided by the product</span>
<span class="sd">            of the input tensor shape; or if it does not match the input&#39;s array size.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reshape(input, (3, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.1  0.3]</span>
<span class="sd">         [ 3.6  0.4]</span>
<span class="sd">         [ 0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">reshape_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="reverse_sequence"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.reverse_sequence.html#mindspore.ops.reverse_sequence">[文档]</a><span class="k">def</span> <span class="nf">reverse_sequence</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">seq_lengths</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses variable length slices.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input to reverse, supporting all number types including bool.</span>
<span class="sd">        seq_lengths (Tensor): Specified reversing length, must be a 1-D vector with int32 or int64 types.</span>
<span class="sd">        seq_dim (int): The dimension where reversal is performed. Required.</span>
<span class="sd">        batch_dim (int): The input is sliced in this dimension. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `seq_dim` or `batch_dim` is not an int.</span>
<span class="sd">        ValueError: If value of `batch_dim` is equal to or greater than length of shape of input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([1, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse_sequence(x, seq_lengths, seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [5. 4. 6.]</span>
<span class="sd">         [9. 8. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([1, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse_sequence(x, seq_lengths, seq_dim=0, batch_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 5. 9.]</span>
<span class="sd">         [4. 2. 6.]</span>
<span class="sd">         [7. 8. 3.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([2, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse_sequence(x, seq_lengths, seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 1. 3.]</span>
<span class="sd">         [5. 4. 6.]</span>
<span class="sd">         [9. 8. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([3, 2, 3]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse_sequence(x, seq_lengths, seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[3. 2. 1.]</span>
<span class="sd">         [5. 4. 6.]</span>
<span class="sd">         [9. 8. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; seq_lengths = Tensor(np.array([4, 4]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse_sequence(x, seq_lengths, seq_dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4. 3. 2. 1.]</span>
<span class="sd">         [8. 7. 6. 5.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ReverseSequence</span><span class="p">(</span><span class="n">seq_dim</span><span class="o">=</span><span class="n">seq_dim</span><span class="p">,</span> <span class="n">batch_dim</span><span class="o">=</span><span class="n">batch_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">seq_lengths</span><span class="p">)</span></div>


<div class="viewcode-block" id="flatten"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.flatten.html#mindspore.ops.flatten">[文档]</a><span class="k">def</span> <span class="nf">flatten</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">end_dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flatten a tensor along dimensions from `start_dim` to `start_dim`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>
<span class="sd">        order (str, optional): Only ``&#39;C&#39;`` and ``&#39;F&#39;`` are supported.</span>
<span class="sd">            ``&#39;C&#39;`` means to flatten in row-major (C-style) order.</span>
<span class="sd">            ``&#39;F&#39;`` means to flatten in column-major (Fortran-style) order. Default: ``&#39;C&#39;`` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        start_dim (int, optional): The first dimension to flatten. Default: ``1`` .</span>
<span class="sd">        end_dim (int, optional): The last dimension to flatten. Default: ``-1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. If no dimensions are flattened, returns the original `input`, otherwise return the flattened Tensor.</span>
<span class="sd">        If `input` is a 0-dimensional Tensor, a 1-dimensional Tensor will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `order` is not string type.</span>
<span class="sd">        ValueError: If `order` is string type, but not &#39;C&#39; or &#39;F&#39;.</span>
<span class="sd">        TypeError: If `start_dim` or `end_dim` is not int.</span>
<span class="sd">        ValueError: If `start_dim` is greater than `end_dim` after canonicalized.</span>
<span class="sd">        ValueError: If `start_dim` or `end_dim` is not in range of [-input.dim, input.dim-1].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.flatten(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 24)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">check_axis_valid</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">ndim</span> <span class="ow">or</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;start_dim&#39; or &#39;end_dim&#39; out of range.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dim_valid</span><span class="p">(</span><span class="n">start_dim</span><span class="p">,</span> <span class="n">end_dim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">start_dim</span> <span class="o">&gt;</span> <span class="n">end_dim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;flatten&#39;, &#39;start_dim&#39; cannot come after &#39;end_dim&#39;.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x_rank</span><span class="p">):</span>
        <span class="n">ndim</span> <span class="o">=</span> <span class="n">x_rank</span> <span class="k">if</span> <span class="n">x_rank</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>
        <span class="n">check_axis_valid</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">axis</span> <span class="k">if</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">ndim</span>

    <span class="c1"># Check the types of arguments.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;flatten&#39;, argument &#39;input&#39; must be Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">start_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">end_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> \
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">start_dim</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">end_dim</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;flatten&#39;, both &#39;start_dim&#39; and &#39;end_dim&#39; must be int.&quot;</span><span class="p">)</span>
    <span class="n">check_flatten_order_const</span><span class="p">(</span><span class="n">order</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">order</span> <span class="o">==</span> <span class="s1">&#39;F&#39;</span><span class="p">:</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="nb">input</span><span class="p">))</span>
        <span class="n">new_order</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tuple_reversed</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">new_order</span><span class="p">)</span>

    <span class="c1"># Handle the default case.</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">shape_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">x_rank</span> <span class="o">=</span> <span class="n">rank_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">start_dim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">end_dim</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x_rank</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">reshape_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
        <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Flatten</span><span class="p">)()(</span><span class="nb">input</span><span class="p">)</span>

    <span class="c1"># Check axis.</span>
    <span class="n">start_dim</span> <span class="o">=</span> <span class="n">canonicalize_axis</span><span class="p">(</span><span class="n">start_dim</span><span class="p">,</span> <span class="n">x_rank</span><span class="p">)</span>
    <span class="n">end_dim</span> <span class="o">=</span> <span class="n">canonicalize_axis</span><span class="p">(</span><span class="n">end_dim</span><span class="p">,</span> <span class="n">x_rank</span><span class="p">)</span>
    <span class="n">check_dim_valid</span><span class="p">(</span><span class="n">start_dim</span><span class="p">,</span> <span class="n">end_dim</span><span class="p">)</span>
    <span class="c1"># If input is a 0-dimensional Tensor, a 1-dimensional Tensor will be returned.</span>
    <span class="k">if</span> <span class="n">x_rank</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">reshape_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
    <span class="c1"># If no dimensions to flatten, return the original object.</span>
    <span class="k">if</span> <span class="n">start_dim</span> <span class="o">==</span> <span class="n">end_dim</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">input</span>
    <span class="c1"># Flatten elements along specified dimensions.</span>
    <span class="n">dim_length</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">start_dim</span>
    <span class="k">while</span> <span class="n">idx</span> <span class="o">&lt;=</span> <span class="n">end_dim</span><span class="p">:</span>
        <span class="n">dim_length</span> <span class="o">*=</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">idx</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[:</span><span class="n">start_dim</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">dim_length</span><span class="p">,)</span> <span class="o">+</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">end_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">reshape_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_select_type_match</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">tensor_type</span><span class="p">,</span> <span class="n">scalar_name</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tensor_type</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the input[</span><span class="si">{</span><span class="n">scalar_name</span><span class="si">}</span><span class="s2">] is int, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;then the input[</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">] must be a Tensor of int32.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="nb">float</span><span class="p">)</span> <span class="ow">and</span> <span class="n">tensor_type</span> <span class="o">!=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the input[</span><span class="si">{</span><span class="n">scalar_name</span><span class="si">}</span><span class="s2">] is float, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;then the input[</span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2">] must be a Tensor of float32.&quot;</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_select_shape_match</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">cond_shape</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">input_shape</span> <span class="o">!=</span> <span class="n">cond_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the cond shape must be same as </span><span class="si">{</span><span class="n">tensor_name</span><span class="si">}</span><span class="s2"> shape.&quot;</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_select_type</span><span class="p">(</span><span class="n">is_cond_tensor</span><span class="p">,</span> <span class="n">is_x_scalar</span><span class="p">,</span> <span class="n">is_y_scalar</span><span class="p">,</span> <span class="n">is_x_tensor</span><span class="p">,</span> <span class="n">is_y_tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_cond_tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the input[cond] must be a Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_x_scalar</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_y_tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the input[x] is int or float, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;then the input[y] must be a Tensor.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">is_y_scalar</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_x_tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For functional operator[select], the input[y] is int or float, &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;then the input[x] must be a Tensor.&quot;</span><span class="p">)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_select_shape_same</span><span class="p">(</span><span class="n">cond_shape</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if input of select has same shape.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cond_shape</span> <span class="o">==</span> <span class="n">x_shape</span> <span class="ow">and</span> <span class="n">x_shape</span> <span class="o">==</span> <span class="n">y_shape</span> <span class="ow">and</span> <span class="n">cond_shape</span> <span class="o">==</span> <span class="n">y_shape</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">get_max_value</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the maximum value of x, y and z.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="n">y</span> <span class="ow">and</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="n">z</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">if</span> <span class="n">y</span> <span class="o">&gt;=</span> <span class="n">x</span> <span class="ow">and</span> <span class="n">y</span> <span class="o">&gt;=</span> <span class="n">z</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">y</span>
    <span class="k">return</span> <span class="n">z</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_calc_broadcast_shape</span><span class="p">(</span><span class="n">cond_shape</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate broadcast shape for select&quot;&quot;&quot;</span>
    <span class="n">converted_shape</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cond_reverse</span> <span class="o">=</span> <span class="n">cond_shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x_reverse</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y_reverse</span> <span class="o">=</span> <span class="n">y_shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="n">get_max_value</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cond_reverse</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_reverse</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_reverse</span><span class="p">))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_len</span><span class="p">:</span>
        <span class="n">cond_element</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cond_reverse</span><span class="p">)</span> <span class="k">else</span> <span class="n">cond_reverse</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">x_element</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_reverse</span><span class="p">)</span> <span class="k">else</span> <span class="n">x_reverse</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">y_element</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_reverse</span><span class="p">)</span> <span class="k">else</span> <span class="n">y_reverse</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">broadcast_element</span> <span class="o">=</span> <span class="n">get_max_value</span><span class="p">(</span><span class="n">cond_element</span><span class="p">,</span> <span class="n">x_element</span><span class="p">,</span> <span class="n">y_element</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">cond_element</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">broadcast_element</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For select, condition input can not broadcast at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_element</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">broadcast_element</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For select, x input can not broadcast at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">y_element</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">broadcast_element</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For select, y input can not broadcast at index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">converted_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">broadcast_element</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">converted_shape</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">converted_shape</span><span class="p">)</span>


<div class="viewcode-block" id="select"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.select.html#mindspore.ops.select">[文档]</a><span class="k">def</span> <span class="nf">select</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The conditional tensor determines whether the corresponding element in the output must be</span>
<span class="sd">    selected from `x` (if true) or `y` (if false) based on the value of each element.</span>

<span class="sd">    It can be defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">        x_i, &amp; \text{if } cond_i \\</span>
<span class="sd">        y_i, &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        cond (Tensor[bool]): The condition tensor, decides which element is chosen.</span>
<span class="sd">          The shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">        x (Union[Tensor, int, float]): The first Tensor or number to be selected.</span>
<span class="sd">          If x is a Tensor, the shape is or can be broadcadt to :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">          If x is an int or a float, it will be cast to the type of int32 or float32,</span>
<span class="sd">          and broadcast to the same shape as y. One of x and y must be a Tensor.</span>
<span class="sd">        y (Union[Tensor, int, float]): The second Tensor or number to be selected.</span>
<span class="sd">          If y is a Tensor, The shape is or can be broadcadt to :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">          If y is an int or a float, it will be cast to the type of int32 or float32,</span>
<span class="sd">          and broadcast to the same shape as x. One of x and y must be a Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `cond`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `y` is not a Tensor, int or float.</span>
<span class="sd">        ValueError: The shapes of inputs can not be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # 1) Both inputs are Tensor</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; cond = Tensor([True, False])</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor([1,2], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.select(cond, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 2.]</span>
<span class="sd">        &gt;&gt;&gt; # 2) y is a float</span>
<span class="sd">        &gt;&gt;&gt; cond = Tensor([True, False])</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = 2.0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.select(cond, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">is_x_scalar</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="n">is_y_scalar</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span>
    <span class="n">is_x_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
    <span class="n">is_y_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
    <span class="n">is_cond_tensor</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span>
    <span class="n">_check_select_type</span><span class="p">(</span><span class="n">is_cond_tensor</span><span class="p">,</span> <span class="n">is_x_scalar</span><span class="p">,</span> <span class="n">is_y_scalar</span><span class="p">,</span> <span class="n">is_x_tensor</span><span class="p">,</span> <span class="n">is_y_tensor</span><span class="p">)</span>
    <span class="n">input_x</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">input_y</span> <span class="o">=</span> <span class="n">y</span>
    <span class="k">if</span> <span class="n">is_x_scalar</span><span class="p">:</span>
        <span class="n">_check_select_shape_match</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cond</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">)</span>
        <span class="n">_check_select_type_match</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">)</span>
        <span class="n">input_x</span> <span class="o">=</span> <span class="n">zeros_like_</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_x</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_y_scalar</span><span class="p">:</span>
        <span class="n">_check_select_shape_match</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cond</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
        <span class="n">_check_select_type_match</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
        <span class="n">input_y</span> <span class="o">=</span> <span class="n">zeros_like_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">input_y</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_y</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">input_y</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_x_tensor</span> <span class="ow">and</span> <span class="n">is_y_tensor</span> <span class="ow">and</span> <span class="n">is_cond_tensor</span><span class="p">:</span>
        <span class="n">x_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">cond_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span>
        <span class="n">all_constant</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">isconstant</span><span class="p">(</span><span class="n">cond_shape</span><span class="p">)</span> <span class="ow">and</span> <span class="n">F</span><span class="o">.</span><span class="n">isconstant</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="ow">and</span> <span class="n">F</span><span class="o">.</span><span class="n">isconstant</span><span class="p">(</span><span class="n">y_shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">all_constant</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">_check_select_shape_same</span><span class="p">(</span><span class="n">cond_shape</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">):</span>
            <span class="n">broadcast_shape</span> <span class="o">=</span> <span class="n">_calc_broadcast_shape</span><span class="p">(</span><span class="n">cond_shape</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">y_shape</span><span class="p">)</span>
            <span class="n">new_cond</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
            <span class="n">new_x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
            <span class="n">new_y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">broadcast_shape</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">tensor_select_</span><span class="p">(</span><span class="n">new_cond</span><span class="p">,</span> <span class="n">new_x</span><span class="p">,</span> <span class="n">new_y</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tensor_select_</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span></div>


<div class="viewcode-block" id="strided_slice"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.strided_slice.html#mindspore.ops.strided_slice">[文档]</a><span class="k">def</span> <span class="nf">strided_slice</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span>
                  <span class="n">begin</span><span class="p">,</span>
                  <span class="n">end</span><span class="p">,</span>
                  <span class="n">strides</span><span class="p">,</span>
                  <span class="n">begin_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">end_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">ellipsis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">new_axis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts a strided slice of a Tensor based on `begin/end` index and `strides`.</span>

<span class="sd">    This operation extracts a fragment of size (end-begin)/strides from the given &#39;input_tensor&#39;.</span>
<span class="sd">    Starting from the beginning position, the fragment continues adding strides to the index until</span>
<span class="sd">    all dimensions are not less than the ending position.</span>

<span class="sd">    Note:</span>
<span class="sd">        - `begin` , `end` and `strides` must have the same shape.</span>
<span class="sd">        - `begin` , `end` and `strides` are all 1-D Tensor,  and their shape size</span>
<span class="sd">          must not greater than the dim of `input_x`.</span>

<span class="sd">    During the slicing process, the fragment (end-begin)/strides are extracted from each dimension.</span>

<span class="sd">    Example: For Tensor `input_x` with shape :math:`(5, 6, 7)`,</span>
<span class="sd">    set `begin`, `end` and `strides` to (1, 3, 2), (3, 5, 6),</span>
<span class="sd">    (1, 1, 2) respectively, then elements from index 1 to 3 are extrected for dim 0, index 3 to 5</span>
<span class="sd">    are extrected for dim 1 and index 2 to 6 with a `stirded` of 2 are extrected for dim 2, this</span>
<span class="sd">    process is equivalent to a pythonic slice `input_x[1:3, 3:5, 2:6:2]`.</span>

<span class="sd">    If the length of `begin` 、 `end` and `strides` is smaller than the dim of `input_x`,</span>
<span class="sd">    then all elements are extracted from the missing dims, it behaves like all the</span>
<span class="sd">    missing dims are filled with zeros, size of that missing dim and ones.</span>

<span class="sd">    Example: For Tensor `input_x` with shape :math:`(5, 6, 7)`,</span>
<span class="sd">    set `begin`, `end` and `strides` to (1, 3),</span>
<span class="sd">    (3, 5), (1, 1) respectively, then elements from index 1 to 3 are extrected</span>
<span class="sd">    for dim 0, index 3 to 5 are extrected for dim 1 and index 3 to 5 are extrected</span>
<span class="sd">    for dim 2, this process is equivalent to a pythonic slice `input_x[1:3, 3:5, 0:7]`.</span>

<span class="sd">    Here&#39;s how a mask works:</span>
<span class="sd">    For each specific mask, it will be converted to a binary representation internally, and then</span>
<span class="sd">    reverse the result to start the calculation. For Tensor `input_x` with</span>
<span class="sd">    shape :math:`(5, 6, 7)`. Given mask value of 3 which</span>
<span class="sd">    can be represented as 0b011. Reverse that we get 0b110, which implies the first and second dim of the</span>
<span class="sd">    original Tensor will be effected by this mask. See examples below, for simplicity all mask mentioned</span>
<span class="sd">    below are all in their reverted binary form:</span>

<span class="sd">    - `begin_mask` and `end_mask`</span>

<span class="sd">      If the ith bit of `begin_mask` is 1, `begin[i]` is ignored and the fullest</span>
<span class="sd">      possible range in that dimension is used instead. `end_mask` is analogous,</span>
<span class="sd">      except with the end range. For Tensor `input_x` with shape :math:`(5, 6, 7, 8)`,  if `begin_mask`</span>
<span class="sd">      is 0b110, `end_mask` is 0b011, the slice `input_x[0:3, 0:6, 2:7:2]` is produced.</span>

<span class="sd">    - `ellipsis_mask`</span>

<span class="sd">      If the ith bit of `ellipsis_mask` is 1, as many unspecified dimensions as needed</span>
<span class="sd">      will be inserted between other dimensions. Only one non-zero bit is allowed</span>
<span class="sd">      in `ellipsis_mask`. For Tensor `input_x` with shape :math:`(5, 6, 7, 8)`,  `input_x[2:,...,:6]`</span>
<span class="sd">      is equivalent to `input_x[2:5,:,:,0:6]` ,  `input_x[2:,...]` is equivalent</span>
<span class="sd">      to `input_x[2:5,:,:,:]`.</span>

<span class="sd">    - `new_axis_mask`</span>

<span class="sd">      If the ith bit of `new_axis_mask` is 1, `begin`, `end` and `strides` are</span>
<span class="sd">      ignored and a new length 1 dimension is added at the specified position</span>
<span class="sd">      in the output Tensor. For Tensor `input_x` with shape :math:`(5, 6, 7)`, if `new_axis_mask`</span>
<span class="sd">      is 0b110,  a new dim is added to the second dim, which will produce</span>
<span class="sd">      a Tensor with shape :math:`(5, 1, 6, 7)`.</span>

<span class="sd">    - `shrink_axis_mask`</span>

<span class="sd">      If the ith bit of `shrink_axis_mask` is 1, `begin`, `end` and `strides`</span>
<span class="sd">      are ignored and dimension i will be shrunk to 0.</span>
<span class="sd">      For Tensor `input_x` with shape :math:`(5, 6, 7)`,</span>
<span class="sd">      if `shrink_axis_mask` is 0b010, it is equivalent to slice `x[:, 5, :]`</span>
<span class="sd">      and results in an output shape of :math:`(5, 7)`.</span>

<span class="sd">    Note:</span>
<span class="sd">        `new_axis_mask` and  `shrink_axis_mask` are not recommended to</span>
<span class="sd">        use at the same time, it might incur unexpected result.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input Tensor to be extracted from.</span>
<span class="sd">        begin (tuple[int]): A tuple which represents the location where to start.</span>
<span class="sd">            Only non-negative int is allowed.</span>
<span class="sd">        end (tuple[int]): A tuple or which represents the maximum location where to end.</span>
<span class="sd">            Only non-negative int is allowed.</span>
<span class="sd">        strides (tuple[int]): A tuple which represents the strides is continuously added</span>
<span class="sd">            before reaching the maximum location. Only int is allowed, it can be negative</span>
<span class="sd">            which results in reversed slicing.</span>
<span class="sd">        begin_mask (int, optional): Starting index of the slice. Default: ``0`` .</span>
<span class="sd">        end_mask (int, optional): Ending index of the slice. Default: ``0`` .</span>
<span class="sd">        ellipsis_mask (int, optional): An int mask, ignore slicing operation when set to 1. Default: ``0`` .</span>
<span class="sd">        new_axis_mask (int, optional): An int mask for adding new dims. Default: ``0`` .</span>
<span class="sd">        shrink_axis_mask (int, optional): An int mask for shrinking dims. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, return the extracts a strided slice of a Tensor based on `begin/end` index and `strides`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `begin_mask`, `end_mask`, `ellipsis_mask`, `new_axis_mask` or</span>
<span class="sd">            `shrink_axis_mask` is not an int.</span>
<span class="sd">        TypeError: If `begin`, `end` or `strides` is not tuple[int].</span>
<span class="sd">        ValueError: If `begin_mask`, `end_mask`, `ellipsis_mask`, `new_axis_mask` or</span>
<span class="sd">            `shrink_axis_mask` is less than 0.</span>
<span class="sd">        ValueError: If `begin`, `end` and `strides` have different shapes.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]],</span>
<span class="sd">        ...                   [[5, 5, 5], [6, 6, 6]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.strided_slice(input_x, (1, 0, 2), (3, 1, 3), (1, 1, 1))</span>
<span class="sd">        &gt;&gt;&gt; # Take this &quot; output = strided_slice(input_x, (1, 0, 2), (3, 1, 3), (1, 1, 1)) &quot; as an example,</span>
<span class="sd">        &gt;&gt;&gt; # start = [1, 0, 2] , end = [3, 1, 3], strides = [1, 1, 1], Find a segment of (start, end),</span>
<span class="sd">        &gt;&gt;&gt; # note that end is an open interval</span>
<span class="sd">        &gt;&gt;&gt; # To facilitate understanding, this operator can be divided into three steps:</span>
<span class="sd">        &gt;&gt;&gt; # Step 1: Calculation of the first dimension:</span>
<span class="sd">        &gt;&gt;&gt; # start = 1, end = 3, strides = 1, So can take 1st, 2nd rows, and then gets the final output at this time.</span>
<span class="sd">        &gt;&gt;&gt; # output_1th =</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [3,3,3]</span>
<span class="sd">        &gt;&gt;&gt; #         [4,4,4]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [5,5,5]</span>
<span class="sd">        &gt;&gt;&gt; #         [6,6,6]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # Step 2: Calculation of the second dimension</span>
<span class="sd">        &gt;&gt;&gt; # 2nd dimension, start = 0, end = 1, strides = 1. So only 0th rows</span>
<span class="sd">        &gt;&gt;&gt; # can be taken, and the output at this time.</span>
<span class="sd">        &gt;&gt;&gt; # output_2nd =</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [3,3,3]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [5,5,5]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # Step 3: Calculation of the third dimension</span>
<span class="sd">        &gt;&gt;&gt; # 3nd dimension,start = 2, end = 3, strides = 1, So can take 2th cols,</span>
<span class="sd">        &gt;&gt;&gt; # and you get the final output at this time.</span>
<span class="sd">        &gt;&gt;&gt; # output_3ed =</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [3]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; #     [</span>
<span class="sd">        &gt;&gt;&gt; #         [5]</span>
<span class="sd">        &gt;&gt;&gt; #     ]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # The final output after finishing is:</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3.]]</span>
<span class="sd">         [[5.]]]</span>
<span class="sd">        &gt;&gt;&gt; # another example like :</span>
<span class="sd">        &gt;&gt;&gt; output = strided_slice(input_x, (1, 0, 0), (2, 1, 3), (1, 1, 1))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 3. 3.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">strided_slice_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">)(</span>
        <span class="n">begin_mask</span><span class="p">,</span> <span class="n">end_mask</span><span class="p">,</span> <span class="n">ellipsis_mask</span><span class="p">,</span> <span class="n">new_axis_mask</span><span class="p">,</span> <span class="n">shrink_axis_mask</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">strided_slice_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span></div>


<div class="viewcode-block" id="slice"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.slice.html#mindspore.ops.slice">[文档]</a><span class="k">def</span> <span class="nf">slice</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Slices a tensor in the specified shape.</span>

<span class="sd">    Slice the tensor `input_x` in shape of `size` and starting at the location specified by `begin`.</span>
<span class="sd">    The slice `begin` represents the offset in each dimension of `input_x`.</span>
<span class="sd">    The slice `size` represents the size of the output tensor.</span>

<span class="sd">    Note:</span>
<span class="sd">        `begin` is zero-based and `size` is one-based.</span>

<span class="sd">    If `size[i]` is -1, all remaining elements in dimension i are included in the slice.</span>
<span class="sd">    This is equivalent to setting :math:`size[i] = input\_x.shape(i) - begin[i]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor.</span>
<span class="sd">        begin (Union[tuple, list]): The beginning of the slice. Only constant value(&gt;=0) is allowed.</span>
<span class="sd">        size (Union[tuple, list]): The size of the slice. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is input `size`, the data type is the same as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `begin` or `size` is neither tuple nor list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; data = Tensor(np.array([[[1, 1, 1], [2, 2, 2]],</span>
<span class="sd">        ...                         [[3, 3, 3], [4, 4, 4]],</span>
<span class="sd">        ...                         [[5, 5, 5], [6, 6, 6]]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.slice(data, (1, 0, 0), (1, 1, 3))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3 3 3]]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.slice(data, (1, 0, 0), (1, 1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3 3]]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.slice(data, (1, 0, 0), (1, 1, 1))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3]]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.slice(data, (1, 1, 0), (1, 1, 3))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4 4 4]]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.slice(data, (1, 0, 1), (1, 1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3 3]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_slice</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span></div>


<div class="viewcode-block" id="concat"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.concat.html#mindspore.ops.concat">[文档]</a><span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.cat()`.</span>

<span class="sd">    Tutorial Examples:</span>
<span class="sd">        - `Tensor - Tensor Operation &lt;https://mindspore.cn/tutorials/en/r2.1/beginner/tensor.html#tensor-operation&gt;`_</span>
<span class="sd">        - `FGSM Network Adversarial Attack - Implementing FGSM</span>
<span class="sd">          &lt;https://mindspore.cn/tutorials/application/en/r2.1/cv/fgsm.html#implementing-fgsm&gt;`_</span>
<span class="sd">        - `Vision Transformer Image Classification - Building ViT as a whole</span>
<span class="sd">          &lt;https://mindspore.cn/tutorials/application/en/r2.1/cv/vit.html#building-vit-as-a-whole&gt;`_</span>
<span class="sd">        - `Sentiment Classification Implemented by RNN - Dense</span>
<span class="sd">          &lt;https://mindspore.cn/tutorials/application/en/r2.1/nlp/sentiment_analysis.html#dense&gt;`_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cat</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="stack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.stack.html#mindspore.ops.stack">[文档]</a><span class="k">def</span> <span class="nf">stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks a list of tensors in specified axis.</span>

<span class="sd">    Stacks the list of input tensors with the same rank `R`, output is a tensor of rank `(R+1)`.</span>

<span class="sd">    Given input tensors of shape :math:`(x_1, x_2, ..., x_R)`. Set the number of input tensors as `N`.</span>
<span class="sd">    If :math:`axis \ge 0`, the shape of the output tensor is</span>
<span class="sd">    :math:`(x_1, x_2, ..., x_{axis}, N, x_{axis+1}, ..., x_R)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (Union[tuple, list]): A Tuple or list of Tensor objects with the same shape and type.</span>
<span class="sd">        axis (int): Dimension to stack. The range is [-(R+1), R+1). Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. A stacked Tensor with the same type as `tensors`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data types of elements in `tensors` are not the same.</span>
<span class="sd">        ValueError: If the length of `tensors` is not greater than zero;</span>
<span class="sd">                    or if axis is out of the range [-(R+1), R+1);</span>
<span class="sd">                    or if the shapes of elements in tensors are not the same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x1 = Tensor(np.array([0, 1]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; input_x2 = Tensor(np.array([2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.stack((input_x1, input_x2), 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">         [2. 3.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_stack</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Stack</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span></div>


<div class="viewcode-block" id="unstack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unstack.html#mindspore.ops.unstack">[文档]</a><span class="k">def</span> <span class="nf">unstack</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Unstacks tensor in specified axis, this is the opposite of :func:`mindspore.ops.stack`.</span>
<span class="sd">    Assuming input is a tensor of rank `R`, output tensors will have rank `(R-1)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">            A tensor to be unstacked and the rank of the tensor must be greater than 0.</span>
<span class="sd">        axis (int): Dimension along which to unpack. Default: ``0`` .</span>
<span class="sd">            Negative values wrap around. The range is [-R, R).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of tensors, the shape of each objects is the same.</span>
<span class="sd">        Given a tensor of shape :math:`(x_1, x_2, ..., x_R)`. If :math:`0 \le axis`,</span>
<span class="sd">        the shape of tensor in output is :math:`(x_1, x_2, ..., x_{axis}, x_{axis+2}, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis is out of the range [-len(input_x.shape), len(input_x.shape)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 1, 1, 1], [2, 2, 2, 2]]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unstack(input_x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[4], dtype=Int64, value= [1, 1, 1, 1]), Tensor(shape=[4], dtype=Int64, value= [2, 2, 2, 2]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_unstack</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Unstack</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_unstack</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="unbind"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unbind.html#mindspore.ops.unbind">[文档]</a><span class="k">def</span> <span class="nf">unbind</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Removes a tensor dimension in specified axis.</span>

<span class="sd">    Unstacks a tensor of rank `R` along axis dimension, and output tensors will have rank `(R-1)`.</span>

<span class="sd">    Given a tensor of shape :math:`(n_1, n_2, ..., n_R)` and a specified `dim`,</span>
<span class="sd">    shape of the output tensors is :math:`(n_1, n_2, ..., n_{dim}, n_{dim+2}, ..., n_R)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape is :math:`(n_1, n_2, ..., n_R)`.</span>
<span class="sd">            A tensor to be unstacked and the rank of the tensor must be greater than 0.</span>
<span class="sd">        dim (int): Dimension along which to unpack. Negative values wrap around. The range is [-R, R). Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of tensors, the shape of each objects is the same.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis is out of the range [-R, R).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unbind(x, dim=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Int64, value=[1, 2, 3]), Tensor(shape=[3], dtype=Int64, value=[4, 5, 6]),</span>
<span class="sd">        Tensor(shape=[3], dtype=Int64, value=[7, 8, 9]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_unstack</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Unstack</span><span class="p">)(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_unstack</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="expand_dims"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.expand_dims.html#mindspore.ops.expand_dims">[文档]</a><span class="k">def</span> <span class="nf">expand_dims</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds an additional dimension to `input_x` at the given axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the specified axis is a negative number, the index is counted</span>
<span class="sd">        backward from the end and starts at 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        axis (int): Specifies the dimension index at which to expand</span>
<span class="sd">            the shape of `input_x`. The value of axis must be in the range</span>
<span class="sd">            `[-input_x.ndim-1, input_x.ndim]`. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(1, x_1, x_2, ..., x_R)` if the</span>
<span class="sd">        value of `axis` is 0. It has the same data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is not in the valid range :math:`[-a.ndim-1, a.ndim]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.expand_dims(input_tensor, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2.]</span>
<span class="sd">          [2. 2.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="unsqueeze"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unsqueeze.html#mindspore.ops.unsqueeze">[文档]</a><span class="k">def</span> <span class="nf">unsqueeze</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds an additional dimension to `input` at the given dim.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(n_1, n_2, ..., n_R)`.</span>
<span class="sd">        dim (int): Specifies the dimension index at which to expand</span>
<span class="sd">            the shape of `input`. The value of `dim` must be in the range</span>
<span class="sd">            `[-input.ndim-1, input.ndim]`. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(1, n_1, n_2, ..., n_R)` if the</span>
<span class="sd">        value of `dim` is 0. It has the same data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dim` is not an int.</span>
<span class="sd">        ValueError: If `dim` is not in the valid range :math:`[-input.ndim-1, input.ndim]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsqueeze(input_tensor, dim=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2.]</span>
<span class="sd">          [2. 2.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="squeeze"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.squeeze.html#mindspore.ops.squeeze">[文档]</a><span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the Tensor after deleting the dimension of size 1 in the specified `axis`.</span>

<span class="sd">    If :math:`axis=None`, it will remove all the dimensions of size 1.</span>
<span class="sd">    If `axis` is specified, it will remove the dimensions of size 1 in the given `axis`.</span>
<span class="sd">    For example, if the dimension is not specified :math:`axis=None`, input shape is (A, 1, B, C, 1, D),</span>
<span class="sd">    then the shape of the output Tensor is (A, B, C, D). If the dimension is specified, the squeeze operation</span>
<span class="sd">    is only performed in the specified dimension. If input shape is (A, 1, B), input Tensor will not be</span>
<span class="sd">    changed when :math:`axis=0` , but when :math:`axis=1` , the shape of the input Tensor will be changed to (A, B).</span>

<span class="sd">    Note:</span>
<span class="sd">        - Please note that in dynamic graph mode, the output Tensor will share data with the input Tensor,</span>
<span class="sd">          and there is no Tensor data copy process.</span>
<span class="sd">        - The dimension index starts at 0 and must be in the range `[-input.ndim, input.ndim]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        axis (Union[int, tuple(int)]): Specifies the dimension indexes of shape to be removed, which will remove</span>
<span class="sd">            all the dimensions of size 1 in the given axis parameter. If specified, it must be int32 or int64.</span>
<span class="sd">            Default: ``None`` , an empty tuple will be used.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(x_1, x_2, ..., x_S)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a tensor.</span>
<span class="sd">        TypeError: If `axis` is neither an int nor tuple.</span>
<span class="sd">        TypeError: If `axis` is a tuple whose elements are not all int.</span>
<span class="sd">        ValueError: If the corresponding dimension of the specified axis isn&#39;t equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.squeeze(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [1. 1.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">squeeze_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Squeeze</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">squeeze_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="transpose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.transpose.html#mindspore.ops.transpose">[文档]</a><span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Permutes the dimensions of the input tensor according to input permutation.</span>

<span class="sd">    For a 1-D array this has no effect, as a transposed vector is simply the same vector.</span>
<span class="sd">    To convert a 1-D array into a 2D column vector please refer the class: mindspore.ops.ExpandDims.</span>
<span class="sd">    For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given,</span>
<span class="sd">    their order indicates how the axes are permuted (see Examples).</span>
<span class="sd">    If axes are not provided and a.shape is :math:`(i[0], i[1], ... i[n-2], i[n-1])`,</span>
<span class="sd">    then a.transpose().shape is :math:`(i[n-1], i[n-2], ... i[1], i[0])`.</span>

<span class="sd">    Note:</span>
<span class="sd">        On GPU and CPU, if the value of `input_perm` is negative, its actual value is `input_perm[i] + rank(input)`.</span>
<span class="sd">        Negative value of `input_perm` is not supported on Ascend.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        input_perm (tuple[int]): The permutation to be converted. The elements in `input_perm` are composed of</span>
<span class="sd">            the indexes of each dimension of `input`. The length of `input_perm` and the shape of `input` must be</span>
<span class="sd">            the same. Only constant value is allowed. Must be in the range [-rank(input), rank(input)).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the type of output tensor is the same as `input` and the shape of output tensor is decided by the</span>
<span class="sd">        shape of `input` and the value of `input_perm`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_perm` is not a tuple.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to length of shape of `input_perm`.</span>
<span class="sd">        ValueError: If the same element exists in `input_perm`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_perm = (0, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.transpose(input, input_perm)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 1.  4.]</span>
<span class="sd">          [ 2.  5.]</span>
<span class="sd">          [ 3.  6.]]</span>
<span class="sd">         [[ 7. 10.]</span>
<span class="sd">          [ 8. 11.]</span>
<span class="sd">          [ 9. 12.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">transpose_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_mul.html#mindspore.ops.scatter_mul">[文档]</a><span class="k">def</span> <span class="nf">scatter_mul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using given values to update tensor value through the mul operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each `i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] \mathrel{*}= \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type. A RuntimeError will be reported</span>
<span class="sd">    when the data types of parameters need to be converted.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor to be updated, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do mul operation whose data type must be int32 or int64.</span>
<span class="sd">        updates (Tensor): The tensor doing the mul operation with `input_x`,</span>
<span class="sd">            the data type is same as `input_x`, the shape is `indices.shape + input_x.shape[1:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `indices` is not an int32 or int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 2. 2.]</span>
<span class="sd">         [4. 4. 4.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [1.0, 1.0, 1.0] = [1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [3.0, 3.0, 3.0] = [6.0, 6.0, 6.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [6.0, 6.0, 6.0] * [7.0, 7.0, 7.0] = [42.0, 42.0, 42.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [42.0, 42.0, 42.0] * [9.0, 9.0, 9.0] = [378.0, 378.0, 378.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[  1.   1.   1.]</span>
<span class="sd">         [378. 378. 378.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[1, 0], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [1, 0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [3.0, 3.0, 3.0] = [3.0, 3.0, 3.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [1.0, 1.0, 1.0] = [2.0, 2.0, 2.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [7.0, 7.0, 7.0] = [14.0, 14.0, 14.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [14.0, 14.0, 14.0] * [9.0, 9.0, 9.0] = [126.0, 126.0, 126.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[  3.   3.   3.]</span>
<span class="sd">         [126. 126. 126.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 1.0, 1.0], [2.0, 2.0, 2.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [0, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [1.0, 1.0, 1.0] = [1.0, 1.0, 1.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [2.0, 2.0, 2.0] * [3.0, 3.0, 3.0] = [6.0, 6.0, 6.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [1.0, 1.0, 1.0] * [7.0, 7.0, 7.0] = [7.0, 7.0, 7.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [6.0, 6.0, 6.0] * [9.0, 9.0, 9.0] = [54.0, 54.0, 54.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [0, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 7.  7.  7.]</span>
<span class="sd">         [54. 54. 54.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_mul_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_max"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_max.html#mindspore.ops.scatter_max">[文档]</a><span class="k">def</span> <span class="nf">scatter_max</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using given values to update tensor value through the max operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each :math:`i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :]</span>
<span class="sd">        = \max(\text{input_x}[\text{indices}[i, ..., j], :], \text{updates}[i, ..., j, :])</span>

<span class="sd">    Inputs of `input_x` and `updates` follow the implicit type conversion rules to keep the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to the relatively highest</span>
<span class="sd">    priority data type. A RuntimeError will be reported when `updates` does not support conversion to the data type</span>
<span class="sd">    required by `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">            The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        indices (Tensor): The index to do max operation whose data type must be mindspore.int32.</span>
<span class="sd">        updates (Tensor): The tensor doing the max operation with `input_x`,</span>
<span class="sd">            the data type is same as `input_x`, the shape is `indices.shape + x.shape[1:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, the type and shape same as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is not an int32 or int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>
<span class="sd">        RuntimeError: On the Ascend platform, the input data dimension of `input_x` , `indices`</span>
<span class="sd">                      and `updates` is greater than 8 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]), mindspore.float32), name=&quot;input_x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.ones([2, 2, 3]) * 88, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_max(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[88. 88. 88.]</span>
<span class="sd">         [88. 88. 88.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_max_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_add.html#mindspore.ops.scatter_add">[文档]</a><span class="k">def</span> <span class="nf">scatter_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using given values to update tensor value through the add operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">        indices (Tensor): The index to do add operation whose data type must be int32 or int64.</span>
<span class="sd">        updates (Tensor): The tensor doing the add operation with `input_x`,</span>
<span class="sd">            the data type is same as `input_x`, the shape is `indices.shape + x.shape[1:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is not an int32 or int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">            is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[7.0, 7.0, 7.0], [9.0, 9.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  1.  1.]</span>
<span class="sd">         [19. 19. 19.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_add_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_min"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_min.html#mindspore.ops.scatter_min">[文档]</a><span class="k">def</span> <span class="nf">scatter_min</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using given values to update tensor value through the min operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each :math:`i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :]</span>
<span class="sd">        = \min(\text{input_x}[\text{indices}[i, ..., j], :], \text{updates}[i, ..., j, :])</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type. A RuntimeError will be reported</span>
<span class="sd">    when `updates` does not support conversion to the data type required by `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">        indices (Tensor): The index to do min operation whose data type must be mindspore.int32 or mindspore.int64.</span>
<span class="sd">        updates (Tensor): The tensor doing the min operation with `input_x`,</span>
<span class="sd">            the data type is same as `input_x`, the shape is `indices.shape + input_x.shape[1:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>
<span class="sd">        RuntimeError: On the Ascend platform, the input data dimension of `input_x` , `indices`</span>
<span class="sd">                      and `updates` is greater than 8 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.zeros((2, 3)), mindspore.float32), name=&quot;input_x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([1, 0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; update = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_min(input_x, indices, update)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [0. 0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_min_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_div.html#mindspore.ops.scatter_div">[文档]</a><span class="k">def</span> <span class="nf">scatter_div</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using given values to update tensor value through the div operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    for each :math:`i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] \mathrel{/}= \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type. A RuntimeError will be reported</span>
<span class="sd">    when `updates` does not support conversion to the data type required by `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">        indices (Tensor): The index to do divide operation whose data type must be mindspore.int32 or</span>
<span class="sd">          mindspore.int64.</span>
<span class="sd">        updates (Tensor): The tensor doing the divide operation with `input_x`, the data type is same as `input_x`,</span>
<span class="sd">          the shape is `indices.shape + input_x.shape[1:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the type of `indices` is not one of the following dtype: int32, int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter is required</span>
<span class="sd">                      when data type conversion of Parameter is not supported.</span>
<span class="sd">        RuntimeError: On the Ascend platform, the input data dimension of `input_x` , `indices`</span>
<span class="sd">                      and `updates` is greater than 8 dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[6.0, 6.0, 6.0], [2.0, 2.0, 2.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[2.0, 2.0, 2.0], [2.0, 2.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[3. 3. 3.]</span>
<span class="sd">         [1. 1. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[105.0, 105.0, 105.0],</span>
<span class="sd">        ...                                      [315.0, 315.0, 315.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[0, 1], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [105.0, 105.0, 105.0] / [1.0, 1.0, 1.0] = [105.0, 105.0, 105.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [315.0, 315.0, 315.0] / [3.0, 3.0, 3.0] = [105.0, 105.0, 105.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [105.0, 105.0, 105.0] / [5.0, 5.0, 5.0] = [21.0, 21.0, 21.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [21.0, 21.0, 21.0] / [7.0, 7.0, 7.0] = [3.0, 3.0, 3.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[5.0, 5.0, 5.0], [7.0, 7.0, 7.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[105. 105. 105.]</span>
<span class="sd">         [  3.   3.   3.]]</span>
<span class="sd">        &gt;&gt;&gt; # for input_x will be updated after the operation is completed. input_x need to be re-initialized.</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[105.0, 105.0, 105.0],</span>
<span class="sd">        ...                                      [315.0, 315.0, 315.0]]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # for indices = [[1, 0], [1, 1]]</span>
<span class="sd">        &gt;&gt;&gt; # step 1: [1, 0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[0] = [105.0, 105.0, 105.0] / [3.0, 3.0, 3.0] = [35.0, 35.0, 35.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [315.0, 315.0, 315.0] / [1.0, 1.0, 1.0] = [315.0, 315.0, 315.0]</span>
<span class="sd">        &gt;&gt;&gt; # step 2: [1, 1]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [315.0, 315.0, 315.0] / [5.0, 5.0, 5.0] = [63.0 63.0 63.0]</span>
<span class="sd">        &gt;&gt;&gt; # input_x[1] = [63.0 63.0 63.0] / [7.0, 7.0, 7.0] = [9.0, 9.0, 9.0]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1.0, 1.0, 1.0], [3.0, 3.0, 3.0]],</span>
<span class="sd">        ...                            [[5.0, 5.0, 5.0], [7.0, 7.0, 7.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[35. 35. 35.]</span>
<span class="sd">         [ 9.  9.  9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_div_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd.html#mindspore.ops.scatter_nd">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scatters a tensor into a new tensor depending on the specified indices.</span>

<span class="sd">    Creates an empty tensor with the given `shape`, and set values by scattering the update tensor</span>
<span class="sd">    depending on indices. The empty tensor has rank :math:`P` and `indices` has rank :math:`Q`.</span>

<span class="sd">    The `shape` is :math:`(s_0, s_1, ..., s_{P-1})`, where :math:`P \ge 1`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)`, where :math:`Q \ge 2` and :math:`N \le P`.</span>

<span class="sd">    The last dimension of `indices` (with length :math:`N` ) indicates slices along the :math:`N` th dimension of the</span>
<span class="sd">    empty tensor.</span>

<span class="sd">    `updates` is a tensor of rank :math:`Q-1+P-N`, and</span>
<span class="sd">    its shape is :math:`(i_0, i_1, ..., i_{Q-2}, s_N, s_{N+1}, ..., s_{P-1})`.</span>

<span class="sd">    If `indices` contains duplicates, the duplicate `updates` are summed.</span>

<span class="sd">    The following figure shows the calculation process of inserting two new value matrices into the first dimension</span>
<span class="sd">    with rank-3:</span>

<span class="sd">    .. image:: ScatterNd.png</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Tensor): Define the index of scattering in the new tensor with int32 or int64 data type.</span>
<span class="sd">            The rank of `indices` must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): Define the source Tensor to be updated.</span>
<span class="sd">            It has shape `indices.shape[:-1] + shape[indices.shape[-1]:]`.</span>
<span class="sd">        shape (tuple[int]): Define the shape of the output tensor, has the same data type as indices.</span>
<span class="sd">            `shape` can not be empty, and the elements in `shape` must be greater than or equal to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the new tensor, has the same type as `update` and the same shape as `shape`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is not a tuple.</span>
<span class="sd">        ValueError: If any element of `shape` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2],</span>
<span class="sd">        ...                             [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[1, 1, 1, 1], [2, 2, 2, 2],</span>
<span class="sd">        ...                             [3, 3, 3, 3], [4, 4, 4, 4]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (4, 4, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd(indices, updates, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3.]</span>
<span class="sd">          [4. 4. 4. 4.]]</span>
<span class="sd">         [[0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]]</span>
<span class="sd">         [[1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3.]</span>
<span class="sd">          [4. 4. 4. 4.]]</span>
<span class="sd">         [[0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([3.2, 1.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (3, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd(indices, updates, shape)</span>
<span class="sd">        &gt;&gt;&gt; # In order to facilitate understanding, explain the operator pseudo-operation process step by step:</span>
<span class="sd">        &gt;&gt;&gt; # Step 1: Generate an empty Tensor of the specified shape according to the shape</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # Step 2: Modify the data at the specified location according to the indicators</span>
<span class="sd">        &gt;&gt;&gt; # 0th row of indices is [0, 1], 0th row of updates is 3.2.</span>
<span class="sd">        &gt;&gt;&gt; # means that the empty tensor in the 0th row and 1st col set to 3.2</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 3.2. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # 1th row of indices is [1, 1], 1th row of updates is 1.1.</span>
<span class="sd">        &gt;&gt;&gt; # means that the empty tensor in the 1th row and 1st col set to 1.1</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 3.2. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 1.1  0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # The final result is as follows:</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 3.2 0.]</span>
<span class="sd">         [0. 1.1 0.]</span>
<span class="sd">         [0. 0.  0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_nd_</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_update"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_update.html#mindspore.ops.scatter_update">[文档]</a><span class="k">def</span> <span class="nf">scatter_update</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates tensor values by using input indices and value.</span>

<span class="sd">    Using given values to update tensor value, along with the input indices.</span>

<span class="sd">    for each `i, ..., j` in `indices.shape`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{input_x}[\text{indices}[i, ..., j], :] = \text{updates}[i, ..., j, :]</span>

<span class="sd">    Inputs of `input_x` and `updates` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">        indices (Tensor): The index of input tensor. With int32 or int64 data type.</span>
<span class="sd">            If there are duplicates in indices, the order for updating is undefined.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">            and updates.shape = indices.shape + input_x.shape[1:].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is not an int32 or an int64.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape + input_x.shape[1:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; np_x = np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]])</span>
<span class="sd">        &gt;&gt;&gt; input_x = mindspore.Parameter(Tensor(np_x, mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; np_updates = np.array([[2.0, 1.2, 1.0], [3.0, 1.2, 1.0]])</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np_updates, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_update(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2. 1.2  1.]</span>
<span class="sd">         [3. 1.2  1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_update_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScatterUpdate</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">scatter_update_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_add.html#mindspore.ops.scatter_nd_add">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse addition to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the add operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">        indices (Tensor): The index to do min operation whose data type must be mindspore.int32 or mindspore.int64.</span>
<span class="sd">            The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor doing the addition operation with `input_x`,</span>
<span class="sd">            the data type is same as `input_x`, the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_add(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 10.  9.  4. 12.  6.  7. 17.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.zeros((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_add(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1 1 1 1]</span>
<span class="sd">          [2 2 2 2]</span>
<span class="sd">          [3 3 3 3]</span>
<span class="sd">          [4 4 4 4]]</span>
<span class="sd">         [[0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]]</span>
<span class="sd">         [[5 5 5 5]</span>
<span class="sd">          [6 6 6 6]</span>
<span class="sd">          [7 7 7 7]</span>
<span class="sd">          [8 8 8 8]]</span>
<span class="sd">         [[0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]</span>
<span class="sd">          [0 0 0 0]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_add_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScatterNdAdd</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_add_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_sub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_sub.html#mindspore.ops.scatter_nd_sub">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_sub</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse subtraction to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the subtraction operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">        indices (Tensor): The index of input tensor, with int32 or int64 data type.</span>
<span class="sd">            The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor doing the subtraction operation with `input_x`, has the same type as input.</span>
<span class="sd">            The shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_sub(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. -6. -3.  4. -2.  6.  7. -1.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.zeros((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_sub(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[-1 -1 -1 -1]</span>
<span class="sd">          [-2 -2 -2 -2]</span>
<span class="sd">          [-3 -3 -3 -3]</span>
<span class="sd">          [-4 -4 -4 -4]]</span>
<span class="sd">         [[ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]]</span>
<span class="sd">         [[-5 -5 -5 -5]</span>
<span class="sd">          [-6 -6 -6 -6]</span>
<span class="sd">          [-7 -7 -7 -7]</span>
<span class="sd">          [-8 -8 -8 -8]]</span>
<span class="sd">         [[ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]</span>
<span class="sd">          [ 0  0  0  0]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_sub_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScatterNdSub</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_sub_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_mul.html#mindspore.ops.scatter_nd_mul">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_mul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies sparse multiplication to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update parameter value through the multiplication operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q, where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): Input parameter.</span>
<span class="sd">        indices (Tensor): The index to do multiplication operation whose data type must be mindspore.int32 or</span>
<span class="sd">            mindspore.int64. The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor to do the multiplication operation with `input_x`.</span>
<span class="sd">            The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 16. 18.  4. 35.  6.  7. 72.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1 1 1 1]</span>
<span class="sd">          [2 2 2 2]</span>
<span class="sd">          [3 3 3 3]</span>
<span class="sd">          [4 4 4 4]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]</span>
<span class="sd">         [[5 5 5 5]</span>
<span class="sd">          [6 6 6 6]</span>
<span class="sd">          [7 7 7 7]</span>
<span class="sd">          [8 8 8 8]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_mul_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ScatterNdMul</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_mul_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_div.html#mindspore.ops.scatter_nd_div">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_div</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applying sparse division to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the div operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q, where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">        indices (Tensor): The index to do div operation whose data type must be mindspore.int32 or mindspore.int64.</span>
<span class="sd">            The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor to do the div operation with `input_x`.</span>
<span class="sd">            The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_div(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.         0.25       0.5        4.         0.71428573 6.</span>
<span class="sd">         7.         0.8888889 ]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)), mindspore.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_div(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.         1.         1.         1.        ]</span>
<span class="sd">          [0.5        0.5        0.5        0.5       ]</span>
<span class="sd">          [0.33333334 0.33333334 0.33333334 0.33333334]</span>
<span class="sd">          [0.25       0.25       0.25       0.25      ]]</span>
<span class="sd">         [[1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]]</span>
<span class="sd">         [[0.2        0.2        0.2        0.2       ]</span>
<span class="sd">          [0.16666667 0.16666667 0.16666667 0.16666667]</span>
<span class="sd">          [0.14285715 0.14285715 0.14285715 0.14285715]</span>
<span class="sd">          [0.125      0.125      0.125      0.125     ]]</span>
<span class="sd">         [[1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]</span>
<span class="sd">          [1.         1.         1.         1.        ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_div_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScatterNdDiv</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_div_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_max"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_max.html#mindspore.ops.scatter_nd_max">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_max</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applying sparse maximum to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update parameter value through the max operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">        indices (Tensor): The index to do maximum operation whose data type must be mindspore.int32 or mindspore.int64.</span>
<span class="sd">            The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor to do the max operation with `input_x`.</span>
<span class="sd">            The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8]), mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_max(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 8. 6. 4. 7. 6. 7. 9.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)), mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_max(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1 1 1 1]</span>
<span class="sd">          [2 2 2 2]</span>
<span class="sd">          [3 3 3 3]</span>
<span class="sd">          [4 4 4 4]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]</span>
<span class="sd">         [[5 5 5 5]</span>
<span class="sd">          [6 6 6 6]</span>
<span class="sd">          [7 7 7 7]</span>
<span class="sd">          [8 8 8 8]]</span>
<span class="sd">         [[1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]</span>
<span class="sd">          [1 1 1 1]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_max_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ScatterNdMax</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_max_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter_nd_min"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd_min.html#mindspore.ops.scatter_nd_min">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd_min</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applying sparse minimum to individual values or slices in a tensor.</span>

<span class="sd">    Using given values to update tensor value through the min operation, along with the input indices.</span>
<span class="sd">    This operation outputs the `input_x` after the update is done, which makes it convenient to use the updated value.</span>

<span class="sd">    `input_x` has rank P and `indices` has rank Q where `Q &gt;= 2`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)` where `N &lt;= P`.</span>

<span class="sd">    The last dimension of `indices` (with length `N` ) indicates slices along the `N` th dimension of `input_x`.</span>

<span class="sd">    `updates` is a tensor of rank `Q-1+P-N`. Its shape is:</span>
<span class="sd">    :math:`(i_0, i_1, ..., i_{Q-2}, x\_shape_N, ..., x\_shape_{P-1})`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Parameter): The target tensor, with data type of Parameter.</span>
<span class="sd">        indices (Tensor): The index to do min operation whose data type must be mindspore.int32 or mindspore.int64.</span>
<span class="sd">            The rank of indices must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): The tensor to do the min operation with `input_x`.</span>
<span class="sd">            The data type is same as `input_x`, and the shape is `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        use_locking (bool): Whether to protect the assignment by a lock. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the updated `input_x`, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `use_locking` is not bool.</span>
<span class="sd">        TypeError: If the dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `input_x` and `updates` are not the same.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to `indices.shape[:-1] + x.shape[indices.shape[-1]:]`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones(8) * 10, mindspore.float32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2], [4], [1], [7]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([6, 7, 8, 9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_min(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [10.  8.  6. 10.  7. 10. 10.  9.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.ones((4, 4, 4)) * 10, mindspore.int32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd_min(input_x, indices, updates, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 1  1  1  1]</span>
<span class="sd">          [ 2  2  2  2]</span>
<span class="sd">          [ 3  3  3  3]</span>
<span class="sd">          [ 4  4  4  4]]</span>
<span class="sd">         [[10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]]</span>
<span class="sd">         [[ 5  5  5  5]</span>
<span class="sd">          [ 6  6  6  6]</span>
<span class="sd">          [ 7  7  7  7]</span>
<span class="sd">          [ 8  8  8  8]]</span>
<span class="sd">         [[10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]</span>
<span class="sd">          [10 10 10 10]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_min_inner</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ScatterNdMin</span><span class="p">)(</span><span class="n">use_locking</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">scatter_nd_min_inner</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="sort"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sort.html#mindspore.ops.sort">[文档]</a><span class="k">def</span> <span class="nf">sort</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sorts the elements of the input tensor along the given dimension in the specified order.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x(Tensor): The input tensor to sort.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        axis (int, optional): The dimension to sort along. Default: ``-1``, means the last dimension.</span>
<span class="sd">            The Ascend backend only supports sorting the last dimension.</span>
<span class="sd">        descending (bool, optional): Controls the sort order. If `descending` is True, the elements</span>
<span class="sd">            are sorted in descending order, or else sorted in ascending order. Default: ``False`` .</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Currently, the data types of Float16, UInt8, Int8, Int16, Int32, Int64 are well supported.</span>
<span class="sd">        If use Float32, it may cause loss of accuracy.</span>

<span class="sd">    Returns:</span>

<span class="sd">        - y1, a tensor whose values are the sorted values, with the same shape and data type as input.</span>
<span class="sd">        - y2, a tensor that consists of the indices of the elements in the original input tensor.</span>
<span class="sd">          Data type is int32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If `descending` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16, float32, uint8, int8, int16, int32, int64.</span>
<span class="sd">        ValueError: If `axis` is not in range of [-len(input_x.shape), len(input_x.shape)).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[8, 2, 1], [5, 9, 3], [4, 6, 7]]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sort(x)</span>
<span class="sd">        &gt;&gt;&gt; # The output below is based on the Ascend platform.</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3, 3], dtype=Float16, value=</span>
<span class="sd">        [[ 1.0000e+00,  2.0000e+00,  8.0000e+00],</span>
<span class="sd">        [ 3.0000e+00,  5.0000e+00,  9.0000e+00],</span>
<span class="sd">        [ 4.0000e+00,  6.0000e+00,  7.0000e+00]]), Tensor(shape=[3, 3], dtype=Int32, value=</span>
<span class="sd">        [[2, 1, 0],</span>
<span class="sd">        [2, 0, 1],</span>
<span class="sd">        [0, 1, 2]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_sort</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Sort</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">descending</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_sort</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="argsort"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.argsort.html#mindspore.ops.argsort">[文档]</a><span class="k">def</span> <span class="nf">argsort</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sorts the input tensor along the given dimension in specified order and return the sorted indices.</span>

<span class="sd">    Args:</span>
<span class="sd">        input(Tensor): The input tensor to sort.</span>
<span class="sd">        axis (int): The axis to sort along. Default: ``-1`` , means the last dimension.</span>
<span class="sd">            The Ascend backend only supports sorting the last dimension.</span>
<span class="sd">        descending (bool): The sort order. If `descending` is True then the elements</span>
<span class="sd">            are sorted in descending order by value. Otherwise sort in ascending order. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the indices of sorted input tensor. Data type is int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[8, 2, 1], [5, 9, 3], [4, 6, 7]]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; sort = ops.argsort(x)</span>
<span class="sd">        &gt;&gt;&gt; print(sort)</span>
<span class="sd">        [[2 1 0]</span>
<span class="sd">         [2 0 1]</span>
<span class="sd">         [0 1 2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_sort</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Sort</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">descending</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">arg_sort</span> <span class="o">=</span> <span class="n">_sort</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">arg_sort</span></div>


<div class="viewcode-block" id="gather"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather.html#mindspore.ops.gather">[文档]</a><span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the slice of the input tensor corresponding to the elements of `input_indices` on the specified `axis`.</span>

<span class="sd">    The following figure shows the calculation process of Gather commonly:</span>

<span class="sd">    .. image:: Gather.png</span>

<span class="sd">    where params represents the input `input_params`, and indices represents the index to be sliced `input_indices`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        1. The value of input_indices must be in the range of `[0, input_param.shape[axis])`, the result is undefined</span>
<span class="sd">           out of range.</span>

<span class="sd">        2. The data type of input_params cannot be</span>
<span class="sd">           `bool_ &lt;https://www.mindspore.cn/docs/en/r2.1/api_python/mindspore.html#mindspore.dtype&gt;`_ on Ascend</span>
<span class="sd">           platform currently.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_params (Tensor): The original Tensor. The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        input_indices (Tensor): Index tensor to be sliced, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">            Specifies the indices of elements of the original Tensor. The data type can be int32 or int64.</span>
<span class="sd">        axis (Union(int, Tensor[int])): Specifies the dimension index to gather indices.</span>
<span class="sd">                                        It must be greater than or equal to `batch_dims`.</span>
<span class="sd">                                        When `axis` is a Tensor, the size must be 1.</span>
<span class="sd">        batch_dims (int): Specifies the number of batch dimensions. It must be less than or euqal to the rank</span>
<span class="sd">                          of `input_indices`. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is</span>
<span class="sd">        :math:`input\_params.shape[:axis] + input\_indices.shape[batch\_dims:] + input\_params.shape[axis + 1:]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError:  If `axis` is not an int or Tensor.</span>
<span class="sd">        ValueError: If `axis` is a Tensor and its size is not 1.</span>
<span class="sd">        TypeError:  If `input_params` is not a tensor.</span>
<span class="sd">        TypeError:  If `input_indices` is not a tensor of type int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case1: input_indices is a Tensor with shape (5, ).</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([1, 2, 3, 4, 5, 6, 7]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2, 4, 2, 6]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 3. 5. 3. 7.]</span>
<span class="sd">        &gt;&gt;&gt; # case2: input_indices is a Tensor with shape (2, 2). When the input_params has one dimension,</span>
<span class="sd">        &gt;&gt;&gt; # the output shape is equal to the input_indices shape.</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([[0, 2], [2, 6]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 3.]</span>
<span class="sd">         [3. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; # case3: input_indices is a Tensor with shape (2, ) and</span>
<span class="sd">        &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 0.</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  2.  3.  4.]</span>
<span class="sd">         [ 9. 10. 11. 12.]]</span>
<span class="sd">        &gt;&gt;&gt; # case4: input_indices is a Tensor with shape (2, ) and</span>
<span class="sd">        &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 1, batch_dims is 1.</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 1</span>
<span class="sd">        &gt;&gt;&gt; batch_dims = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis, batch_dims)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  7. 10.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_gather</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Gather</span><span class="p">)(</span><span class="n">batch_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_gather</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather_d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather_d.html#mindspore.ops.gather_d">[文档]</a><span class="k">def</span> <span class="nf">gather_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers elements along an axis specified by dim.</span>

<span class="sd">    Refer to :func:`mindspore.ops.gather_elements` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dim = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather_d(x, dim, index)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">         [4 3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gather_d_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather_elements"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather_elements.html#mindspore.ops.gather_elements">[文档]</a><span class="k">def</span> <span class="nf">gather_elements</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers elements along an axis specified by dim.</span>

<span class="sd">    For a 3-D tensor, the output is:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        output[i][j][k] = x[index[i][j][k]][j][k]  # if dim == 0</span>

<span class="sd">        output[i][j][k] = x[i][index[i][j][k]][k]  # if dim == 1</span>

<span class="sd">        output[i][j][k] = x[i][j][index[i][j][k]]  # if dim == 2</span>

<span class="sd">    `input` and `index` have the same length of dimensions, and all dimensions except `dim` have the same size.</span>
<span class="sd">    If `dim` = i, `input` is an n-D tensor with shape :math:`(z_0, z_1, ..., z_i, ..., z_{n-1})`,</span>
<span class="sd">    the `index` must be an n-D tensor with shape :math:`(z_0, z_1, ..., y, ..., z_{n-1})`</span>
<span class="sd">    where `y`&gt;=1 and the output will have the same shape with `index`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">        dim (int): The axis along which to index. It must be int32 or int64. The value range is [-input.ndim,</span>
<span class="sd">            input.ndim).</span>
<span class="sd">        index (Tensor): The indices of elements to gather. It can be one of the following data types:</span>
<span class="sd">            int32, int64. The value range of each index element is [-input.shape(dim), input.shape(dim)).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as index tensor, the shape of tensor is :math:`(z_0, z_1, ..., y, ..., z_{n-1})`,</span>
<span class="sd">        and has the same data type with `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `dim` or `index` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to length of shape of `index`.</span>
<span class="sd">        ValueError: If the size of the dimension except `dim` is not equal between `input` and `index`.</span>
<span class="sd">        ValueError: If the value of `dim` is not in the expected range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dim = 1</span>
<span class="sd">        &gt;&gt;&gt; output = mindspore.ops.gather_elements(x, dim, index)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">         [4 3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gather_d_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather_nd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather_nd.html#mindspore.ops.gather_nd">[文档]</a><span class="k">def</span> <span class="nf">gather_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers slices from a tensor by indices.</span>

<span class="sd">    Using given indices to gather slices from a tensor with a specified shape.</span>

<span class="sd">    `indices` is an K-dimensional integer tensor. Supposes it as a (K-1)-dimensional tensor and each element of it</span>
<span class="sd">    defines a slice of `input_x`:</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[(i_0, ..., i_{K-2})] = input\_x[indices[(i_0, ..., i_{K-2})]]</span>

<span class="sd">    The last dimension of `indices` can not more than the rank of `input_x`:</span>
<span class="sd">    :math:`indices.shape[-1] &lt;= input\_x.rank`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor to gather values.</span>
<span class="sd">        indices (Tensor): The index tensor, with int32 or int64 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as `input_x` and the shape is</span>
<span class="sd">        :math:`indices\_shape[:-1] + input\_x\_shape[indices\_shape[-1]:]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of `indices`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather_nd(input_x, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.1  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gather_nd_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_add.html#mindspore.ops.tensor_scatter_add">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by adding the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When multiple values are given for the same</span>
<span class="sd">    index, the updated result will be the sum of all values. This operation is almost</span>
<span class="sd">    equivalent to using ScatterNdAdd, except that the updates are applied on output `Tensor`</span>
<span class="sd">    instead of input `Parameter`.</span>

<span class="sd">    The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of `input_x[indices]`. For more details, see use cases.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output\left [indices  \right ] = input\_x + update</span>

<span class="sd">    Note:</span>
<span class="sd">        - On GPU, if some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">          the corresponding `updates` will not be updated to self tensor.</span>
<span class="sd">        - On CPU, if some values of the `indices` are out of bound, raising an index error.</span>
<span class="sd">        - On Ascend, out of bound checking is not supported, if some values of the `indices` are out of bound,</span>
<span class="sd">          unknown errors may be caused.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">            The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as input,</span>
<span class="sd">            and updates. And the shape should be</span>
<span class="sd">            equal to :math:`indices.shape[:-1] + input\_x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>
<span class="sd">        RuntimeError: If a value of `indices` is not in `input_x` on CPU backend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_add(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 3.1  0.3  3.6]</span>
<span class="sd">         [ 0.4  0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">tensor_scatter_add_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_sub"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_sub.html#mindspore.ops.tensor_scatter_sub">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_sub</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by subtracting the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When multiple values are provided for the same</span>
<span class="sd">    index, the result of the update will be to subtract these values respectively. This operation is almost</span>
<span class="sd">    equivalent to using :class:`mindspore.ops.ScatterNdSub` , except that the updates are applied on output `Tensor`</span>
<span class="sd">    instead of input `Parameter`.</span>

<span class="sd">    The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of `input_x[indices]`. For more details, see use cases.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[indices] = input\_x - update</span>

<span class="sd">    Note:</span>
<span class="sd">        On GPU, if some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">        the corresponding `updates` will not be updated to self tensor. On CPU, if some values of</span>
<span class="sd">        the `indices` are out of bound, raising an index error. On Ascend, out of bound checking is</span>
<span class="sd">        not supported, if some values of the `indices` are out of bound, unknown errors may be caused.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">            The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as `input_x`,</span>
<span class="sd">            and the shape of `updates` should be equal to indices.shape[:-1] + input_x.shape[indices.shape[-1]:].</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>
<span class="sd">        RuntimeError: If a value of `indices` is not in `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_sub(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-3.3000002  0.3        3.6      ]</span>
<span class="sd">         [ 0.4        0.5       -3.2      ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">tensor_scatter_sub_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_max"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_max.html#mindspore.ops.tensor_scatter_max">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_max</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    By comparing the value at the position indicated by `indices` in `input_x` with the value in the `updates`,</span>
<span class="sd">    the value at the index will eventually be equal to the largest one to create a new tensor.</span>

<span class="sd">    The last axis of the index is the depth of each index vector. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of input_x[indices].</span>

<span class="sd">    .. math::</span>
<span class="sd">        output\left [indices  \right ] = \max(input\_x, update)</span>

<span class="sd">    Note:</span>
<span class="sd">        - On GPU, if some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">          the corresponding `updates` will not be updated to self tensor.</span>
<span class="sd">        - On CPU, if some values of the `indices` are out of bound, raising an index error.</span>
<span class="sd">        - On Ascend, out of bound checking is not supported, if some values of the `indices` are out of bound,</span>
<span class="sd">          unknown errors may be caused.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The dimension of `input_x` must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type must be int32 or int64.</span>
<span class="sd">            The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the `input_x` tensor, has the same type as input,</span>
<span class="sd">            and updates.shape should be equal to :math:`indices.shape[:-1] + input\_x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>
<span class="sd">        RuntimeError: If a value of `indices` is not in `input_x` on CPU backend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">        &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">        &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">        &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">        &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_max(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; # 5, Perform the max operation for the first time:</span>
<span class="sd">        &gt;&gt;&gt; #      first_input_x = Max(input_x[0][0], updates[0]) = [[1.0, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; # 6, Perform the max operation for the second time:</span>
<span class="sd">        &gt;&gt;&gt; #      second_input_x = Max(input_x[0][0], updates[1]) = [[2.2, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 2.2  0.3  3.6]</span>
<span class="sd">         [ 0.4  0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_scatter_max_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_min"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_min.html#mindspore.ops.tensor_scatter_min">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_min</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    By comparing the value at the position indicated by `indices` in `input_x` with the value in the `updates`,</span>
<span class="sd">    the value at the index will eventually be equal to the smallest one to create a new tensor.</span>

<span class="sd">    The last axis of the index is the depth of each index vector. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of `input_x[indices]`. For more details, see case below.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output\left [indices  \right ] = \min(input\_x, update)</span>

<span class="sd">    Note:</span>
<span class="sd">        - On GPU, if some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">          the corresponding `updates` will not be updated to self tensor.</span>
<span class="sd">        - On CPU, if some values of the `indices` are out of bound, raising an index error.</span>
<span class="sd">        - On Ascend, out of bound checking is not supported, if some values of the `indices` are out of bound,</span>
<span class="sd">          unknown errors may be caused.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The dimension of `input_x` must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">            The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as `input_x`</span>
<span class="sd">            And the shape of `updates` should be</span>
<span class="sd">            equal to :math:`indices.shape[:-1] + input\_x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>
<span class="sd">        RuntimeError: If a value of `indices` is not in `input_x` on CPU backend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_min(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ -0.1  0.3  3.6]</span>
<span class="sd">        [ 0.4  0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_scatter_min_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_elements"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_elements.html#mindspore.ops.tensor_scatter_elements">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_elements</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Write all elements in `updates` to the index specified by `indices` in `input_x` according to the reduction</span>
<span class="sd">    operation specified by `reduction`.</span>
<span class="sd">    `axis` controls the direction of the scatter operation.</span>

<span class="sd">    `tensor_scatter_elements` takes three inputs `input_x`, `updates` and `indices` of the same rank r &gt;= 1.</span>

<span class="sd">    For a 3-D tensor, the output is:</span>

<span class="sd">    .. code-block::</span>

<span class="sd">        output[indices[i][j][k]][j][k] = updates[i][j][k]  # if axis == 0, reduction == &quot;none&quot;</span>

<span class="sd">        output[i][indices[i][j][k]][k] += updates[i][j][k]  # if axis == 1, reduction == &quot;add&quot;</span>

<span class="sd">        output[i][j][indices[i][j][k]] = updates[i][j][k]  # if axis == 2, reduction == &quot;none&quot;</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The order in which updates are applied is nondeterministic, meaning that if there are multiple index vectors</span>
<span class="sd">          in `indices` that correspond to the same position, the value of that position in the output will be</span>
<span class="sd">          nondeterministic.</span>
<span class="sd">        - On Ascend, the reduction only support set to &quot;none&quot; for now.</span>
<span class="sd">        - On Ascend, the data type of `input_x` must be float16 or float32.</span>

<span class="sd">    Note:</span>
<span class="sd">        If some values of the `indices` exceed the upper or lower bounds of the index of `input_x`, instead of raising</span>
<span class="sd">        an index error, the corresponding `updates` will not be updated to `input_x`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor. The rank must be at least 1.</span>
<span class="sd">        indices (Tensor): The index of `input_x` to do scatter operation whose data type must be mindspore.int32 or</span>
<span class="sd">            mindspore.int64. Same rank as  `input_x`. And accepted range is [-s, s) where s is the size along axis.</span>
<span class="sd">        updates (Tensor): The tensor doing the scatter operation with `input_x`, has the same type as `input_x` and</span>
<span class="sd">            the same shape as `indices`.</span>
<span class="sd">        axis (int): Which axis to scatter. Accepted range is [-r, r) where r = rank(input_x). Default: ``0``.</span>
<span class="sd">        reduction (str): Which reduction operation to scatter, supports ``&quot;none&quot;`` , ``&quot;add&quot;`` . Default: ``&quot;none&quot;``.</span>
<span class="sd">            When `reduction` is set to ``&quot;none&quot;``, `updates` will be assigned to `input_x` according to  `indices`.</span>
<span class="sd">            When `reduction` is set to ``&quot;add&quot;``, `updates` will be added to `input_x` according to  `indices`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If anyone of the rank among `input_x`, `indices` and `updates` less than 1.</span>
<span class="sd">        ValueError: If the shape of `updates` is not equal to the shape of `indices`.</span>
<span class="sd">        ValueError: If the rank of `updates` is not equal to the rank of `input_x`.</span>
<span class="sd">        RuntimeError: If the data type of `input_x` and `updates` conversion of Parameter</span>
<span class="sd">            is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Parameter</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1, 2, 3, 4, 5]]), mindspore.int32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[2, 4]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[8, 8]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 1</span>
<span class="sd">        &gt;&gt;&gt; reduction = &quot;none&quot;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_elements(input_x, indices, updates, axis, reduction)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1, 2, 8, 4, 8]]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Parameter(Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.int32), name=&quot;x&quot;)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[1, -1, 2], [0, 2, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[1, 2, 2], [4, 5, 8]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; reduction = &quot;add&quot;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_elements(input_x, indices, updates, axis, reduction)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[5, 2, 3], [5, 5, 14], [7, 15, 11]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_tensor_scatter_elements</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">TensorScatterElements</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_tensor_scatter_elements</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="scatter"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter.html#mindspore.ops.scatter">[文档]</a><span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update the value in `src` to `input` according to the specified index.</span>
<span class="sd">    Refer to :func:`mindspore.ops.tensor_scatter_elements` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The target tensor. The rank of `input` must be at least 1.</span>
<span class="sd">        axis (int): Which axis to scatter. Accepted range is [-r, r) where r = rank(input).</span>
<span class="sd">        index (Tensor): The index to do update operation whose data type must be mindspore.int32 or</span>
<span class="sd">            mindspore.int64. Same rank as `input` . And accepted range is [-s, s) where s is the size along axis.</span>
<span class="sd">        src (Tensor): The tensor doing the update operation with `input` , has the same type as `input` ,</span>
<span class="sd">            and the shape of `src` should be equal to the shape of `index` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `index` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If anyone of the rank among `input` , `index` and `src` less than 1.</span>
<span class="sd">        ValueError: If the shape of `src` is not equal to the shape of `index` .</span>
<span class="sd">        ValueError: If the rank of `src` is not equal to the rank of `input` .</span>
<span class="sd">        RuntimeError: If the data type of `input` and `src` conversion of Parameter</span>
<span class="sd">            is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[1, 2, 3, 4, 5]]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; src = Tensor(np.array([[8, 8]]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor(np.array([[2, 4]]), dtype=ms.int64)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.scatter(input=input, axis=1, index=index, src=src)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[1. 2. 8. 4. 8.]]</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.zeros((5, 5)), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; src = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor(np.array([[0, 0, 0], [2, 2, 2], [4, 4, 4]]), dtype=ms.int64)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.scatter(input=input, axis=0, index=index, src=src)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[1. 2. 3. 0. 0.]</span>
<span class="sd">        [0. 0. 0. 0. 0.]</span>
<span class="sd">        [4. 5. 6. 0. 0.]</span>
<span class="sd">        [0. 0. 0. 0. 0.]</span>
<span class="sd">        [7. 8. 9. 0. 0.]]</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.zeros((5, 5)), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; src = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor(np.array([[0, 2, 4], [0, 2, 4], [0, 2, 4]]), dtype=ms.int64)</span>
<span class="sd">        &gt;&gt;&gt; out = ops.scatter(input=input, axis=1, index=index, src=src)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[1. 0. 2. 0. 3.]</span>
<span class="sd">        [4. 0. 5. 0. 6.]</span>
<span class="sd">        [7. 0. 8. 0. 9.]</span>
<span class="sd">        [0. 0. 0. 0. 0.]</span>
<span class="sd">        [0. 0. 0. 0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor_scatter_elements</span><span class="p">(</span><span class="n">input_x</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">index</span><span class="p">,</span> <span class="n">updates</span><span class="o">=</span><span class="n">src</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_get_slice_scatter_const</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the rank of input, embedded dimensions and index.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="k">if</span> <span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">axis</span> <span class="o">+</span> <span class="n">x_rank</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">start</span> <span class="k">if</span> <span class="n">start</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">start</span> <span class="k">if</span> <span class="n">start</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">start</span> <span class="o">+</span> <span class="n">x_rank</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">end</span> <span class="k">if</span> <span class="n">end</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">end</span> <span class="k">if</span> <span class="n">end</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">end</span> <span class="o">+</span> <span class="n">x_rank</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">end</span> <span class="k">if</span> <span class="n">end</span> <span class="o">&lt;</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="k">else</span> <span class="n">x_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="n">index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">builtins</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x_rank</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">axis</span>


<div class="viewcode-block" id="slice_scatter"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.slice_scatter.html#mindspore.ops.slice_scatter">[文档]</a><span class="k">def</span> <span class="nf">slice_scatter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Slice the input Tensor in the specified dimension and overlay the slice results with the source Tensor.</span>
<span class="sd">    The `input` is sliced along the specified dimension. The start position of the slice is `start` ,</span>
<span class="sd">    the end position is `end` , and the step size is `step` .</span>
<span class="sd">    Then the slicing result is overwritten with `src` to get the output Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The target Tensor.</span>
<span class="sd">        src (Tensor): The source Tensor.</span>
<span class="sd">        axis (int, optional): The dimension of `input` to be sliced. Default: ``0`` .</span>
<span class="sd">        start (int, optional): The start index to slice in the specified dimension.</span>
<span class="sd">            Default: ``None``, `start` is ``0`` .</span>
<span class="sd">        end (int, optional): The end index to slice in the specified dimension.</span>
<span class="sd">            Default: ``None``, `end` is the length of `input` in the specified dimension.</span>
<span class="sd">        step (int, optional): Step size. Default: ``1``, the distance from the next slice element is ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor after embedding, has the same shape and type as `input` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `src` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` or `step` is not an integer.</span>
<span class="sd">        TypeError: If `start` or `end` is not ``None`` or an integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; a = ms.ops.zeros((4, 6))</span>
<span class="sd">        &gt;&gt;&gt; b = ms.ops.ones((4, 3))</span>
<span class="sd">        &gt;&gt;&gt; output = ms.ops.slice_scatter(a, b, axis=1, start=0, end=5, step=2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0. 1. 0. 1. 0.]</span>
<span class="sd">         [1. 0. 1. 0. 1. 0.]</span>
<span class="sd">         [1. 0. 1. 0. 1. 0.]</span>
<span class="sd">         [1. 0. 1. 0. 1. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">input_rank</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">_get_slice_scatter_const</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>

    <span class="n">src_shape</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">index_shape</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">),)</span> <span class="o">+</span> <span class="n">input_shape</span><span class="p">[</span><span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="n">index_tensor</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">builtins</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
        <span class="n">index_tensor</span> <span class="o">=</span> <span class="n">index_tensor</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">index_shape</span> <span class="o">==</span> <span class="n">src_shape</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">builtins</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">input_rank</span> <span class="o">-</span> <span class="n">axis</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">index_tensor</span> <span class="o">=</span> <span class="n">index_tensor</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">index_tensor</span> <span class="o">=</span> <span class="n">index_tensor</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tensor_scatter_elements</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">index_tensor</span><span class="p">,</span> <span class="n">updates</span><span class="o">=</span><span class="n">src</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">builtins</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">axis</span><span class="p">):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="n">input_rank</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="n">src_shape</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">builtins</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">src_shape</span><span class="p">)):</span>
            <span class="n">index_tensor</span> <span class="o">=</span> <span class="n">index_tensor</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">index</span><span class="p">),)</span> <span class="o">+</span> <span class="n">src_shape</span><span class="p">)</span>
    <span class="n">index_tensor</span> <span class="o">=</span> <span class="n">index_tensor</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">tensor_scatter_elements</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">indices</span><span class="o">=</span><span class="n">index_tensor</span><span class="p">,</span> <span class="n">updates</span><span class="o">=</span><span class="n">src</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="select_scatter"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.select_scatter.html#mindspore.ops.select_scatter">[文档]</a><span class="k">def</span> <span class="nf">select_scatter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    On the specified dimension `axis` of `input` , `src` is scattered into `input` on the specified `index` of `input` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The target Tensor.</span>
<span class="sd">        src (Tensor): The source Tensor.</span>
<span class="sd">        axis (int): The dimension of `input` to be embedded.</span>
<span class="sd">        index (int): The location of scattering on the specified dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor after embedding, has the same shape and type as `input` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `src` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` or `index` is not an integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; a = ms.ops.zeros((2, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; b = ms.ops.ones((2, 3))</span>
<span class="sd">        &gt;&gt;&gt; output = ms.ops.select_scatter(a, b, axis=1, index=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[0. 0. 0.]</span>
<span class="sd">          [1. 1. 1.]</span>
<span class="sd">          [0. 0. 0.]]</span>
<span class="sd">         [[0. 0. 0.]</span>
<span class="sd">          [1. 1. 1.]</span>
<span class="sd">          [0. 0. 0.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">src</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">slice_scatter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">index</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="space_to_batch_nd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.space_to_batch_nd.html#mindspore.ops.space_to_batch_nd">[文档]</a><span class="k">def</span> <span class="nf">space_to_batch_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides a tensor&#39;s spatial dimensions into blocks and combines the block sizes with the original batch.</span>

<span class="sd">    This operation will divide spatial dimensions into blocks with `block_size`,</span>
<span class="sd">    and after division, the output tensor&#39;s spatial dimension is the corresponding number of blocks.</span>
<span class="sd">    The output tensor&#39;s batch dimension is the product of the original batch and the product of `block_size`.</span>
<span class="sd">    Before division, the spatial dimensions of the input are zero padded according to paddings if necessary.</span>
<span class="sd">    Assume input shape is :math:`(n, c_1, ... c_k, w_1, ..., w_M)`, then the shape of the output tensor will be</span>
<span class="sd">    :math:`(n&#39;, c_1, ... c_k, w&#39;_1, ..., w&#39;_M)`, where</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            n&#39; = n*(block\_size[0] * ... * block\_size[M]) \\</span>
<span class="sd">            w&#39;_i = (w_i + paddings[i][0] + paddings[i][1])//block\_size[i]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. It must be a 4-D tensor on Ascend.</span>
<span class="sd">        block_size (Union[list(int), tuple(int), int]): The block size of dividing block with all value greater</span>
<span class="sd">            than 1. If `block_size` is a tuple or list, the length of `block_size` is M corresponding to the</span>
<span class="sd">            number of spatial dimensions. If `block_size` is an int, the block size of M dimensions are the same,</span>
<span class="sd">            equal to `block_size`. M must be 2 on Ascend.</span>
<span class="sd">        paddings (Union[tuple, list]): The padding values for spatial dimensions, containing M subtraction list.</span>
<span class="sd">            Each contains 2 integer values. All values must be greater than 0.</span>
<span class="sd">            `paddings[i]` specifies the paddings for the spatial dimension i,</span>
<span class="sd">            which corresponds to the input dimension i + offset.</span>
<span class="sd">            It is required that input_shape[i+offset]+paddings[i][0]+paddings[i][1] is divisible by block_size[i].</span>
<span class="sd">            M must be 2 on Ascend.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the output tensor with the same data type as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If `block_size` is not one dimensional when `block_size` is a list or tuple.</span>
<span class="sd">        ValueError: If the length of `block_size` is not 2 on Ascend.</span>
<span class="sd">        ValueError: If the element of `block_size` is not an integer larger than 1.</span>
<span class="sd">        ValueError: If shape of `paddings` is not (M, 2), where M is the length of `block_size`.</span>
<span class="sd">        ValueError: If the element of `paddings` is not an integer larger than 0.</span>
<span class="sd">        TypeError: If `block_size` is not one of list, tuple, int.</span>
<span class="sd">        TypeError: If `paddings` is neither list nor tuple.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; block_size = [2, 2]</span>
<span class="sd">        &gt;&gt;&gt; paddings = [[0, 0], [0, 0]]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1, 2], [3, 4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.space_to_batch_nd(input_x, block_size, paddings)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.]]]</span>
<span class="sd">         [[[2.]]]</span>
<span class="sd">         [[[3.]]]</span>
<span class="sd">         [[[4.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_space_to_batch_nd</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">SpaceToBatchND</span><span class="p">)(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">paddings</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_space_to_batch_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="batch_to_space_nd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.batch_to_space_nd.html#mindspore.ops.batch_to_space_nd">[文档]</a><span class="k">def</span> <span class="nf">batch_to_space_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">,</span> <span class="n">crops</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides batch dimension with blocks and interleaves these blocks back into spatial dimensions.</span>

<span class="sd">    This operation will divide batch dimension N into blocks with block_shape, the output tensor&#39;s N dimension</span>
<span class="sd">    is the corresponding number of blocks after division. The output tensor&#39;s :math:`w_1, ..., w_M` dimension is</span>
<span class="sd">    the product of original :math:`w_1, ..., w_M` dimension and block_shape with given amount to crop from dimension,</span>
<span class="sd">    respectively.</span>

<span class="sd">    If the input shape is :math:`(n, c_1, ... c_k, w_1, ..., w_M)`, the output shape is</span>
<span class="sd">    :math:`(n&#39;, c_1, ... c_k, w&#39;_1, ..., w&#39;_M)`, where</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            n&#39; = n//(block\_shape[0]*...*block\_shape[M-1]) \\</span>
<span class="sd">            w&#39;_i = w_i*block\_shape[i-1]-crops[i-1][0]-crops[i-1][1]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. It must be greater or equal to 2-D tensor(equal to 4-D tensor on Ascend),</span>
<span class="sd">            batch dimension must be divisible by product of `block_shape`.</span>
<span class="sd">        block_shape (Union[list(int), tuple(int), int]): The block shape of dividing block with all value greater</span>
<span class="sd">            than or equal to 1. If `block_shape` is a tuple or list, the length of `block_shape` is M corresponding</span>
<span class="sd">            to the number of spatial dimensions. If `block_shape` is an int, the block size of M dimensions are the</span>
<span class="sd">            same, equal to `block_shape`. In this case of Ascend, M must be 2.</span>
<span class="sd">        crops (Union[list(int), tuple(int)]): The crops values for spatial dimensions, containing M subtraction list.</span>
<span class="sd">            Each contains 2 integer values. All values must be &gt;= 0. crops[i] specifies the crops values for spatial</span>
<span class="sd">            dimension i, which corresponds to input dimension i + offset,where offset = N-M, and N is the number of</span>
<span class="sd">            input dimensions. It is required that</span>
<span class="sd">            :math:`input\_shape[i+offset]*block\_shape[i] &gt; crops[i][0]+crops[i][1]`</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the output tensor with the same type as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `block_shape` is not one of list, tuple, int.</span>
<span class="sd">        TypeError: If `crops` is neither list nor tuple.</span>
<span class="sd">        ValueError: If `block_shape` is not one dimensional when `block_shape` is a list or tuple.</span>
<span class="sd">        ValueError: If the length of `block_shape` is not 2 on Ascend.</span>
<span class="sd">        ValueError: If the element of `block_shape` is not an integer larger than or euqal to 1.</span>
<span class="sd">        ValueError: If shape of `crops` is not (M, 2), where M is the length of `block_shape`.</span>
<span class="sd">        ValueError: If the element of `crops` is not an integer larger than or euqal to 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; block_shape = [2, 2]</span>
<span class="sd">        &gt;&gt;&gt; crops = [[0, 0], [0, 0]]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1]]], [[[2]]], [[[3]]], [[[4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.batch_to_space_nd(input_x, block_shape, crops)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.  2.]</span>
<span class="sd">           [3.  4.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">_batch_to_space_ndv2</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchToSpaceNDV2</span><span class="p">)()</span>
        <span class="k">return</span> <span class="n">_batch_to_space_ndv2</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">block_shape</span><span class="p">,</span> <span class="n">crops</span><span class="p">)</span>
    <span class="n">_batch_to_space_nd</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BatchToSpaceND</span><span class="p">)(</span><span class="n">block_shape</span><span class="p">,</span> <span class="n">crops</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_batch_to_space_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="nonzero"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.nonzero.html#mindspore.ops.nonzero">[文档]</a><span class="k">def</span> <span class="nf">nonzero</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a Tensor of the positions of all non-zero values.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor, its rank should be greater than or eaqual to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a 2-D Tensor whose data type is int64, containing the positions of all non-zero values of the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not Tensor.</span>
<span class="sd">        ValueError: If dim of `x` equals to 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1,  0], [-5, 0]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nonzero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 0]</span>
<span class="sd">         [0 1 0]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 0, 2, 0, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nonzero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0]</span>
<span class="sd">         [2]</span>
<span class="sd">         [4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nonzero_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="matrix_diag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_diag.html#mindspore.ops.matrix_diag">[文档]</a><span class="k">def</span> <span class="nf">matrix_diag</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_rows</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_cols</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor with the contents in `x` as k[0]-th to k[1]-th diagonals of a matrix, with everything else padded</span>
<span class="sd">    with `padding_value`. `num_rows` and `num_cols` specify the dimension of the innermost matrix of the output. If both</span>
<span class="sd">    are not specified, the op assumes the innermost matrix of output Tensor is square and infers its size from `k` and</span>
<span class="sd">    the innermost dimension of `x`. If the `num_rows` and `num_cols` specify only one of them, the operator will derive</span>
<span class="sd">    the smallest legal value as the dimension of output. Moreover, when only one diagonal is given</span>
<span class="sd">    (k is an integer or k[0] == k[1]), the first to the second innermost dimension of `x` is the batch size. Otherwise,</span>
<span class="sd">    the second innermost dimension is not a part of batch size.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The diagonal Tensor.</span>
<span class="sd">        k (Union[int, Tensor], optional): Diagonal offsets. A Tensor of type int32. Positive value means superdiagonal,</span>
<span class="sd">            0 refers to the main diagonal, and negative value means subdiagonals. `k` can be a single integer</span>
<span class="sd">            (for a single diagonal) or a pair of integers specifying the low and high ends of a matrix band.</span>
<span class="sd">            k[0] must not be larger than k[1]. The value must be in the range of given or derivated `num_rows`</span>
<span class="sd">            and `num_cols`, meaning value of k must be in (-num_rows, num_cols). Default: ``0`` .</span>
<span class="sd">        num_rows (Union[int, Tensor], optional): The number of rows of the output Tensor. A Tensor of type int32 with</span>
<span class="sd">            only one value. If `num_rows` is -1, indicating that the innermost matrix of the output Tensor is a square</span>
<span class="sd">            matrix, and the real number of rows will be derivated by other inputs. That is</span>
<span class="sd">            :math:`num\_rows = x.shape[-1] - min(k[1], 0)`. Otherwise, the value must be equal or greater than</span>
<span class="sd">            :math:`x.shape[-1] - min(k[1], 0)`. Default: ``-1`` .</span>
<span class="sd">        num_cols (Union[int, Tensor], optional): The number of columns of</span>
<span class="sd">            the output Tensor. A Tensor of type int32 with only one value.</span>
<span class="sd">            If `num_cols` is -1, indicating that the innermost matrix of the output</span>
<span class="sd">            Tensor is a square matrix, and the real number of columns will be derivated by other inputs.</span>
<span class="sd">            That is :math:`num\_cols = x.shape[-1] + max(k[0], 0)`. Otherwise, the value must be equal or</span>
<span class="sd">            greater than :math:`x.shape[-1] - min(k[1], 0)`.  Default: ``-1`` .</span>
<span class="sd">        padding_value (Union[int, float, Tensor], optional): The number to fill the area outside the specified</span>
<span class="sd">            diagonal band. A Tensor with only one value. Have the same dtype as x. Default: ``0`` .</span>
<span class="sd">        align (str, optional): specifies how superdiagonals and subdiagonals should be aligned.</span>
<span class="sd">            Supported values: ``&quot;RIGHT_LEFT&quot;`` , ``&quot;LEFT_RIGHT&quot;`` , ``&quot;LEFT_LEFT&quot;`` , ``&quot;RIGHT_RIGHT&quot;`` .</span>
<span class="sd">            Default: ``&quot;RIGHT_LEFT&quot;`` .</span>

<span class="sd">            - When set to &quot;RIGHT_LEFT&quot;, the alignment of superdiagonals will be towards the right side</span>
<span class="sd">              (padding the row on the left), while subdiagonals will be towards the left side</span>
<span class="sd">              (padding the row on the right)</span>
<span class="sd">            - When set to &quot;LEFT_RIGHT&quot;, the alignment of superdiagonals will be towards the left side</span>
<span class="sd">              (padding the row on the right), while subdiagonals will be towards the right side</span>
<span class="sd">              (padding the row on the left)</span>
<span class="sd">            - When set to &quot;LEFT_LEFT&quot;, the alignment of  both superdiagonals and subdiagonals will be towards</span>
<span class="sd">              the left side(padding the row on the right).</span>
<span class="sd">            - When set to &quot;RIGHT_RIGHT&quot;, the alignment of both superdiagonals and subdiagonals will be towards</span>
<span class="sd">              the right side(padding the row on the left).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor. Has the same type as `x`.</span>
<span class="sd">        Suppose `x` has r dimensions with shape :math:`(I, J, ..., M, N)` . The output Tensor has rank r + 1 with shape</span>
<span class="sd">        :math:`(I, J, ..., M, num\_rows, num\_cols)` when only one diagonal is given (k is an integer or k[0] == k[1]).</span>
<span class="sd">        Otherwise, it has rank r with shape :math:`(I, J, ..., num\_rows, num\_cols)` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not Tensor.</span>
<span class="sd">        TypeError: If input `x` and `padding_value` are not the same dtype.</span>
<span class="sd">        TypeError: If `k`, `num_rows` or `num_cols` is not int32 dtype.</span>
<span class="sd">        ValueError: If rank of `k` is not equal to 0 or 1.</span>
<span class="sd">        ValueError: If rank of `num_rows`, `num_cols` or `padding_value` is not equal to 0.</span>
<span class="sd">        ValueError: If size of `k` is not equal to 1 or 2.</span>
<span class="sd">        ValueError: If the value of `k` is not in (-num_rows, num_cols).</span>
<span class="sd">        ValueError: If k[1] is not greater equal to k[0] when k[0] != k[1].</span>
<span class="sd">        ValueError: If rank of `x` is not greater than or is equal to 1 when k is an integer or k[0] == k[1].</span>
<span class="sd">        ValueError: If rank of `x` is not greater than or is equal to 2 when k[0] != k[1].</span>
<span class="sd">        ValueError: If x.shape[-2] is not equal to k[1] - k[0] + 1 when k[0] != k[1].</span>
<span class="sd">        ValueError: If `num_rows` and `num_cols` do not match the dimensions of `x` and the values of `k`.</span>
<span class="sd">        ValueError: If `align` is not a string or not in the valid set of values.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[8, 9, 0],</span>
<span class="sd">        ...                      [1, 2, 3],</span>
<span class="sd">        ...                      [0, 4, 5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; k =Tensor(np.array([-1, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_rows = Tensor(np.array(3), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_cols = Tensor(np.array(3), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; padding_value = Tensor(np.array(11), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_diag(x, k, num_rows, num_cols, padding_value, align=&#39;LEFT_RIGHT&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  8. 11.]</span>
<span class="sd">         [ 4.  2.  9.]</span>
<span class="sd">         [11.  5.  3.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">num_rows</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_cols</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_cols</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">num_cols</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">num_cols</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding_value</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">padding_value</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">padding_value</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">matrix_diag_v3</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MatrixDiagV3</span><span class="p">)(</span><span class="n">align</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matrix_diag_v3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">num_rows</span><span class="p">,</span> <span class="n">num_cols</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">)</span></div>


<div class="viewcode-block" id="matrix_diag_part"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_diag_part.html#mindspore.ops.matrix_diag_part">[文档]</a><span class="k">def</span> <span class="nf">matrix_diag_part</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the diagonal part of input tensor.</span>
<span class="sd">    Returns a tensor with the k[0]-th to k[1]-th diagonals of `x`. Some diagonals are shorter than</span>
<span class="sd">    max_diag_len and need to be padded. Input k and padding_value must be const Tensor when taking Graph mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor with rank r, where r &gt;= 2.</span>
<span class="sd">        k (Union[int, Tensor], optional): A Tensor of type int32. Diagonal offset(s). Positive value means</span>
<span class="sd">            superdiagonal, 0 refers to the main diagonal, and negative value means subdiagonals. k can be</span>
<span class="sd">            a single integer (for a single diagonal) or a pair of integers specifying the low and high ends</span>
<span class="sd">            of a matrix band. k[0] must not be larger than k[1]. The value of k has restructions, meaning</span>
<span class="sd">            value of k must be in (-x.shape[-2], x.shape[-1]). Default: ``0``.</span>
<span class="sd">        padding_value (Union[int, float, Tensor], optional): A Tensor with only one value. Have the same dtype as x.</span>
<span class="sd">            The number to fill the area outside the specified diagonal band. Default: ``0`` .</span>
<span class="sd">        align (str, optional): An optional string from: ``&quot;RIGHT_LEFT&quot;`` , ``&quot;LEFT_RIGHT&quot;`` ,</span>
<span class="sd">            ``&quot;LEFT_LEFT&quot;`` , ``&quot;RIGHT_RIGHT&quot;`` . Align is a string specifying how superdiagonals and subdiagonals</span>
<span class="sd">            should be aligned, respectively. ``&quot;RIGHT_LEFT&quot;`` aligns superdiagonals to the right (left-pads the row)</span>
<span class="sd">            and subdiagonals to the left (right-pads the row). Default: ``&quot;RIGHT_LEFT&quot;`` . Default: ``&quot;RIGHT_LEFT&quot;``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor. Has the same type as `x`.</span>
<span class="sd">        Assume `x` has r dimensions :math:`(I, J, ..., M, N)` . Let `max_diag_len` be the maximum length among all</span>
<span class="sd">        diagonals to be extracted, :math:`max\_diag\_len = min(M + min(k[1], 0), N + min(-k[0], 0))`</span>
<span class="sd">        Let `num_diags` be the number of diagonals to extract, :math:`num\_diags = k[1] - k[0] + 1`.</span>
<span class="sd">        If :math:`num\_diags == 1`, the output tensor is of rank r - 1 with shape :math:`(I, J, ..., L, max\_diag\_len)`</span>
<span class="sd">        Otherwise, the output tensor has rank r with dimensions :math:`(I, J, ..., L, num\_diags, max\_diag\_len)` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not Tensor.</span>
<span class="sd">        TypeError: If input `x` and `padding_value` are not the same dtype.</span>
<span class="sd">        TypeError: If `k` is not int32 dtype.</span>
<span class="sd">        ValueError: If `align` is not a string or not in the valid range.</span>
<span class="sd">        ValueError: If rank of `k` is not equal to 0 or 1.</span>
<span class="sd">        ValueError: If rank of `padding_value` is not equal to 0.</span>
<span class="sd">        ValueError: If rank of `x` is not greater equal to 2.</span>
<span class="sd">        ValueError: If size of `k` is not equal to 1 or 2.</span>
<span class="sd">        ValueError: If k[1] is not greater equal to k[0] in case the size of `k` is 2.</span>
<span class="sd">        ValueError: If the value of `k` is not in (-x.shape[-2], x.shape[-1]).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3, 4],</span>
<span class="sd">        ...                      [5, 6, 7, 8],</span>
<span class="sd">        ...                      [9, 8, 7, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; k =Tensor(np.array([1, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; padding_value = Tensor(np.array(9), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_diag_part(x, k, padding_value, align=&#39;RIGHT_LEFT&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[9. 9. 4.]</span>
<span class="sd">         [9. 3. 8.]</span>
<span class="sd">         [2. 7. 6.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix_diag_part_v3</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MatrixDiagPartV3</span><span class="p">)(</span><span class="n">align</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matrix_diag_part_v3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">)</span></div>


<div class="viewcode-block" id="matrix_set_diag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.matrix_set_diag.html#mindspore.ops.matrix_set_diag">[文档]</a><span class="k">def</span> <span class="nf">matrix_set_diag</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">diagonal</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;RIGHT_LEFT&quot;</span><span class="p">):</span> <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a batched matrix tensor with new batched diagonal values.</span>
<span class="sd">    Given x and diagonal, this operation returns a tensor with the same shape and values as x, except for the specified</span>
<span class="sd">    diagonals of the innermost matrices. These will be overwritten by the values in diagonal. Some diagonals are shorter</span>
<span class="sd">    than max_diag_len and need to be padded.</span>
<span class="sd">    The diagonal :math:`shape[-2]` must be equal to num_diags calculated by :math:`k[1] - k[0] + 1`.</span>
<span class="sd">    The diagonal :math:`shape[-1]` must be</span>
<span class="sd">    equal to the longest diagonal value max_diag_len calculated</span>
<span class="sd">    by :math:`min(x.shape[-2] + min(k[1], 0), x.shape[-1] + min(-k[0], 0))`.</span>
<span class="sd">    Let x have r + 1 dimensions :math:`(I, J, ..., L, M, N)` .</span>
<span class="sd">    The diagonal tensor has rank r with shape :math:`(I, J, ..., L, max\_diag\_len)`</span>
<span class="sd">    when k is an integer or :math:`k[0] == k[1]`. Otherwise, it has rank r + 1</span>
<span class="sd">    with shape :math:`(I, J, ... L, num\_diags, max\_diag\_len)` .</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Rank r + 1, where r &gt;= 1.</span>
<span class="sd">        diagonal (Tensor): A Tensor. Have the same dtype as x. Rank r when k is an integer or :math:`k[0] == k[1]`.</span>
<span class="sd">            Otherwise, it has rank r + 1.</span>
<span class="sd">        k (Union[int, Tensor], optional): A int32 Scalar or int32 Tensor. Diagonal offset(s). Positive value means</span>
<span class="sd">            superdiagonal, 0 refers to the main diagonal, and negative value means subdiagonals. k can be a</span>
<span class="sd">            single integer (for a single diagonal) or a pair of integers specifying the low and high ends of</span>
<span class="sd">            a matrix band. k[0] must not be larger than k[1].</span>
<span class="sd">            The alue of k has restructions, meaning value of k must be in :math:`(-x.shape[-2], x.shape[-1])`.</span>
<span class="sd">            Input k must be const Tensor when taking Graph mode. Default: ``0`` .</span>
<span class="sd">        align (str, optional): An optional string from: ``&quot;RIGHT_LEFT&quot;`` (default), ``&quot;LEFT_RIGHT&quot;`` , ``&quot;LEFT_LEFT&quot;`` ,</span>
<span class="sd">            ``&quot;RIGHT_RIGHT&quot;`` . Align is a string specifying how superdiagonals and subdiagonals should be aligned,</span>
<span class="sd">            respectively. ``&quot;RIGHT_LEFT&quot;`` aligns superdiagonals to the right (left-pads the row) and subdiagonals</span>
<span class="sd">            to the left (right-pads the row).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, The same type as x. Let x has r+1 dimensions :math:`(I, J, ..., L, M, N)` .</span>
<span class="sd">        The output is a tensor of rank r+1 with dimensions :math:`(I, J, ..., L, M, N)` , the same as input x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input `x` or `diagonal` is not Tensor.</span>
<span class="sd">        TypeError: If input `x` and `diagonal` are not the same dtype.</span>
<span class="sd">        TypeError: If `k` is not int32 dtype.</span>
<span class="sd">        ValueError: If `align` is not a string or not in the valid range.</span>
<span class="sd">        ValueError: If rank of `k` is not equal to 0 or 1.</span>
<span class="sd">        ValueError: If rank of `x` is not greater equal to 2.</span>
<span class="sd">        ValueError: If size of `k` is not equal to 1 or 2.</span>
<span class="sd">        ValueError: If k[1] is not greater equal to k[0] in case the size of `k` is 2.</span>
<span class="sd">        ValueError: If the `diagonal` rank size don&#39;t match with input `x` rank size.</span>
<span class="sd">        ValueError: If the `diagonal` shape value don&#39;t match with input `x` shape value.</span>
<span class="sd">        ValueError: If the diagonal :math:`shape[-2]` is not equal to num_diags calculated by :math:`k[1]-k[0]+1`.</span>
<span class="sd">        ValueError: If the value of `k` is not in :math:`(-x.shape[-2], x.shape[-1])`.</span>
<span class="sd">        ValueError: If the diagonal.shape[-1] is not equal to the max_diag_len calculated by</span>
<span class="sd">            :math:`min(x.shape[-2] + min(k[1],</span>
<span class="sd">            0), x.shape[-1] + min(-k[0], 0))`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[7, 7, 7, 7],</span>
<span class="sd">        ...                      [7, 7, 7, 7],</span>
<span class="sd">        ...                      [7, 7, 7, 7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; diagonal = Tensor(np.array([[0, 9, 1],</span>
<span class="sd">        ...                             [6, 5, 8],</span>
<span class="sd">        ...                             [1, 2, 3],</span>
<span class="sd">        ...                             [4, 5, 0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; k = Tensor(np.array([-1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; align = &#39;RIGHT_LEFT&#39;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_set_diag(x, diagonal, k, align)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 6. 9. 7.]</span>
<span class="sd">         [4. 2. 5. 1.]</span>
<span class="sd">         [7. 5. 3. 8.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (3, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix_set_diag_v3_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MatrixSetDiagV3</span><span class="p">)(</span><span class="n">align</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">matrix_set_diag_v3_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">diagonal</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span></div>


<div class="viewcode-block" id="meshgrid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.meshgrid.html#mindspore.ops.meshgrid">[文档]</a><span class="k">def</span> <span class="nf">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates coordinate matrices from given coordinate tensors.</span>

<span class="sd">    Given N one-dimensional coordinate tensors, returns a tuple outputs of N N-D</span>
<span class="sd">    coordinate tensors for evaluating expressions on an N-D grid.</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (List[Tensor]): List of 1-D tensors.</span>
<span class="sd">            The length of inputs should be greater than 1. The data type is Number.</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        indexing (str, optional): Cartesian (&#39;xy&#39;, default) or</span>
<span class="sd">            matrix (&#39;ij&#39;) indexing of output. Valid options: xy&#39; or &#39;ij&#39;. In the 2-D case with</span>
<span class="sd">            inputs of length `M` and `N`, the outputs are of shape :math:`(N, M)`</span>
<span class="sd">            for &#39;xy&#39; indexing and :math:`(M, N)` for &#39;ij&#39; indexing. In the 3-D</span>
<span class="sd">            case with inputs of length `M`, `N` and `P`, outputs are of shape</span>
<span class="sd">            :math:`(N, M, P)` for &#39;xy&#39; indexing and :math:`(M, N, P)` for &#39;ij&#39; indexing. Default: ``&#39;xy&#39;`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensors, a Tuple of N N-D Tensor objects. The data type is the same with the Inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `indexing` is not a str or `inputs` is not a tuple.</span>
<span class="sd">        ValueError: If `indexing` is neither &#39;xy&#39; nor &#39;ij&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([5, 6, 7]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; z = Tensor(np.array([8, 9, 0, 1, 2]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.meshgrid(x, y, z, indexing=&#39;xy&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3, 4, 5], dtype=Int32, value=</span>
<span class="sd">         [[[1, 1, 1, 1, 1],</span>
<span class="sd">           [2, 2, 2, 2, 2],</span>
<span class="sd">           [3, 3, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 4, 4]],</span>
<span class="sd">          [[1, 1, 1, 1, 1],</span>
<span class="sd">           [2, 2, 2, 2, 2],</span>
<span class="sd">           [3, 3, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 4, 4]],</span>
<span class="sd">          [[1, 1, 1, 1, 1],</span>
<span class="sd">           [2, 2, 2, 2, 2],</span>
<span class="sd">           [3, 3, 3, 3, 3],</span>
<span class="sd">           [4, 4, 4, 4, 4]]]),</span>
<span class="sd">         Tensor(shape=[3, 4, 5], dtype=Int32, value=</span>
<span class="sd">         [[[5, 5, 5, 5, 5],</span>
<span class="sd">           [5, 5, 5, 5, 5],</span>
<span class="sd">           [5, 5, 5, 5, 5],</span>
<span class="sd">           [5, 5, 5, 5, 5]],</span>
<span class="sd">          [[6, 6, 6, 6, 6],</span>
<span class="sd">           [6, 6, 6, 6, 6],</span>
<span class="sd">           [6, 6, 6, 6, 6],</span>
<span class="sd">           [6, 6, 6, 6, 6]],</span>
<span class="sd">          [[7, 7, 7, 7, 7],</span>
<span class="sd">           [7, 7, 7, 7, 7],</span>
<span class="sd">           [7, 7, 7, 7, 7],</span>
<span class="sd">           [7, 7, 7, 7, 7]]]),</span>
<span class="sd">         Tensor(shape=[3, 4, 5], dtype=Int32, value=</span>
<span class="sd">         [[[8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2]],</span>
<span class="sd">          [[8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2]],</span>
<span class="sd">          [[8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2],</span>
<span class="sd">           [8, 9, 0, 1, 2]]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">meshgrid_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Meshgrid</span><span class="p">)(</span><span class="n">indexing</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">meshgrid_op</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span></div>


<div class="viewcode-block" id="affine_grid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.affine_grid.html#mindspore.ops.affine_grid">[文档]</a><span class="k">def</span> <span class="nf">affine_grid</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a 2D or 3D flow field (sampling grid) based on `theta`, a batch of affine matrices.</span>

<span class="sd">    Args:</span>
<span class="sd">        theta (Tensor): The input tensor of flow field whose dtype is float16, float32.</span>
<span class="sd">            Input batch of affine matrices with shape :math:`(N, 2, 3)` for 2D grid or :math:`(N, 3, 4)` for 3D grid.</span>
<span class="sd">        size (tuple[int]): The target output image size.</span>
<span class="sd">            The value of target output with format :math:`(N, C, H, W)` for 2D grid or :math:`(N, C, D, H, W)` for 3D</span>
<span class="sd">            grid.</span>
<span class="sd">        align_corners (bool, optional): Geometrically, each pixel of input is viewed as a squqre instead of dot.</span>
<span class="sd">            If ``True`` , consider extremum -1 and 1 referring to the centers of the pixels rather than pixel corners.</span>
<span class="sd">            The default value is ``False`` , extremum -1 and 1 refer to the corners of the pixels, so that sampling is</span>
<span class="sd">            irrelevant to resolution of the image. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a tensor whose data type is same as &#39;theta&#39;, and the shape is :math:`(N, H, W, 2)` for 2D grid</span>
<span class="sd">        or :math:`(N, D, H, W, 3)` for 3D grid.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `theta` is not a Tensor or `size` is not a tuple.</span>
<span class="sd">        ValueError: If the shape of `theta` is not :math:`(N, 2, 3)` or :math:`(N, 3, 4)`.</span>
<span class="sd">        ValueError: If the size of `size` is not 4 or 5.</span>
<span class="sd">        ValueError: If the shape of `theta` is :math:`(N, 2, 3)`, the size of `size` is not 4;</span>
<span class="sd">                    If the shape of `theta` is :math:`(N, 3, 4)`, the size of `size` is not 5.</span>
<span class="sd">        ValueError: If the size[0] is not equal to the shape[0] of theta.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; theta = Tensor([[[0.8, 0.5, 0],[-0.5, 0.8, 0]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; out_size = (1, 3, 2, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.affine_grid(theta, out_size, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[-0.78333336 -0.06666666]</span>
<span class="sd">        [-0.25       -0.4       ]</span>
<span class="sd">        [ 0.28333336 -0.73333335]]</span>
<span class="sd">        [[-0.28333336  0.73333335]</span>
<span class="sd">        [ 0.25        0.4       ]</span>
<span class="sd">        [ 0.78333336  0.06666666]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">affine_grid_op</span> <span class="o">=</span> <span class="n">AffineGrid</span><span class="p">(</span><span class="n">align_corners</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">affine_grid_op</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast_to"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.broadcast_to.html#mindspore.ops.broadcast_to">[文档]</a><span class="k">def</span> <span class="nf">broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span> <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts input tensor to a given shape. The dim of input shape must be smaller</span>
<span class="sd">    than or equal to that of target shape. Suppose input shape is :math:`(x_1, x_2, ..., x_m)`,</span>
<span class="sd">    target shape is :math:`(*, y_1, y_2, ..., y_m)`, where :math:`*` means any additional dimension.</span>
<span class="sd">    The broadcast rules are as follows:</span>

<span class="sd">    Compare the value of :math:`x_m` and :math:`y_m`, :math:`x_{m-1}` and :math:`y_{m-1}`, ...,</span>
<span class="sd">    :math:`x_1` and :math:`y_1` consecutively and</span>
<span class="sd">    decide whether these shapes are broadcastable and what the broadcast result is.</span>

<span class="sd">    If the value pairs at a specific dim are equal, then that value goes right into that dim of output shape.</span>
<span class="sd">    With an input shape :math:`(2, 3)`, target shape :math:`(2, 3)` , the inferred output shape is :math:`(2, 3)`.</span>

<span class="sd">    If the value pairs are unequal, there are three cases:</span>

<span class="sd">    Case 1: If the value of the target shape in the dimension is -1, the value of the</span>
<span class="sd">    output shape in the dimension is the value of the corresponding input shape in the dimension.</span>
<span class="sd">    With an input shape :math:`(3, 3)`, target</span>
<span class="sd">    shape :math:`(-1, 3)`, the output shape is :math:`(3, 3)`.</span>

<span class="sd">    Case 2: If the value of target shape in the dimension is not -1, but the corresponding</span>
<span class="sd">    value in the input shape is 1, then the corresponding value of the output shape</span>
<span class="sd">    is that of the target shape. With an input shape :math:`(1, 3)`, target</span>
<span class="sd">    shape :math:`(8, 3)`, the output shape is :math:`(8, 3)`.</span>

<span class="sd">    Case 3: If the corresponding values of the two shapes do not satisfy the above cases,</span>
<span class="sd">    it means that broadcasting from the input shape to the target shape is not supported.</span>

<span class="sd">    So far we got the last m dims of the outshape, now focus on the first :math:`*` dims, there are</span>
<span class="sd">    two cases:</span>

<span class="sd">    If the first :math:`*` dims of output shape does not have -1 in it, then fill the input</span>
<span class="sd">    shape with ones until their length are the same, and then refer to</span>
<span class="sd">    Case 2 mentioned above to calculate the output shape. With target shape :math:`(3, 1, 4, 1, 5, 9)`,</span>
<span class="sd">    input shape :math:`(1, 5, 9)`, the filled input shape will be :math:`(1, 1, 1, 1, 5, 9)` and thus the</span>
<span class="sd">    output shape is :math:`(3, 1, 4, 1, 5, 9)`.</span>

<span class="sd">    If the first :math:`*` dims of output shape have -1 in it, it implies this -1 is corresponding to</span>
<span class="sd">    a non-existing dim so they&#39;re not broadcastable. With target shape :math:`(3, -1, 4, 1, 5, 9)`,</span>
<span class="sd">    input shape :math:`(1, 5, 9)`, instead of operating the dim-filling process first, it raises errors directly.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>
<span class="sd">        shape (tuple): The target shape to broadcast. Can be fully specified, or have -1 in one position</span>
<span class="sd">                       where it will be substituted by the input tensor&#39;s shape in that position, see example.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the given `shape` and the same data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is not a tuple.</span>
<span class="sd">        ValueError: If the target and input shapes are incompatible, or if a - 1 in the target shape is in an invalid</span>
<span class="sd">                    location.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; shape = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.broadcast_to(x, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [1. 2. 3.]]</span>
<span class="sd">        &gt;&gt;&gt; shape = (-1, 2)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1], [2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.broadcast_to(x, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">         [2. 2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="n">F</span><span class="o">.</span><span class="n">is_sequence_value_unknown</span><span class="p">(</span><span class="n">shape</span><span class="p">):</span>
        <span class="n">_dyn_broadcast_to</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">DynamicBroadcastTo</span><span class="p">)()</span>
        <span class="k">return</span> <span class="n">_dyn_broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
    <span class="n">_broadcast_to</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">BroadcastTo</span><span class="p">)(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="unsorted_segment_min"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unsorted_segment_min.html#mindspore.ops.unsorted_segment_min">[文档]</a><span class="k">def</span> <span class="nf">unsorted_segment_min</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of a tensor along segments.</span>

<span class="sd">    The following figure shows the calculation process of unsorted_segment_min:</span>

<span class="sd">    .. image:: UnsortedSegmentMin.png</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text { output }_i=\text{min}_{j \ldots} \text { data }[j \ldots]</span>

<span class="sd">    where :math:`min` over tuples :math:`j...` such that :math:`segment\_ids[j...] == i`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - If the segment_id i is absent in the segment_ids, then output[i] will be filled with</span>
<span class="sd">          the maximum value of the x&#39;s type.</span>
<span class="sd">        - The `segment_ids` must be non-negative tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape is :math:`(x_1, x_2, ..., x_R)`. With float16, float32 or int32 data type.</span>
<span class="sd">        segment_ids (Tensor): TThe label indicates the segment to which each element belongs.</span>
<span class="sd">            Set the shape as :math:`(x_1, x_2, ..., x_N)`, where 0 &lt; N &lt;= R.</span>
<span class="sd">        num_segments (int): The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `num_segments` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `segment_ids` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_min(x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [4. 2. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">unsorted_segment_min_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentMin</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">unsorted_segment_min_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>


<div class="viewcode-block" id="unsorted_segment_max"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unsorted_segment_max.html#mindspore.ops.unsorted_segment_max">[文档]</a><span class="k">def</span> <span class="nf">unsorted_segment_max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum along segments of a tensor.</span>

<span class="sd">    The following figure shows the calculation process of unsorted_segment_max:</span>

<span class="sd">    .. image:: UnsortedSegmentMax.png</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text { output }_i=\text{max}_{j \ldots} \text { data }[j \ldots]</span>

<span class="sd">    where :math:`max` over tuples :math:`j...` such that :math:`segment\_ids[j...] == i`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - If the segment_id i is absent in the segment_ids, then output[i] will be filled with</span>
<span class="sd">          the minimum value of the x&#39;s type.</span>
<span class="sd">        - The `segment_ids` must be non-negative tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape is :math:`(x_1, x_2, ..., x_R)`. With float16, float32 or int32 data type.</span>
<span class="sd">        segment_ids (Tensor): TThe label indicates the segment to which each element belongs.</span>
<span class="sd">            Set the shape as :math:`(x_1, x_2, ..., x_N)`, where 0 &lt; N &lt;= R.</span>
<span class="sd">        num_segments (int): The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `num_segments` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `segment_ids` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_max(x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [4. 5. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">unsorted_segment_max_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentMax</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">unsorted_segment_max_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>


<div class="viewcode-block" id="unsorted_segment_prod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unsorted_segment_prod.html#mindspore.ops.unsorted_segment_prod">[文档]</a><span class="k">def</span> <span class="nf">unsorted_segment_prod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the product of a tensor along segments.</span>

<span class="sd">    The following figure shows the calculation process of unsorted_segment_prod:</span>

<span class="sd">    .. image:: UnsortedSegmentProd.png</span>

<span class="sd">    Note:</span>
<span class="sd">        - If the segment_id i is absent in the segment_ids, then output[i] will be filled with 1.</span>
<span class="sd">        - The `segment_ids` must be non-negative tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape is :math:`(x_1, x_2, ..., x_R)`. With float16, float32 or int32 data type.</span>
<span class="sd">        segment_ids (Tensor): A `1-D` tensor whose shape is :math:`(x_1)`,</span>
<span class="sd">                              the value must be non-negative tensor. The data type must be int32.</span>
<span class="sd">        num_segments (int): The value specifies the number of distinct `segment_ids`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, set the number of `num_segments` as `N`, the shape is :math:`(N, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `num_segments` is not an int.</span>
<span class="sd">        ValueError: If length of shape of `segment_ids` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [4, 2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor(np.array([0, 1, 0]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_prod(x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4. 4. 3.]</span>
<span class="sd">         [4. 5. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">unsorted_segment_prod_</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">UnsortedSegmentProd</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">unsorted_segment_prod_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>


<div class="viewcode-block" id="index_fill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.index_fill.html#mindspore.ops.index_fill">[文档]</a><span class="k">def</span> <span class="nf">index_fill</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fills the elements under the `axis` dimension of the input Tensor `x` with the input `value`</span>
<span class="sd">    by selecting the indices in the order given in `index`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input Tensor.  The supported data type is Number or Bool.</span>
<span class="sd">        axis (Union[int, Tensor]): Dimension along which to fill the input Tensor. Only supports</span>
<span class="sd">            an int number or a 0-dimensional Tensor, whose data type is int32 or int64.</span>
<span class="sd">        index (Tensor): Indices of the input Tensor to fill in. The dtype must be int32.</span>
<span class="sd">        value (Union[bool, int, float, Tensor]): Value to fill the returned Tensor. If `value` is</span>
<span class="sd">            a Tensor, it must be a 0-dimensional Tensor and has the same dtype as `x`. Otherwise,</span>
<span class="sd">            the `value` will be cast to a 0-dimensional Tensor with the same data type as `x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as input Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is neither int number nor Tensor.</span>
<span class="sd">        TypeError: When `axis` is a Tensor, its dtype is not int32 or int64.</span>
<span class="sd">        TypeError: If `index` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `index` is not int32.</span>
<span class="sd">        TypeError: If `value` is not a bool, int, float, or Tensor.</span>
<span class="sd">        TypeError: When `value` is a Tensor, the dtype of `x` and `value` are not the same.</span>
<span class="sd">        ValueError: If `axis` is a Tensor and its rank is not equal to 0.</span>
<span class="sd">        ValueError: If the rank of `index` is greater than 1D.</span>
<span class="sd">        ValueError: When `value` is a Tensor and its rank is not equal to 0.</span>
<span class="sd">        RuntimeError: If the value of `axis` is out the range of `[-x.ndim, x.ndim - 1]`.</span>
<span class="sd">        RuntimeError: If the values of `index` are out the range of `[-x.shape[axis], x.shape[axis]-1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor([0, 2], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(-2.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.index_fill(x, 1, index, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[-2. 2. -2.]</span>
<span class="sd">         [-2. 5. -2.]</span>
<span class="sd">         [-2. 8. -2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">)):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">cast_</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">index_fill_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checks axes are with the bounds of ndim&quot;&quot;&quot;</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">axis</span>


<div class="viewcode-block" id="index_select"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.index_select.html#mindspore.ops.index_select">[文档]</a><span class="k">def</span> <span class="nf">index_select</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a new Tensor that accesses the values of `input` along the specified `axis` dimension</span>
<span class="sd">    using the indices specified in `index`. The new Tensor has the same number of dimensions as `input`,</span>
<span class="sd">    with the size of the `axis` dimension being equal to the length of `index`, and the size of all other</span>
<span class="sd">    dimensions will be unchanged from the original `input` Tensor.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The value of index must be in the range of `[0, input.shape[axis])`, the result is undefined out of range.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>
<span class="sd">        axis (int): The dimension to be indexed.</span>
<span class="sd">        index (Tensor): A 1-D Tensor with the indices to access in `input` along the specified axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as input Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `index` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not int number.</span>
<span class="sd">        ValueError: If the value of `axis` is out the range of `[-input.ndim, input.ndim - 1]`.</span>
<span class="sd">        ValueError: If the dimension of `index` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.arange(16).astype(np.float32).reshape(2, 2, 4))</span>
<span class="sd">        &gt;&gt;&gt; print(input)</span>
<span class="sd">        [[[ 0.  1.  2.  3.]</span>
<span class="sd">          [ 4.  5.  6.  7.]]</span>
<span class="sd">         [[ 8.  9. 10. 11.]</span>
<span class="sd">          [12. 13. 14. 15.]]]</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor([0,], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.index_select(input, 1, index)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[ 0.  1.  2.  3.]]</span>
<span class="sd">         [[ 8.  9. 10. 11.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;index_select&#39;, `input` and `index` must be all tensors.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">index</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;index_select&#39;, the dimension of `index` must be 1, but got </span><span class="si">{</span><span class="n">index</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gather_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="population_count"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.population_count.html#mindspore.ops.population_count">[文档]</a><span class="k">def</span> <span class="nf">population_count</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes element-wise population count(a.k.a bitsum, bitcount).</span>
<span class="sd">    For each entry in `input_x`, calculates the number of 1 bits in the binary representation of that entry.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Tensor of any dimension. The data type must be int16 or uint16 (Ascend).</span>
<span class="sd">            The data type must be int8, int16, int32, int64, uint8, uint16, uint32, uint64 (CPU and GPU).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape as the input, and the data type is uint8.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not int16, uint16 (Ascend).</span>
<span class="sd">        TypeError: If dtype of `input_x` is not int8, int16, int32, int64, uint8, uint16, uint32, uint64 (CPU and GPU).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([0, 1, 3], mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.population_count(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">population_count_</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="c1">##############################</span>
<span class="c1"># Type Conversion Functions.</span>
<span class="c1">##############################</span>


<div class="viewcode-block" id="is_tensor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.is_tensor.html#mindspore.ops.is_tensor">[文档]</a><span class="k">def</span> <span class="nf">is_tensor</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check whether the input object is a :class:`mindspore.Tensor` .</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Object): input object.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Bool. Return True if `obj` is a Tensor, otherwise, return False.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor([1.9, 2.2, 3.1])</span>
<span class="sd">        &gt;&gt;&gt; ops.is_tensor(a)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">)</span></div>


<div class="viewcode-block" id="is_nonzero"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.is_nonzero.html#mindspore.ops.is_nonzero">[文档]</a><span class="k">def</span> <span class="nf">is_nonzero</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine whether the input Tensor contains 0 or False. The input can only be a single element.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Bool, returns False if the input Tensor contains a unit element of 0 or a single element of False,</span>
<span class="sd">        otherwise returns True.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not Tensor.</span>
<span class="sd">        ValueError: If the element number of `input` is not equal to 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor([[[False]]])</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor([[3.5]])</span>
<span class="sd">        &gt;&gt;&gt; out1 = ops.is_nonzero(x1)</span>
<span class="sd">        &gt;&gt;&gt; print(out1)</span>
<span class="sd">        False</span>
<span class="sd">        &gt;&gt;&gt; out2 = ops.is_nonzero(x2)</span>
<span class="sd">        &gt;&gt;&gt; print(out2)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For is_nonzero, the input must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s1">.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For is_nonzero, the numel of input must be 1, but got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">bool</span><span class="p">(</span><span class="n">out</span><span class="p">)</span></div>


<div class="viewcode-block" id="scalar_cast"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scalar_cast.html#mindspore.ops.scalar_cast">[文档]</a><span class="k">def</span> <span class="nf">scalar_cast</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Casts the input scalar to another type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (scalar): The input scalar. Only constant value is allowed.</span>
<span class="sd">        input_y (mindspore.dtype): The type to be cast. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Scalar. The type is the same as the python type corresponding to `input_y`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input_x` nor `input_y` is a constant value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scalar_cast(255.0, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        255</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scalar_cast_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_mul.html#mindspore.ops.tensor_scatter_mul">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_mul</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by multiplying the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When divided values are provided for the same</span>
<span class="sd">    index, the result of the update will multiply these values respectively. Except that</span>
<span class="sd">    the updates are applied on output `Tensor` instead of input `Parameter`.</span>

<span class="sd">    The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of `input_x[indices]`. For more details, see use cases.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[indices] = input\_x \times update</span>

<span class="sd">    Note:</span>
<span class="sd">        - If some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">          the corresponding `updates` will not be updated to `input_x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The dimension of `input_x` must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64. The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the input tensor, has the same type as `input_x`,</span>
<span class="sd">            and the shape of `updates` should be equal to</span>
<span class="sd">            :math:`indices.shape[:-1] + input\_x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>
<span class="sd">        RuntimeError: If a value of `indices` is not in `input_x` on CPU backend.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; # Next, demonstrate the approximate operation process of this operator:</span>
<span class="sd">        &gt;&gt;&gt; # 1, indices[0] = [0, 0], indices[1] = [0, 0]</span>
<span class="sd">        &gt;&gt;&gt; # 2, And input_x[0, 0] = -0.1</span>
<span class="sd">        &gt;&gt;&gt; # 3, So input_x[indices] = [-0.1, -0.1]</span>
<span class="sd">        &gt;&gt;&gt; # 4, Satisfy the above formula: input_x[indices].shape=(2) == updates.shape=(2)</span>
<span class="sd">        &gt;&gt;&gt; # 5, Perform the multiply operation for the first time:</span>
<span class="sd">        &gt;&gt;&gt; #      first_input_x = input_x[0][0] * updates[0] = [[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; # 6, Perform the multiply operation for the second time:</span>
<span class="sd">        &gt;&gt;&gt; #      second_input_x = input_x[0][0] * updates[1] = [[-0.22, 0.3, 3.6], [0.4, 0.5, -3.2]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_mul(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.22  0.3   3.6  ]</span>
<span class="sd">         [ 0.4   0.5   -3.2 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_scatter_mul_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<div class="viewcode-block" id="tensor_scatter_div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_scatter_div.html#mindspore.ops.tensor_scatter_div">[文档]</a><span class="k">def</span> <span class="nf">tensor_scatter_div</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a new tensor by dividing the values from the positions in `input_x` indicated by</span>
<span class="sd">    `indices`, with values from `updates`. When divided values are provided for the same</span>
<span class="sd">    index, the result of the update will be to divided these values respectively. Except that</span>
<span class="sd">    the updates are applied on output `Tensor` instead of input `Parameter`.</span>

<span class="sd">    The last axis of `indices` is the depth of each index vectors. For each index vector,</span>
<span class="sd">    there must be a corresponding value in `updates`. The shape of `updates` should be</span>
<span class="sd">    equal to the shape of `input_x[indices]`. For more details, see use cases.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output\left [indices  \right ] = input\_x \div update</span>

<span class="sd">    Note:</span>
<span class="sd">        - On GPU, if some values of the `indices` are out of bound, instead of raising an index error,</span>
<span class="sd">          the corresponding `updates` will not be updated to self tensor.</span>
<span class="sd">        - On CPU, if some values of the `indices` are out of bound, raising an index error.</span>
<span class="sd">        - On Ascend, out of bound checking is not supported, if some values of the `indices` are out of bound,</span>
<span class="sd">          unknown errors may be caused.</span>
<span class="sd">        - The operator can&#39;t handle division by 0 exceptions, so the user needs to make sure</span>
<span class="sd">          there is no 0 value in `updates`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The dimension of input_x must be no less than indices.shape[-1].</span>
<span class="sd">        indices (Tensor): The index of input tensor whose data type is int32 or int64.</span>
<span class="sd">            The rank must be at least 2.</span>
<span class="sd">        updates (Tensor): The tensor to update the `input_x` tensor, has the same type as `input_x`.</span>
<span class="sd">            And the shape of `updates` should be</span>
<span class="sd">            equal to :math:`indices.shape[:-1] + input\_x.shape[indices.shape[-1]:]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of shape of `indices`.</span>
<span class="sd">        RuntimeError: If a value of `indices` is not in `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [0, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([1.0, 2.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_scatter_div(input_x, indices, updates)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.05  0.3  3.6  ]</span>
<span class="sd">         [ 0.4   0.5  -3.2 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tensor_scatter_div_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">scalar_to_array</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The  interface is deprecated. Please use the :func:`mindspore.ops.scalar_to_tensor` instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarToArray</span><span class="p">()(</span><span class="n">input_x</span><span class="p">)</span>


<div class="viewcode-block" id="scalar_to_tensor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scalar_to_tensor.html#mindspore.ops.scalar_to_tensor">[文档]</a><span class="k">def</span> <span class="nf">scalar_to_tensor</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a scalar to a `Tensor`, and converts the data type to the specified type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Union[bool, int, float]): The input is a scalar. Only constant value is allowed.</span>
<span class="sd">        dtype (mindspore.dtype): The target data type. Only constant value is allowed. Default: ``mstype.float32``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. 0-D Tensor and the content is the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is neither bool nor int nor float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; data = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scalar_to_tensor(data, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scalar_to_tensor_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="tuple_to_array"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tuple_to_array.html#mindspore.ops.tuple_to_array">[文档]</a><span class="k">def</span> <span class="nf">tuple_to_array</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a tuple to a tensor.</span>

<span class="sd">    If the type of the first number in the tuple is integer, the data type of the output tensor is int.</span>
<span class="sd">    Otherwise, the data type of the output tensor is float.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (tuple): A tuple of numbers. These numbers have the same type. Only constant value is allowed.</span>
<span class="sd">            The shape is :math:`(N,*)` where :math:`*` means any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, if the input tuple contains `N` numbers, then the shape of the output tensor is (N,).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a tuple.</span>
<span class="sd">        ValueError: If length of `input_x` is less than or equal to 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = (1,2,3)</span>
<span class="sd">        &gt;&gt;&gt; print(type(input_x))</span>
<span class="sd">        &lt;class &#39;tuple&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tuple_to_array(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(type(output))</span>
<span class="sd">        &lt;class &#39;mindspore.common.tensor.Tensor&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span>
    <span class="k">return</span> <span class="n">tuple_to_tensor_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="masked_select"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.masked_select.html#mindspore.ops.masked_select">[文档]</a><span class="k">def</span> <span class="nf">masked_select</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new 1-D Tensor which indexes the `x` tensor according to the boolean `mask`.</span>
<span class="sd">    The shapes of the `mask` tensor and the `x` tensor don&#39;t need to match, but they must be broadcastable.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        mask (Tensor[bool]): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1-D Tensor, with the same type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `mask` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `mask` is not bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4]), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; mask = Tensor(np.array([1, 0, 1, 0]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.masked_select(x, mask)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">masked_select_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span></div>


<div class="viewcode-block" id="masked_fill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.masked_fill.html#mindspore.ops.masked_fill">[文档]</a><span class="k">def</span> <span class="nf">masked_fill</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fills elements of Tensor with value where mask is True.</span>
<span class="sd">    The shapes of `input_x` and `mask` need to be the same or broadcastable.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The source Tensor whose data type is one of bool, uint8, int8, int16, int32,</span>
<span class="sd">                    int64, float16, float32, float64, complex64, complex128.</span>
<span class="sd">        mask (Tensor[bool]): The boolean mask.</span>
<span class="sd">        value (Union[float, Tensor]): The value to fill in with, which dtype is the same as `input_x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `mask` is not bool.</span>
<span class="sd">        TypeError: If `input_x` or `mask` is not a Tensor.</span>
<span class="sd">        ValueError: If the shapes of `input_x` and `mask` could not be broadcast.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `value` is not one of bool, uint8, int8, int16, int32,</span>
<span class="sd">                   int64, float16, float32, float64, complex64, complex128.</span>
<span class="sd">        TypeError: If dtype of `value` is different from that of `input_x`.</span>
<span class="sd">        TypeError: If `value` is neither float number nor Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mask = Tensor(np.array([True, True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.masked_fill(input_x, mask, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5 0.5 3.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">scalar_to_tensor_</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">input_x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">masked_fill_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">MaskedFill</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">masked_fill_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="diag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.diag.html#mindspore.ops.diag">[文档]</a><span class="k">def</span> <span class="nf">diag</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a diagonal tensor with a given diagonal values.</span>

<span class="sd">    Assume `input` has dimensions :math:`(D_1,... D_k)` , the output is a tensor of</span>
<span class="sd">    rank 2k with dimensions :math:`(D_1,..., D_k, D_1,..., D_k)` where:</span>
<span class="sd">    :math:`output[i_1,..., i_k, i_1,..., i_k] = input[i_1,..., i_k]` and 0 everywhere else.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        ValueError: If rank of `input` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.diag(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 0 0 0]</span>
<span class="sd">         [0 2 0 0]</span>
<span class="sd">         [0 0 3 0]</span>
<span class="sd">         [0 0 0 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">diag_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="diagflat"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.diagflat.html#mindspore.ops.diagflat">[文档]</a><span class="k">def</span> <span class="nf">diagflat</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a 2-D Tensor which diagonal is the flattened `input` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor, which is flattened and set as the diagonal of the output.</span>
<span class="sd">        offset (int, optional): `offset` controls which diagonal to choose. Default: ``0`` .</span>

<span class="sd">            - When `offset` is zero, the diagonal chosen is the main diagonal.</span>
<span class="sd">            - When `offset` is a positive integer, the diagonal chosen is up the main diagonal.</span>
<span class="sd">            - When `offset` is a negative integer, the diagonal chosen is down the main diagonal.</span>

<span class="sd">    Returns:</span>
<span class="sd">        The 2-D Tensor, whose diagonal is the flattened `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a tensor.</span>
<span class="sd">        TypeError: If `offset` is not an integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 2], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.diagflat(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 0.]</span>
<span class="sd">         [0. 0. 2.]</span>
<span class="sd">         [0. 0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For diagflat, the input x must be tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">offset</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For diagflat, the offset must be int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">offset</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">offset_abs</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">offset</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">zeros</span><span class="p">((</span><span class="n">offset_abs</span><span class="p">,</span> <span class="n">offset_abs</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">diag</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">offset</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">pad_y</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span> <span class="o">+</span> <span class="n">offset_abs</span><span class="p">,</span> <span class="n">offset_abs</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">pad_x</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">offset_abs</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">offset</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">cat</span><span class="p">((</span><span class="n">pad_x</span><span class="p">,</span> <span class="n">res</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">cat</span><span class="p">((</span><span class="n">res</span><span class="p">,</span> <span class="n">pad_y</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">cat</span><span class="p">((</span><span class="n">res</span><span class="p">,</span> <span class="n">pad_x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">cat</span><span class="p">((</span><span class="n">pad_y</span><span class="p">,</span> <span class="n">res</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="col2im"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.col2im.html#mindspore.ops.col2im">[文档]</a><span class="k">def</span> <span class="nf">col2im</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Combines an array of sliding local blocks into a large containing tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): 4D tensor with data type float16 or float.</span>
<span class="sd">        output_size (Tensor): 1D tensor with 2 elements of data type int.</span>
<span class="sd">        kernel_size (Union[int, tuple[int], list[int]]): The size of the kernel, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Must be specified.</span>
<span class="sd">        dilation (Union[int, tuple[int], list[int]]): The size of the dilation, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width.</span>
<span class="sd">        padding_value (Union[int, tuple[int], list[int]]): The size of the padding, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width.</span>
<span class="sd">        stride (Union[int, tuple[int], list[int]]): The size of the stride, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 4D Tensor, with same type as &#39;input_x&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If :attr:`kernel_size`, `dilation`, `padding_value`, `stride` data type is not in</span>
<span class="sd">            Union[int, tuple[int], list[int]].</span>
<span class="sd">        ValueError: If :attr:`kernel_size`, `dilation`, `padding_value`, `stride` value is not</span>
<span class="sd">            greater than zero or elements number more than 2.</span>
<span class="sd">        ValueError: If :attr:`padding_value` value is less than zero or elements number more than 2.</span>
<span class="sd">        ValueError: If input_x.shape[2] != kernel_size[0] * kernel_size[1].</span>
<span class="sd">        ValueError: If input_x.shape[3] does not match the calculated number of sliding blocks.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(input_data=np.random.rand(16, 16, 4, 25), dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_size = Tensor(input_data=[8, 8], dtype=mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.col2im(x, output_size, [2, 2], [2, 2], [2, 2], [2, 2])</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 16, 8, 8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">c2i</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Col2Im</span><span class="p">)(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">c2i</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_split_int</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">split_size_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor `x` into multiple sub-tensors along the axis according to the given `split_size_or_sections`</span>
<span class="sd">    with int type.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arr_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">length_along_dim</span> <span class="o">=</span> <span class="n">arr_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">split_size_or_sections</span> <span class="o">&gt;</span> <span class="n">length_along_dim</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">length_along_dim</span> <span class="o">%</span> <span class="n">split_size_or_sections</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">sections</span> <span class="o">=</span> <span class="n">length_along_dim</span> <span class="o">//</span> <span class="n">split_size_or_sections</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">sections</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">num_sections</span> <span class="o">=</span> <span class="n">length_along_dim</span> <span class="o">//</span> <span class="n">split_size_or_sections</span>
        <span class="n">length1</span> <span class="o">=</span> <span class="n">num_sections</span> <span class="o">*</span> <span class="n">split_size_or_sections</span>
        <span class="n">length2</span> <span class="o">=</span> <span class="n">length_along_dim</span> <span class="o">-</span> <span class="n">length1</span>
        <span class="n">start1</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">size1</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">length1</span><span class="p">)</span>
        <span class="n">start2</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">start1</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">length1</span><span class="p">)</span>
        <span class="n">size2</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">length2</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">num_sections</span><span class="p">)(</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start1</span><span class="p">,</span> <span class="n">size1</span><span class="p">))</span> <span class="o">+</span> \
              <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start2</span><span class="p">,</span> <span class="n">size2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="k">def</span> <span class="nf">_split_sub_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">split_size_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor `x` into multiple sub-tensors along the axis according to the given `split_size_or_sections`</span>
<span class="sd">    with type of tuple or list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">new_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">split_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">split_size_or_sections</span><span class="p">):</span>
        <span class="n">new_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">split_size</span><span class="p">)</span>
    <span class="n">new_indices</span> <span class="o">=</span> <span class="n">new_indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">sub_tensors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">begin</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ms_arrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_indices</span><span class="p">)):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">new_indices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">begin</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">new_indices</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">end</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">sliced_tensor</span> <span class="o">=</span> <span class="n">strided_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">begin</span><span class="p">),</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">end</span><span class="p">),</span> <span class="n">strides</span><span class="p">)</span>
        <span class="n">sub_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sliced_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sub_tensors</span>


<div class="viewcode-block" id="split"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.split.html#mindspore.ops.split">[文档]</a><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">split_size_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the Tensor into chunks along the given axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensor (Tensor): A Tensor to be divided.</span>
<span class="sd">        split_size_or_sections (Union[int, tuple(int), list(int)]):</span>
<span class="sd">            If `split_size_or_sections` is an int type, `tensor` will be split into equally sized chunks,</span>
<span class="sd">            each chunk with size `split_size_or_sections`. Last chunk will be smaller than `split_size_or_sections`</span>
<span class="sd">            if `tensor.shape[axis]` is not divisible by `split_size_or_sections`.</span>
<span class="sd">            If `split_size_or_sections` is a list type, then `tensor` will be split into len(split_size_or_sections)</span>
<span class="sd">            chunks with sizes `split_size_or_sections` along the given `axis`.</span>
<span class="sd">        axis (int): The axis along which to split. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `tensor` is not Tensor.</span>
<span class="sd">        TypeError: If argument `axis` is not Tensor.</span>
<span class="sd">        ValueError: If argument `axis` is out of range of :math:`[-tensor.ndim, tensor.ndim)` .</span>
<span class="sd">        TypeError: If each element in &#39;split_size_or_sections&#39; is not integer.</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not int, tuple(int) or list(int).</span>
<span class="sd">        ValueError: The sum of &#39;split_size_or_sections&#39; is not equal to x.shape[axis].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(9).astype(&quot;float32&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.split(Tensor(input_x), 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32, value= [ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[3], dtype=Float32, value= [ 3.00000000e+00,  4.00000000e+00,  5.00000000e+00]),</span>
<span class="sd">         Tensor(shape=[3], dtype=Float32, value= [ 6.00000000e+00,  7.00000000e+00,  8.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;expect `tensor` is a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Type of Argument `axis` should be integer but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">arr_axis</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">split_size_or_sections</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">split_size_or_sections</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">_split_int</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">split_size_or_sections</span><span class="p">,</span> <span class="n">arr_axis</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For split, the value of &#39;split_size_or_sections&#39; must be more than zero, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">split_size_or_sections</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_size_or_sections</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">split_size_or_sections</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">int</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Each element in &#39;split_size_or_sections&#39; should be integer, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">item</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Each element in &#39;split_size_or_sections&#39; should be non-negative, &quot;</span>
                                <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">split_size_or_sections</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">split_size_or_sections</span><span class="p">)</span> <span class="o">!=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">arr_axis</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The sum of &#39;split_size_or_sections&#39; should be equal to </span><span class="si">{</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">arr_axis</span><span class="p">]</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">split_size_or_sections</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_split_sub_tensors</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">split_size_or_sections</span><span class="p">,</span> <span class="n">arr_axis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Type of Argument `split_size_or_sections` should be integer, tuple(int) or list(int), &quot;</span> \
                        <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">split_size_or_sections</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">res</span><span class="p">)</span></div>


<div class="viewcode-block" id="tril"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tril.html#mindspore.ops.tril">[文档]</a><span class="k">def</span> <span class="nf">tril</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the lower triangle part of &#39;input&#39; (elements that contain the diagonal and below),</span>
<span class="sd">    and set the other elements to zeros.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A Tensor with shape :math:`(x_1, x_2, ..., x_R)`. The rank must be at least 2.</span>
<span class="sd">          Supporting all number types including bool.</span>
<span class="sd">        diagonal (int, optional): An optional attribute indicates the diagonal to consider, default: 0,</span>
<span class="sd">            indicating the main diagonal.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the same shape and data type as the input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `diagonal` is not an int.</span>
<span class="sd">        TypeError: If the type of `x` is neither number nor bool.</span>
<span class="sd">        ValueError: If the rank of `x` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; result = ops.tril(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 1  0  0  0]</span>
<span class="sd">         [ 5  6  0  0]</span>
<span class="sd">         [10 11 12  0]</span>
<span class="sd">         [14 15 16 17]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; result = ops.tril(x, diagonal=1)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 1  2  0  0]</span>
<span class="sd">         [ 5  6  7  0]</span>
<span class="sd">         [10 11 12 13]</span>
<span class="sd">         [14 15 16 17]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; result = ops.tril(x, diagonal=-1)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 0  0  0  0]</span>
<span class="sd">         [ 5  0  0  0]</span>
<span class="sd">         [10 11  0  0]</span>
<span class="sd">         [14 15 16  0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tril_</span> <span class="o">=</span> <span class="n">Tril</span><span class="p">(</span><span class="n">diagonal</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tril_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="triu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.triu.html#mindspore.ops.triu">[文档]</a><span class="k">def</span> <span class="nf">triu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>  <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the upper triangle part of &#39;input&#39; (elements that contain the diagonal and below),</span>
<span class="sd">    and set the other elements to zeros.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor with shape :math:`(M, N, *)` where * means any number of additional dimensions.</span>
<span class="sd">        diagonal (int, optional): An optional attribute indicates the diagonal to consider, default: 0,</span>
<span class="sd">            indicating the main diagonal.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a tensor has the same shape and data type as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `diagonal` is not an int.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; result = ops.triu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 1  2  3  4]</span>
<span class="sd">         [ 0  6  7  8]</span>
<span class="sd">         [ 0  0 12 13]</span>
<span class="sd">         [ 0  0  0 17]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; result = ops.triu(x, diagonal=1)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 0  2  3  4]</span>
<span class="sd">         [ 0  0  7  8]</span>
<span class="sd">         [ 0  0  0 13]</span>
<span class="sd">         [ 0  0  0  0]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[ 1,  2,  3,  4],</span>
<span class="sd">        ...                      [ 5,  6,  7,  8],</span>
<span class="sd">        ...                      [10, 11, 12, 13],</span>
<span class="sd">        ...                      [14, 15, 16, 17]]))</span>
<span class="sd">        &gt;&gt;&gt; result = ops.triu(x, diagonal=-1)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[ 1  2  3  4]</span>
<span class="sd">         [ 5  6  7  8]</span>
<span class="sd">         [ 0 11 12 13]</span>
<span class="sd">         [ 0  0 16 17]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Triu</span><span class="p">)(</span><span class="n">diagonal</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check axes are within the number of dimensions of tensor x and normalize the negative axes.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)]): Axes of the tensor.</span>
<span class="sd">        ndim (int): The number of dimensions of the tensor.</span>

<span class="sd">    Return:</span>
<span class="sd">        Axis (Union[int, tuple(int)]). If input is integer, return integer, else tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;axis should be integers, not </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="o">-</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="n">ax</span> <span class="o">&lt;</span> <span class="n">ndim</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;axis </span><span class="si">{</span><span class="n">ax</span><span class="si">}</span><span class="s1"> is out of bounds for array of dimension </span><span class="si">{</span><span class="n">ndim</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">canonicalizer</span><span class="p">(</span><span class="n">ax</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ax</span> <span class="o">+</span> <span class="n">ndim</span> <span class="k">if</span> <span class="n">ax</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">ax</span>

    <span class="n">axis</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">canonicalizer</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span> <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">axis</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">el</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">axis</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">axis</span><span class="p">))</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;duplicate axis in </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_list_comprehensions</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_tuple</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a new list or tuple by list comprehension.</span>

<span class="sd">    Args:</span>
<span class="sd">        obj (Union[int, list, tuple]):</span>
<span class="sd">            If integer, it will be the length of the returned tuple/list.</span>
<span class="sd">        item: The value to be filled. Default: ``None`` .</span>
<span class="sd">            If ``None`` , the values in the new list/tuple are the same as obj</span>
<span class="sd">            or range(obj) when obj is integer.</span>
<span class="sd">        return_tuple(bool): If ``true`` , returns tuple, else returns list.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List or tuple.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lst</span> <span class="o">=</span> <span class="n">obj</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">lst</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ms_arrange</span><span class="p">(</span><span class="n">obj</span><span class="p">):</span>
            <span class="n">lst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">item</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lst</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">lst</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">return_tuple</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_tuple_setitem</span><span class="p">(</span><span class="n">tup</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tuple with specified `idx` set to `value`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tup</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span>
    <span class="n">tup</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_tensor_split_sub_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor `x` into multiple sub-tensors along the axis according to the given `indices_or_sections`</span>
<span class="sd">    with type of tuple or list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">length_along_dim</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="n">indices_or_sections</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">)</span>
    <span class="n">indices_or_sections</span> <span class="o">+=</span> <span class="p">(</span><span class="n">length_along_dim</span><span class="p">,)</span>

    <span class="n">sub_tensors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">begin</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ms_arrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">)):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">indices_or_sections</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">begin</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">indices_or_sections</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">end</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">idx</span>
        <span class="n">sliced_tensor</span> <span class="o">=</span> <span class="n">strided_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">begin</span><span class="p">),</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">end</span><span class="p">),</span> <span class="n">strides</span><span class="p">)</span>
        <span class="n">sub_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sliced_tensor</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">sub_tensors</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_tensor_split_sub_int</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor `x` into multiple sub-tensors along the axis according to the given `indices_or_sections`</span>
<span class="sd">    with type if int.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">arr_shape</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">length_along_dim</span> <span class="o">=</span> <span class="n">arr_shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">indices_or_sections</span> <span class="o">&gt;</span> <span class="n">length_along_dim</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">length_along_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">indices_or_sections_n</span> <span class="o">=</span> <span class="p">[</span><span class="n">length_along_dim</span><span class="p">,</span> <span class="n">length_along_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">res2</span> <span class="o">=</span> <span class="n">_tensor_split_sub_tensors</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">indices_or_sections_n</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">length_along_dim</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">):</span>
            <span class="n">res</span> <span class="o">+=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">res2</span><span class="p">)[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">elif</span> <span class="n">length_along_dim</span> <span class="o">%</span> <span class="n">indices_or_sections</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">num_long_tensor</span> <span class="o">=</span> <span class="n">length_along_dim</span> <span class="o">%</span> <span class="n">indices_or_sections</span>
        <span class="n">num_short_tensor</span> <span class="o">=</span> <span class="n">indices_or_sections</span> <span class="o">-</span> <span class="n">num_long_tensor</span>
        <span class="n">length1</span> <span class="o">=</span> <span class="n">num_long_tensor</span> <span class="o">*</span> <span class="p">(</span><span class="n">length_along_dim</span> <span class="o">//</span> <span class="n">indices_or_sections</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">length2</span> <span class="o">=</span> <span class="n">length_along_dim</span> <span class="o">-</span> <span class="n">length1</span>
        <span class="n">start1</span> <span class="o">=</span> <span class="n">_list_comprehensions</span><span class="p">(</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">size1</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">length1</span><span class="p">)</span>
        <span class="n">start2</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">start1</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">length1</span><span class="p">)</span>
        <span class="n">size2</span> <span class="o">=</span> <span class="n">_tuple_setitem</span><span class="p">(</span><span class="n">arr_shape</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">length2</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">num_long_tensor</span><span class="p">)(</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start1</span><span class="p">,</span> <span class="n">size1</span><span class="p">))</span> <span class="o">+</span> \
              <span class="n">P</span><span class="o">.</span><span class="n">Split</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">num_short_tensor</span><span class="p">)(</span><span class="n">tensor_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start2</span><span class="p">,</span> <span class="n">size2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span>


<div class="viewcode-block" id="tensor_split"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.tensor_split.html#mindspore.ops.tensor_split">[文档]</a><span class="k">def</span> <span class="nf">tensor_split</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors along the given axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]):</span>

<span class="sd">            - If `indices_or_sections` is an integer n, input tensor will be split into n sections.</span>

<span class="sd">              - If :math:`input.shape(axis)` can be divisible by n, sub-sections will have equal size</span>
<span class="sd">                :math:`input.shape(axis) / n` .</span>
<span class="sd">              - If :math:`input.shape(axis)` is not divisible by n, the first :math:`input.shape(axis) % n` sections</span>
<span class="sd">                will have size :math:`input.shape(axis) // n + 1` , and the rest will have</span>
<span class="sd">                size :math:`input.shape(axis) // n` .</span>
<span class="sd">            - If `indices_or_sections` is of type tuple(int) or list(int), the input tensor will be split at the</span>
<span class="sd">              indices in the list or tuple. For example, given parameters :math:`indices\_or\_sections=[1, 4]`</span>
<span class="sd">              and :math:`axis=0` , the input tensor will be split into sections :math:`input[:1]` ,</span>
<span class="sd">              :math:`input[1:4]` , and :math:`input[4:]` .</span>

<span class="sd">        axis (int): The axis along which to split. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `input` is not Tensor.</span>
<span class="sd">        TypeError: If argument `axis` is not int.</span>
<span class="sd">        ValueError: If argument `axis` is out of range of :math:`[-input.ndim, input.ndim)` .</span>
<span class="sd">        TypeError: If each element in &#39;indices_or_sections&#39; is not integer.</span>
<span class="sd">        TypeError: If argument `indices_or_sections` is not int, tuple(int) or list(int).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(9).astype(&quot;float32&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_split(Tensor(input_x), 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3], dtype=Float32, value= [ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[3], dtype=Float32, value= [ 3.00000000e+00,  4.00000000e+00,  5.00000000e+00]),</span>
<span class="sd">        Tensor(shape=[3], dtype=Float32, value= [ 6.00000000e+00,  7.00000000e+00,  8.00000000e+00]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;expect `x` is a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Type of Argument `axis` should be integer but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">handle_axis</span> <span class="o">=</span> <span class="n">_canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">indices_or_sections</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">_tensor_split_sub_int</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">handle_axis</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For tensor_split, the value of &#39;indices_or_sections&#39; must be more than zero &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">indices_or_sections</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">indices_or_sections</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">int</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Each element in &#39;indices_or_sections&#39; should be integer, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">_tensor_split_sub_tensors</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="n">handle_axis</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Type of Argument `indices_or_sections` should be integer, tuple(int) or list(int), &quot;</span> \
                        <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">indices_or_sections</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">res</span></div>


<div class="viewcode-block" id="vsplit"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.vsplit.html#mindspore.ops.vsplit">[文档]</a><span class="k">def</span> <span class="nf">vsplit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits `input` with two or more dimensions, into multiple sub-tensors vertically</span>
<span class="sd">    according to `indices_or_sections`.</span>

<span class="sd">    It is equivalent to `ops.tensor_split` with :math:`axis=0` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]): See argument in :func:`mindspore.ops.tensor_split`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(9).reshape((3, 3)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.vsplit(Tensor(input_x), 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 3], dtype=Float32, value=[[ 0.00000000e+00,  1.00000000e+00,  2.00000000e+00]]),</span>
<span class="sd">         Tensor(shape=[1, 3], dtype=Float32, value=[[ 3.00000000e+00,  4.00000000e+00,  5.00000000e+00]]),</span>
<span class="sd">         Tensor(shape=[1, 3], dtype=Float32, value=[[ 6.00000000e+00,  7.00000000e+00,  8.00000000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;expect `x` is a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;vsplit expect `x` is a Tensor with at least 1 dimension, but got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tensor_split</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="hsplit"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hsplit.html#mindspore.ops.hsplit">[文档]</a><span class="k">def</span> <span class="nf">hsplit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors horizontally.</span>
<span class="sd">    It is equivalent to `ops.tensor_split` with :math:`axis=1` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]): See argument in :func:`mindspore.ops.tensor_split`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not Tensor.</span>
<span class="sd">        ValueError: If dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(6).reshape((2, 3)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hsplit(Tensor(input_x), 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 1], dtype=Float32, value=[[ 0.00000000e+00], [ 3.00000000e+00]]),</span>
<span class="sd">         Tensor(shape=[2, 1], dtype=Float32, value=[[ 1.00000000e+00], [ 4.00000000e+00]]),</span>
<span class="sd">         Tensor(shape=[2, 1], dtype=Float32, value=[[ 2.00000000e+00], [ 5.00000000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;expect `x` is a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;hsplit expect `x` is a Tensor with at least 2 dimension, but got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tensor_split</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span></div>


<div class="viewcode-block" id="dsplit"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.dsplit.html#mindspore.ops.dsplit">[文档]</a><span class="k">def</span> <span class="nf">dsplit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits a tensor into multiple sub-tensors along the 3rd axis.</span>
<span class="sd">    It is equivalent to `ops.tensor_split` with :math:`axis=2` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A Tensor to be divided.</span>
<span class="sd">        indices_or_sections (Union[int, tuple(int), list(int)]): See argument in :func:`mindspore.ops.tensor_split`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of sub-tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = np.arange(6).reshape((1, 2, 3)).astype(&#39;float32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.dsplit(Tensor(input_x), 3)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2, 1], dtype=Float32, value=[[[ 0.00000000e+00], [ 3.00000000e+00]]]),</span>
<span class="sd">         Tensor(shape=[1, 2, 1], dtype=Float32, value=[[[ 1.00000000e+00], [ 4.00000000e+00]]]),</span>
<span class="sd">         Tensor(shape=[1, 2, 1], dtype=Float32, value=[[[ 2.00000000e+00], [ 5.00000000e+00]]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;expect `x` is a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;dsplit expect `x` is a Tensor with at least 3 dimension, but got </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tensor_split</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices_or_sections</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_init_and_select_elem</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">initial</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">cmp_fn</span><span class="p">):</span>    <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize the input according to Initial, and select the element according to where.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">initial</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">cmp_fn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">where</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">where</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">where</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">where</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">where</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">where</span><span class="o">.</span><span class="n">shape</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">where</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;initial value must be provided for where masks&#39;</span><span class="p">)</span>
        <span class="n">where</span> <span class="o">=</span> <span class="n">where</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">initial</span> <span class="o">=</span> <span class="n">initial</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">where</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">initial</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span>


<div class="viewcode-block" id="max"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.max.html#mindspore.ops.max">[文档]</a><span class="k">def</span> <span class="nf">max</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>    <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the maximum value along with the given axis for the input tensor. It returns the maximum values and</span>
<span class="sd">    indices.</span>

<span class="sd">    Note:</span>
<span class="sd">        - In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</span>
<span class="sd">        - When `axis` is ``None``, `keepdims` and subsequent parameters have no</span>
<span class="sd">          effect. At the same time, the index is fixed to return 0.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If there are multiple maximum values, the index of the first maximum value is used.</span>
<span class="sd">        - The value range of &quot;axis&quot; is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of &quot;input&quot;.</span>

<span class="sd">    Also see: :class:`mindspore.ops.ArgMaxWithValue`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor, can be any dimension. Complex tensor is not supported for now.</span>
<span class="sd">        axis (int): The dimension to reduce. Default: ``None`` .</span>
<span class="sd">        keepdims (bool): Whether to reduce dimension, if true, the output will keep same dimension with the input,</span>
<span class="sd">            the output will reduce dimension if false. Default: ``False`` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        initial (scalar, optional): The minimum value of an output element. Must be present to allow computation</span>
<span class="sd">            on empty slice. Default: ``None`` .</span>
<span class="sd">        where (Tensor[bool], optional): A Tensor indicating whether to replace the primitive value in `input`</span>
<span class="sd">            with the value in `initial`. If ``True`` , do not replace, otherwise replace. For the index of ``True``</span>
<span class="sd">            in `where`, the corresponding value in `initial` must be assigned. Default: ``None`` , which indicates</span>
<span class="sd">            ``True`` by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the maximum value of the input</span>
<span class="sd">        tensor.</span>

<span class="sd">        - values (Tensor) - The maximum value of input tensor, with the same shape as index, and same dtype as x.</span>
<span class="sd">        - index (Tensor) - The index for the maximum value of the input tensor, with dtype int32. If `keepdims`</span>
<span class="sd">          is true, the shape of output tensors is :math:`(input_1, input_2, ..., input_{axis-1}, 1, input_{axis+1},</span>
<span class="sd">          ..., input_N)` . Otherwise, the shape is :math:`(input_1, input_2, ..., input_{axis-1}, input_{axis+1},</span>
<span class="sd">          ..., input_N)` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not Tensor.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If `initial` is not a number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, index,  = ops.max(x, keepdims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output, index)</span>
<span class="sd">        0.7 0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">reduce_max</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;max&#39;, &#39;initial&#39; must be a scalar, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;max&#39;, &#39;axis&#39; must be int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">_init_and_select_elem</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">initial</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">maximum</span><span class="p">)</span>
    <span class="n">argmax_with_value_op</span> <span class="o">=</span> <span class="n">ArgMaxWithValue</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="n">indices</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">argmax_with_value_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">indices</span></div>


<div class="viewcode-block" id="argmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.argmax.html#mindspore.ops.argmax">[文档]</a><span class="k">def</span> <span class="nf">argmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the indices of the maximum values of a tensor across a dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor.</span>
<span class="sd">        dim (Union[int, None], optional): The dimension to reduce. If `dim` is ``None`` , the indices of the maximum</span>
<span class="sd">            value within the flattened input will be returned. Default: ``None`` .</span>
<span class="sd">        keepdim (bool, optional): Whether the output tensor retains the specified</span>
<span class="sd">            dimension. Ignored if `dim` is None. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, indices of the maximum values across a dimension.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keepdim` is not bool.</span>
<span class="sd">        ValueError: If `dim` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 20, 5], [67, 8, 9], [130, 24, 15]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.argmax(x, dim=-1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 0 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;keepdim&quot;</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="s2">&quot;argmax&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">bool_</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">is_dim_none</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">reshape_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">is_dim_none</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Argmax</span><span class="p">)(</span><span class="n">dim</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">keepdim</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_dim_none</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<div class="viewcode-block" id="min"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.min.html#mindspore.ops.min">[文档]</a><span class="k">def</span> <span class="nf">min</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">where</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>    <span class="c1"># pylint: disable=redefined-outer-name</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the minimum value along with the given axis for the input tensor. It returns the minimum values and</span>
<span class="sd">    indices.</span>

<span class="sd">    Note:</span>
<span class="sd">        - In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</span>
<span class="sd">        - When `axis` is ``None``, `keepdims` and subsequent parameters have no</span>
<span class="sd">          effect. At the same time, the index is fixed to return 0.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If there are multiple minimum values, the index of the first minimum value is used.</span>
<span class="sd">        - The value range of &quot;axis&quot; is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of &quot;x&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor, can be any dimension. Complex tensor is not supported for now.</span>
<span class="sd">        axis (int): The dimension to reduce. Default: ``None`` .</span>
<span class="sd">        keepdims (bool): Whether to reduce dimension, if ``True`` the output will keep the same dimension as the input,</span>
<span class="sd">            the output will reduce dimension if ``False`` . Default: ``False`` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        initial (scalar, optional): The maximum value of an output element. Must be present to allow computation</span>
<span class="sd">            on empty slice. Default: ``None`` .</span>
<span class="sd">        where (Tensor[bool], optional): A Tensor indicating whether to replace the primitive value in `input`</span>
<span class="sd">            with the value in `initial`. If ``True`` , do not replace, otherwise replace. For the index of ``True``</span>
<span class="sd">            in `where`, the corresponding value in `initial` must be assigned. Default: ``None`` , which indicates</span>
<span class="sd">            ``True``  by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the minimum value of the input</span>
<span class="sd">        tensor.</span>

<span class="sd">        - **values** (Tensor) - The minimum value of input tensor, with the same</span>
<span class="sd">          shape as `index`, and same dtype as `x`.</span>
<span class="sd">        - **index** (Tensor) - The index for the minimum value of the input tensor, with dtype int32. If `keepdims`</span>
<span class="sd">          is true, the shape of output tensors is :math:`(input_1, input_2, ..., input_{axis-1}, 1, input_{axis+1},</span>
<span class="sd">          ..., input_N)` . Otherwise, the shape is :math:`(input_1, input_2, ..., input_{axis-1}, input_{axis+1},</span>
<span class="sd">          ..., input_N)` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not Tensor.</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If `initial` is not a number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, index = ops.min(x, keepdims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output, index)</span>
<span class="sd">        0.0 0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">reduce_min</span><span class="p">(</span><span class="nb">input</span><span class="p">),</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">initial</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">initial</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;min&#39;, &#39;initial&#39; must be a scalar, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">initial</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;min&#39;, &#39;axis&#39; must be int, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">_init_and_select_elem</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">initial</span><span class="p">,</span> <span class="n">where</span><span class="p">,</span> <span class="n">ops</span><span class="o">.</span><span class="n">minimum</span><span class="p">)</span>
    <span class="n">argmin_with_value_</span> <span class="o">=</span> <span class="n">ArgMinWithValue</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="n">keepdims</span><span class="p">)</span>
    <span class="n">indices</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="n">argmin_with_value_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">indices</span></div>


<div class="viewcode-block" id="aminmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.aminmax.html#mindspore.ops.aminmax">[文档]</a><span class="k">def</span> <span class="nf">aminmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It returns the minimum and maximum value along the given axis of input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor, can be any dimension. Set the shape of input tensor as</span>
<span class="sd">          :math:`(x_1, x_2, ..., x_N)` .</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">        axis (int, optional): The dimension to reduce. The value range of `axis` is [-rank, rank),</span>
<span class="sd">            where &quot;rank&quot; is the dimension of `input`. If `axis` is None, computes the minimum and maximum value</span>
<span class="sd">            along the entire input tensor. Default: ``0`` .</span>
<span class="sd">        keepdims (bool, optional): Whether to maintain dimension. When set to True, the output will keep the same</span>
<span class="sd">            dimension as the input, or the dimension specified by `axis` is reduced. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple (Tensor), containing the minimum value and maximum value of the input tensor.</span>

<span class="sd">        - If `keepdims` is True, the shape of output tensors is</span>
<span class="sd">          :math:`(x_1, x_2, ..., x_{axis-1}, 1, x_{axis+1}, ..., x_N)`.</span>
<span class="sd">        - If `keepdims` is False, the shape of output tensors is</span>
<span class="sd">          :math:`(x_1, x_2, ..., x_{axis-1}, x_{axis+1}, ..., x_N)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keepdims` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int and not None.</span>
<span class="sd">        ValueError: If `axis` is not in range [-rank, rank).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output0, output1 = ops.aminmax(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output0, output1)</span>
<span class="sd">        0.0 0.7</span>
<span class="sd">        &gt;&gt;&gt; output2, output3 = ops.aminmax(x, axis=-1, keepdims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output2, output3)</span>
<span class="sd">        [0.] [0.7]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0.0, 0.4, 0.6, 0.7, 0.1], [0.78, 0.97, 0.5, 0.82, 0.99]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output4, output5 = ops.aminmax(x, axis=None, keepdims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output4, output5)</span>
<span class="sd">        [[0.]] [[0.99]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">output0</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
        <span class="n">output1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">keepdims</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">output0</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output0</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
            <span class="n">output1</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output1</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output0</span><span class="p">,</span> <span class="n">output1</span>
    <span class="n">argmin_with_value_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ArgMinWithValue</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="n">argmax_with_value_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ArgMaxWithValue</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">output0</span> <span class="o">=</span> <span class="n">argmin_with_value_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">output1</span> <span class="o">=</span> <span class="n">argmax_with_value_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output0</span><span class="p">,</span> <span class="n">output1</span></div>


<div class="viewcode-block" id="narrow"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.narrow.html#mindspore.ops.narrow">[文档]</a><span class="k">def</span> <span class="nf">narrow</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a narrowed tensor from input tensor, and</span>
<span class="sd">    the dimension axis is input from start to start + length.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the tensor to narrow.</span>
<span class="sd">        axis (int): the axis along which to narrow.</span>
<span class="sd">        start (int): the starting dimension.</span>
<span class="sd">        length (int): the distance to the ending dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor.</span>

<span class="sd">        - output (Tensors) - The narrowed tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input is not a tensor or tuple or list of tensors.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.narrow(x, 0, 0, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1 2 3]</span>
<span class="sd">         [ 4 5 6]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.narrow(x, 1, 1, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 2 3]</span>
<span class="sd">         [ 5 6]</span>
<span class="sd">         [ 8 9]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="s2">&quot;narrow&quot;</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">)</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">-</span> <span class="n">start</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">)</span>

    <span class="n">begins</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span>
    <span class="n">begins</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">start</span>
    <span class="n">sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">sizes</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">length</span>
    <span class="k">return</span> <span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">begins</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span></div>


<div class="viewcode-block" id="unsorted_segment_sum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unsorted_segment_sum.html#mindspore.ops.unsorted_segment_sum">[文档]</a><span class="k">def</span> <span class="nf">unsorted_segment_sum</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sum of a tensor along segments.</span>

<span class="sd">    Calculates a tensor such that :math:`\text{output}[i] = \sum_{segment\_ids[j] == i} \text{data}[j, \ldots]`, where</span>
<span class="sd">    :math:`j,...` is a tuple describing the index of element in data.</span>
<span class="sd">    `segment_ids` selects which elements in data to sum</span>
<span class="sd">    up. Segment_ids does not need to be sorted, and it does not need to cover all values in the entire valid value</span>
<span class="sd">    range.</span>

<span class="sd">    The following figure shows the calculation process of unsorted_segment_sum:</span>

<span class="sd">    .. image:: UnsortedSegmentSum.png</span>

<span class="sd">    Note:</span>
<span class="sd">        - If the segment_id i is absent in the segment_ids, then output[i] will be filled with 0.</span>
<span class="sd">        - On Ascend, if the value of segment_id is less than 0 or greater than the length of the input data shape, an</span>
<span class="sd">          execution error will occur.</span>

<span class="sd">    If the sum of the given segment_ids :math:`i` is empty, then :math:`\text{output}[i] = 0`. If the given segment_ids</span>
<span class="sd">    is negative, the value will be ignored. &#39;num_segments&#39; must be equal to the number of different segment_ids.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Input Tensor contains the data to be summed.</span>
<span class="sd">          The shape is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        segment_ids (Tensor): TThe label indicates the segment to which each element belongs.</span>
<span class="sd">            Set the shape as :math:`(x_1, x_2, ..., x_N)`, where 0 &lt; N &lt;= R.</span>
<span class="sd">        num_segments (Union[int, Tensor], optional): Set :math:`z` as num_segments, it can be an int or 0-D Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is :math:`(z, x_{N+1}, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `num_segments` is not an int or 0-D Tensor.</span>
<span class="sd">        ValueError: If length of shape of `segment_ids` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 1, 2], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 4</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_sum(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 3. 4. 0.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4, 2, 5], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 1, 2, 3, 4], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 6</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_sum(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 3. 4. 2. 5. 0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">unsorted_segment_sum_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>



<div class="viewcode-block" id="topk"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.topk.html#mindspore.ops.topk">[文档]</a><span class="k">def</span> <span class="nf">topk</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">largest</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds values and indices of the `k` largest or smallest entries along a given dimension.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If sorted is set to False, it will use the aicpu operator, the performance may be reduced. In addition, due to</span>
<span class="sd">          different memory layout and traversal methods on different platforms, the display order of calculation results</span>
<span class="sd">          may be inconsistent when `sorted` is False.</span>

<span class="sd">    If the `input` is a one-dimensional Tensor, finds the `k` largest  or smallest entries in the Tensor,</span>
<span class="sd">    and outputs its value and index as a Tensor. values[`k`] is the `k` largest item in `input`,</span>
<span class="sd">    and its index is indices [`k`].</span>

<span class="sd">    For a multi-dimensional matrix,</span>
<span class="sd">    calculates the first or last `k` entries in a given dimension, therefore:</span>

<span class="sd">    .. math::</span>

<span class="sd">        values.shape = indices.shape</span>

<span class="sd">    If the two compared elements are the same, the one with the smaller index value is returned first.</span>

<span class="sd">    Note:</span>
<span class="sd">        Currently, Ascend/CPU supported all common data types except bool and complex type,</span>
<span class="sd">        but GPU only supports float16, float32 currently.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input to be computed.</span>
<span class="sd">        k (int): The number of top or bottom elements to be computed along the last dimension, constant input is needed.</span>
<span class="sd">        dim (int, optional): The dimension to sort along. Default: ``None`` .</span>
<span class="sd">        largest (bool, optional): If largest is ``False``  then the k smallest elements are returned.</span>
<span class="sd">            Default: ``True`` .</span>
<span class="sd">        sorted (bool, optional): If ``True`` , the obtained elements will be sorted by the values in descending order.</span>
<span class="sd">            If ``False`` , the obtained elements will not be sorted. Default: ``True`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple consisting of `values` and `indexes`.</span>

<span class="sd">        - values (Tensor): The `k` largest or smallest elements in each slice of the given dimension.</span>
<span class="sd">        - indices (Tensor): The indices of values within the last dimension of input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sorted` is not a bool.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `k` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor([[0.5368, 0.2447, 0.4302, 0.9673],</span>
<span class="sd">        ...                [0.4388, 0.6525, 0.4685, 0.1868],</span>
<span class="sd">        ...                [0.3563, 0.5152, 0.9675, 0.8230]], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.topk(x, 2, dim=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[3, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 9.67299998e-01,  5.36800027e-01],</span>
<span class="sd">         [ 6.52499974e-01,  4.68499988e-01],</span>
<span class="sd">         [ 9.67499971e-01,  8.23000014e-01]]), Tensor(shape=[3, 2], dtype=Int32, value=</span>
<span class="sd">        [[3, 0],</span>
<span class="sd">         [1, 2],</span>
<span class="sd">         [2, 3]]))</span>
<span class="sd">        &gt;&gt;&gt; output2 = ops.topk(x, 2, dim=1, largest=False)</span>
<span class="sd">        &gt;&gt;&gt; print(output2)</span>
<span class="sd">        (Tensor(shape=[3, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 2.44700000e-01,  4.30200011e-01],</span>
<span class="sd">         [ 1.86800003e-01,  4.38800007e-01],</span>
<span class="sd">         [ 3.56299996e-01,  5.15200019e-01]]), Tensor(shape=[3, 2], dtype=Int32, value=</span>
<span class="sd">        [[1, 2],</span>
<span class="sd">         [3, 0],</span>
<span class="sd">         [0, 1]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">top_k_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">TopK</span><span class="p">)(</span><span class="nb">sorted</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">largest</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="o">-</span><span class="nb">input</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">dim</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">largest</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">top_k_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
            <span class="n">values</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="o">-</span><span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">values</span><span class="p">,</span> <span class="n">indices</span>
        <span class="k">return</span> <span class="n">top_k_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">top_k_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">largest</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span></div>


<span class="k">def</span> <span class="nf">expand</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor where the dimension of size is expanded to a larger size.</span>

<span class="sd">    Note:</span>
<span class="sd">        - If the `size` for a dimension is -1, it means no change for the size of that dimension.</span>
<span class="sd">        - When a Tensor is expanded to a larger number of dimensions, the new ones will be appended at</span>
<span class="sd">          the front, and for the new dimensions, the `size` can not be -1.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): A Tensor to be expanded.</span>
<span class="sd">        size (Tensor): The expanded shape of `input_x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        y (Tensor) - Tensor after expansion whose shape is `size`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `size` is not Tensor.</span>
<span class="sd">        TypeError: If the type of `size` is not one of the following dtype: int16, int32, int64.</span>
<span class="sd">        ValueError: If the size of `size` is less than the size of `input_x.shape`.</span>
<span class="sd">        ValueError: If `size` is not a 1-D tensor.</span>
<span class="sd">        ValueError: If the expanded `size` is not equal to the existing shape of `input_x` at a dimension</span>
<span class="sd">            that is not 1.</span>
<span class="sd">        ValueError: If the expanded `size` &lt; 0 and it is in a leading position, corresponding to</span>
<span class="sd">            a non-existing dimension in `input_x`.</span>
<span class="sd">        ValueError: If the number of elements of output is more than 1000000.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[2], [3], [4]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; size = Tensor(np.array([3,4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.expand(input_x, size)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[2. 2. 2. 2.]</span>
<span class="sd">         [3. 3. 3. 3.]</span>
<span class="sd">         [4. 4. 4. 4.]]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(2, mindspore.int16)</span>
<span class="sd">        &gt;&gt;&gt; size = Tensor(np.array([1, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.expand(input_x, size)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">expand_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Expand</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">expand_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_fold_param</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the parameters of fold op.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="s1">&#39;fold&#39;</span><span class="p">)</span>
    <span class="n">param</span> <span class="o">=</span> <span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">param</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="s1">&#39;fold&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">param_name</span> <span class="o">==</span> <span class="s2">&quot;padding&quot;</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int_sequence</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="s1">&#39;fold&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int_sequence</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="s1">&#39;fold&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">param</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_fold_input</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the rank of fold&#39;s input.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor_</span><span class="p">))</span> <span class="ow">or</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For array function &#39;fold&#39;, &#39;input&#39; must be a 3-D tensor.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="fold"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fold.html#mindspore.ops.fold">[文档]</a><span class="k">def</span> <span class="nf">fold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Combines an array of sliding local blocks into a large containing tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The input must be a 3-dimensional Tensor with shape :math:`(N, C \times H, W)` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): 3-D Tensor, supported dtypes: float16, float32, float64, complex64 and complex128.</span>
<span class="sd">        output_size (Tensor): 1D tensor with `2` elements of data type int.</span>
<span class="sd">        kernel_size (Union[int, tuple[int], list[int]]): The size of the kernel, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Must be specified.</span>
<span class="sd">        dilation (Union[int, tuple[int], list[int]], optional): The size of the dilation, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: ``1`` .</span>
<span class="sd">        padding (Union[int, tuple[int], list[int]], optional): The size of the padding, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: ``0`` .</span>
<span class="sd">        stride (Union[int, tuple[int], list[int]], optional): The size of the stride, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, with same type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size`, `dilation`, `padding`, `stride` data type is not int, tuple or list.</span>
<span class="sd">        ValueError: If `kernel_size`, `dilation`, `stride` value is not</span>
<span class="sd">            greater than zero or elements number more than `2`.</span>
<span class="sd">        ValueError: If `padding` value is less than zero or elements number more than `2`.</span>
<span class="sd">        ValueError: If `input.shape[1] != kernel_size[0] * kernel_size[1]`</span>
<span class="sd">        ValueError: If `input.shape[2]` does not match the calculated number of sliding blocks.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(input_data=np.random.rand(16, 64, 25), dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_size = Tensor(input_data=[8, 8], dtype=mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fold(x, output_size, [2, 2], [2, 2], [2, 2], [2, 2])</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 16, 8, 8)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_fold_input</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_fold_param</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">)</span>
    <span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_fold_param</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="s2">&quot;dilation&quot;</span><span class="p">)</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_check_fold_param</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s2">&quot;padding&quot;</span><span class="p">)</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_check_fold_param</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">)</span>
    <span class="n">fold_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Col2Im</span><span class="p">)(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">r_shape</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">+</span> <span class="n">input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">r_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fold_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_unfold_params</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="n">param_size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the parameters of unfold op.&quot;&quot;&quot;</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">],</span> <span class="s1">&#39;unfold&#39;</span><span class="p">)</span>
    <span class="n">param</span> <span class="o">=</span> <span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">param</span>
    <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">param_name</span> <span class="o">+</span> <span class="s2">&quot; size&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">param_size</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span> <span class="s1">&#39;unfold&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">param_name</span> <span class="o">==</span> <span class="s2">&quot;padding&quot;</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int_sequence</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="s1">&#39;unfold&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int_sequence</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_name</span><span class="p">,</span> <span class="s1">&#39;unfold&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">param</span>


<div class="viewcode-block" id="unfold"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unfold.html#mindspore.ops.unfold">[文档]</a><span class="k">def</span> <span class="nf">unfold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts sliding local blocks from a batched input tensor.</span>

<span class="sd">    Consider a batched input tensor of shape :math:`(N, C, *)`,</span>
<span class="sd">    where :math:`N` is the batch dimension, :math:`C` is the channel dimension,</span>
<span class="sd">    and :math:`*` represent arbitrary spatial dimensions. This operation flattens</span>
<span class="sd">    each sliding `Kernel_size`- sized block within the spatial dimensions</span>
<span class="sd">    of input `x` into a column (i.e., last dimension) of a 3-D output</span>
<span class="sd">    tensor of shape :math:`(N, C \times \prod(\text{kernel_size}), L)`, where</span>
<span class="sd">    :math:`C \times \prod(\text{kernel_size})` is the total number of values</span>
<span class="sd">    within each block (a block has :math:`\prod(\text{kernel_size})` spatial</span>
<span class="sd">    locations each containing a `C`-channeled vector), and :math:`L` is</span>
<span class="sd">    the total number of such blocks:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \prod_d \left\lfloor\frac{\text{spatial_size}[d] + 2 \times \text{pads}[d] %</span>
<span class="sd">            - \text{dilations}[d] \times (\text{kernel_size}[d] - 1) - 1}{\text{strides}[d]} + 1\right\rfloor,</span>

<span class="sd">    where :math:`\text{spatial_size}` is formed by the spatial dimensions</span>
<span class="sd">    of input `x` (:math:`*` above), and :math:`d` is over all spatial</span>
<span class="sd">    dimensions.</span>

<span class="sd">    Therefore, indexing `output` at the last dimension (column dimension)</span>
<span class="sd">    gives all values within a certain block.</span>

<span class="sd">    The `dilation`, `padding` and `stride` arguments specify</span>
<span class="sd">    how the sliding blocks are retrieved.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The output is a 3-dimensional Tensor whose shape is</span>
<span class="sd">          :math:`(N, C \times \prod(\text{kernel_size}), L)` .</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): 4-D Tensor, supported dtypes: float16, float32, float64, complex64 and complex128.</span>
<span class="sd">        kernel_size (Union[int, tuple[int], list[int]]): The size of the kernel, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Must be specified.</span>
<span class="sd">        dilation (Union[int, tuple[int], list[int]], optional): The dilation of the window, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: ``1`` .</span>
<span class="sd">        padding (Union[int, tuple[int], list[int]], optional): The pad of the window, that must be</span>
<span class="sd">            a tuple/list of one or two `int` for height and width.</span>
<span class="sd">            If one int, pad_height = pad_width.</span>
<span class="sd">            If two int, pad_height = padding[0], pad_width = padding[1].</span>
<span class="sd">            Default: ``0`` .</span>
<span class="sd">        stride (Union[int, tuple[int], list[int]], optional): The stride of the window, should be two int</span>
<span class="sd">            for height and width. If type is int, it means that height equal with width. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, with same type as `input` . And its shape is as described above.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If any data type of `kernel_size`, `stride`, `dilation`, `kernel_size` is not int, tuple or list.</span>
<span class="sd">        ValueError: If `kernel_size`, `dilation`, `stride` value is not</span>
<span class="sd">            greater than zero or elements number more than `2`.</span>
<span class="sd">        ValueError: If `padding` value is less than zero.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(4, 4, 32, 32), mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unfold(x, kernel_size=3, dilation=1, stride=1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 36, 900)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_unfold_params</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_unfold_params</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_check_unfold_params</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s2">&quot;padding&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">stride</span> <span class="o">=</span> <span class="n">_check_unfold_params</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">unfold_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Im2Col</span><span class="p">)(</span><span class="n">ksizes</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                                        <span class="n">strides</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                                        <span class="n">dilations</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                                        <span class="n">pads</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">unfold_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">tmp_shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">tmp_shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span> <span class="o">+</span> <span class="n">tmp_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_diagonal_axes</span><span class="p">(</span><span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">,</span> <span class="n">x_ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check the parameters of unfold op.&quot;&quot;&quot;</span>
    <span class="n">axes</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_axis_valid</span><span class="p">((</span><span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">),</span> <span class="n">x_ndim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">axes</span>


<div class="viewcode-block" id="diagonal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.diagonal.html#mindspore.ops.diagonal">[文档]</a><span class="k">def</span> <span class="nf">diagonal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns specified diagonals of `input`.</span>

<span class="sd">    If `input` is 2-D, returns the diagonal of `input` with the given offset.</span>
<span class="sd">    If `input` has more than two</span>
<span class="sd">    dimensions, then the axes specified by `dim1` and `dim2` are used to determine</span>
<span class="sd">    the 2-D sub-array whose diagonal is returned. In this case, remove the `dim1` and `dim2` dimensions of `input`</span>
<span class="sd">    and insert the last dimension of `input` by the diagonal elements determined by `dim1` and `dim2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Array from which the diagonals are taken.</span>
<span class="sd">        offset (int, optional): Offset of the diagonal from the main diagonal.</span>
<span class="sd">            Can be positive or negative. Default: ``0`` .</span>
<span class="sd">        dim1 (int, optional): Axis to be used as the first axis of the 2-D</span>
<span class="sd">            sub-arrays from which the diagonals should be taken. Defaults to</span>
<span class="sd">            first axis (0). Default: ``0`` .</span>
<span class="sd">        dim2 (int, optional): Axis to be used as the second axis of the 2-D</span>
<span class="sd">            sub-arrays from which the diagonals should be taken. Defaults to</span>
<span class="sd">            second axis (1). Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, if `input` is 2-D, then `input` 1-D array containing the diagonal. If</span>
<span class="sd">        ``input.ndim &gt; 2``, then the dimensions specified by `dim1` and `dim2` are removed,</span>
<span class="sd">        and a new axis inserted at the end corresponding to the diagonal.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if `dim1` or `dim2` are not an int.</span>
<span class="sd">        ValueError: if the input tensor has less than two dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[0, 1], [2, 3]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.diagonal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x_ndim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span>
    <span class="k">if</span> <span class="n">x_ndim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;ops.diagonal requires an array of at least two dimensions&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;dim1&quot;</span><span class="p">,</span> <span class="n">dim1</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;diagonal&quot;</span><span class="p">)</span>
    <span class="n">_check_attr_dtype</span><span class="p">(</span><span class="s2">&quot;dim2&quot;</span><span class="p">,</span> <span class="n">dim2</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="s2">&quot;diagonal&quot;</span><span class="p">)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dtype</span>

    <span class="n">axes</span> <span class="o">=</span> <span class="n">_check_diagonal_axes</span><span class="p">(</span><span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">,</span> <span class="n">x_ndim</span><span class="p">)</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ms_arrange</span><span class="p">(</span><span class="n">x_ndim</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
            <span class="n">perm</span> <span class="o">+=</span> <span class="p">(</span><span class="n">i</span><span class="p">,)</span>
    <span class="n">perm</span> <span class="o">+=</span> <span class="n">axes</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span>

    <span class="n">x_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>

    <span class="n">fill_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Fill</span><span class="p">)()</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Eye</span><span class="p">)()(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">offset</span> <span class="o">&gt;=</span> <span class="n">m</span> <span class="ow">or</span> <span class="n">offset</span> <span class="o">&lt;=</span> <span class="o">-</span><span class="n">n</span><span class="p">:</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">fill_op</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">offset</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">e_left</span> <span class="o">=</span> <span class="n">fill_op</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">offset</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">e_right</span> <span class="o">=</span> <span class="n">e</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">m</span> <span class="o">-</span> <span class="n">offset</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">e</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">)(</span><span class="mi">1</span><span class="p">)((</span><span class="n">e_left</span><span class="p">,</span> <span class="n">e_right</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">offset</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">e_upper</span> <span class="o">=</span> <span class="n">fill_op</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="n">offset</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">e_lower</span> <span class="o">=</span> <span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">n</span> <span class="o">+</span> <span class="n">offset</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
            <span class="n">e</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">)(</span><span class="mi">0</span><span class="p">)((</span><span class="n">e_upper</span><span class="p">,</span> <span class="n">e_lower</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">)</span>

    <span class="n">prod_val</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Mul</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">)()(</span><span class="n">prod_val</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">begin</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">ms_arrange</span><span class="p">(</span><span class="n">x_ndim</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">begin</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span>
    <span class="n">last_dim_begin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">offset</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="n">begin</span> <span class="o">+=</span> <span class="p">(</span><span class="n">last_dim_begin</span><span class="p">,)</span>
    <span class="n">res_size</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">last_dim_end</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">offset</span><span class="p">)))))</span> <span class="o">-</span> <span class="n">last_dim_begin</span>
    <span class="k">if</span> <span class="n">last_dim_end</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">([])</span>
    <span class="n">res_size</span> <span class="o">+=</span> <span class="p">(</span><span class="n">last_dim_end</span><span class="p">,)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Slice</span><span class="p">)()(</span><span class="n">res</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">res_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_check_is_tensor</span><span class="p">(</span><span class="n">param_name</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">cls_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns True if input is Tensor.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For </span><span class="si">{</span><span class="n">cls_name</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">param_name</span><span class="si">}</span><span class="s2"> must be a Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_diagonal_scatter_shape</span><span class="p">(</span><span class="n">diag_shape</span><span class="p">,</span> <span class="n">src_shape</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">diag_shape</span> <span class="o">!=</span> <span class="n">src_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For diagonal_scatter, the shape of src should equal to the shape of input diagonal,&quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got src.shape </span><span class="si">{</span><span class="n">src_shape</span><span class="si">}</span><span class="s2"> and diagonal shape </span><span class="si">{</span><span class="n">diag_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="diagonal_scatter"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.diagonal_scatter.html#mindspore.ops.diagonal_scatter">[文档]</a><span class="k">def</span> <span class="nf">diagonal_scatter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `dim1` and `dim2` specify the two dimensions of `input`,</span>
<span class="sd">    the elements in these two dimensions will be treated as elements of a matrix,</span>
<span class="sd">    and `src` is embedded on the diagonal of the matrix.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor, whose dimension is larger than 1.</span>
<span class="sd">        src (Tensor): The source Tensor to embed.</span>
<span class="sd">        offset (int, optional): `offset` controls which diagonal to choose. Default: ``0`` .</span>

<span class="sd">            - When `offset` is zero, the diagonal chosen is the main diagonal.</span>
<span class="sd">            - When `offset` is a positive integer, the diagonal chosen is up the main diagonal.</span>
<span class="sd">            - When `offset` is a negative integer, the diagonal chosen is down the main diagonal.</span>

<span class="sd">        dim1 (int, optional): Axis to be used as the first axis of the 2-D</span>
<span class="sd">            sub-arrays from which the diagonals should be taken. Default: ``0`` .</span>
<span class="sd">        dim2 (int, optional): Axis to be used as the second axis of the 2-D</span>
<span class="sd">            sub-arrays from which the diagonals should be taken. Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor after embedding, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `src` is not a Tensor.</span>
<span class="sd">        TypeError: If `offset` , `dim1` or `dim2` is not an integer.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; input = ms.ops.zeros((3,3))</span>
<span class="sd">        &gt;&gt;&gt; src = ms.ops.ones(2)</span>
<span class="sd">        &gt;&gt;&gt; out = ms.ops.diagonal_scatter(input, src, 1, dim1=1, dim2=0)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[0. 0. 0.]</span>
<span class="sd">         [1. 0. 0.]</span>
<span class="sd">         [0. 1. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">&quot;diagonal_scatter&quot;</span><span class="p">)</span>
    <span class="n">_check_is_tensor</span><span class="p">(</span><span class="s2">&quot;src&quot;</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="s2">&quot;diagonal_scatter&quot;</span><span class="p">)</span>
    <span class="n">_check_is_int</span><span class="p">(</span><span class="n">offset</span><span class="p">,</span> <span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="s2">&quot;diagonal_scatter&quot;</span><span class="p">)</span>
    <span class="n">_check_is_int</span><span class="p">(</span><span class="n">dim1</span><span class="p">,</span> <span class="s2">&quot;dim1&quot;</span><span class="p">,</span> <span class="s2">&quot;diagonal_scatter&quot;</span><span class="p">)</span>
    <span class="n">_check_is_int</span><span class="p">(</span><span class="n">dim2</span><span class="p">,</span> <span class="s2">&quot;dim2&quot;</span><span class="p">,</span> <span class="s2">&quot;diagonal_scatter&quot;</span><span class="p">)</span>
    <span class="n">input_diag</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">diagonal</span><span class="p">(</span><span class="n">offset</span><span class="p">,</span> <span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">)</span>
    <span class="n">_check_diagonal_scatter_shape</span><span class="p">(</span><span class="n">input_diag</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">embed</span> <span class="o">=</span> <span class="n">ones_like</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
    <span class="n">embed</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">embed</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">)</span>
    <span class="n">embed</span> <span class="o">=</span> <span class="nb">input</span> <span class="o">*</span> <span class="n">embed</span>
    <span class="n">src</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">+</span> <span class="n">src</span> <span class="o">-</span> <span class="n">embed</span></div>


<span class="k">def</span> <span class="nf">lstsq</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the solutions of the least squares and minimum norm problems of full-rank</span>
<span class="sd">    matrix `x` of size :math:`(m \times n)` and matrix `a` of size :math:`(m \times k)`.</span>

<span class="sd">    If :math:`m \geq n`, `lstsq` solves the least-squares problem:</span>

<span class="sd">    .. math::</span>

<span class="sd">       \begin{array}{ll}</span>
<span class="sd">       \min_y &amp; \|xy-a\|_2.</span>
<span class="sd">       \end{array}</span>

<span class="sd">    If :math:`m &lt; n`, `lstsq` solves the least-norm problem:</span>

<span class="sd">    .. math::</span>

<span class="sd">       \begin{array}{llll}</span>
<span class="sd">       \min_y &amp; \|y\|_2 &amp; \text{subject to} &amp; xy = a.</span>
<span class="sd">       \end{array}</span>

<span class="sd">    where `y` is the returned tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The :math:`(m \times n)` matrix equivalent to :math:`x` in above.</span>
<span class="sd">            The input tensor whose data type is float16, float32 or float64.</span>
<span class="sd">        A (Tensor): The :math:`(m \times k)` matrix equivalent to :math:`a` in above.</span>
<span class="sd">            The input tensor whose data type is float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the least squares or minimum norm problems solution, which has shape :math:`(n \times k)`.</span>
<span class="sd">        The data type is the same with `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `A` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` or `A` is not one of: float16, float32, float64.</span>
<span class="sd">        TypeError: If the dtypes of `input` and `A` are not the same.</span>
<span class="sd">        ValueError: If the dimension of `input` is not equal to 2.</span>
<span class="sd">        ValueError: If the dimension of `A` is not equal to 2 or 1.</span>
<span class="sd">        ValueError: If the length of input_dims[0] is not equal to the length of A_dims[0].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[2,1,5],[3,5,1],[1,1,1]]),mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([[10,5],[15,8],[7,4]]),mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.lstsq(x, a)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[17.000002  11.000002 ]</span>
<span class="sd">         [-6.5000005 -4.500001 ]</span>
<span class="sd">         [-3.500002  -2.5000017]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lstsq_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Lstsq</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">lstsq_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>


<div class="viewcode-block" id="mvlgamma"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mvlgamma.html#mindspore.ops.mvlgamma">[文档]</a><span class="k">def</span> <span class="nf">mvlgamma</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the results of the multivariate log-gamma function with dimension `p` element-wise.</span>

<span class="sd">    The mathematical calculation process of Mvlgamma is shown as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \log (\Gamma_{p}(input))=C+\sum_{i=1}^{p} \log (\Gamma(input-\frac{i-1}{2}))</span>

<span class="sd">    where :math:`C = \log(\pi) \times \frac{p(p-1)}{4}` and :math:`\Gamma(\cdot)` is the Gamma function.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of the multivariate log-gamma function,</span>
<span class="sd">          which must be one of the following types: float32, float64.</span>
<span class="sd">          The shape is :math:`(N,*)`, where :math:`*` means any number of additional dimensions.</span>
<span class="sd">          And the value of any element in `input` must be greater than :math:`(p - 1) / 2`.</span>
<span class="sd">        p (int): The number of dimensions. And the value of `p` must be greater than or equal to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is neither float32 nor float64.</span>
<span class="sd">        TypeError: If `p` is not an int.</span>
<span class="sd">        ValueError: If `p` is less than 1.</span>
<span class="sd">        ValueError: If not all elements of `input` are greater than :math:`(p - 1) / 2`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 5], [4, 2, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.mvlgamma(x, p=3)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[2.694925 5.402975 9.140645]</span>
<span class="sd">         [5.402975 1.596312 13.64045]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mvlgamma_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Mvlgamma</span><span class="p">)(</span><span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mvlgamma_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="argwhere"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.argwhere.html#mindspore.ops.argwhere">[文档]</a><span class="k">def</span> <span class="nf">argwhere</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a Tensor of the positions of all non-zero values.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. The data type is Number or Bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a 2-D Tensor whose data type is int64, containing the positions of all non-zero values of the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not Tensor.</span>
<span class="sd">        ValueError: If dim of `input` equals to 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1,  0], [-5, 0]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.argwhere(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 0]</span>
<span class="sd">         [0 1 0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nonzero_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="column_stack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.column_stack.html#mindspore.ops.column_stack">[文档]</a><span class="k">def</span> <span class="nf">column_stack</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks 1-D tensors as columns into a 2-D tensor. Tensors of other dimension are stacked as-is,</span>
<span class="sd">    like :func:`mindspore.ops.hstack`.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (Union[tuple[Tensor], list[Tensor]]): A sequence of tensors. All</span>
<span class="sd">            of them must have the same shape except the axis to be concatenated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        2-D Tensor, formed by stacking the given tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tensors` is not list or tuple.</span>
<span class="sd">        TypeError: If element in `tensors` is not Tensor.</span>
<span class="sd">        ValueError: If `tensors` is empty.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor([1, 1, 1])</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor([2, 2, 2])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.column_stack((x1, x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 2]</span>
<span class="sd">         [1 2]</span>
<span class="sd">         [1 2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For column_stack, the input must be list or tuple of tensors, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">trans_x</span> <span class="o">=</span> <span class="p">()</span>
    <span class="n">_expand_dims</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For column_stack, the input element must be tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">_expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">_expand_dims</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">trans_x</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">trans_x</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For column_stack, the input must have at least 1 tensor, but got 0.&quot;</span><span class="p">)</span>
    <span class="n">_concat</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">)(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_concat</span><span class="p">(</span><span class="n">trans_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="hstack"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.hstack.html#mindspore.ops.hstack">[文档]</a><span class="k">def</span> <span class="nf">hstack</span><span class="p">(</span><span class="n">tensors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Stacks tensors in sequence horizontally.</span>
<span class="sd">    This is equivalent to concatenation along the second axis, except for 1-D tensors</span>
<span class="sd">    where it concatenates along the first axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (Union[tuple[Tensor], list[Tensor]]): A sequence of tensors. The</span>
<span class="sd">            tensors must have the same shape along all but the second axis, except</span>
<span class="sd">            1-D tensors which can be any length.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Stacked Tensor, formed by stacking the given tensors.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `tensors` is not list or tuple.</span>
<span class="sd">        TypeError: If element in `tensors` is not Tensor.</span>
<span class="sd">        ValueError: If `tensors` is empty.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor([1, 1, 1])</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor([2, 2, 2])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hstack((x1, x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 1. 1. 2. 2. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For hstack, the input must be list or tuple, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="n">tuple_of_tensor</span> <span class="o">=</span> <span class="p">()</span>
    <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">tensors</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For hstack, the input element must be tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">tensor</span> <span class="o">=</span> <span class="n">expand_dims_</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">tuple_of_tensor</span> <span class="o">+=</span> <span class="p">(</span><span class="n">tensor</span><span class="p">,)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">tuple_of_tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For hstack, the input must have at least 1 tensor, but got 0.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">tuple_of_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">_concat</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">)(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_concat</span><span class="p">(</span><span class="n">tuple_of_tensor</span><span class="p">)</span>
    <span class="n">_concat</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Concat</span><span class="p">)(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_concat</span><span class="p">(</span><span class="n">tuple_of_tensor</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_axis_valid</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks axis are valid given ndim, and returns axis that can be passed</span>
<span class="sd">    to the built-in operator (non-negative, int or tuple).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="n">ndim</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">axis</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">)):</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">_check_check_axis_in_range</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ndim</span><span class="p">),</span> <span class="n">axis</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">axis</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_check_check_axis_in_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">ndim</span><span class="p">),)</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_get_moved_perm</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function for movedim, returns permutation after moving axis</span>
<span class="sd">    from source to destination.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dest_sorted_idx</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">destination</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="n">operator</span><span class="o">.</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">1</span><span class="p">))]</span>
    <span class="n">axis_orig</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">builtins</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">source</span><span class="p">]</span>

    <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">m</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dest_sorted_idx</span><span class="p">:</span>
        <span class="c1"># inserts an axis that has been moved, denoted by n, and axis that remain</span>
        <span class="c1"># in their original position, indexed from k to k + n - m, into index m in</span>
        <span class="c1"># the list of permuted axis</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">destination</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span>
        <span class="n">perm</span> <span class="o">+=</span> <span class="n">axis_orig</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">j</span><span class="p">]</span>
        <span class="n">perm</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">source</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">k</span> <span class="o">+=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">m</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">perm</span> <span class="o">+=</span> <span class="n">axis_orig</span><span class="p">[</span><span class="n">k</span><span class="p">:]</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span>


<div class="viewcode-block" id="movedim"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.movedim.html#mindspore.ops.movedim">[文档]</a><span class="k">def</span> <span class="nf">movedim</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Moves axis of an array from source to destination.</span>

<span class="sd">    Other axis remain in their original order.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The tensor array whose axis should be reordered.</span>
<span class="sd">            The dimension of `x` must not be 0.</span>
<span class="sd">        source (Union[int, sequence[int]]): Original positions of the</span>
<span class="sd">            axis to move. The length of `source` and `destination` must be the same.</span>
<span class="sd">        destination (Union[int, sequence[int]]): Destination positions</span>
<span class="sd">            for each of the original axis. The length of `source` and `destination` must be the same.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, array with moved axis.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If axis are out of the range of `[-x.ndim, x.ndim)`, or</span>
<span class="sd">            if the axis contain duplicates.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case1 : moving single axis</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.zeros((3, 4, 5)))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.movedim(x, 0, -1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 5, 3)</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : moving multiple axes</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.zeros((3, 4, 5)))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.movedim(x, (0, 2), (1, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 3, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="n">destination</span> <span class="o">=</span> <span class="n">_check_axis_valid</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">source</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">destination</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;For `source` and `destination` arguments, the number of elements must be the same, but got &#39;source&#39;:&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">source</span><span class="p">)</span><span class="si">}</span><span class="s2"> and &#39;destination&#39;: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">destination</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">_get_moved_perm</span><span class="p">(</span><span class="n">ndim</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">)()(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span></div>


<div class="viewcode-block" id="moveaxis"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.moveaxis.html#mindspore.ops.moveaxis">[文档]</a><span class="k">def</span> <span class="nf">moveaxis</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for `ops.movedim`. Moves axis of an array from source to destination.</span>

<span class="sd">    Refer to :func:`mindspore.ops.movedim` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.zeros((3, 4, 5)))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.moveaxis(x, 0, -1)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 5, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">movedim</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">destination</span><span class="p">)</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_swapaxes_axis</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">ndim</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_swapaxes_axis</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">ndim</span><span class="p">)</span>


<div class="viewcode-block" id="swapaxes"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.swapaxes.html#mindspore.ops.swapaxes">[文档]</a><span class="k">def</span> <span class="nf">swapaxes</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis0</span><span class="p">,</span> <span class="n">axis1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Interchange two axes of a tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input(Tensor): Input tensor.</span>
<span class="sd">        axis0 (int): First axis.</span>
<span class="sd">        axis1 (int): Second axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Transposed tensor, has the same data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `input` is not Tensor.</span>
<span class="sd">        TypeError: If `axis0` or `axis1` is not integer.</span>
<span class="sd">        ValueError: If `axis0` or `axis1` is not in the range of :math:`[-ndim, ndim-1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones((2,3,4), dtype=np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.swapaxes(input, 0, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 3, 2)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For ops.swapaxes, parameter `input` must be Tensor, but got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">axis0</span><span class="p">,</span> <span class="n">axis1</span> <span class="o">=</span> <span class="n">_check_swapaxes_axis</span><span class="p">((</span><span class="n">axis0</span><span class="p">,</span> <span class="n">axis1</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis0</span> <span class="o">==</span> <span class="n">axis1</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">input</span>
    <span class="k">if</span> <span class="n">axis0</span> <span class="o">&gt;</span> <span class="n">axis1</span><span class="p">:</span>
        <span class="n">axis0</span><span class="p">,</span> <span class="n">axis1</span> <span class="o">=</span> <span class="n">axis1</span><span class="p">,</span> <span class="n">axis0</span>

    <span class="n">perm</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">make_range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">axis1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="nb">input</span><span class="o">.</span><span class="n">ndim</span><span class="p">:</span>
        <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis0</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span><span class="p">:</span><span class="n">axis1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                   <span class="n">perm</span><span class="p">[</span><span class="n">axis0</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">axis1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis0</span><span class="p">:</span><span class="n">axis0</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">new_perm</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">axis0</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis1</span><span class="p">:</span><span class="n">axis1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
                   <span class="n">perm</span><span class="p">[</span><span class="n">axis0</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span><span class="n">axis1</span><span class="p">]</span> <span class="o">+</span> <span class="n">perm</span><span class="p">[</span><span class="n">axis0</span><span class="p">:</span><span class="n">axis0</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Transpose</span><span class="p">)()(</span><span class="nb">input</span><span class="p">,</span> <span class="n">new_perm</span><span class="p">)</span></div>


<div class="viewcode-block" id="swapdims"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.swapdims.html#mindspore.ops.swapdims">[文档]</a><span class="k">def</span> <span class="nf">swapdims</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim0</span><span class="p">,</span> <span class="n">dim1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Interchange two dims of a tensor.</span>
<span class="sd">    This function is equivalent to :func:`mindspore.ops.swapaxes` function.</span>

<span class="sd">    Args:</span>
<span class="sd">        input(Tensor): Input tensor.</span>
<span class="sd">        dim0 (int): First dim.</span>
<span class="sd">        dim1 (int): Second dim.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Transposed tensor, has the same data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If argument `input` is not Tensor.</span>
<span class="sd">        TypeError: If `dim0` or `dim1` is not integer.</span>
<span class="sd">        ValueError: If `dim0` or `dim1` is not in the range of :math:`[-ndim, ndim-1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones((2,3,4), dtype=np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.swapdims(input, 0, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 3, 2)</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim0</span><span class="p">,</span> <span class="n">dim1</span><span class="p">)</span></div>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_is_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">op_name</span><span class="p">):</span>
    <span class="n">arg_value</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">op_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">arg_value</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_positive_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">op_name</span><span class="p">):</span>
    <span class="n">arg_value</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">op_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">arg_value</span>


<span class="nd">@constexpr</span>
<span class="k">def</span> <span class="nf">_check_axis_range</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">op_name</span><span class="p">):</span>
    <span class="n">arg_value</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="o">-</span><span class="n">limit</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">op_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">arg_value</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_cal_repeat_dims</span><span class="p">(</span><span class="n">x_rank</span><span class="p">,</span> <span class="n">rep</span><span class="p">,</span> <span class="n">expand_axis</span><span class="p">):</span>
    <span class="n">rep_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">rep_dims</span><span class="p">[</span><span class="n">expand_axis</span><span class="p">]</span> <span class="o">=</span> <span class="n">rep</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">rep_dims</span><span class="p">)</span>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_cal_reshape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">rep</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
    <span class="n">x_reshape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span>
    <span class="n">x_reshape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="o">*=</span> <span class="n">rep</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">x_reshape</span><span class="p">)</span>


<div class="viewcode-block" id="repeat_interleave"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.repeat_interleave.html#mindspore.ops.repeat_interleave">[文档]</a><span class="k">def</span> <span class="nf">repeat_interleave</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Repeat elements of a tensor along an axis, like `numpy.repeat`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The tensor to repeat values for. Must be of type: float16,</span>
<span class="sd">            float32, int8, uint8, int16, int32, or int64.</span>
<span class="sd">        repeats (Union[int, tuple, list, Tensor]): The number of times to repeat, must be positive.</span>
<span class="sd">        axis (int, optional): The axis along which to repeat, Default: ``None``. if dims is None,</span>
<span class="sd">            the input Tensor will be flattened and the output will alse be flattened.</span>

<span class="sd">    Returns:</span>
<span class="sd">        One tensor with values repeated along the specified axis. If input has shape</span>
<span class="sd">        :math:`(s1, s2, ..., sn)` and axis is i, the output will have shape :math:`(s1, s2, ...,</span>
<span class="sd">        si * repeats, ..., sn)`. The output type will be the same as the type of `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[0, 1, 2], [3, 4, 5]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.repeat_interleave(input, repeats=2, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 1 2]</span>
<span class="sd">         [0 1 2]</span>
<span class="sd">         [3 4 5]</span>
<span class="sd">         [3 4 5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">axis</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">repeats</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">repeats</span> <span class="o">=</span> <span class="n">TensorToList</span><span class="p">()(</span><span class="n">repeats</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">repeats</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="repeat_elements"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.repeat_elements.html#mindspore.ops.repeat_elements">[文档]</a><span class="k">def</span> <span class="nf">repeat_elements</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rep</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Repeat elements of a tensor along an axis, like `np.repeat` .</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The tensor to repeat values for. Must be of type: float16,</span>
<span class="sd">            float32, int8, uint8, int16, int32, or int64.</span>
<span class="sd">        rep (int): The number of times to repeat, must be positive.</span>
<span class="sd">        axis (int): The axis along which to repeat. Default: 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        One tensor with values repeated along the specified axis. If x has shape</span>
<span class="sd">        :math:`(s1, s2, ..., sn)` and axis is i, the output will have shape :math:`(s1, s2, ..., si * rep, ..., sn)`.</span>
<span class="sd">        The output type will be the same as the type of `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : repeat on axis 0</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0, 1, 2], [3, 4, 5]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.repeat_elements(x, rep = 2, axis = 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 1 2]</span>
<span class="sd">         [0 1 2]</span>
<span class="sd">         [3 4 5]</span>
<span class="sd">         [3 4 5]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : repeat on axis 1</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0, 1, 2], [3, 4, 5]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.repeat_elements(x, rep = 2, axis = 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 1 1 2 2]</span>
<span class="sd">         [3 3 4 4 5 5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">const_utils</span><span class="o">.</span><span class="n">check_type_valid</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="s1">&#39;input x&#39;</span><span class="p">)</span>
    <span class="n">rep</span> <span class="o">=</span> <span class="n">_check_positive_int</span><span class="p">(</span><span class="n">rep</span><span class="p">,</span> <span class="s2">&quot;rep&quot;</span><span class="p">,</span> <span class="s2">&quot;repeat_elements&quot;</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_is_int</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="s2">&quot;repeat_elements&quot;</span><span class="p">)</span>
    <span class="n">shape_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>
    <span class="n">rank_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Rank</span><span class="p">()</span>
    <span class="n">tile_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Tile</span><span class="p">()</span>
    <span class="n">expand_dims_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
    <span class="n">x_rank</span> <span class="o">=</span> <span class="n">rank_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">axis</span> <span class="o">=</span> <span class="n">_check_axis_range</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x_rank</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="s2">&quot;repeat_elements&quot;</span><span class="p">)</span>
    <span class="n">expand_axis</span> <span class="o">=</span> <span class="n">axis</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">x_expand</span> <span class="o">=</span> <span class="n">expand_dims_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">expand_axis</span><span class="p">)</span>
    <span class="n">rep_dims</span> <span class="o">=</span> <span class="n">_cal_repeat_dims</span><span class="p">(</span><span class="n">x_rank</span><span class="p">,</span> <span class="n">rep</span><span class="p">,</span> <span class="n">expand_axis</span><span class="p">)</span>
    <span class="n">x_expand</span> <span class="o">=</span> <span class="n">tile_op</span><span class="p">(</span><span class="n">x_expand</span><span class="p">,</span> <span class="n">rep_dims</span><span class="p">)</span>
    <span class="n">x_shape</span> <span class="o">=</span> <span class="n">shape_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_reshape</span> <span class="o">=</span> <span class="n">_cal_reshape</span><span class="p">(</span><span class="n">x_shape</span><span class="p">,</span> <span class="n">rep</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="n">x_rep</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">x_expand</span><span class="p">,</span> <span class="n">x_reshape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_rep</span></div>


<span class="nd">@_primexpr</span>
<span class="k">def</span> <span class="nf">_check_sequence_mask_input_len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">prim_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">msg_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39;, the&quot;</span> <span class="k">if</span> <span class="n">prim_name</span> <span class="k">else</span> <span class="s2">&quot;The&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">input_shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> input_shape must be greater than 0, but got </span><span class="si">{</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="c1"># broadcast only supports 7d shape</span>
    <span class="n">shape_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">shape_size</span> <span class="o">&gt;=</span> <span class="mi">7</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">msg_prefix</span><span class="si">}</span><span class="s2"> dimension of input_shape must be less than 7, but got </span><span class="si">{</span><span class="n">shape_size</span><span class="si">}</span><span class="s2">d.&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="sequence_mask"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sequence_mask.html#mindspore.ops.sequence_mask">[文档]</a><span class="k">def</span> <span class="nf">sequence_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a mask tensor representing the first N positions of each cell.</span>

<span class="sd">    If `lengths` has shape :math:`(d_1, d_2, ..., d_n)`, then the resulting tensor mask has type and shape</span>
<span class="sd">    :math:`(d_1, d_2, ..., d_n, maxlen)`, with mask :math:`[i_1, i_2, ..., i_n, j] = (j &lt; lengths[i_1, i_2, ..., i_n])`.</span>

<span class="sd">    Args:</span>
<span class="sd">        lengths (Tensor): Tensor to calculate the mask for. All values in this tensor should be</span>
<span class="sd">            less than or equal to `maxlen`. Values greater than `maxlen` will be treated as `maxlen`.</span>
<span class="sd">        maxlen (int): size of the last dimension of returned tensor. Must be positive and same</span>
<span class="sd">            type as elements in `lengths`. Default is ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        One mask tensor of shape `lengths.shape + (maxlen,)` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lengths` is not a Tensor.</span>
<span class="sd">        TypeError: If `maxlen` is not an int.</span>
<span class="sd">        TypeError: If dtype of `lengths` is neither int32 nor int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: When maxlen is assigned</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sequence_mask(x, 5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True False False False False]</span>
<span class="sd">         [ True  True False False False]</span>
<span class="sd">         [ True  True  True False False]</span>
<span class="sd">         [ True  True  True  True False]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: When there is 0 in x</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 3], [2, 0]]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sequence_mask(x, 5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ True False False False False]</span>
<span class="sd">          [ True  True  True False False]]</span>
<span class="sd">         [[ True  True False False False]</span>
<span class="sd">          [False False False False False]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: when the maxlen is not assigned</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 3], [2, 4]]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sequence_mask(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ True False False False ]</span>
<span class="sd">          [ True  True  True False ]]</span>
<span class="sd">         [[ True  True False False ]</span>
<span class="sd">          [ True  True  True  True ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">argmax_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ArgMaxWithValue</span><span class="p">()</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>
    <span class="n">range_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Range</span><span class="p">()</span>
    <span class="n">expand_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ExpandDims</span><span class="p">()</span>
    <span class="n">cast_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>
    <span class="n">to_tensor_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ScalarToTensor</span><span class="p">()</span>
    <span class="n">shape_op</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Shape</span><span class="p">()</span>

    <span class="n">const_utils</span><span class="o">.</span><span class="n">check_type_valid</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">lengths</span><span class="p">),</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="s1">&#39;lengths&#39;</span><span class="p">)</span>
    <span class="n">_check_sequence_mask_input_len</span><span class="p">(</span><span class="n">shape_op</span><span class="p">(</span><span class="n">lengths</span><span class="p">),</span> <span class="s2">&quot;sequence_mask&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">maxlen</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">flatten_data</span> <span class="o">=</span> <span class="n">reshape_op</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
        <span class="n">flatten_data</span> <span class="o">=</span> <span class="n">cast_op</span><span class="p">(</span><span class="n">flatten_data</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">argmax_op</span><span class="p">(</span><span class="n">flatten_data</span><span class="p">)</span>
        <span class="n">maxlen</span> <span class="o">=</span> <span class="n">cast_op</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">maxlen</span> <span class="o">=</span> <span class="n">_check_positive_int</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="s2">&quot;maxlen&quot;</span><span class="p">,</span> <span class="s2">&quot;sequence_mask&quot;</span><span class="p">)</span>
        <span class="n">maxlen</span> <span class="o">=</span> <span class="n">to_tensor_op</span><span class="p">(</span><span class="n">maxlen</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">range_vector</span> <span class="o">=</span> <span class="n">range_op</span><span class="p">(</span><span class="n">to_tensor_op</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">maxlen</span><span class="p">,</span> <span class="n">to_tensor_op</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">expand_op</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">range_vector</span> <span class="o">&lt;</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="k">def</span> <span class="nf">top_k</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    `top_k` is deprecated, please use `ops.topk` instead.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">top_k_</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">TopK</span><span class="p">)(</span><span class="nb">sorted</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">top_k_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>


<div class="viewcode-block" id="deepcopy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.deepcopy.html#mindspore.ops.deepcopy">[文档]</a><span class="k">def</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a deepcopy of input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a deepcopy of `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[0, 1], [2, 1]], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.deepcopy(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 1]</span>
<span class="sd">         [2 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_deepcopy</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">P</span><span class="o">.</span><span class="n">Identity</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">_deepcopy</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;unique&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unique_with_pad&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unique_consecutive&#39;</span><span class="p">,</span>
    <span class="s1">&#39;eye&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_band_part&#39;</span><span class="p">,</span>
    <span class="s1">&#39;padding&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fill&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fill_&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fills&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tile&#39;</span><span class="p">,</span>
    <span class="s1">&#39;size&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ger&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ones&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ones_like&#39;</span><span class="p">,</span>
    <span class="s1">&#39;zeros&#39;</span><span class="p">,</span>
    <span class="s1">&#39;zeros_like&#39;</span><span class="p">,</span>
    <span class="s1">&#39;shape&#39;</span><span class="p">,</span>
    <span class="s1">&#39;shape_&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reverse&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reverse_sequence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hamming_window&#39;</span><span class="p">,</span>
    <span class="s1">&#39;chunk&#39;</span><span class="p">,</span>
    <span class="s1">&#39;full&#39;</span><span class="p">,</span>
    <span class="s1">&#39;full_like&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dyn_shape&#39;</span><span class="p">,</span>
    <span class="s1">&#39;rank&#39;</span><span class="p">,</span>
    <span class="s1">&#39;range&#39;</span><span class="p">,</span>
    <span class="s1">&#39;arange&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reshape&#39;</span><span class="p">,</span>
    <span class="s1">&#39;reshape_&#39;</span><span class="p">,</span>
    <span class="s1">&#39;flatten&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_slice&#39;</span><span class="p">,</span>
    <span class="s1">&#39;strided_slice&#39;</span><span class="p">,</span>
    <span class="s1">&#39;slice&#39;</span><span class="p">,</span>
    <span class="s1">&#39;slice_scatter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;select_scatter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;cat&#39;</span><span class="p">,</span>
    <span class="s1">&#39;concat&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unbind&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unstack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;is_tensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scalar_cast&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scalar_to_array&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scalar_to_tensor&#39;</span><span class="p">,</span>
    <span class="s1">&#39;space_to_batch_nd&#39;</span><span class="p">,</span>
    <span class="s1">&#39;batch_to_space_nd&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tuple_to_array&#39;</span><span class="p">,</span>
    <span class="s1">&#39;expand_dims&#39;</span><span class="p">,</span>
    <span class="s1">&#39;squeeze&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unsqueeze&#39;</span><span class="p">,</span>
    <span class="s1">&#39;transpose&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_sub&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_mul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_max&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_nd_min&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_sub&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_mul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_max&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_min&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_scatter_elements&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unsorted_segment_min&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unsorted_segment_max&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unsorted_segment_prod&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gather&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gather_d&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gather_elements&#39;</span><span class="p">,</span>
    <span class="s1">&#39;gather_nd&#39;</span><span class="p">,</span>
    <span class="s1">&#39;one_hot&#39;</span><span class="p">,</span>
    <span class="s1">&#39;masked_fill&#39;</span><span class="p">,</span>
    <span class="s1">&#39;masked_select&#39;</span><span class="p">,</span>
    <span class="s1">&#39;where&#39;</span><span class="p">,</span>
    <span class="s1">&#39;narrow&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ravel&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_add&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_mul&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_max&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_min&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_div&#39;</span><span class="p">,</span>
    <span class="s1">&#39;scatter_update&#39;</span><span class="p">,</span>
    <span class="s1">&#39;select&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tril&#39;</span><span class="p">,</span>
    <span class="s1">&#39;triu&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nonzero&#39;</span><span class="p">,</span>
    <span class="s1">&#39;is_nonzero&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_diag&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_diag_part&#39;</span><span class="p">,</span>
    <span class="s1">&#39;matrix_set_diag&#39;</span><span class="p">,</span>
    <span class="s1">&#39;diag&#39;</span><span class="p">,</span>
    <span class="s1">&#39;diagflat&#39;</span><span class="p">,</span>
    <span class="s1">&#39;meshgrid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;affine_grid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;meshgrid&#39;</span><span class="p">,</span>
    <span class="s1">&#39;broadcast_to&#39;</span><span class="p">,</span>
    <span class="s1">&#39;col2im&#39;</span><span class="p">,</span>
    <span class="s1">&#39;split&#39;</span><span class="p">,</span>
    <span class="s1">&#39;tensor_split&#39;</span><span class="p">,</span>
    <span class="s1">&#39;vsplit&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hsplit&#39;</span><span class="p">,</span>
    <span class="s1">&#39;dsplit&#39;</span><span class="p">,</span>
    <span class="s1">&#39;index_fill&#39;</span><span class="p">,</span>
    <span class="s1">&#39;index_select&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max&#39;</span><span class="p">,</span>
    <span class="s1">&#39;argmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;min&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unsorted_segment_sum&#39;</span><span class="p">,</span>
    <span class="s1">&#39;population_count&#39;</span><span class="p">,</span>
    <span class="s1">&#39;topk&#39;</span><span class="p">,</span>
    <span class="s1">&#39;expand&#39;</span><span class="p">,</span>
    <span class="s1">&#39;fold&#39;</span><span class="p">,</span>
    <span class="s1">&#39;unfold&#39;</span><span class="p">,</span>
    <span class="s1">&#39;diagonal&#39;</span><span class="p">,</span>
    <span class="s1">&#39;diagonal_scatter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;lstsq&#39;</span><span class="p">,</span>
    <span class="s1">&#39;mvlgamma&#39;</span><span class="p">,</span>
    <span class="s1">&#39;swapaxes&#39;</span><span class="p">,</span>
    <span class="s1">&#39;swapdims&#39;</span><span class="p">,</span>
    <span class="s1">&#39;searchsorted&#39;</span><span class="p">,</span>
    <span class="s1">&#39;argsort&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sequence_mask&#39;</span><span class="p">,</span>
    <span class="s1">&#39;repeat_elements&#39;</span><span class="p">,</span>
    <span class="s1">&#39;repeat_interleave&#39;</span><span class="p">,</span>
    <span class="s1">&#39;argwhere&#39;</span><span class="p">,</span>
    <span class="s1">&#39;column_stack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;hstack&#39;</span><span class="p">,</span>
    <span class="s1">&#39;movedim&#39;</span><span class="p">,</span>
    <span class="s1">&#39;moveaxis&#39;</span><span class="p">,</span>
    <span class="s1">&#39;aminmax&#39;</span><span class="p">,</span>
    <span class="s1">&#39;sort&#39;</span><span class="p">,</span>
    <span class="s1">&#39;top_k&#39;</span><span class="p">,</span>
    <span class="s1">&#39;deepcopy&#39;</span>
<span class="p">]</span>
<span class="n">__all__</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2022, MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>