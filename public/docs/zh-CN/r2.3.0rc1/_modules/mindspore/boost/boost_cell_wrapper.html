<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.boost.boost_cell_wrapper &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script><script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/js/theme.js"></script><script src="../../../_static/underscore.js"></script><script src="../../../_static/doctools.js"></script><script src="../../../_static/js/mermaid-9.3.0.js"></script><script src="../../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/tensor_view.html">张量视图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/distributed_training_design.html">分布式并行原生</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.hal.html">mindspore.hal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/enveriment_preparation.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/model_development/model_development.html">网络搭建对比</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/static_graph_syntax/operators.html">静态图语法-运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/static_graph_syntax/statements.html">静态图语法-Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/static_graph_syntax/python_builtin_functions.html">静态图语法-Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">模块代码</a> &raquo;</li>
      <li>mindspore.boost.boost_cell_wrapper</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>mindspore.boost.boost_cell_wrapper 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2021-2022 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;Boost Mode Cell Wrapper.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.wrap</span> <span class="kn">import</span> <span class="n">TrainOneStepCell</span>
<span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="kn">from</span> <span class="nn">mindspore.context</span> <span class="kn">import</span> <span class="n">ParallelMode</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._utils</span> <span class="kn">import</span> <span class="n">_get_global_rank</span><span class="p">,</span> <span class="n">_get_device_num</span><span class="p">,</span> <span class="n">_get_gradients_mean</span>
<span class="kn">from</span> <span class="nn">mindspore.communication.management</span> <span class="kn">import</span> <span class="n">get_group_size</span><span class="p">,</span> <span class="n">create_group</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.cell</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">SequentialCell</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common.sparse_tensor</span> <span class="kn">import</span> <span class="n">RowTensorInner</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">ParameterTuple</span>
<span class="kn">from</span> <span class="nn">mindspore.nn.wrap.grad_reducer</span> <span class="kn">import</span> <span class="n">DistributedGradReducer</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.math_ops</span> <span class="kn">import</span> <span class="n">NPUGetFloatStatusV2</span><span class="p">,</span> <span class="n">NPUClearFloatStatusV2</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">composite</span> <span class="k">as</span> <span class="n">C</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">operations</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.boost.boost</span> <span class="kn">import</span> <span class="n">AutoBoost</span>
<span class="kn">from</span> <span class="nn">mindspore.boost.grad_freeze</span> <span class="kn">import</span> <span class="n">FreezeOpt</span><span class="p">,</span> <span class="n">freeze_cell</span>
<span class="kn">from</span> <span class="nn">mindspore.boost.adasum</span> <span class="kn">import</span> <span class="n">AdaSum</span>
<span class="kn">from</span> <span class="nn">mindspore.boost.dim_reduce</span> <span class="kn">import</span> <span class="n">DimReduce</span>
<span class="kn">from</span> <span class="nn">mindspore.boost.grad_accumulation</span> <span class="kn">import</span> <span class="n">gradient_accumulation_op</span><span class="p">,</span> <span class="n">gradient_clear_op</span>
<span class="kn">from</span> <span class="nn">mindspore.boost.base</span> <span class="kn">import</span> <span class="n">_load_local_pca_mat</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;BoostTrainOneStepCell&quot;</span><span class="p">,</span> <span class="s2">&quot;BoostTrainOneStepWithLossScaleCell&quot;</span><span class="p">]</span>

<span class="n">_get_delta_weight</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;_get_delta_weight&quot;</span><span class="p">)</span>


<span class="nd">@_get_delta_weight</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_get_delta_weight_process</span><span class="p">(</span><span class="n">new_parameter</span><span class="p">,</span> <span class="n">old_parameter</span><span class="p">):</span>
    <span class="n">delta_w</span> <span class="o">=</span> <span class="n">old_parameter</span> <span class="o">-</span> <span class="n">new_parameter</span>
    <span class="k">return</span> <span class="n">delta_w</span>


<span class="n">_save_weight</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;_save_weight&quot;</span><span class="p">)</span>


<span class="nd">@_save_weight</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_save_weight_process</span><span class="p">(</span><span class="n">new_parameter</span><span class="p">,</span> <span class="n">old_parameter</span><span class="p">):</span>
    <span class="n">P</span><span class="o">.</span><span class="n">Assign</span><span class="p">()(</span><span class="n">new_parameter</span><span class="p">,</span> <span class="n">old_parameter</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_parameter</span>


<span class="n">_grad_scale</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;grad_scale&quot;</span><span class="p">)</span>
<span class="n">reciprocal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reciprocal</span><span class="p">()</span>


<span class="nd">@_grad_scale</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">tensor_grad_scale</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;grad scale function for tensor&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">grad</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">grad</span><span class="p">))</span>


<span class="nd">@_grad_scale</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;RowTensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">tensor_grad_scale_row_tensor</span><span class="p">(</span><span class="n">scale</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;grad scale function for row tensor&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">RowTensorInner</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">indices</span><span class="p">,</span>
                          <span class="n">grad</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">reciprocal</span><span class="p">(</span><span class="n">scale</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">)),</span>
                          <span class="n">grad</span><span class="o">.</span><span class="n">dense_shape</span><span class="p">)</span>


<span class="n">_grad_overflow</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">MultitypeFuncGraph</span><span class="p">(</span><span class="s2">&quot;_grad_overflow&quot;</span><span class="p">)</span>
<span class="n">grad_overflow</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">FloatStatus</span><span class="p">()</span>


<span class="nd">@_grad_overflow</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;Tensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_tensor_grad_overflow</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">grad_overflow</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>


<span class="nd">@_grad_overflow</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;RowTensor&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_tensor_grad_overflow_row_tensor</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">grad_overflow</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_OutputToFloat16</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
    <span class="s2">&quot;Wrap cell for amp. Cast network output back to float16&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">op</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_OutputToFloat16</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_op</span> <span class="o">=</span> <span class="n">op</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_op</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>


<div class="viewcode-block" id="BoostTrainOneStepCell"><a class="viewcode-back" href="../../../api_python/mindspore.boost.html#mindspore.boost.BoostTrainOneStepCell">[文档]</a><span class="k">class</span> <span class="nc">BoostTrainOneStepCell</span><span class="p">(</span><span class="n">TrainOneStepCell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Boost Network training package class.</span>

<span class="sd">    Wraps the network with an optimizer. The resulting Cell is trained with input &#39;\*inputs&#39;.</span>
<span class="sd">    The backward graph will be created in the construct function to update the parameter, and different</span>
<span class="sd">    parallel modes are available for training.</span>

<span class="sd">    Args:</span>
<span class="sd">        network (Cell): The training network. The network only supports single output.</span>
<span class="sd">        optimizer (Union[Cell]): Optimizer for updating the weights.</span>
<span class="sd">        sens (numbers.Number): The scaling number to be filled as the input of backpropagation.</span>
<span class="sd">            Default: ``None`` , which is ``1.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **\*inputs** (Tuple(Tensor)) - Tuple of input tensors with shape :math:`(N, \ldots)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor means the loss value, the shape of which is usually :math:`()`.</span>

<span class="sd">        - loss(Tensor): A scalar Tensor.</span>
<span class="sd">        - overflow(Tensor): A scalar Tensor which type is bool.</span>
<span class="sd">        - loss scaling value(Tensor): A scalar Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `sens` is not a number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import boost</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import nn</span>
<span class="sd">        &gt;&gt;&gt; # Define the network structure of LeNet5. Refer to</span>
<span class="sd">        &gt;&gt;&gt; # https://gitee.com/mindspore/docs/blob/r2.3.q1/docs/mindspore/code/lenet.py</span>
<span class="sd">        &gt;&gt;&gt; net = LeNet5()</span>
<span class="sd">        &gt;&gt;&gt; loss_fn = nn.SoftmaxCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; optim = nn.Momentum(net.trainable_params(), learning_rate=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; #1) Using the WithLossCell existing provide</span>
<span class="sd">        &gt;&gt;&gt; loss_net = nn.WithLossCell(net, loss_fn)</span>
<span class="sd">        &gt;&gt;&gt; train_net = boost.BoostTrainOneStepCell(loss_net, optim)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; #2) Using user-defined WithLossCell</span>
<span class="sd">        &gt;&gt;&gt; class MyWithLossCell(nn.Cell):</span>
<span class="sd">        ...    def __init__(self, backbone, loss_fn):</span>
<span class="sd">        ...        super(MyWithLossCell, self).__init__(auto_prefix=False)</span>
<span class="sd">        ...        self._backbone = backbone</span>
<span class="sd">        ...        self._loss_fn = loss_fn</span>
<span class="sd">        ...</span>
<span class="sd">        ...    def construct(self, x, y, label):</span>
<span class="sd">        ...        out = self._backbone(x, y)</span>
<span class="sd">        ...        return self._loss_fn(out, label)</span>
<span class="sd">        ...</span>
<span class="sd">        ...    @property</span>
<span class="sd">        ...    def backbone_network(self):</span>
<span class="sd">        ...        return self._backbone</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; loss_net = MyWithLossCell(net, loss_fn)</span>
<span class="sd">        &gt;&gt;&gt; train_net = boost.BoostTrainOneStepCell(loss_net, optim)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BoostTrainOneStepCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">HyperMap</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">freeze</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">FreezeOpt</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_strategy</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="s1">&#39;train_strategy&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span> <span class="o">=</span> <span class="n">AutoBoost</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_grad_accumulation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_grad_accumulation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_grad_accumulation</span> <span class="o">&amp;</span> \
                                     <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">boost_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;grad_accumulation&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_accumulation_step</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_grad_accumulation</span><span class="p">:</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">max_accumulation_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">grad_accumulation_step</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_accumulation_step</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_accumulation_step</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">use_grad_accumulation</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accumulation_step</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accumulation_step&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_grad_accumulation</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;grad_accumulation&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">enable_dim_reduce</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_dim_reduce_enable</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_dim_reduce</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__init_dim_reduce</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">freeze_nets</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">_get_gradients_mean</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="n">_get_device_num</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">freeze_nets</span> <span class="o">=</span> <span class="n">freeze_cell</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">,</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_grad_accumulation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span>
                                           <span class="bp">self</span><span class="o">.</span><span class="n">max_accumulation_step</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">enable_adasum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">check_adasum_enable</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sync_tensor</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_adasum</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">__init_adasum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">freeze</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_freeze_process</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">sense_flag</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_no_sens_impl</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">sens</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_grad_accumulation</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_process</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">sens</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_dim_reduce</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">sens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_clone</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">))</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_adasum</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">adasum_process</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">loss</span>

<div class="viewcode-block" id="BoostTrainOneStepCell.gradient_freeze_process"><a class="viewcode-back" href="../../../api_python/mindspore.boost.html#mindspore.boost.BoostTrainOneStepCell.gradient_freeze_process">[文档]</a>    <span class="k">def</span> <span class="nf">gradient_freeze_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gradient freeze algorithm process.</span>

<span class="sd">        Args:</span>
<span class="sd">            inputs (tuple(Tensor)): Tuple of input tensors with shape :math:`(N, \ldots)`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            - **loss** (Tensor) -  Network loss, tensor with shape :math:`()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_strategy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span>
            <span class="n">max_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">freeze_nets</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_strategy</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">]</span>
            <span class="n">max_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_strategy</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">freeze_nets</span><span class="p">[</span><span class="n">step</span><span class="p">](</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&gt;=</span> <span class="n">max_index</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">loss</span></div>

<div class="viewcode-block" id="BoostTrainOneStepCell.gradient_accumulation_process"><a class="viewcode-back" href="../../../api_python/mindspore.boost.html#mindspore.boost.BoostTrainOneStepCell.gradient_accumulation_process">[文档]</a>    <span class="k">def</span> <span class="nf">gradient_accumulation_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">sens</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gradient accumulation algorithm process.</span>

<span class="sd">        Args:</span>
<span class="sd">            loss (Tensor): Tensor with shape :math:`()`.</span>
<span class="sd">            grads (tuple(Tensor)): Tuple of gradient tensors.</span>
<span class="sd">            sens (Tensor): Tensor with shape :math:`()`.</span>
<span class="sd">            inputs (tuple(Tensor)): Tuple of input tensors with shape :math:`(N, \ldots)`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            - **loss** (Tensor) - Network loss, tensor with shape :math:`()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">gradient_accumulation_op</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_accumulation_step</span><span class="p">),</span>
                                             <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation</span><span class="p">,</span> <span class="n">grads</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">accumulation_step</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accumulation_step</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_accumulation_step</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_dim_reduce</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation</span><span class="p">,</span> <span class="n">sens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
                                                      <span class="bp">self</span><span class="o">.</span><span class="n">weights_clone</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_adasum</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">adasum_process</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">accumulation_step</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">accumulation_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">gradient_clear_op</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_accumulation</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">loss</span></div>

<div class="viewcode-block" id="BoostTrainOneStepCell.adasum_process"><a class="viewcode-back" href="../../../api_python/mindspore.boost.html#mindspore.boost.BoostTrainOneStepCell.adasum_process">[文档]</a>    <span class="k">def</span> <span class="nf">adasum_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adasum algorithm process.</span>

<span class="sd">        Args:</span>
<span class="sd">            loss (Tensor): Tensor with shape :math:`()`.</span>
<span class="sd">            grads (tuple(Tensor)): Tuple of gradient tensors.</span>

<span class="sd">        Returns:</span>
<span class="sd">            - **loss** (Tensor) - Network loss, tensor with shape :math:`()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">))</span>
        <span class="n">rank_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">start</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">server_rank</span><span class="p">]:</span> <span class="bp">self</span><span class="o">.</span><span class="n">end</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">server_rank</span><span class="p">]]</span>
        <span class="n">grad_clone</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">grad_clone</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">delta_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_get_delta_weight</span><span class="p">),</span> <span class="n">rank_weights</span><span class="p">,</span> <span class="n">grad_clone</span><span class="p">)</span>
        <span class="n">adasum_res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adasum</span><span class="p">(</span><span class="n">delta_w</span><span class="p">,</span> <span class="n">rank_weights</span><span class="p">,</span> <span class="n">grad_clone</span><span class="p">)</span>
        <span class="n">sync_tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sync_tensor</span><span class="p">,</span> <span class="n">adasum_res</span><span class="p">)</span>
        <span class="n">sync_flag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adasum</span><span class="o">.</span><span class="n">sync_barrier</span><span class="p">(</span><span class="n">sync_tensor</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_number</span><span class="p">):</span>
            <span class="n">weight_tuple</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">start</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="bp">self</span><span class="o">.</span><span class="n">end</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
            <span class="n">node_rank</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">weight_tuple</span><span class="p">,</span> <span class="n">sync_flag</span><span class="p">)</span>
            <span class="n">update_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adasum</span><span class="o">.</span><span class="n">broadcast_list</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">node_rank</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">server_rank</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_save_weight</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_clone</span><span class="p">,</span> <span class="n">update_weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_save_weight</span><span class="p">),</span> <span class="n">weight_tuple</span><span class="p">,</span> <span class="n">update_weights</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div>

<div class="viewcode-block" id="BoostTrainOneStepCell.check_adasum_enable"><a class="viewcode-back" href="../../../api_python/mindspore.boost.html#mindspore.boost.BoostTrainOneStepCell.check_adasum_enable">[文档]</a>    <span class="k">def</span> <span class="nf">check_adasum_enable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check adasum enable.</span>

<span class="sd">        Returns:</span>
<span class="sd">            - **enable_adasum** (bool) - Check whether the Adasum algorithm is enabled.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;adasum&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="n">_rank_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
        <span class="n">_device_number</span> <span class="o">=</span> <span class="mi">8</span>
        <span class="n">group_number</span> <span class="o">=</span> <span class="n">_rank_size</span> <span class="o">//</span> <span class="n">_device_number</span>
        <span class="n">is_enable</span> <span class="o">=</span> <span class="nb">bool</span><span class="p">(</span><span class="n">group_number</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">group_number</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">group_number</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">is_enable</span></div>

<div class="viewcode-block" id="BoostTrainOneStepCell.check_dim_reduce_enable"><a class="viewcode-back" href="../../../api_python/mindspore.boost.html#mindspore.boost.BoostTrainOneStepCell.check_dim_reduce_enable">[文档]</a>    <span class="k">def</span> <span class="nf">check_dim_reduce_enable</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check dim_reduce enable.</span>

<span class="sd">        Returns:</span>
<span class="sd">            - **enable_dim_reduce** (bool) - Check whether the dimensionality reduction second-order training</span>
<span class="sd">              algorithm is enabled.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="s2">&quot;dim_reduce&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span></div>

    <span class="k">def</span> <span class="nf">_no_sens_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;construct implementation when the &#39;sens&#39; parameter is passed in.&quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">sens</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_no_sens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_grad_accumulation</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_process</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">sens</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_dim_reduce</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">sens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_clone</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_adasum</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">adasum_process</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">__init_dim_reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;dim reduce algorithm init method.&quot;&quot;&quot;</span>
        <span class="n">local_pca_mat_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">local_pca_mat_path</span>
        <span class="n">rho</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">rho</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">gamma</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">alpha</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">sigma</span>
        <span class="n">_rank</span> <span class="o">=</span> <span class="n">_get_global_rank</span><span class="p">()</span>
        <span class="n">_rank_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="o">==</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span> <span class="k">else</span> <span class="n">get_group_size</span><span class="p">()</span>
        <span class="n">n_components</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">n_components</span>
        <span class="n">timeout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">timeout</span>
        <span class="n">pca_mat</span> <span class="o">=</span> <span class="n">_load_local_pca_mat</span><span class="p">(</span><span class="n">local_pca_mat_path</span><span class="p">,</span> <span class="n">timeout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_clone</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;weights_clone&quot;</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim_reduce</span> <span class="o">=</span> <span class="n">DimReduce</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">pca_mat</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span>
                                    <span class="n">alpha</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">_rank</span><span class="p">,</span> <span class="n">_rank_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__init_adasum</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;adasum algorithm init method.&quot;&quot;&quot;</span>
        <span class="n">_rank</span> <span class="o">=</span> <span class="n">_get_global_rank</span><span class="p">()</span>
        <span class="n">_rank_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
        <span class="n">_device_number</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">device_number</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device_number</span> <span class="o">=</span> <span class="n">_device_number</span>
        <span class="n">group_number</span> <span class="o">=</span> <span class="n">_rank_size</span> <span class="o">//</span> <span class="n">_device_number</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">server_rank</span> <span class="o">=</span> <span class="n">_rank</span> <span class="o">%</span> <span class="n">_device_number</span>
        <span class="n">parameter_rank_number</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">//</span> <span class="n">_device_number</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="o">*</span> <span class="n">parameter_rank_number</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">_device_number</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">parameter_rank_number</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">_device_number</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>

        <span class="n">current_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">start</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">server_rank</span><span class="p">]:</span> <span class="bp">self</span><span class="o">.</span><span class="n">end</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">server_rank</span><span class="p">]]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_clone</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="n">current_weights</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;delta_weight&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adasum</span> <span class="o">=</span> <span class="n">AdaSum</span><span class="p">(</span><span class="n">_rank</span><span class="p">,</span> <span class="n">_device_number</span><span class="p">,</span> <span class="n">group_number</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_clone</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">degree</span> <span class="o">//</span> <span class="n">group_number</span><span class="p">)</span>
        <span class="n">group_list</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">group_number</span><span class="p">)]</span>
        <span class="n">current_index</span> <span class="o">=</span> <span class="n">_rank</span> <span class="o">//</span> <span class="n">_device_number</span>
        <span class="n">server_group_name</span> <span class="o">=</span> <span class="s2">&quot;allreduce_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">current_index</span><span class="p">)</span>
        <span class="n">create_group</span><span class="p">(</span><span class="n">server_group_name</span><span class="p">,</span> <span class="n">group_list</span><span class="p">[</span><span class="n">current_index</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span> <span class="o">=</span> <span class="n">DistributedGradReducer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">degree</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">server_group_name</span><span class="p">)</span></div>


<div class="viewcode-block" id="BoostTrainOneStepWithLossScaleCell"><a class="viewcode-back" href="../../../api_python/mindspore.boost.html#mindspore.boost.BoostTrainOneStepWithLossScaleCell">[文档]</a><span class="k">class</span> <span class="nc">BoostTrainOneStepWithLossScaleCell</span><span class="p">(</span><span class="n">BoostTrainOneStepCell</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Boost Network training with loss scaling.</span>

<span class="sd">    This is a training step with loss scaling. It takes a network, an optimizer and possibly a scale update</span>
<span class="sd">    Cell as args. The loss scale value can be updated in both host side or device side. The</span>
<span class="sd">    BoostTrainOneStepWithLossScaleCell will be compiled to be graph which takes `*inputs` as input data.</span>
<span class="sd">    The Tensor type of `scale_sense` is acting as loss scaling value. If you want to update it on host side,</span>
<span class="sd">    the value must be provided. If the Tensor type of `scale_sense` is not given, the loss scale update logic</span>
<span class="sd">    must be provide by Cell type of `scale_sense`.</span>

<span class="sd">    Args:</span>
<span class="sd">        network (Cell): The training network. The network only supports single output.</span>
<span class="sd">        optimizer (Cell): Optimizer for updating the weights.</span>
<span class="sd">        scale_sense (Union[Tensor, Cell]): If this value is Cell type, the loss scaling update logic cell.If this value</span>
<span class="sd">            is Tensor type, :func:`mindspore.nn.TrainOneStepWithLossScaleCell.set_sense_scale` can be called to update</span>
<span class="sd">            loss scale factor, Tensor with shape :math:`()` or :math:`(1,)`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **\*inputs** (Tuple(Tensor)) - Tuple of input tensors with shape :math:`(N, \ldots)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the loss, overflow flag and current loss scaling value.</span>

<span class="sd">        - **loss** (Tensor) -  Tensor with shape :math:`()`.</span>
<span class="sd">        - **overflow** (Tensor) -  Tensor with shape :math:`()`, type is bool.</span>
<span class="sd">        - **loss scaling value** (Tensor) -  Tensor with shape :math:`()`</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `scale_sense` is neither Cell nor Tensor.</span>
<span class="sd">        ValueError: If shape of `scale_sense` is neither :math:`(1,)` nor :math:`()`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter, nn</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.nn import WithLossCell</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import boost</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, in_features, out_features):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.weight = Parameter(Tensor(np.ones([in_features, out_features]).astype(np.float32)),</span>
<span class="sd">        ...                                 name=&#39;weight&#39;)</span>
<span class="sd">        ...         self.matmul = ops.MatMul()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         output = self.matmul(x, self.weight)</span>
<span class="sd">        ...         return output</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; size, in_features, out_features = 16, 16, 10</span>
<span class="sd">        &gt;&gt;&gt; #1) when the type of scale_sense is Cell:</span>
<span class="sd">        &gt;&gt;&gt; net = Net(in_features, out_features)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MSELoss()</span>
<span class="sd">        &gt;&gt;&gt; optimizer = nn.Momentum(net.trainable_params(), learning_rate=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; net_with_loss = WithLossCell(net, loss)</span>
<span class="sd">        &gt;&gt;&gt; manager = nn.DynamicLossScaleUpdateCell(loss_scale_value=2**12, scale_factor=2, scale_window=1000)</span>
<span class="sd">        &gt;&gt;&gt; train_network = boost.BoostTrainOneStepWithLossScaleCell(net_with_loss, optimizer, scale_sense=manager)</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.ones([out_features, in_features]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.ones([out_features,]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = train_network(input, labels)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; #2) when the type of scale_sense is Tensor:</span>
<span class="sd">        &gt;&gt;&gt; net = Net(in_features, out_features)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.MSELoss()</span>
<span class="sd">        &gt;&gt;&gt; optimizer = nn.Momentum(net.trainable_params(), learning_rate=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; net_with_loss = WithLossCell(net, loss)</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.ones([size, in_features]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; label = Tensor(np.zeros([size, out_features]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; scaling_sens = Tensor(np.full((1), np.finfo(np.float32).max), dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; train_network = boost.BoostTrainOneStepWithLossScaleCell(net_with_loss, optimizer, scale_sense=scaling_sens)</span>
<span class="sd">        &gt;&gt;&gt; output = train_network(inputs, label)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">scale_sense</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BoostTrainOneStepWithLossScaleCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">less_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LessEqual</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">allreduce</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AllReduce</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_distributed</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parallel_mode</span> <span class="o">!=</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gpu_target</span> <span class="o">=</span> <span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_scaling_manager</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base0</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_all</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">ReduceAll</span><span class="p">(</span><span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logic_not</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">LogicalNot</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Equal</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_boost</span><span class="o">.</span><span class="n">boost_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;loss_scale_group&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enable_enhanced_amp</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_sense</span><span class="p">,</span> <span class="n">Cell</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">scale_sense</span><span class="p">,</span> <span class="s2">&quot;set_loss_scale_status&quot;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The scale_sense must be enhanced amp Cell, bug got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">scale_sense</span><span class="p">)))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_scaling_manager</span> <span class="o">=</span> <span class="n">scale_sense</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss_scale_groups</span> <span class="o">=</span> <span class="n">scale_sense</span><span class="o">.</span><span class="n">loss_scale_groups</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_enhanced_amp</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_do_keep_mix_fp32</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enable_enhanced_amp</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_sense</span><span class="p">,</span> <span class="n">Cell</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">loss_scaling_manager</span> <span class="o">=</span> <span class="n">scale_sense</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scale_sense</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">scale_sense</span><span class="o">.</span><span class="n">get_loss_scale</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span>
                                             <span class="n">name</span><span class="o">=</span><span class="s2">&quot;scale_sense&quot;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_sense</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">scale_sense</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="ow">or</span> <span class="n">scale_sense</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">scale_sense</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">scale_sense</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;scale_sense&#39;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The shape of scale_sense must be (1,) or (), </span><span class="se">\</span>
<span class="s2">                                     but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">scale_sense</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The scale_sense must be Cell or Tensor, but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">scale_sense</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">(</span><span class="o">*</span><span class="n">inputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_enhanced_amp</span><span class="p">:</span>
            <span class="n">scaling_sens</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">)</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
            <span class="n">cond</span><span class="p">,</span> <span class="n">scaling_sens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_enhanced_amp_process_overflow_status</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scaling_sens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_sense</span>
            <span class="n">status</span><span class="p">,</span> <span class="n">scaling_sens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_overflow_check</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">)</span>
            <span class="n">scaling_sens_filled</span> <span class="o">=</span> <span class="n">C</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">scaling_sens</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>

            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">network</span><span class="p">,</span> <span class="n">weights</span><span class="p">)(</span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">scaling_sens_filled</span><span class="p">)</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_grad_scale</span><span class="p">,</span> <span class="n">scaling_sens</span><span class="p">),</span> <span class="n">grads</span><span class="p">)</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_reducer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>

            <span class="c1"># get the overflow buffer</span>
            <span class="n">cond</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_overflow_status</span><span class="p">(</span><span class="n">status</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
            <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_process_loss_scale</span><span class="p">(</span><span class="n">cond</span><span class="p">)</span>
            <span class="c1"># if there is no overflow, do optimize</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">overflow</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__multi_update</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">scaling_sens_filled</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">scaling_sens</span>

    <span class="k">def</span> <span class="nf">__multi_update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">scaling_sens_filled</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;enable multi-algorithm&#39;s process&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_grad_accumulation</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_accumulation_process</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">scaling_sens_filled</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_dim_reduce</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_reduce</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">scaling_sens_filled</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
                                                      <span class="bp">self</span><span class="o">.</span><span class="n">weights_clone</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_adasum</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">adasum_process</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">grads</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">_get_dynamic_overflow_status</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Judge whether the current network overflows.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **param** (Tensor) - Whether the overflow occurs or not.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            bool, overflow value.</span>
<span class="sd">            float, update ratio.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">flag_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base0</span><span class="p">,</span> <span class="n">param</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reducer_flag</span><span class="p">:</span>
            <span class="n">flag_reduce</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">flag_sum</span><span class="p">)</span>
            <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logic_not</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">flag_reduce</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logic_not</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">flag_sum</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">overflow</span><span class="p">:</span>
            <span class="n">update_ratio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_ratio</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">update_ratio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">growth_ratio</span>
        <span class="k">return</span> <span class="n">overflow</span><span class="p">,</span> <span class="n">update_ratio</span>

    <span class="k">def</span> <span class="nf">_enhanced_amp_process_overflow_status</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Enhanced hybrid precision update loss scale and update weights.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **grads** (Tuple(Tensor)) - Tuple of gradients.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            bool, overflow value.</span>
<span class="sd">            float, loss scale value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">overflow_global_flag</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">loss_scale_temp</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">overflow_status_list</span><span class="p">:</span>
            <span class="n">overflow</span><span class="p">,</span> <span class="n">update_ratio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_dynamic_overflow_status</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">overflow</span><span class="p">:</span>
                <span class="n">overflow_global_flag</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">new_loss_scale_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_scaling_manager</span><span class="o">.</span><span class="n">update_loss_scale_status</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">update_ratio</span><span class="p">)</span>
            <span class="n">loss_scale_temp</span> <span class="o">+=</span> <span class="p">(</span><span class="n">new_loss_scale_value</span><span class="p">,)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_loss_scale</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span>
            <span class="n">layer</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">P</span><span class="o">.</span><span class="n">Less</span><span class="p">()(</span><span class="n">overflow_global_flag</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">base</span><span class="p">):</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_grad_scale</span><span class="p">),</span> <span class="n">loss_scale_temp</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
            <span class="n">overflow_global_flag</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">overflow_global_flag</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">overflow_global_flag</span><span class="p">,</span> <span class="n">loss_scale_temp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_set_sense_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sens</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If the user has set the sens in the training process and wants to reassign the value, he can call</span>
<span class="sd">        this function again to make modification, and sens needs to be of type Tensor.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **sens** (Tensor) - The new sense whose shape and type are the same with original `scale_sense`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_sense</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sens</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_sense</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">sens</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The input type must be Tensor, but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">sens</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_start_overflow_check</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pre_cond</span><span class="p">,</span> <span class="n">compute_input</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Start floating-point overflow detection. Create and clear the overflow detection state.</span>

<span class="sd">        Specify the argument &#39;pre_cond&#39; and &#39;compute_input&#39; to make sure overflow status is cleared at the right time.</span>
<span class="sd">        Taking this situation as an example, we need to execute state clearing after loss calculation and then detect</span>
<span class="sd">        overflow in the process of gradient calculation. In this case, pre_cond should be the output of the loss</span>
<span class="sd">        function, and compute_input should be the input of gradients-computing function.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **pre_cond** (Tensor) - A precondition for starting overflow detection. It determines the executing order</span>
<span class="sd">              of overflow state clearing and prior processions. It makes sure that the function &#39;start_overflow&#39;</span>
<span class="sd">              clears status after finishing the process of precondition.</span>
<span class="sd">            - **compute_input** (object) - The input of subsequent process. Overflow detection should be performed on a</span>
<span class="sd">              certain computation. Set `compute_input` as the input of the computation, to ensure overflow status is</span>
<span class="sd">              cleared before executing the computation.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            Tuple[object, object], the first value is False for GPU backend, while it is an instance of</span>
<span class="sd">            NPUAllocFloatStatus for other backend. The status is used to detect overflow during overflow detection.</span>
<span class="sd">            The second value is the same as the input of `compute_input`, but contains some information about the</span>
<span class="sd">            execution order.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">status</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu_target</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">status</span><span class="p">,</span> <span class="n">pre_cond</span><span class="p">)</span>
            <span class="c1"># clear overflow buffer</span>
            <span class="n">clear_status</span> <span class="o">=</span> <span class="n">NPUClearFloatStatusV2</span><span class="p">()(</span><span class="n">status</span><span class="p">)</span>
            <span class="n">compute_input</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">compute_input</span><span class="p">,</span> <span class="n">clear_status</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">status</span><span class="p">,</span> <span class="n">compute_input</span>

    <span class="k">def</span> <span class="nf">_get_overflow_status</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">status</span><span class="p">,</span> <span class="n">compute_output</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get floating-point overflow status.</span>

<span class="sd">        Get overflow results after executing the target process for overflow detection.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **status** (object) - A status instance used to detect the overflow.</span>
<span class="sd">            - **compute_output** - Overflow detection should be performed on a certain computation. Set `compute_output`</span>
<span class="sd">              as the output of the computation, to ensure overflow status is acquired before executing the</span>
<span class="sd">              computation.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            bool, whether the overflow occurs or not.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpu_target</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">status</span><span class="p">,</span> <span class="n">compute_output</span><span class="p">)</span>
            <span class="n">get_status</span> <span class="o">=</span> <span class="n">NPUGetFloatStatusV2</span><span class="p">()(</span><span class="n">status</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_distributed</span><span class="p">:</span>
                <span class="c1"># sum overflow flag over devices</span>
                <span class="n">flag_reduce</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">get_status</span><span class="p">)</span>
                <span class="c1"># get_status not equal to [0]*8 means overflow</span>
                <span class="n">flag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base0</span><span class="p">,</span> <span class="n">flag_reduce</span><span class="p">)</span>
                <span class="n">status</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">status</span><span class="p">,</span> <span class="n">flag</span><span class="p">)</span>
                <span class="c1"># distributed needs to skip allreduce to avoid its overflow affecting the next step</span>
                <span class="n">clear_status</span> <span class="o">=</span> <span class="n">NPUClearFloatStatusV2</span><span class="p">()(</span><span class="n">status</span><span class="p">)</span>
                <span class="n">flag</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">flag</span><span class="p">,</span> <span class="n">clear_status</span><span class="p">)</span>
                <span class="n">overall_finite</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">flag</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">status</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">status</span><span class="p">,</span> <span class="n">get_status</span><span class="p">)</span>
                <span class="n">clear_status</span> <span class="o">=</span> <span class="n">NPUClearFloatStatusV2</span><span class="p">()(</span><span class="n">status</span><span class="p">)</span>
                <span class="n">get_status</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">depend</span><span class="p">(</span><span class="n">get_status</span><span class="p">,</span> <span class="n">clear_status</span><span class="p">)</span>
                <span class="n">flag</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base0</span><span class="p">,</span> <span class="n">get_status</span><span class="p">)</span>
                <span class="n">overall_finite</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduce_all</span><span class="p">(</span><span class="n">flag</span><span class="p">)</span>
            <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logic_not</span><span class="p">(</span><span class="n">overall_finite</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">flag_sum</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hyper_map</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">_grad_overflow</span><span class="p">),</span> <span class="n">compute_output</span><span class="p">)</span>
            <span class="n">flag_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">AddN</span><span class="p">()(</span><span class="n">flag_sum</span><span class="p">)</span>
            <span class="c1"># convert flag_sum to scalar</span>
            <span class="n">flag_sum</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()(</span><span class="n">flag_sum</span><span class="p">,</span> <span class="p">(()))</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_distributed</span><span class="p">:</span>
                <span class="c1"># sum overflow flag over devices</span>
                <span class="n">flag_reduce</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">allreduce</span><span class="p">(</span><span class="n">flag_sum</span><span class="p">)</span>
                <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">less_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base</span><span class="p">,</span> <span class="n">flag_reduce</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">overflow</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">less_equal</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base</span><span class="p">,</span> <span class="n">flag_sum</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">overflow</span>

    <span class="k">def</span> <span class="nf">_process_loss_scale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">overflow</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate loss scale according to the overflow.</span>

<span class="sd">        Inputs:</span>
<span class="sd">            - **overflow** (bool) - Whether the overflow occurs or not.</span>

<span class="sd">        Outputs:</span>
<span class="sd">            bool, overflow value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_scaling_manager</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_scaling_manager</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale_sense</span><span class="p">,</span> <span class="n">overflow</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">overflow</span>

    <span class="k">def</span> <span class="nf">_init_enhanced_amp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Init enhanced hybrid precision.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parent</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params_len</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_rank</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params_len</span><span class="p">)]</span>
        <span class="n">index</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">loss_scale_number</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_scale_groups</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">loss_scale_group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_scale_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loss_scale_group</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">continue</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_union</span><span class="p">(</span><span class="n">index</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
                <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">parent_set</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_loss_scale</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">parent_set</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduce_ratio</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">growth_ratio</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mf">1000.0</span><span class="p">),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">overflow_status_list</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">]),</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
                                                             <span class="n">name</span><span class="o">=</span><span class="s1">&#39;mix_layer_status_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                                                   <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">loss_scale_number</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_scaling_manager</span><span class="o">.</span><span class="n">set_loss_scale_status</span><span class="p">(</span><span class="n">loss_scale_number</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_scaling_manager</span><span class="o">.</span><span class="n">get_loss_scale</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_get_root</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get parent id.</span>

<span class="sd">        Args:</span>
<span class="sd">            i (int): the current parameters&#39;s id.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Number, the parent id.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">[</span><span class="n">i</span><span class="p">]]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_root</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_union</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Aggregate parameters of the same category.</span>

<span class="sd">        Args:</span>
<span class="sd">            i (int): the last parameters&#39;s id.</span>
<span class="sd">            j (int): the current parameters&#39;s id.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">i_root</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_root</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="n">j_root</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_root</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_rank</span><span class="p">[</span><span class="n">i_root</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_rank</span><span class="p">[</span><span class="n">j_root</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">[</span><span class="n">j_root</span><span class="p">]</span> <span class="o">=</span> <span class="n">i_root</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layer_rank</span><span class="p">[</span><span class="n">i_root</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_rank</span><span class="p">[</span><span class="n">i_root</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_rank</span><span class="p">[</span><span class="n">j_root</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">[</span><span class="n">j_root</span><span class="p">]</span> <span class="o">=</span> <span class="n">i_root</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parent</span><span class="p">[</span><span class="n">i_root</span><span class="p">]</span> <span class="o">=</span> <span class="n">j_root</span>

    <span class="k">def</span> <span class="nf">_do_keep_mix_fp32</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">network</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Keep enhanced amp cell of type float32.</span>

<span class="sd">        Args:</span>
<span class="sd">            network (Cell): The training network.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cells</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">name_cells</span><span class="p">()</span>
        <span class="n">change</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">cells</span><span class="p">:</span>
            <span class="n">subcell</span> <span class="o">=</span> <span class="n">cells</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">subcell</span> <span class="o">==</span> <span class="n">network</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="s2">&quot;GroupLossScaleManager&quot;</span> <span class="ow">in</span> <span class="n">subcell</span><span class="o">.</span><span class="n">cls_name</span><span class="p">:</span>
                <span class="n">network</span><span class="o">.</span><span class="n">_cells</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">_OutputToFloat16</span><span class="p">(</span><span class="n">subcell</span><span class="o">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>  <span class="c1"># pylint: disable=W0212</span>
                <span class="n">change</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_do_keep_mix_fp32</span><span class="p">(</span><span class="n">subcell</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">SequentialCell</span><span class="p">)</span> <span class="ow">and</span> <span class="n">change</span><span class="p">:</span>
            <span class="n">network</span><span class="o">.</span><span class="n">cell_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">cells</span><span class="p">())</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>