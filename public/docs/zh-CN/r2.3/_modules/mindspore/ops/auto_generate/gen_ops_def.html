<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.auto_generate.gen_ops_def &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script><script src="../../../../_static/js/mermaid-9.3.0.js"></script><script src="../../../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">分布式并行原生</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.hal.html">mindspore.hal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">网络搭建对比</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/operators.html">静态图语法-运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/statements.html">静态图语法-Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/python_builtin_functions.html">静态图语法-Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">模块代码</a> &raquo;</li>
      <li>mindspore.ops.auto_generate.gen_ops_def</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>mindspore.ops.auto_generate.gen_ops_def 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2023 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators definition generated by gen_ops.py, includes functions and primitive classes.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">Primitive</span><span class="p">,</span> <span class="n">prim_arg_register</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">signature</span> <span class="k">as</span> <span class="n">sig</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common._decorator</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._primitive_cache</span> <span class="kn">import</span> <span class="n">_get_cache_prim</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.auto_generate.gen_arg_dtype_cast</span> <span class="kn">import</span> <span class="n">type_it</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.auto_generate.gen_arg_handler</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">OpDtype</span>
<span class="kn">from</span> <span class="nn">mindspore.common._stub_tensor</span> <span class="kn">import</span> <span class="n">_convert_stub</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_contiguous</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_copy</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_cos</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_sigmoid_grad</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_sigmoid</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_silu_grad</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_silu</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_sin</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_softmax_backward</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_softmax</span>


<span class="k">class</span> <span class="nc">AbsGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AbsGrad()</span>
<span class="sd">        out = prim(x, dout)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.abs_grad(x, dout)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.abs_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Abs</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Abs()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.abs(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.abs` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ACosGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ACosGrad()</span>
<span class="sd">        out = prim(x, dout)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.acos_grad(x, dout)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.acos_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ACos</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ACos()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.acos(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.acos` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AcoshGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AcoshGrad()</span>
<span class="sd">        out = prim(out, dout)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.acosh_grad(out, dout)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.acosh_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Acosh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Acosh()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.acosh(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.acosh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Add()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.add(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.add` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Addcdiv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Addcdiv()</span>
<span class="sd">        out = prim(input, tensor1, tensor2, value)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.addcdiv_(input, tensor1, tensor2, value)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.addcdiv_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Addcmul</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Addcmul()</span>
<span class="sd">        out = prim(input, tensor1, tensor2, value)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.addcmul_(input, tensor1, tensor2, value)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.addcmul_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Angle</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Angle()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.angle(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.angle` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ApplyCamePart1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ApplyCamePart1()</span>
<span class="sd">        out = prim(grad, eps)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.apply_came_part1(grad, eps)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.apply_came_part1` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ApplyCamePart2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ApplyCamePart2()</span>
<span class="sd">        out = prim(grad, sum_grad_r, sum_grad_c, sum_grad_rc, r, c, beta2, sum_r, global_shape)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.apply_came_part2(grad, sum_grad_r, sum_grad_c, sum_grad_rc, r, c, beta2, sum_r, global_shape)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.apply_came_part2` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_grad_r&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_grad_c&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_grad_rc&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_r&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;global_shape&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">sum_grad_r</span><span class="p">,</span> <span class="n">sum_grad_c</span><span class="p">,</span> <span class="n">sum_grad_rc</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">sum_r</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">global_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">sum_grad_r</span><span class="p">,</span> <span class="n">sum_grad_c</span><span class="p">,</span> <span class="n">sum_grad_rc</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">sum_r</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ApplyCamePart3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ApplyCamePart3()</span>
<span class="sd">        out = prim(u, m, eps, beta1, clip_threshold, sum_square_u, global_shape, use_first_moment)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.apply_came_part3(u, m, eps, beta1, clip_threshold, sum_square_u, global_shape, use_first_moment)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.apply_came_part3` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;u&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;eps&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;clip_threshold&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_square_u&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;global_shape&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;use_first_moment&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">clip_threshold</span><span class="p">,</span> <span class="n">sum_square_u</span><span class="p">,</span> <span class="n">global_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_first_moment</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">clip_threshold</span><span class="p">,</span> <span class="n">sum_square_u</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">,</span> <span class="n">use_first_moment</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ApplyCamePart4</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ApplyCamePart4()</span>
<span class="sd">        out = prim(param, m, r, c, weight_decay, lr, beta3, sum_r, sum_u_r, sum_u_c, sum_u_rc, global_shape)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.apply_came_part4(param, m, r, c, weight_decay, lr, beta3, sum_r, sum_u_r, sum_u_c, sum_u_rc, global_shape)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.apply_came_part4` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;param&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta3&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_r&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_u_r&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_u_c&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_u_rc&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;global_shape&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta3</span><span class="p">,</span> <span class="n">sum_r</span><span class="p">,</span> <span class="n">sum_u_r</span><span class="p">,</span> <span class="n">sum_u_c</span><span class="p">,</span> <span class="n">sum_u_rc</span><span class="p">,</span> <span class="n">global_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta3</span><span class="p">,</span> <span class="n">sum_r</span><span class="p">,</span> <span class="n">sum_u_r</span><span class="p">,</span> <span class="n">sum_u_c</span><span class="p">,</span> <span class="n">sum_u_rc</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Argmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Argmax(axis, output_type)</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.argmax_(input, axis, output_type)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.argmax_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;output_type&quot;</span><span class="p">,</span> <span class="n">output_type</span><span class="p">,</span> <span class="n">dtype_to_type_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_type</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Argmin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Argmin(axis, output_type)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.argmin_(x, axis, output_type)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.argmin_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;output_type&quot;</span><span class="p">,</span> <span class="n">output_type</span><span class="p">,</span> <span class="n">dtype_to_type_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_type</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AsinGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AsinGrad()</span>
<span class="sd">        out = prim(x, dout)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.asin_grad(x, dout)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.asin_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Asin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Asin()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.asin(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.asin` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AsinhGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AsinhGrad()</span>
<span class="sd">        out = prim(out, dout)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.asinh_grad(out, dout)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.asinh_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Asinh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Asinh()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.asinh(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.asinh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="AssignAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AssignAdd.html#mindspore.ops.AssignAdd">[文档]</a><span class="k">class</span> <span class="nc">AssignAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AssignAdd()</span>
<span class="sd">        out = prim(ref, value)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.assign_add(ref, value)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.assign_add` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;ref&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ref</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">ref</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Assign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Assign()</span>
<span class="sd">        out = prim(variable, value)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.assign(variable, value)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.assign` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Atan2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Atan2()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.atan2(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.atan2` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AtanGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AtanGrad()</span>
<span class="sd">        out = prim(x, dout)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.atan_grad(x, dout)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.atan_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Atan</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Atan()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.atan(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.atan` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Atanh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Atanh()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.atanh(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.atanh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AvgPoolGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AvgPoolGrad(kernel_size, strides, pad_mode, data_format)</span>
<span class="sd">        out = prim(x, out, dout)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops._avg_pool_grad(x, out, dout, kernel_size, strides, pad_mode, data_format)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops._avg_pool_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">to_kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">to_strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AvgPool</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AvgPool(kernel_size, strides, pad_mode, data_format)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.avg_pool(x, kernel_size, strides, pad_mode, data_format)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.avg_pool` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">to_kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">to_strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BatchNormGradGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.BatchNormGradGrad(is_training, epsilon, data_format)</span>
<span class="sd">        out = prim(x, dy, scale, saved_mean, saved_variance, dout_dx, dout_dscale, dout_dbias)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.batch_norm_grad_grad(x, dy, scale, saved_mean, saved_variance, dout_dx, dout_dscale, dout_dbias, is_training, epsilon, data_format)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.batch_norm_grad_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">saved_mean</span><span class="p">,</span> <span class="n">saved_variance</span><span class="p">,</span> <span class="n">dout_dx</span><span class="p">,</span> <span class="n">dout_dscale</span><span class="p">,</span> <span class="n">dout_dbias</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">saved_mean</span><span class="p">,</span> <span class="n">saved_variance</span><span class="p">,</span> <span class="n">dout_dx</span><span class="p">,</span> <span class="n">dout_dscale</span><span class="p">,</span> <span class="n">dout_dbias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BatchNormGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.BatchNormGrad(is_training, epsilon, data_format)</span>
<span class="sd">        out = prim(dout, x, scale, saved_mean, saved_variance, reserve)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.batch_norm_grad(dout, x, scale, saved_mean, saved_variance, reserve, is_training, epsilon, data_format)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.batch_norm_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">saved_mean</span><span class="p">,</span> <span class="n">saved_variance</span><span class="p">,</span> <span class="n">reserve</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">saved_mean</span><span class="p">,</span> <span class="n">saved_variance</span><span class="p">,</span> <span class="n">reserve</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Betainc</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Betainc()</span>
<span class="sd">        out = prim(a, b, x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.betainc(a, b, x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.betainc` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BiasAddGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.BiasAddGrad(data_format)</span>
<span class="sd">        out = prim(dout)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.bias_add_grad(dout, data_format)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.bias_add_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BiasAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.BiasAdd(data_format)</span>
<span class="sd">        out = prim(input_x, bias)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.bias_add(input_x, bias, data_format)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.bias_add` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BoolNot</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.BoolNot()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.bool_not(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.bool_not` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Ceil</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Ceil()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.ceil(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.ceil` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.CeLU(alpha)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.celu_(x, alpha)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.celu_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CholeskyGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.CholeskyGrad()</span>
<span class="sd">        out = prim(x, grad)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cholesky_grad(x, grad)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cholesky_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CholeskyInverse</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.CholeskyInverse(upper)</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cholesky_inverse_(input_x, upper)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cholesky_inverse_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;upper&quot;</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Cholesky</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Cholesky(upper)</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cholesky_(input_x, upper)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cholesky_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;upper&quot;</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Complex</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Complex()</span>
<span class="sd">        out = prim(real, imag)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.complex(real, imag)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.complex` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Concat</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Concat(axis)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.concat_(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.concat_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Conj</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Conj()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.conj(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.conj` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Contiguous</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Contiguous()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.contiguous(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.contiguous` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_contiguous</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span>

<span class="k">class</span> <span class="nc">Copy</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Copy()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.copy(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.copy` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span>

<span class="k">class</span> <span class="nc">Cos</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Cos()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cos(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cos` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_cos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]))</span>

<span class="k">class</span> <span class="nc">Cosh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Cosh()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cosh(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cosh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CumProd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.CumProd(exclusive, reverse)</span>
<span class="sd">        out = prim(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cum_prod(x, axis, exclusive, reverse)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cum_prod` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;exclusive&quot;</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">exclusive</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CumSum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.CumSum(exclusive, reverse)</span>
<span class="sd">        out = prim(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cum_sum(x, axis, exclusive, reverse)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cum_sum` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;exclusive&quot;</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">exclusive</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Cummax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Cummax(axis)</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cummax(input, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cummax` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Cummin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Cummin(axis)</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cummin_(input, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cummin_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Diag</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Diag()</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.diag(input_x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.diag` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Diagonal</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Diagonal(offset, dim1, dim2)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.diagonal(x, offset, dim1, dim2)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.diagonal` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;dim1&quot;</span><span class="p">,</span> <span class="n">dim1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;dim2&quot;</span><span class="p">,</span> <span class="n">dim2</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim2</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Div</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Div()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.div_(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.div_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Eig</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Eig(compute_v)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.eig_(x, compute_v)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.eig_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_v</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">EluGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.EluGrad()</span>
<span class="sd">        out = prim(dout, out)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.elu_grad(dout, out)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.elu_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Elu</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Elu(alpha)</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.elu(input_x, alpha)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.elu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Equal</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Equal()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.equal(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.equal` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Erf</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Erf()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.erf(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.erf` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Erfc</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Erfc()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.erfc(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.erfc` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Erfinv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Erfinv()</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.erfinv(input_x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.erfinv` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Exp()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.exp(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.exp` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ExpandDims</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ExpandDims()</span>
<span class="sd">        out = prim(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.expand_dims(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.expand_dims` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Expm1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Expm1()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.expm1(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.expm1` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Eye</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Eye()</span>
<span class="sd">        out = prim(n, m, dtype)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.eye_(n, m, dtype)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.eye_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype_to_type_id</span><span class="p">(</span><span class="s1">&#39;Eye&#39;</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">FastGeLUGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FastGeLUGrad()</span>
<span class="sd">        out = prim(dy, x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.fast_gelu_grad(dy, x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.fast_gelu_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FastGeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FastGeLU()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.fast_gelu(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.fast_gelu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FFTWithSize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FFTWithSize(signal_ndim, inverse, real, norm, onesided, signal_sizes)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.fft_with_size_(x, signal_ndim, inverse, real, norm, onesided, signal_sizes)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.fft_with_size_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">inverse</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;backward&#39;</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">signal_sizes</span><span class="o">=</span><span class="p">()):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;signal_ndim&quot;</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;inverse&quot;</span><span class="p">,</span> <span class="n">inverse</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;real&quot;</span><span class="p">,</span> <span class="n">real</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="n">norm</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;onesided&quot;</span><span class="p">,</span> <span class="n">onesided</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;signal_sizes&quot;</span><span class="p">,</span> <span class="n">signal_sizes</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">signal_ndim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">real</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">onesided</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">signal_sizes</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FFTShift</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FFTShift(forward)</span>
<span class="sd">        out = prim(x, axes)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.fftshift(x, axes, forward)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.fftshift` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axes&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">forward</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;forward&quot;</span><span class="p">,</span> <span class="n">forward</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Flatten()</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.flatten_(input_x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.flatten_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FloorDiv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FloorDiv()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.floor_div(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.floor_div` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FloorMod</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FloorMod()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.floor_mod(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.floor_mod` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Floor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Floor()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.floor(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.floor` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GatherDGradV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GatherDGradV2()</span>
<span class="sd">        out = prim(x, dim, index, dout)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gather_d_grad_v2(x, dim, index, dout)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gather_d_grad_v2` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GatherD</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GatherD()</span>
<span class="sd">        out = prim(x, dim, index)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gather_d(x, dim, index)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gather_d` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GatherNd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GatherNd()</span>
<span class="sd">        out = prim(input_x, indices)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gather_nd(input_x, indices)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gather_nd` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Gather</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Gather(batch_dims)</span>
<span class="sd">        out = prim(input_params, input_indices, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gather(input_params, input_indices, axis, batch_dims)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gather` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;batch_dims&quot;</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dims</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Gcd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Gcd()</span>
<span class="sd">        out = prim(x1, x2)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gcd(x1, x2)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gcd` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GeLUGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GeLUGrad()</span>
<span class="sd">        out = prim(dy, x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gelu_grad(dy, x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gelu_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GeLU()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gelu_(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gelu_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Geqrf</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Geqrf()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.geqrf(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.geqrf` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GreaterEqual</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GreaterEqual()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.greater_equal(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.greater_equal` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Greater</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Greater()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.greater(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.greater` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GridSampler2DGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GridSampler2DGrad(interpolation_mode, padding_mode, align_corners)</span>
<span class="sd">        out = prim(grad, input_x, grid)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.grid_sampler_2d_grad(grad, input_x, grid, interpolation_mode, padding_mode, align_corners)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.grid_sampler_2d_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;interpolation_mode&quot;</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;padding_mode&quot;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GridSampler2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GridSampler2D(interpolation_mode, padding_mode, align_corners)</span>
<span class="sd">        out = prim(input_x, grid)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.grid_sampler_2d(input_x, grid, interpolation_mode, padding_mode, align_corners)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.grid_sampler_2d` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;interpolation_mode&quot;</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;padding_mode&quot;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GridSampler3DGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GridSampler3DGrad(interpolation_mode, padding_mode, align_corners)</span>
<span class="sd">        out = prim(grad, input_x, grid)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.grid_sampler_3d_grad(grad, input_x, grid, interpolation_mode, padding_mode, align_corners)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.grid_sampler_3d_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;interpolation_mode&quot;</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;padding_mode&quot;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">GridSampler3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GridSampler3D(interpolation_mode, padding_mode, align_corners)</span>
<span class="sd">        out = prim(input_x, grid)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.grid_sampler_3d(input_x, grid, interpolation_mode, padding_mode, align_corners)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.grid_sampler_3d` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;interpolation_mode&quot;</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;padding_mode&quot;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">HShrinkGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.HShrinkGrad(lambd)</span>
<span class="sd">        out = prim(gradients, features)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.hshrink_grad(gradients, features, lambd)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.hshrink_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;lambd&quot;</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">HShrink</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.HShrink(lambd)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.hshrink(x, lambd)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.hshrink` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;lambd&quot;</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">HSigmoidGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.HSigmoidGrad()</span>
<span class="sd">        out = prim(grads, input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.hsigmoid_grad(grads, input_x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.hsigmoid_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">input_x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">HSigmoid</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.HSigmoid()</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.hsigmoid(input_x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.hsigmoid` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">HSwishGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.HSwishGrad()</span>
<span class="sd">        out = prim(y_grad, x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.hswish_grad(y_grad, x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.hswish_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_grad</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">y_grad</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">HSwish</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.HSwish()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.hardswish_(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.hardswish_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Identity</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Identity()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.deepcopy(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.deepcopy` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LayerNormGradGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LayerNormGradGrad(begin_norm_axis, begin_params_axis)</span>
<span class="sd">        out = prim(x, dy, variance, mean, gamma, d_dx, d_dg, d_db)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.layer_norm_grad_grad(x, dy, variance, mean, gamma, d_dx, d_dg, d_db, begin_norm_axis, begin_params_axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.layer_norm_grad_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_norm_axis&quot;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_params_axis&quot;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">d_dx</span><span class="p">,</span> <span class="n">d_dg</span><span class="p">,</span> <span class="n">d_db</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">d_dx</span><span class="p">,</span> <span class="n">d_dg</span><span class="p">,</span> <span class="n">d_db</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_params_axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LayerNormGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LayerNormGrad(begin_norm_axis, begin_params_axis)</span>
<span class="sd">        out = prim(x, dy, variance, mean, gamma)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.layer_norm_grad(x, dy, variance, mean, gamma, begin_norm_axis, begin_params_axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.layer_norm_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_norm_axis&quot;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_params_axis&quot;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_params_axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LayerNorm(begin_norm_axis, begin_params_axis, epsilon)</span>
<span class="sd">        out = prim(input_x, gamma, beta)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.layer_norm(input_x, gamma, beta, begin_norm_axis, begin_params_axis, epsilon)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.layer_norm` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_norm_axis&quot;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_params_axis&quot;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_params_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LessEqual</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LessEqual()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.less_equal(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.less_equal` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Less</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Less()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.less(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.less` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LinSpace</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LinSpace()</span>
<span class="sd">        out = prim(start, stop, num)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.lin_space_(start, stop, num)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.lin_space_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">num</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Log1p</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Log1p()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.log1p(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.log1p` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LogMatrixDeterminant</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LogMatrixDeterminant()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.log_matrix_determinant(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.log_matrix_determinant` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Log</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Log()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.log(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.log` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;cust_aicpu&quot;</span><span class="p">,</span> <span class="s1">&#39;Log&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;base&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;shift&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LogSoftmaxGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LogSoftmaxGrad(axis)</span>
<span class="sd">        out = prim(logits, grad)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.log_softmax_grad(logits, grad, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.log_softmax_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LogSoftmax(axis)</span>
<span class="sd">        out = prim(logits)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.log_softmax(logits, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.log_softmax` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LogicalAnd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LogicalAnd()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.logical_and_(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.logical_and_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LogicalNot</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LogicalNot()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.logical_not_(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.logical_not_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LogicalOr</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LogicalOr()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.logical_or_(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.logical_or_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LogicalXor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LogicalXor()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.logical_xor_(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.logical_xor_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LogitGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LogitGrad(eps)</span>
<span class="sd">        out = prim(grad, input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.logit_grad(grad, input, eps)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.logit_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Logit</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Logit(eps)</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.logit(input, eps)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.logit` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MaskedFill</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.MaskedFill()</span>
<span class="sd">        out = prim(input, mask, value)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.masked_fill(input, mask, value)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.masked_fill` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MatrixDeterminant</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.MatrixDeterminant()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.matrix_determinant(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.matrix_determinant` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MatrixExp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.MatrixExp()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.matrix_exp(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.matrix_exp` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MaximumGradGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.MaximumGradGrad(grad_x, grad_y)</span>
<span class="sd">        out = prim(x, y, dx, dy)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.maximum_grad_grad(x, y, dx, dy, grad_x, grad_y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.maximum_grad_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad_y</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_x&quot;</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_y&quot;</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MaximumGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.MaximumGrad(grad_x, grad_y)</span>
<span class="sd">        out = prim(x, y, grads)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.maximum_grad(x, y, grads, grad_x, grad_y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.maximum_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad_y</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_x&quot;</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_y&quot;</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Maximum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Maximum()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.maximum(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.maximum` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MinimumGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.MinimumGrad(grad_x, grad_y)</span>
<span class="sd">        out = prim(x1, x2, grads)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.minimum_grad(x1, x2, grads, grad_x, grad_y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.minimum_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad_y</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_x&quot;</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_y&quot;</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Minimum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Minimum()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.minimum(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.minimum` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Mul</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Mul()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.mul(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.mul` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_READ</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_READ</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NanToNum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.NanToNum(nan, posinf, neginf)</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.nan_to_num_(input, nan, posinf, neginf)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.nan_to_num_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">,</span> <span class="n">nan</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;posinf&quot;</span><span class="p">,</span> <span class="n">posinf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;neginf&quot;</span><span class="p">,</span> <span class="n">neginf</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">posinf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">neginf</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Neg</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Neg()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.neg(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.neg` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NextAfter</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.NextAfter()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.next_after(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.next_after` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NLLLossGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.NLLLossGrad(reduction, ignore_index)</span>
<span class="sd">        out = prim(logits, loss_grad, labels, weight, total_weight)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.nllloss_grad(logits, loss_grad, labels, weight, total_weight, reduction, ignore_index)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.nllloss_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;ignore_index&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss_grad</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">total_weight</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss_grad</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">total_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NLLLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.NLLLoss(reduction, ignore_index)</span>
<span class="sd">        out = prim(logits, labels, weight)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.nll_loss_(logits, labels, weight, reduction, ignore_index)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.nll_loss_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;ignore_index&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NonZero</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.NonZero()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.non_zero(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.non_zero` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NotEqual</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.NotEqual()</span>
<span class="sd">        out = prim(x1, x2)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.not_equal(x1, x2)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.not_equal` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OneHot</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.OneHot(axis)</span>
<span class="sd">        out = prim(indices, depth, on_value, off_value)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.one_hot_(indices, depth, on_value, off_value, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.one_hot_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">OnesLike</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.OnesLike()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.ones_like_(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.ones_like_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Pow</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Pow()</span>
<span class="sd">        out = prim(x1, x2)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.pow(x1, x2)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.pow` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PReLUGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.PReLUGrad()</span>
<span class="sd">        out = prim(dy, x, weight)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.prelu_grad(dy, x, weight)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.prelu_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PReLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.PReLU()</span>
<span class="sd">        out = prim(x, weight)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.prelu(x, weight)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.prelu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Qr</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Qr(full_matrices)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.qr_(x, full_matrices)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.qr_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_matrices</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">RandpermV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.RandpermV2(seed, offset, dtype)</span>
<span class="sd">        out = prim(n)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.randperm(n, seed, offset, dtype)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.randperm` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;RandpermV2&#39;</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TENSOR</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;RandpermV2&#39;</span><span class="p">,</span> <span class="s1">&#39;offset&#39;</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TENSOR</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">dtype_to_type_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Range</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Range(maxlen)</span>
<span class="sd">        out = prim(start, limit, delta)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.range(start, limit, delta, maxlen)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.range` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;maxlen&quot;</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxlen</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">RealDiv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.RealDiv()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.real_div(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.real_div` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Real</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Real()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.real(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.real` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReciprocalGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReciprocalGrad()</span>
<span class="sd">        out = prim(y, dy)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reciprocal_grad(y, dy)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reciprocal_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Reciprocal</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Reciprocal()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reciprocal_(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reciprocal_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReduceAll</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReduceAll(keep_dims)</span>
<span class="sd">        out = prim(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reduce_all(x, axis, keep_dims)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reduce_all` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReduceAny</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReduceAny(keep_dims)</span>
<span class="sd">        out = prim(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reduce_any(x, axis, keep_dims)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reduce_any` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReduceMax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReduceMax(keep_dims)</span>
<span class="sd">        out = prim(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reduce_max(x, axis, keep_dims)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reduce_max` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReduceMean</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReduceMean(keep_dims)</span>
<span class="sd">        out = prim(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reduce_mean(x, axis, keep_dims)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reduce_mean` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReduceMin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReduceMin(keep_dims)</span>
<span class="sd">        out = prim(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reduce_min(x, axis, keep_dims)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reduce_min` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReduceProd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReduceProd(keep_dims)</span>
<span class="sd">        out = prim(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reduce_prod(x, axis, keep_dims)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reduce_prod` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReduceStd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReduceStd(axis, unbiased, keep_dims)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reduce_std(x, axis, unbiased, keep_dims)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reduce_std` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[],</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;ReduceStd&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TENSOR</span><span class="p">),</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;unbiased&quot;</span><span class="p">,</span> <span class="n">unbiased</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unbiased</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReduceSum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReduceSum(keep_dims, skip_mode)</span>
<span class="sd">        out = prim(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reduce_sum(x, axis, keep_dims, skip_mode)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reduce_sum` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">skip_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;skip_mode&quot;</span><span class="p">,</span> <span class="n">skip_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_mode</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReLU6Grad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReLU6Grad()</span>
<span class="sd">        out = prim(y_backprop, x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops._relu6_grad(y_backprop, x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops._relu6_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReLU6</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReLU6()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.relu6(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.relu6` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReluGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReluGrad()</span>
<span class="sd">        out = prim(y_backprop, x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops._relu_grad(y_backprop, x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops._relu_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReLU()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.relu(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.relu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Reshape</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Reshape()</span>
<span class="sd">        out = prim(input, shape)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reshape(input, shape)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reshape` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeBicubicGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ResizeBicubicGrad(align_corners, half_pixel_centers)</span>
<span class="sd">        out = prim(grads, image)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.resize_bicubic_grad(grads, image, align_corners, half_pixel_centers)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.resize_bicubic_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeBicubic</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ResizeBicubic(align_corners, half_pixel_centers)</span>
<span class="sd">        out = prim(image, size)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.resize_bicubic(image, size, align_corners, half_pixel_centers)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.resize_bicubic` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeBilinearGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ResizeBilinearGrad(align_corners, half_pixel_centers)</span>
<span class="sd">        out = prim(grads, image)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.resize_bilinear_grad(grads, image, align_corners, half_pixel_centers)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.resize_bilinear_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeBilinearV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ResizeBilinearV2(align_corners, half_pixel_centers)</span>
<span class="sd">        out = prim(image, size)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.resize_bilinear(image, size, align_corners, half_pixel_centers)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.resize_bilinear` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeLinear1DGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ResizeLinear1DGrad(coordinate_transformation_mode)</span>
<span class="sd">        out = prim(grads, x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.resize_linear_1d_grad(grads, x, coordinate_transformation_mode)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.resize_linear_1d_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="o">=</span><span class="s1">&#39;align_corners&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;coordinate_transformation_mode&quot;</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coordinate_transformation_mode</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeLinear1D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ResizeLinear1D(coordinate_transformation_mode)</span>
<span class="sd">        out = prim(x, size)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.resize_linear_1d(x, size, coordinate_transformation_mode)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.resize_linear_1d` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="o">=</span><span class="s1">&#39;align_corners&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;coordinate_transformation_mode&quot;</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coordinate_transformation_mode</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeNearestNeighborGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ResizeNearestNeighborGrad(align_corners, half_pixel_centers)</span>
<span class="sd">        out = prim(grads, size)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.resize_nearest_neighbor_grad(grads, size, align_corners, half_pixel_centers)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.resize_nearest_neighbor_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeNearestNeighbor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ResizeNearestNeighbor(size, align_corners, half_pixel_centers)</span>
<span class="sd">        out = prim(image_in)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.resize_nearest_neighbor(image_in, size, align_corners, half_pixel_centers)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.resize_nearest_neighbor` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;ResizeNearestNeighbor&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image_in</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">image_in</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeNearestNeighborV2Grad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ResizeNearestNeighborV2Grad(align_corners, half_pixel_centers)</span>
<span class="sd">        out = prim(grads, size)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.resize_nearest_neighbor_v2_grad(grads, size, align_corners, half_pixel_centers)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.resize_nearest_neighbor_v2_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeNearestNeighborV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ResizeNearestNeighborV2(align_corners, half_pixel_centers)</span>
<span class="sd">        out = prim(image, size)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.resize_nearest_neighbor_v2(image, size, align_corners, half_pixel_centers)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.resize_nearest_neighbor_v2` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ReverseV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReverseV2(axis)</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reverse_v2(input, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reverse_v2` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;ReverseV2&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">RightShift</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.RightShift()</span>
<span class="sd">        out = prim(input_x, input_y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.right_shift(input_x, input_y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.right_shift` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Roll</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Roll(shift, axis)</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.roll_(input, shift, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.roll_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;shift&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;Roll&#39;</span><span class="p">,</span> <span class="s1">&#39;shift&#39;</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="p">(</span><span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">),</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;Roll&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">),</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shift</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Round</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Round()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.round(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.round` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">RsqrtGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.RsqrtGrad()</span>
<span class="sd">        out = prim(y_backprop, x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops._rsqrt_grad(y_backprop, x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops._rsqrt_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Rsqrt</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Rsqrt()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.rsqrt(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.rsqrt` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ScalarToTensor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ScalarToTensor()</span>
<span class="sd">        out = prim(input_x, dtype)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.scalar_to_tensor(input_x, dtype)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.scalar_to_tensor` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype_to_type_id</span><span class="p">(</span><span class="s1">&#39;ScalarToTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">ScatterNd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ScatterNd()</span>
<span class="sd">        out = prim(indices, updates, shape)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.scatter_nd(indices, updates, shape)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.scatter_nd` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SigmoidGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.SigmoidGrad()</span>
<span class="sd">        out = prim(y, dy)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops._sigmoid_grad(y, dy)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops._sigmoid_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_sigmoid_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">]))</span>

<span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sigmoid()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sigmoid(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sigmoid` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]))</span>

<span class="k">class</span> <span class="nc">SiLUGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.SiLUGrad()</span>
<span class="sd">        out = prim(dout, x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops._silu_grad(dout, x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops._silu_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_silu_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">dout</span><span class="p">,</span> <span class="n">x</span><span class="p">]))</span>

<span class="k">class</span> <span class="nc">SiLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.SiLU()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.silu(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.silu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_silu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]))</span>

<span class="k">class</span> <span class="nc">Sin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sin()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sin(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sin` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_sin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span>

<span class="k">class</span> <span class="nc">Sinc</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sinc()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sinc(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sinc` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Sinh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sinh()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sinh(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sinh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SoftmaxBackward</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.SoftmaxBackward()</span>
<span class="sd">        out = prim(dout, out, dim)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.softmax_backward(dout, out, dim)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.softmax_backward` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dout&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;out&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_softmax_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">dout</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="p">]))</span>

<span class="k">class</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Softmax(axis)</span>
<span class="sd">        out = prim(logits)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.softmax(logits, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.softmax` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;Softmax&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]))</span>

<span class="k">class</span> <span class="nc">Split</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Split(axis, output_num)</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.split_(input_x, axis, output_num)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.split_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_num</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;output_num&quot;</span><span class="p">,</span> <span class="n">output_num</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_num</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SqrtGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.SqrtGrad()</span>
<span class="sd">        out = prim(dy, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sqrt_grad(dy, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sqrt_grad` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Sqrt</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sqrt()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sqrt_(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sqrt_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Square()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.square_(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.square_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TensorCopySlices</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.TensorCopySlices()</span>
<span class="sd">        out = prim(x, value, begin, end, strides)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.tensor_copy_slices(x, value, begin, end, strides)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.tensor_copy_slices` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TensorShape</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.TensorShape()</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.tensor_shape(input_x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.tensor_shape` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Trace</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Trace()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.trace(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.trace` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Transpose</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Transpose()</span>
<span class="sd">        out = prim(x, perm)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.transpose(x, perm)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.transpose` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">TupleToTensor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.TupleToTensor()</span>
<span class="sd">        out = prim(input_tuple, dtype)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.tuple_to_tensor(input_tuple, dtype)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.tuple_to_tensor` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_tuple&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tuple</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_tuple</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype_to_type_id</span><span class="p">(</span><span class="s1">&#39;TupleToTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">View</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.View()</span>
<span class="sd">        out = prim(input, shape)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.view(input, shape)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.view` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ZerosLike</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ZerosLike()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.zeros_like_(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.zeros_like_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">abs_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for abs operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">abs_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">AbsGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">abs_grad_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns absolute value of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = |input_i|</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-1.0, 1.0, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.abs(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 1. 0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">abs_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Abs</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">abs_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">acos_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ACosGrad of input element-wise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">acos_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ACosGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">acos_grad_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">acos</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arccosine of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \cos^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.acos(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.737726  1.5307857 1.2661036 0.9764105]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">acos_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ACos</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">acos_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">acosh_grad</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of Acosh operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">acosh_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">AcoshGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">acosh_grad_op</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">acosh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic cosine of the inputs element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \cosh^{-1}(input_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Given an input tensor input, the function computes inverse hyperbolic cosine of every element.</span>
<span class="sd">        Input range is [1, inf].</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of inverse hyperbolic cosine function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.acosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.9624237 1.7627472 5.298292 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">acosh_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Acosh</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">acosh_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds other value to input Tensor.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} + other_{i}</span>

<span class="sd">    Note:</span>
<span class="sd">        - When the two inputs have different shapes,</span>
<span class="sd">          they must be able to broadcast to a common shape.</span>
<span class="sd">        - The two inputs can not be bool type at the same time,</span>
<span class="sd">          [True, Tensor(True, bool\_), Tensor(np.array([True]), bool\_)] are all considered bool type.</span>
<span class="sd">        - The two inputs comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one of the input `input` , `other` after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, number.Number, bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: x and y are both Tensor.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 7. 9.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: x is a scalar and y is a Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 6. 7.]</span>
<span class="sd">        &gt;&gt;&gt; # the data type of x is int32, the data type of y is float32,</span>
<span class="sd">        &gt;&gt;&gt; # and the output is the data format of higher precision float32.</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">add_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Add</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">add_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">addcdiv_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the element-wise division of tensor tensor1 by tensor tensor2,</span>
<span class="sd">    multiply the result by the scalar value and add it to input_data.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y[i] = input[i] + value[i] * (tensor1[i] / tensor2[i])</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The tensor to be added.</span>
<span class="sd">        tensor1 (Tensor): The numerator tensor.</span>
<span class="sd">        tensor2 (Tensor): The denominator tensor.</span>
<span class="sd">        value (Tensor): The multiplier for tensor1/tensor2.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as tensor1/tensor2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `tensor1`, `tensor2`, `input` is not tensor.</span>
<span class="sd">        ValueError: If `tensor1` could not be broadcast to a tensor with shape of `tensor2`.</span>
<span class="sd">        ValueError: If `value` could not be broadcast to tensors with shapes of `tensor1/tensor2`.</span>
<span class="sd">        ValueError: If `input` could not be broadcast to tensors with shapes of `value*(tensor1/tensor2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([4, 3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.addcdiv_(input_data, x1, x2, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1.25      1.6666667 2.5       5.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">addcdiv_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Addcdiv</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">addcdiv_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">addcmul_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the element-wise product of tensor tensor1 and tensor tensor2,</span>
<span class="sd">    multiply the result by the scalar value and add it to input_data.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[i] = input[i] + value[i] * (tensor1[i] * tensor2[i])</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The tensor to be added.</span>
<span class="sd">        tensor1 (Tensor): The tensor to be multiplied.</span>
<span class="sd">        tensor2 (Tensor): The tensor to be multiplied.</span>
<span class="sd">        value (Tensor): The multiplier for tensor1*tensor2.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as x1*x2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `tensor1`, `tensor2`, `input` is not Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not one of: float32, float16, int32.</span>
<span class="sd">        TypeError: If dtype of `tensor1` or `tensor2` is not one of: float32, float16, int32.</span>
<span class="sd">        TypeError: If dtype of `value` is not one of: float32, float16, int32.</span>
<span class="sd">        ValueError: If `tensor1` could not be broadcast to a tensor with shape of `tensor2`.</span>
<span class="sd">        ValueError: If `value` could not be broadcast to tensors with shapes of `tensor1` * `tensor2`.</span>
<span class="sd">        ValueError: If `input` could not be broadcast to tensors with shapes of `value*(tensor1*tensor2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1], [2], [3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([[1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.addcmul_(input_data, x1, x2, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 2.  3.  4.]</span>
<span class="sd">        [ 3.  5.  7.]</span>
<span class="sd">        [ 4.  7. 10.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">addcmul_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Addcmul</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">addcmul_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">angle</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the element-wise argument of a complex tensor.</span>
<span class="sd">    The elements in input are considered to be complex numbers of the form a+bj, where a is the real part and b</span>
<span class="sd">    is the imaginary part. The argument returned by this function is of the form :math:`atan2(b, a)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      input (Tensor):</span>
<span class="sd">        The input tensor. types: complex64, complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tensor, has the float32 or float64 type and the same shape as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `input` is not a Tensor.</span>
<span class="sd">      TypeError:</span>
<span class="sd">        If the dtype of `input` is not one of: complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">      ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">      &gt;&gt;&gt; import mindspore</span>
<span class="sd">      &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">      &gt;&gt;&gt; input = Tensor([-1.5 + 7.8j, 3 + 5.75j], mindspore.complex64)</span>
<span class="sd">      &gt;&gt;&gt; output = ops.angle(input)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [1.7607845 1.0899091]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">angle_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Angle</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">angle_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">argmax_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the maximum value along a specified `axis` of a Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. :math:`(N, *)` where :math:`*` means, any number of additional</span>
<span class="sd">              dimensions.</span>
<span class="sd">        axis (int): Axis where the Argmax operation applies to. Default: ``-1`` .</span>
<span class="sd">        output_type (:class:`mindspore.dtype`): Output data type.</span>
<span class="sd">            Supported types: ``mstype.int32`` , ``mstype.int64`` . Default: ``mstype.int32`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, indices of the max value of input tensor across the axis.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[1, 20, 5], [67, 8, 9], [130, 24, 15]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.argmax_(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 0 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">argmax_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Argmax</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">output_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">argmax_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">argmin_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the minimum value along a specified `axis` of a Tensor.</span>

<span class="sd">    If the shape of input tensor is :math:`(x_1, ..., x_N)`, the shape of the output tensor is</span>
<span class="sd">    :math:`(x_1, ..., x_{axis-1}, x_{axis+1}, ..., x_N)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor. The shape is :math:`(N, *)` where :math:`*` means, </span>
<span class="sd">            any number of additional dimensions.</span>
<span class="sd">        axis (int): Axis where the Argmin operation applies to. Default: ``-1`` .</span>
<span class="sd">        output_type (:class:`mindspore.dtype`): Output data type.</span>
<span class="sd">            Supported types: ``mstype.int32`` , ``mstype.int64`` . Default: ``mstype.int32`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose dtype is determined by `output_type`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If `output_type` is neither int32 nor int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([2.0, 3.1, 1.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index = ops.argmin_(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(index)</span>
<span class="sd">        2</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">argmin_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Argmin</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">output_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">argmin_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">asin_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes AsinGrad of input element-wise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">asin_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">AsinGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">asin_grad_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">asin</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arcsine of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \sin^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32, float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.asin(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.8330704  0.04001067  0.30469266  0.5943858 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">asin_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Asin</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">asin_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">asinh_grad</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of Asinh operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">asinh_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">AsinhGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">asinh_grad_op</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">asinh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \sinh^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of inverse hyperbolic sine function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-5.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.asinh(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.3124382  1.1947632  1.8184465  5.298342 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">asinh_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Asinh</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">asinh_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="assign_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.assign_add.html#mindspore.ops.assign_add">[文档]</a><span class="k">def</span> <span class="nf">assign_add</span><span class="p">(</span><span class="n">ref</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates a `Parameter` by adding a value to it.</span>

<span class="sd">    Args of `variable` and `value` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>
<span class="sd">    If `value` is a number, the number is automatically converted to Tensor,</span>
<span class="sd">    and the data type is consistent with the Tensor data type involved in the operation.</span>

<span class="sd">    Note:</span>
<span class="sd">        Since `variable` is a data type Parameter, the data type cannot be changed,</span>
<span class="sd">        so only the type of `value` is allowed to be promoted to the type of `variable`.</span>
<span class="sd">        And the conversion type supported by different devices will be different,</span>
<span class="sd">        it is recommended to use the same data type when using this operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        variable (Parameter): The `Parameter`.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        value (Tensor): The value to be added to the `variable`.</span>
<span class="sd">            It must have the same shape as `variable`.</span>
<span class="sd">            it is recommended to use the same data type when using this operator.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as original `variable`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `value` is neither Number nor Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `variable` and `value` conversion of Parameter</span>
<span class="sd">                    is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; variable = mindspore.Parameter(initializer(1, [1], mindspore.int32), name=&quot;global_step&quot;)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(np.ones([1]).astype(np.int32) * 100)</span>
<span class="sd">        &gt;&gt;&gt; ops.assign_add(variable, value)</span>
<span class="sd">        &gt;&gt;&gt; print(variable.asnumpy())</span>
<span class="sd">        [101]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">assign_add_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">AssignAdd</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">assign_add_op</span><span class="p">(</span><span class="n">ref</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Assigns `Parameter` with a value.</span>

<span class="sd">    Args of `variable` and `value` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **variable** (Parameter) - The `Parameter`. :math:`(N,*)` where :math:`*` means,</span>
<span class="sd">        any number of additional dimensions, its rank should be less than 8.</span>
<span class="sd">        - **value** (Tensor) - The value to be assigned, has the same shape with `variable`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as original `variable`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `variable` is not a Parameter.</span>
<span class="sd">        TypeError: If `value` is not a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `variable` and `value` conversion of Parameter</span>
<span class="sd">                    is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([2.0], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; variable = mindspore.Parameter(Tensor([1.0], mindspore.float32), name=&quot;variable&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ops.assign(variable, value)</span>
<span class="sd">        &gt;&gt;&gt; print(variable.asnumpy())</span>
<span class="sd">        [2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">assign_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Assign</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">assign_op</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">atan2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns arctangent of input/other element-wise.</span>

<span class="sd">    It returns :math:`\theta\ \in\ [-\pi, \pi]`</span>
<span class="sd">    such that :math:`input = r*\sin(\theta), other = r*\cos(\theta)`, where :math:`r = \sqrt{input^2 + other^2}`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Arg `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">          If they have different data types, the lower precision data type will be converted to relatively the</span>
<span class="sd">          highest precision data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor, Number.number): The input tensor or scalar.</span>
<span class="sd">        other (Tensor, Number.number): The input tensor or scalar. It has the same shape with `input` or</span>
<span class="sd">            its shape is able to broadcast with `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is same as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor or scalar.</span>
<span class="sd">        RuntimeError: If the data type of `input` and `other` conversion of Parameter is required</span>
<span class="sd">                    when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atan2(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.7853982]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">atan2_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Atan2</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">atan2_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">atan_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes AtanGrad of input element-wise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">atan_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">AtanGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">atan_grad_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">atan</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the trigonometric inverse tangent of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \tan^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            The data type should be one of the following types: float16, float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atan(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7853982 0.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">atan_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Atan</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">atan_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">atanh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic tangent of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \tanh^{-1}(input_{i})</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            The data type should be one of the following types: float16, float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0, -0.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atanh(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.         -0.54930615]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">atanh_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Atanh</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">atanh_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_avg_pool_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradients of the avg pool operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">avg_pool_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">AvgPoolGrad</span><span class="p">)(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_pool_grad_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">avg_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average pooling operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the average value,</span>
<span class="sd">            is an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively. Default: 1.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: 1.</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is &#39;same&#39; or &#39;valid&#39;.</span>
<span class="sd">            Default: &#39;valid&#39;.</span>

<span class="sd">            - same: The height and width of the output are the same as the input divided by &#39;strides&#39;</span>
<span class="sd">            and rounded up.</span>

<span class="sd">            - valid: Returns the output of the valid calculation without filling. Redundant pixels that</span>
<span class="sd">            do not satisfy the calculation will be discarded.</span>
<span class="sd">        data_format (str): The format of input and output data. It should be &#39;NHWC&#39; or &#39;NCHW&#39;.</span>
<span class="sd">            Default: &#39;NCHW&#39;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output(Tensor), with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; nor &#39;NHWC&#39;.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         result = ops.avg_pool_(x, kernel_size=2, strides=1, pad_mode=&quot;VALID&quot;, data_format=&quot;NCHW&quot;)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape(1, 3, 3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 2.5   3.5   4.5]</span>
<span class="sd">        [ 6.5   7.5   8.5]]</span>
<span class="sd">        [[14.5  15.5  16.5]</span>
<span class="sd">        [18.5  19.5  20.5]]</span>
<span class="sd">        [[26.5  27.5  28.5]</span>
<span class="sd">        [30.5  31.5  32.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">avg_pool_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">AvgPool</span><span class="p">)(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avg_pool_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">betainc</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the regularized incomplete beta function</span>
<span class="sd">    :math:`I_{x}(a, b)`. It is defined as the ratio of the incomplete beta function</span>
<span class="sd">    to the complete beta function:</span>

<span class="sd">    .. math::</span>

<span class="sd">    I_{x}(a, b)=\frac{B(x ; a, b)}{B(a, b)}</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>

<span class="sd">    B(x ; a, b)=\int_{0}^{x} t^{a-1}(1-t)^{b-1} dt</span>

<span class="sd">    is the incomplete beta function and</span>

<span class="sd">    .. math::</span>

<span class="sd">    B(a, b) = \int_0^1 t^{a-1} (1-t)^{b-1} dt</span>

<span class="sd">    is the complete beta function.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **a** (Tensor) - Peak location of beta distribution.</span>
<span class="sd">              A Tensor of types: float32, float64.</span>
<span class="sd">        - **b** (Tensor) - Spread of the beta distribution.</span>
<span class="sd">              A Tensor, must have the same dtype and shape as `a` .</span>
<span class="sd">        - **x** (Tensor) - Upper limit of integration of the incomplete beta function.</span>
<span class="sd">              A Tensor, must have the same dtype and shape as `a` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, has the same dtype and shape as `a` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `a` is not float32 nor float64.</span>
<span class="sd">        TypeError: If either dtype of `b` and `x` is not the same as the `a`.</span>
<span class="sd">        ValueError: If either shape of `b` and `x` is not the same as the `a`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([0.3, 0.1, 0.4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.array([0.4, 0.5, 0.9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.2, 0.6, 0.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.betainc(a, b, x)</span>
<span class="sd">        &gt;&gt;&gt; print(betainc(output))</span>
<span class="sd">        [0.41462693 0.8706035  0.7298298 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">betainc_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Betainc</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">betainc_op</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">bias_add_grad</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients of BiasAdd.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bias_add_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">BiasAddGrad</span><span class="p">)(</span><span class="n">data_format</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bias_add_grad_op</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">bias_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the sum of the input Tensor and the bias Tensor. Before adding, the bias Tensor will be broadcasted to be</span>
<span class="sd">    consistent with the shape of the input Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The shape can be 2-5 dimensions.</span>
<span class="sd">        - **bias** (Tensor) - The bias tensor, with shape :math:`(C)`. C must be the same as channel dimension C of</span>
<span class="sd">            `input_x`.</span>
<span class="sd">        - **data_format** (str, optional): The format of input and output data.</span>
<span class="sd">                            It should be ``&quot;NHWC&quot;`` , ``&quot;NCHW&quot;`` or ``&quot;NCDHW&quot;`` .</span>
<span class="sd">                            Default is ``&quot;NCHW&quot;`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `data_format` is not a str.</span>
<span class="sd">        ValueError: If value of `data_format` is not in the range of [&#39;NHWC&#39;,&#39;NCHW&#39;,&#39;NCDHW&#39;].</span>
<span class="sd">        TypeError: If `input_x` or `bias` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `bias` is inconsistent.</span>
<span class="sd">        TypeError: If dimension of `input_x` is not in the range [2, 5].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.random.random(3).reshape((3,)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bias_add(input_x, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bias_add_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">BiasAdd</span><span class="p">)(</span><span class="n">data_format</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bias_add_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">bool_not</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bool_not `not` of bool input.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The inputs can be constant/variable value. Usage is the same as &#39;not&#39; in Python.</span>
<span class="sd">        This primitive only have &#39;CPU&#39; implementation, for other platform, it runs using heterogeneous.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Scalar) - A constant or variable scalar, the type can be bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Scalar, the type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` are not bool scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bool_not_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">BoolNot</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">bool_not_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">ceil</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds a tensor up to the closest integer element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ceil` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor with a dtype of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ceil(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2.  3. -1.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ceil(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        3.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ceil_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Ceil</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">ceil_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">celu_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes CeLU (Continuously differentiable exponential linear units) of input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.celu` for more details.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor) - The input tensor with a dtype of float16 or float32.</span>
<span class="sd">        alpha (float, optional): - The :math:`\alpha` value for the Celu formulation. Default: ``1.0`` .</span>
<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-2.0, -1.0, 1.0, 2.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.celu_(input_x, alpha=1.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.86466473 -0.63212055  1.          2.        ]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.celu(input_x, alpha=1.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        2.1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">celu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">CeLU</span><span class="p">)(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">celu_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">cholesky_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the reverse mode backpropgated gradient of the Cholesky algorithm.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **x** (Tensor) - A tensor with float32 or float64 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor with float32 or float64 data type. `grad` should have</span>
<span class="sd">          the same dtype with `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `a` and `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If x is not Tensor.</span>
<span class="sd">        TypeError: If grad is not Tensor.</span>
<span class="sd">        TypeError: If dtype of input x and grad is not float64 nor float32,</span>
<span class="sd">        TypeError: If x has different dtype with grad.</span>
<span class="sd">        ValueError: If input tensor&#39;s last two dims are not equal,</span>
<span class="sd">        ValueError: If the shape of x and grad mismatch.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cholesky_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">CholeskyGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">cholesky_grad_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">cholesky_inverse_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the inverse of the positive definite matrix using cholesky matrix factorization given its Cholesky factor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cholesky_inverse` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        upper(bool, optional): Whether to return a lower or upper triangular matrix. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor whose rank is 2. Supported dtypes: float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 1], [1, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.cholesky_inverse_(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 5.0  -3.0 ]</span>
<span class="sd">        [-3.0   2.0 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cholesky_inverse_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">CholeskyInverse</span><span class="p">)(</span><span class="n">upper</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cholesky_inverse_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">cholesky_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the Cholesky decomposition on a single or a batch of symmetric positive-definite matrices.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cholesky` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor) - Tensor of shape :math:`(*, N, N)`, where :math:`*` is zero or more batch dimensions</span>
<span class="sd">        consisting of symmetric positive-definite matrices, with float32 or float64 data type.</span>
<span class="sd">        upper (bool, optional): - Flag that indicates whether to return a upper or lower triangular matrix.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1.0, 1.0], [1.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cholesky_(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0.]</span>
<span class="sd">        [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cholesky_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Cholesky</span><span class="p">)(</span><span class="n">upper</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cholesky_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">complex</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a complex Tensor from the real part and the imag part.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **real** (Tensor) - The real input tensor. types: float32, float64.</span>
<span class="sd">        - **imag** (Tensor) - The imag input tensor. types: float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the complex type.</span>

<span class="sd">    Raises:</span>
<span class="sd">    TypeError: If the dtype of input is not one of: float32, float64.</span>
<span class="sd">    TypeError: If the dtypes of two inputs are not same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; real = Tensor(np.array([1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; imag = Tensor(np.array([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.complex(real, imag)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.+2.j]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">complex_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Complex</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">complex_op</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">concat_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Connect input tensors along with the given axis.</span>

<span class="sd">    The input data is a tuple or a list of tensors. These tensors have the same rank :math:`R`.</span>
<span class="sd">    Set the given axis as :math:`m`, and :math:`0 \le m &lt; R`. Set the number of input tensors as :math:`N`.</span>
<span class="sd">    For the :math:`i`-th tensor :math:`t_i`, it has the shape of :math:`(x_1, x_2, ..., x_{mi}, ..., x_R)`.</span>
<span class="sd">    :math:`x_{mi}` is the :math:`m`-th dimension of the :math:`t_i`. Then, the shape of the output tensor is</span>

<span class="sd">    .. math::</span>

<span class="sd">        (x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[tuple, list]): A tuple or a list of input tensors.</span>
<span class="sd">            Suppose there are two tensors in this tuple or list, namely t1 and t2.</span>
<span class="sd">            To perform `concat` in the axis 0 direction, except for the :math:`0`-th axis,</span>
<span class="sd">            all other dimensions should be equal, that is,</span>
<span class="sd">            :math:`t1.shape[1] = t2.shape[1], t1.shape[2] = t2.shape[2], ..., t1.shape[R-1] = t2.shape[R-1]`,</span>
<span class="sd">            where :math:`R` represents the rank of tensor.</span>
<span class="sd">        axis (int, optional): The specified axis, whose value is in range :math:`[-R, R)`. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is :math:`(x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)`.</span>
<span class="sd">            The data type is the same with `tensors`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `tensors` have different dimension of tensor.</span>
<span class="sd">        ValueError: If `axis` not in range :math:`[-R, R)`.</span>
<span class="sd">        RuntimeError: If tensor&#39;s shape in `tensors` except for `axis` are different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x1 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; input_x2 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.concat((input_x1, input_x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">        [2. 1.]</span>
<span class="sd">        [0. 1.]</span>
<span class="sd">        [2. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.concat((input_x1, input_x2), 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 0. 1.]</span>
<span class="sd">        [2. 1. 2. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concat_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Concat</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">concat_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">conj</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor of complex numbers that are the complex conjugate of each element in input.</span>

<span class="sd">    The complex numbers in input must be of the form a + bj, where a is the real part and b is the imaginary part.</span>

<span class="sd">    The complex conjugate returned by this operation is of the form a - bj.</span>

<span class="sd">    If input is real, it is returned unchanged.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor to compute to. Must have numeric type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">    TypeError: If the dtype of input is not a numeric type.</span>
<span class="sd">    TypeError: If the input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.asarray(np.complex(1.3+0.4j)), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conj(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (1.3-0.4j)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">conj_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Conj</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">conj_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">contiguous</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a Tensor into a continuous-memory Tensor that contains the same data as the original Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A contiguous in memory tensor containing the same data as self tensor.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6]], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.transpose(x, (1, 0))</span>
<span class="sd">        &gt;&gt;&gt; y.contiguous()</span>
<span class="sd">        &gt;&gt;&gt; y[:, 1] = 1</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [4. 5. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">contiguous_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Contiguous</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">contiguous_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">copy_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Copy</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">copy_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">cos</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes cosine of input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        output_i = \cos(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`. </span>
<span class="sd">        The dtype of output is float32 when dtype of `input` is in</span>
<span class="sd">        [bool, int8, uint8, int16, int32, int64]. Otherwise output has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: On CPU or GPU: If dtype of `input` is not float16, float32 or float64, complex64, complex128.</span>
<span class="sd">                   On Ascend: If type of `input` is not bool, int8, uint8, int16, int32, int64, float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cos(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.971338 0.6748758 0.95233357 0.9959527]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cos_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Cos</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">cos_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">cosh</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic cosine of input element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cosh` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.0289385 1.364684 1.048436 1.0040528]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        4.144313</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cosh_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Cosh</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">cosh_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">cum_prod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative product of the tensor x along axis.</span>
<span class="sd">    For example, if input is a vector of size N, the result will also be a vector of size N, with elements.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = x_1 * x_2 * x_3 * ... * x_i</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor.</span>
<span class="sd">        axis (int): The dimensions to compute the cumulative product.</span>
<span class="sd">            Only constant value is allowed.</span>
<span class="sd">        exclusive (bool): If ``True`` , perform exclusive cumulative product. Default: ``False`` .</span>
<span class="sd">        reverse (bool): If ``True`` , reverse the result along axis. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `exclusive` or `reverse` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is None.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; a, b, c, = 1, 2, 3</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([a, b, c]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output0 = ops.cum_prod(x, 0, False, False) # output=[a, a * b, a * b * c]</span>
<span class="sd">        &gt;&gt;&gt; output1 = ops.cum_prod(x, 0, True, False) # output=[1, a, a * b]</span>
<span class="sd">        &gt;&gt;&gt; output2 = ops.cum_prod(x, 0, False, True) # output=[a * b * c, b * c, c]</span>
<span class="sd">        &gt;&gt;&gt; output3 = ops.cum_prod(x, 0, True, True) # output=[b * c, c, 1]</span>
<span class="sd">        &gt;&gt;&gt; print(output0)</span>
<span class="sd">        [1. 2. 6.]</span>
<span class="sd">        &gt;&gt;&gt; print(output1)</span>
<span class="sd">        [1. 1. 2.]</span>
<span class="sd">        &gt;&gt;&gt; print(output2)</span>
<span class="sd">        [6. 6. 3.]</span>
<span class="sd">        &gt;&gt;&gt; print(output3)</span>
<span class="sd">        [6. 3. 1.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [5, 3, 5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output4 = ops.cum_prod(x, 0, False, False)</span>
<span class="sd">        &gt;&gt;&gt; output5 = ops.cum_prod(x, 1, False, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output4)</span>
<span class="sd">        [[ 1.  2.  3.]</span>
<span class="sd">         [ 4. 10. 18.]</span>
<span class="sd">         [20. 30. 90.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output5)</span>
<span class="sd">        [[  1.   2.   6.]</span>
<span class="sd">         [  4.  20. 120.]</span>
<span class="sd">         [  5.  15.  75.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cum_prod_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">CumProd</span><span class="p">)(</span><span class="n">exclusive</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cum_prod_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">cum_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative sum of input tensor along axis.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = x_1 + x_2 + x_3 + ... + x_i</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to accumulate.</span>
<span class="sd">        axis (int): The axis to accumulate the tensor&#39;s value. Only constant value is allowed.</span>
<span class="sd">          Must be in the range [-rank(input), rank(input)).</span>
<span class="sd">        exclusive (bool): By default, this op performs an inclusive cumsum, which means that the first</span>
<span class="sd">            element of the input is identical to the first element of the output. Default: ``False`` .</span>
<span class="sd">        reverse (bool): If ``True`` , perform inverse cumulative sum. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of the output tensor is consistent with the input tensor&#39;s.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `exclusive` or `reverse` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; cumsum = ops.cum_sum</span>
<span class="sd">        &gt;&gt;&gt; # case 1: along the axis 0</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 0, False, False)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 3.  4.  6. 10.]</span>
<span class="sd">         [ 4. 10. 13. 19.]</span>
<span class="sd">         [ 8. 13. 21. 26.]</span>
<span class="sd">         [ 9. 16. 28. 35.]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: along the axis 1</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 1, False, False)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 3.  7. 13. 23.]</span>
<span class="sd">         [ 1.  7. 14. 23.]</span>
<span class="sd">         [ 4.  7. 15. 22.]</span>
<span class="sd">         [ 1.  4. 11. 20.]]</span>
<span class="sd">        &gt;&gt;&gt; # Next demonstrate exclusive and reverse, along axis 1</span>
<span class="sd">        &gt;&gt;&gt; # case 3: exclusive = True</span>
<span class="sd">        &gt;&gt;&gt; cumsum = ops.cum_sum</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 1, True, False)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 0.  3.  7. 13.]</span>
<span class="sd">         [ 0.  1.  7. 14.]</span>
<span class="sd">         [ 0.  4.  7. 15.]</span>
<span class="sd">         [ 0.  1.  4. 11.]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: reverse = True</span>
<span class="sd">        &gt;&gt;&gt; cumsum = ops.cum_sum</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 1, False, True)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[23. 20. 16. 10.]</span>
<span class="sd">         [23. 22. 16.  9.]</span>
<span class="sd">         [22. 18. 15.  7.]</span>
<span class="sd">         [20. 19. 16.  9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cum_sum_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">CumSum</span><span class="p">)(</span><span class="n">exclusive</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cum_sum_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">cummax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cummax_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Cummax</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cummax_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">cummin_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the cumulative minimum of elements and the index.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">        axis (int): The axis to accumulate the tensor&#39;s value. Must be in the range [-rank(input), rank(input)).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tuple of 2 Tensors(values, indices), containing the cumulative minimum of elements and the index,</span>
<span class="sd">        The shape of each output tensor is the same as input `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; func = ops.cummin</span>
<span class="sd">        &gt;&gt;&gt; output = func(a, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [-0.2284 -0.6628 -0.6628 -0.6628 -1.3298 -1.3298]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1])</span>
<span class="sd">        [0 1 1 1 4 4]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cummin_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Cummin</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cummin_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">diag</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a diagonal tensor with a given diagonal values.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.diag` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.diag(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 0 0 0]</span>
<span class="sd">         [0 2 0 0]</span>
<span class="sd">         [0 0 3 0]</span>
<span class="sd">         [0 0 0 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">diag_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Diag</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">diag_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<div class="viewcode-block" id="diagonal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.diagonal.html#mindspore.ops.diagonal">[文档]</a><span class="k">def</span> <span class="nf">diagonal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns specified diagonals of `input`.</span>

<span class="sd">    If `input` is 2-D, returns the diagonal of `input` with the given offset.</span>
<span class="sd">    If `input` has more than two</span>
<span class="sd">    dimensions, then the axes specified by `dim1` and `dim2` are used to determine</span>
<span class="sd">    the 2-D sub-array whose diagonal is returned. In this case, remove the `dim1` and `dim2` dimensions of `input`</span>
<span class="sd">    and insert the last dimension of `input` by the diagonal elements determined by `dim1` and `dim2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Array from which the diagonals are taken.</span>
<span class="sd">        offset (int, optional): Offset of the diagonal from the main diagonal.</span>
<span class="sd">            Can be positive or negative. Default: ``0`` .</span>
<span class="sd">        dim1 (int, optional): Axis to be used as the first axis of the 2-D</span>
<span class="sd">            sub-arrays from which the diagonals should be taken. Defaults to</span>
<span class="sd">            first axis (0). Default: ``0`` .</span>
<span class="sd">        dim2 (int, optional): Axis to be used as the second axis of the 2-D</span>
<span class="sd">            sub-arrays from which the diagonals should be taken. Defaults to</span>
<span class="sd">            second axis (1). Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, if `input` is 2-D, then `input` 1-D array containing the diagonal. If</span>
<span class="sd">        ``input.ndim &gt; 2``, then the dimensions specified by `dim1` and `dim2` are removed,</span>
<span class="sd">        and a new axis inserted at the end corresponding to the diagonal.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if `dim1` or `dim2` are not an int.</span>
<span class="sd">        ValueError: if the input tensor has less than two dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[0, 1], [2, 3]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.diagonal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">diagonal_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Diagonal</span><span class="p">)(</span><span class="n">offset</span><span class="p">,</span> <span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">diagonal_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">div_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the quotient of dividing the first input tensor by the second input tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">    out_{i} = x_{i} / y_{i}</span>

<span class="sd">    Note:</span>
<span class="sd">        - When the two inputs have different shapes, they must be able to broadcast to a common shape.</span>
<span class="sd">        - The two inputs can not be bool type at the same time,</span>
<span class="sd">          [True, Tensor(True, bool\_), Tensor(np.array([True]), bool\_)] are all considered bool type.</span>
<span class="sd">        - The two inputs comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, which is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one of the input `x` , `y` after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 :has same data type and shape of the two inputs</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; div = ops.div</span>
<span class="sd">        &gt;&gt;&gt; output = div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.3333334  2.5        2.        ]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type and shape of the two inputs</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(2, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.  2.5  3.]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">div_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Div</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">div_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">eig_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the eigenvalues and eigenvectors of a square matrix(batch square matrices).</span>

<span class="sd">    Args:</span>
<span class="sd">      x (Tensor) - Square matrices of shape :math:`(*, N, N)`, with float32, float64, complex64 or</span>
<span class="sd">          complex128 data type.</span>
<span class="sd">      compute_v (bool, optional): If ``True`` , compute both eigenvalues and eigenvectors;</span>
<span class="sd">          If `False`, just eigenvalues will be computed. Default: ``False`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">      eigen_values (Tensor) - Shape :math:`(*, N)`. Each inner most vector represents eigenvalues of</span>
<span class="sd">          the corresponding matrix. The eigenvalues may not have an order.</span>
<span class="sd">      eigen_vectors (Tensor) - If `compute_v` is `False`, it&#39;s an empty tensor. Otherwise, this tensor has</span>
<span class="sd">          shape :math:`(*, N, N)`, whose columns represent normalized (unit length) eigenvectors of corresponding</span>
<span class="sd">          eigenvalues.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If `compute_v` is not a bool.</span>
<span class="sd">       TypeError: If dtype of `x` is not one of: float64, float32, complex64 or complex128.</span>
<span class="sd">       TypeError: If `x` is not a Tensor.</span>
<span class="sd">       ValueError: If `x` is not a square(batch squares).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">       ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1.0, 0.0], [0.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; u, v = ops.eig_(input_x, True)</span>
<span class="sd">        &gt;&gt;&gt; print(u)</span>
<span class="sd">        [1.+0.j 2.+0.j]</span>
<span class="sd">        &gt;&gt;&gt; print(v)</span>
<span class="sd">        [[1.+0.j 0.+0.j]</span>
<span class="sd">         [0.+0.j 1.+0.j]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">eig_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Eig</span><span class="p">)(</span><span class="n">compute_v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">eig_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">elu_grad</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradients of Elu operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">elu_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">EluGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">elu_grad_op</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential Linear Unit activation function.</span>

<span class="sd">    Applies the exponential linear unit function element-wise.</span>
<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ELU}(x)= \left\{</span>
<span class="sd">        \begin{array}{align}</span>
<span class="sd">            \alpha(e^{x}  - 1) &amp; \text{if } x \le 0\\</span>
<span class="sd">            x &amp; \text{if } x \gt 0\\</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Where :math:`x` is the element of input Tensor `input_x`, :math:`\alpha` is param `alpha`,</span>
<span class="sd">    it determines the smoothness of ELU.</span>
<span class="sd">    The picture about ELU looks like this `ELU &lt;https://en.wikipedia.org/wiki/</span>
<span class="sd">    Activation_function#/media/File:Activation_elu.svg&gt;`_ .</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of ELU is a Tensor of any dimension with data type of float16 or float32.</span>
<span class="sd">        alpha (float, optional): The alpha value of ELU, the data type is float. Only support &#39;1.0&#39; currently.</span>
<span class="sd">            Default: ``1.0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `alpha` is not a float.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `alpha` is not equal to 1.0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.elu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.63212055  4.         -0.99966455]</span>
<span class="sd">         [ 2.         -0.99326205  9.        ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">elu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Elu</span><span class="p">)(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">elu_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the equivalence between two tensors element-wise.</span>

<span class="sd">    The second argument can be a number or a tensor whose shape is broadcastable with the first argument and vise versa.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } input_{i} = other_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } input_{i} \ne other_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Note:</span>
<span class="sd">        - `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The shapes of the inputs can be broadcasted to each other.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number]): The first input is a number or</span>
<span class="sd">            a tensor whose data type is number.</span>
<span class="sd">        other (Union[Tensor, Number]): The second input is a number or</span>
<span class="sd">            a tensor whose data type is number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor or number.Number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: The shape of two inputs are different</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 2, 3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.eq(x, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: The shape of two inputs are the same</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([1, 2, 3], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor([1, 2, 4], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.eq(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">equal_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Equal</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">equal_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">erf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Gauss error function of `input` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        erf(x)=\frac{2} {\sqrt{\pi}} \int\limits_0^{x} e^{-t^{2}} dt</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of Gaussian error function. Its data type</span>
<span class="sd">            must be float16 float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erf(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.8427168   0.          0.8427168   0.99530876  0.99997765]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">erf_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Erf</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">erf_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">erfc</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the complementary error function of `input` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">    erfc(x) = 1 - \frac{2} {\sqrt{\pi}} \int\limits_0^{x} e^{-t^{2}} dt</span>

<span class="sd">    Args:</span>
<span class="sd">      input (Tensor): The input tensor with a dtype of float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `input` is not a Tensor.</span>
<span class="sd">      TypeError: If dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">      ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">      &gt;&gt;&gt; import mindspore</span>
<span class="sd">      &gt;&gt;&gt; import numpy as np</span>
<span class="sd">      &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">      &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">      &gt;&gt;&gt; output = ops.erfc(x)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [1.8427168e+00 1.0000000e+00 1.5728319e-01 4.6912432e-03 2.2351742e-05]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">erfc_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Erfc</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">erfc_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">erfinv</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the result of the inverse error function with `input`, which is defined in the range `(-1, 1)` as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        erfinv(erf(x)) = x</span>

<span class="sd">    where :math:`x` is the `input`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute with, with data type float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0, 0.5, -0.9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erfinv(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.          0.47695306 -1.1630805 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">erfinv_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Erfinv</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">erfinv_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">       out_i = e^{x_i}</span>

<span class="sd">    Inputs:</span>
<span class="sd">       - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">       Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">       ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">       &gt;&gt;&gt; import mindspore</span>
<span class="sd">       &gt;&gt;&gt; import numpy as np</span>
<span class="sd">       &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">       &gt;&gt;&gt; x = Tensor(np.array([0.0, 1.0, 3.0]), mindspore.float32)</span>
<span class="sd">       &gt;&gt;&gt; output = ops.exp(x)</span>
<span class="sd">       &gt;&gt;&gt; print(output)</span>
<span class="sd">       [ 1.        2.718282 20.085537]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">exp_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Exp</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">exp_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds an additional dimension to `input_x` at the given axis.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the specified axis is a negative number, the index is counted</span>
<span class="sd">        backward from the end and starts at 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        axis (int): Specifies the dimension index at which to expand</span>
<span class="sd">            the shape of `input_x`. The value of axis must be in the range</span>
<span class="sd">            `[-input_x.ndim-1, input_x.ndim]`. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(1, x_1, x_2, ..., x_R)` if the</span>
<span class="sd">        value of `axis` is 0. It has the same data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is not in the valid range :math:`[-a.ndim-1, a.ndim]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.expand_dims(input_tensor, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2.]</span>
<span class="sd">          [2. 2.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">expand_dims_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ExpandDims</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">expand_dims_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">expm1</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential then minus 1 of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">      out_i = e^{x_i} - 1</span>

<span class="sd">    Args:</span>
<span class="sd">      input (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">      ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">      &gt;&gt;&gt; import mindspore</span>
<span class="sd">      &gt;&gt;&gt; import numpy as np</span>
<span class="sd">      &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">      &gt;&gt;&gt; x = Tensor(np.array([0.0, 1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">      &gt;&gt;&gt; output = ops.expm1(x)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [ 0.        1.718282  6.389056 53.598152]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">expm1_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Expm1</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">expm1_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">eye_</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor with ones on the diagonal and zeros in the rest.</span>

<span class="sd">    Note:</span>
<span class="sd">      - An anti-diagonal Tensor can be produced by Eye and ReverseV2 operators,</span>
<span class="sd">      but ReverseV2 only supports Ascend and GPU platforms currently.</span>
<span class="sd">      - The data type of returned tensor can be float16, float32, int8, int16, int32, int64, uint8 or bool on Ascend platforms.</span>


<span class="sd">    Args:</span>
<span class="sd">      n (int): The number of rows of returned tensor. Constant value only.</span>
<span class="sd">      m (int): The number of columns of returned tensor. Constant value only.</span>
<span class="sd">      dtype (mindspore.dtype): MindSpore&#39;s dtype, the data type of the returned tensor.</span>
<span class="sd">        The data type can be bool or Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tensor, a tensor with ones on the diagonal and the rest of elements are zero. The shape of `output` depends on</span>
<span class="sd">      the user&#39;s Inputs `n` and `m`. And the data type depends on Inputs `dtype`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `m` or `n` is not an int.</span>
<span class="sd">      ValueError: If `m` or `n` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">      ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">      &gt;&gt;&gt; import mindspore</span>
<span class="sd">      &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">      &gt;&gt;&gt; output = ops.eye(2, 2, mindspore.int32)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [[1 0]</span>
<span class="sd">       [0 1]]</span>
<span class="sd">      &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">      Int32</span>
<span class="sd">      &gt;&gt;&gt; output = ops.eye(1, 2, mindspore.float32)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [[1. 0.]]</span>
<span class="sd">      &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">      Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">eye_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Eye</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">eye_op</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fast_gelu_grad</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradients of FastGeLU operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fast_gelu_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">FastGeLUGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">fast_gelu_grad_op</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fast_gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fast Gaussian Error Linear Units activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.fast_gelu` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input to compute the FastGeLU with data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; fast_gelu = ops.FastGeLU()</span>
<span class="sd">        &gt;&gt;&gt; output = fast_gelu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.5418735e-01  3.9921875e+00 -9.7473649e-06]</span>
<span class="sd">        [ 1.9375000e+00 -1.0052517e-03  8.9824219e+00]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fast_gelu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">FastGeLU</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">fast_gelu_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fft_with_size_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">inverse</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;backward&#39;</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">signal_sizes</span><span class="o">=</span><span class="p">()):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fourier transform, can be adjusted by parameters to achieve FFT/IFFT/RFFT/IRFFT.</span>

<span class="sd">    For fft, it computes the following expression:</span>

<span class="sd">    .. math::</span>
<span class="sd">        X[\omega_1, \dots, \omega_d] =</span>
<span class="sd">            \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]</span>
<span class="sd">             e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</span>

<span class="sd">    where :math:`d` = `signal_ndim` is number of dimensions for the</span>
<span class="sd">    signal, and :math:`N_i` is the size of signal dimension :math:`i`.</span>

<span class="sd">    For ifft, it computes the following expression:</span>

<span class="sd">    .. math::</span>
<span class="sd">        X[\omega_1, \dots, \omega_d] =</span>
<span class="sd">            \frac{1}{\prod_{i=1}^d N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]</span>
<span class="sd">             e^{\ j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</span>

<span class="sd">    where :math:`d` = `signal_ndim` is number of dimensions for the</span>
<span class="sd">    signal, and :math:`N_i` is the size of signal dimension :math:`i`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - FFT/IFFT requires complex64 or complex128 inputs, return complex64 or complex128 outputs.</span>
<span class="sd">        - RFFT requires bool, uint8, int8, int16, int32, int64, float32 and float64 inputs,</span>
<span class="sd">          return complex64 or complex128 outputs.</span>
<span class="sd">        - IRFFT requires complex64 or complex128 inputs, return float32 or float64 outputs.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The dimension of the input tensor must be greater than or equal to signal_ndim.</span>
<span class="sd">        signal_ndim (int): The number of dimensions in each signal, this controls how many dimensions</span>
<span class="sd">            of the fourier transform are realized, can only be 1, 2 or 3.</span>
<span class="sd">        inverse (bool): Whether it is the inverse transformation, used to select from FFT and RFFT or IFFT and IRFFT.</span>

<span class="sd">            - when set to ``True``: IFFT and IRFFT.</span>
<span class="sd">            - when set to ``False``: FFT and RFFT.</span>

<span class="sd">        real (bool): Whether it is the real transformation, combines with `inverse` to select a specific</span>
<span class="sd">            transformation mode:</span>

<span class="sd">            - `inverse` is ``False`` ,  `real` is ``False`` : corresponds to FFT.</span>
<span class="sd">            - `inverse` is ``True`` , `real` is ``False`` : corresponds to IFFT.</span>
<span class="sd">            - `inverse` is ``False`` , `real` is ``True`` : corresponds to RFFT.</span>
<span class="sd">            - `inverse` is ``True`` , `real` is ``True``  : corresponds to IRFFT.</span>

<span class="sd">        norm (str, optional): The normalization, optional values: [ ``&quot;backward&quot;`` , ``&quot;forward&quot;`` , ``&quot;ortho&quot;`` ].</span>
<span class="sd">            Default value: ``&quot;backward&quot;`` .</span>

<span class="sd">            - ``&quot;backward&quot;`` has the direct transforms unscaled and the inverse transforms scaled by :math:`1/n`,</span>
<span class="sd">              where n is the input x&#39;s element numbers.</span>
<span class="sd">            - ``&quot;ortho&quot;`` has both direct and inverse transforms are scaled by :math:`1/\sqrt n`.</span>
<span class="sd">            - ``&quot;forward&quot;`` has the direct transforms scaled by :math:`1/n` and the inverse transforms unscaled.</span>

<span class="sd">        onesided (bool, optional): Controls whether the input is halved to avoid redundancy. Default: ``True`` .</span>
<span class="sd">        signal_sizes (tuple, optional): Size of the original signal (the signal before rfft, no batch dimension),</span>
<span class="sd">            only in IRFFT mode and set `onesided` to ``True`` requires the parameter, the following conditions must be</span>
<span class="sd">            satisfied. Default: ``()`` .</span>

<span class="sd">            - The length of `signal_sizes` is equal to the signal_ndim of the IRFFT:</span>
<span class="sd">              :math:`len(signal\_sizes)=signal\_ndim`.</span>
<span class="sd">            - The last dimension of `signal_sizes` divided by 2 is equal to</span>
<span class="sd">              the last dimension of the IRFFT input: :math:`signal\_size[-1]/2+1=x.shape[-1]`.</span>
<span class="sd">            - `signal_sizes` has exactly the same dimensions as the input shape</span>
<span class="sd">              except for the last dimension: :math:`signal\_sizes[:-1]=x.shape[:-1]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tensor containing the complex-to-complex, real-to-complex or complex-to-real Fourier transform result.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input type of FFT/IFFT/IRFFT is not one of: complex64, complex128.</span>
<span class="sd">        TypeError: If the input type is not Tensor.</span>
<span class="sd">        ValueError: If `x` dimension is less than signal_ndim.</span>
<span class="sd">        ValueError: If signal_ndim is greater than 3 or less than 1.</span>
<span class="sd">        ValueError: If norm is none of &quot;backward&quot;, &quot;forward&quot; or &quot;ortho&quot;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case FFT: signal_ndim: 1, inverse: False, real: False.</span>
<span class="sd">        &gt;&gt;&gt; fft_in = Tensor(np.array([2, 1, 2]), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; fft_output = ops.fft_with_size_(fft_in, signal_ndim=1, inverse=False, real=False)</span>
<span class="sd">        &gt;&gt;&gt; print(fft_output)</span>
<span class="sd">        [5.        +0.j         0.5       +0.86602545j 0.50000006-0.8660255j ]</span>
<span class="sd">        &gt;&gt;&gt; # case IFFT: signal_ndim: 1, inverse: True, real: False.</span>
<span class="sd">        &gt;&gt;&gt; ifft_in = fft_output</span>
<span class="sd">        &gt;&gt;&gt; ifft_output = ops.fft_with_size_(ifft_in, signal_ndim=1, inverse=True, real=False)</span>
<span class="sd">        &gt;&gt;&gt; print(ifft_output)</span>
<span class="sd">        [2.        -1.9868216e-08j 0.99999994+0.0000000e+00j</span>
<span class="sd">         1.9999999 +7.9472862e-08j]</span>
<span class="sd">        &gt;&gt;&gt; # case RFFT2D: signal_ndim: 2, inverse: False, real: True.</span>
<span class="sd">        &gt;&gt;&gt; rfft_in = Tensor(np.array([[2, 1, 2], [3, 1, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rfft_output = ops.fft_with_size_(rfft_in, signal_ndim=2, inverse=False, real=True)</span>
<span class="sd">        &gt;&gt;&gt; print(rfft_output)</span>
<span class="sd">        [[ 1.5000000e+01+1.1920929e-07j -2.3841858e-07+5.1961522e+00j]</span>
<span class="sd">         [-5.0000000e+00-2.9802322e-08j  9.9999988e-01-3.4641016e+00j]]</span>
<span class="sd">        &gt;&gt;&gt; # case IRFFT2D: signal_ndim: 2, inverse: True, real: True.</span>
<span class="sd">        &gt;&gt;&gt; irfft_in = rfft_output</span>
<span class="sd">        &gt;&gt;&gt; irfft_output = ops.fft_with_size_(irfft_in, signal_ndim=2, inverse=True, real=True, signal_sizes=rfft_in.shape)</span>
<span class="sd">        &gt;&gt;&gt; print(irfft_output)</span>
<span class="sd">        [[2.         1.         2.        ]</span>
<span class="sd">         [3.         0.99999994 5.9999995 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fft_with_size_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">FFTWithSize</span><span class="p">)(</span><span class="n">signal_ndim</span><span class="p">,</span> <span class="n">inverse</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">norm</span><span class="p">,</span> <span class="n">onesided</span><span class="p">,</span> <span class="n">signal_sizes</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fft_with_size_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">flatten_</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flattens a tensor without changing its batch size on the 0-th axis.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)` to be flattened, where :math:`N` is batch size.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N, X)`, where :math:`X` is</span>
<span class="sd">        the product of the remaining dimension.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.flatten_(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 24)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">flatten_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Flatten</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">flatten_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">floor_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise and round down to the closest integer.</span>

<span class="sd">    Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = \\text{floor}( \\frac{x_i}{y_i})</span>

<span class="sd">    where the :math:`floor` indicates the Floor operator, for more details,</span>
<span class="sd">    please refer to the :class:`mindspore.ops.Floor` operator.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        other (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` nor `other` is a Tensor, number.Number or bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor_divide(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1 -1]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(2.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor_divide(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">floor_div_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">FloorDiv</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">floor_div_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">floor_mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the remainder of division element-wise. It&#39;s a flooring divide.</span>
<span class="sd">    E.g. :math:`floor(x / y) * y + mod(x, y) = x`.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\text{floor}(x_{i} // y_{i})</span>

<span class="sd">    where the :math:`floor` indicates the Floor operator, for more details,</span>
<span class="sd">    please refer to the :class:`mindspore.ops.Floor` operator.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - Data of input `y` should not be 0, or the maximum value of its dtype will be returned.</span>
<span class="sd">        - When the elements of input exceeds 2048 , the accuracy of operator cannot guarantee the requirement of</span>
<span class="sd">          double thousandths in the mini form.</span>
<span class="sd">        - Due to different architectures, the calculation results of this operator on NPU and CPU may be inconsistent.</span>
<span class="sd">        - If shape is expressed as :math:`(D1, D2 ..., Dn)`, then D1\*D2... \*DN&lt;=1000000,n&lt;=8.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision of the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor or number.Number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor_mod(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">floor_mod_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">FloorMod</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">floor_mod_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">floor</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds a tensor down to the closest integer element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \lfloor x_i \rfloor</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor, its data type must be float16,</span>
<span class="sd">            float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not in [float16, float32, float64].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  2. -2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">floor_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Floor</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">floor_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gather_d_grad_v2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradient for the GatherD operation. Note that the operator &quot;GatherDGrad&quot; has been abandoned.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gather_d_grad_v2_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">GatherDGradV2</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">gather_d_grad_v2_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gather_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers elements along an axis specified by dim.</span>

<span class="sd">    Refer to :func:`mindspore.ops.gather_elements` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dim = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather_d(x, dim, index)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">         [4 3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gather_d_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">GatherD</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">gather_d_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gather_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers slices from a tensor by indices.</span>

<span class="sd">    Using given indices to gather slices from a tensor with a specified shape.</span>

<span class="sd">    `indices` is an K-dimensional integer tensor.</span>
<span class="sd">    Supposes it as a (K-1)-dimensional tensor and each element of it defines a slice of `input_x`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        output[(i_0, ..., i_{K-2})] = input\_x[indices[(i_0, ..., i_{K-2})]]</span>

<span class="sd">    The last dimension of `indices` can not more than the rank of `input_x`:</span>
<span class="sd">        :math:`indices.shape[-1] &lt;= input\_x.rank`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor to gather values.</span>
<span class="sd">        indices (Tensor): The index tensor, with int32 or int64 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as `input_x` and the shape is</span>
<span class="sd">        :math:`indices\_shape[:-1] + input\_x\_shape[indices\_shape[-1]:]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of `indices`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather_nd(input_x, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.1  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gather_nd_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">GatherNd</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">gather_nd_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the slice of the input tensor corresponding to the elements of `input_indices` on the specified `axis`.</span>

<span class="sd">    The following figure shows the calculation process of Gather commonly:</span>

<span class="sd">      .. image:: Gather.png</span>

<span class="sd">      where params represents the input `input_params`, and indices represents the index to be sliced `input_indices`.</span>

<span class="sd">      .. note::</span>
<span class="sd">      1. The value of input_indices must be in the range of `[0, input_param.shape[axis])`.</span>
<span class="sd">      On CPU and GPU, an error is raised if an out of bound indice is found. On Ascend, the results may be</span>
<span class="sd">      undefined.</span>

<span class="sd">      2. The data type of input_params cannot be</span>
<span class="sd">      `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ on Ascend</span>
<span class="sd">      platform currently.</span>

<span class="sd">    Args:</span>
<span class="sd">      input_params (Tensor): The original Tensor. The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">      input_indices (Tensor): Index tensor to be sliced, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">        Specifies the indices of elements of the original Tensor. The data type can be int32 or int64.</span>
<span class="sd">      axis (Union(int, Tensor[int])): Specifies the dimension index to gather indices.</span>
<span class="sd">        It must be greater than or equal to `batch_dims`.</span>
<span class="sd">        When `axis` is a Tensor, the size must be 1.</span>
<span class="sd">      batch_dims (int): Specifies the number of batch dimensions. It must be less than or euqal to the rank</span>
<span class="sd">        of `input_indices`. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tensor, the shape of tensor is</span>
<span class="sd">      :math:`input\_params.shape[:axis] + input\_indices.shape[batch\_dims:] + input\_params.shape[axis + 1:]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError:  If `axis` is not an int or Tensor.</span>
<span class="sd">      ValueError: If `axis` is a Tensor and its size is not 1.</span>
<span class="sd">      TypeError:  If `input_params` is not a tensor.</span>
<span class="sd">      TypeError:  If `input_indices` is not a tensor of type int.</span>
<span class="sd">      RuntimeError: If `input_indices` is out of range `[0, input_param.shape[axis])` on CPU or GPU.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">      ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">      &gt;&gt;&gt; import mindspore</span>
<span class="sd">      &gt;&gt;&gt; import numpy as np</span>
<span class="sd">      &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">      &gt;&gt;&gt; # case1: input_indices is a Tensor with shape (5, ).</span>
<span class="sd">      &gt;&gt;&gt; input_params = Tensor(np.array([1, 2, 3, 4, 5, 6, 7]), mindspore.float32)</span>
<span class="sd">      &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2, 4, 2, 6]), mindspore.int32)</span>
<span class="sd">      &gt;&gt;&gt; axis = 0</span>
<span class="sd">      &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [1. 3. 5. 3. 7.]</span>
<span class="sd">      &gt;&gt;&gt; # case2: input_indices is a Tensor with shape (2, 2). When the input_params has one dimension,</span>
<span class="sd">      &gt;&gt;&gt; # the output shape is equal to the input_indices shape.</span>
<span class="sd">      &gt;&gt;&gt; input_indices = Tensor(np.array([[0, 2], [2, 6]]), mindspore.int32)</span>
<span class="sd">      &gt;&gt;&gt; axis = 0</span>
<span class="sd">      &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [[1. 3.]</span>
<span class="sd">       [3. 7.]]</span>
<span class="sd">      &gt;&gt;&gt; # case3: input_indices is a Tensor with shape (2, ) and</span>
<span class="sd">      &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 0.</span>
<span class="sd">      &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">      &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2]), mindspore.int32)</span>
<span class="sd">      &gt;&gt;&gt; axis = 0</span>
<span class="sd">      &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [[ 1.  2.  3.  4.]</span>
<span class="sd">       [ 9. 10. 11. 12.]]</span>
<span class="sd">      &gt;&gt;&gt; # case4: input_indices is a Tensor with shape (2, ) and</span>
<span class="sd">      &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 1, batch_dims is 1.</span>
<span class="sd">      &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">      &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2, 1]), mindspore.int32)</span>
<span class="sd">      &gt;&gt;&gt; axis = 1</span>
<span class="sd">      &gt;&gt;&gt; batch_dims = 1</span>
<span class="sd">      &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis, batch_dims)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [ 1.  7. 10.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gather_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Gather</span><span class="p">)(</span><span class="n">batch_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gather_op</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gcd</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes greatest common divisor of input tensors element-wise.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: int32, int64.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **x1** (Tensor) - The first input tensor.</span>
<span class="sd">        - **x2** (Tensor) - The second input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher precision in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `x1` or `x2` is not int32 or int64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([7, 8, 9]))</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([14, 6, 12]))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.gcd(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [7 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gcd_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Gcd</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">gcd_op</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gelu_grad</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradients of GeLU operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gelu_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">GeLUGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">gelu_grad_op</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gelu_</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Error Linear Units activation function.</span>

<span class="sd">    GeLU is described in the paper `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_.</span>
<span class="sd">    And also please refer to `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>
<span class="sd">    &lt;https://arxiv.org/abs/1810.04805&gt;`_.</span>

<span class="sd">    GeLU is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        GELU(x_i) = x_i*P(X &lt; x_i)</span>

<span class="sd">    where :math:`P` is the cumulative distribution function of the standard Gaussian distribution,</span>
<span class="sd">    :math:`x_i` is the input element.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **x** (Tensor) - The input of the activation function GeLU, the data type is float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.gelu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [0.841192  1.9545976  2.9963627]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gelu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">GeLU</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">gelu_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">geqrf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Decomposes a matrix into the product of an orthogonal matrix `Q` and an upper triangular matrix `R`.</span>
<span class="sd">    The process is called QR decomposition: :math:`A = QR`.</span>

<span class="sd">    Both `Q` and `R` matrices are stored in the same output tensor `y`.</span>
<span class="sd">    The elements of `R` are stored on and above the diagonal, whereas elementary reflectors</span>
<span class="sd">    (or Householder vectors) implicitly defining matrix `Q` are stored below the diagonal.</span>

<span class="sd">    This function returns two tensors (`y`, `tau`).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(*, m, n)`, input must be a matrix greater than or equal to 2D,</span>
<span class="sd">        with dtype of float32, float64, complex64, complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - Tensor of shape :math:`(*, m, n)`, has the same dtype as the `x`.</span>
<span class="sd">        - **tau** (Tensor) - Tensor of shape :math:`(*, p)` and :math:`p = min(m, n)`, has the same dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `x` is not float32, float64, complex64 or complex128.</span>
<span class="sd">        ValueError: If dimension of `x` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-2.0, -1.0], [1.0, 2.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y, tau = ops.geqrf(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 2.236068   1.7888544]</span>
<span class="sd">        [-0.236068   1.3416407]]</span>
<span class="sd">        &gt;&gt;&gt; print(tau)</span>
<span class="sd">        [1.8944271 0.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">geqrf_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Geqrf</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">geqrf_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">greater_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given two Tensors, compares them element-wise to check if each element in the first</span>
<span class="sd">    Tensor is greater than or equal to the corresponding element in the second Tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ge` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">        a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">        a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; greater_equal = ops.greater_equal()</span>
<span class="sd">        &gt;&gt;&gt; output = greater_equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">greater_equal_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">GreaterEqual</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">greater_equal_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">greater</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare the value of the input parameters :math:`x,y` element-wise, and the output result is a bool value.</span>

<span class="sd">    Refer to :func:`mindspore.ops.gt` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">        a bool or a tensor whose data type is</span>
<span class="sd">        `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">        `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, which is a number.Number or</span>
<span class="sd">        a bool or a tensor whose data type is</span>
<span class="sd">        `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">        `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; greater = ops.greater()</span>
<span class="sd">        &gt;&gt;&gt; output = greater(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">greater_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Greater</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">greater_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">grid_sampler_2d_grad</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for GridSampler2D operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **grad** (Tensor) - A 4-D tensor whose dtype is float16 or float32 and whose shape is :math:`(N, C,</span>
<span class="sd">        H_{out}, W_{out})`. The shape is inconsistent with the shape of the output result of forward calculation.</span>
<span class="sd">        - **input_x** (Tensor) - A 4-D tensor whose dtype is the same as `grad` and whose shape is :math:`(N, C,</span>
<span class="sd">        H_{in}, W_{in})`.</span>
<span class="sd">        - **grid** (Tensor) - A 4-D tensor whose dtype is the same as `grad` and whose</span>
<span class="sd">        shape is :math:`(N, H_{out}, W_{out}, 2)`.</span>
<span class="sd">        interpolation_mode (str): An optional string specifying the interpolation method. The optional values are</span>
<span class="sd">            &quot;bilinear&quot; or &quot;nearest&quot;. Default: &quot;bilinear&quot;.</span>
<span class="sd">        padding_mode (str): An optional string specifying the pad method. The optional values are &quot;zeros&quot;, &quot;border&quot; or</span>
<span class="sd">            &quot;reflection&quot;. Default: &quot;zeros&quot;.</span>
<span class="sd">        align_corners (bool): An optional bool. If &quot;true&quot;, the centers of the corner pixels of the input and output</span>
<span class="sd">            tensors are aligned. Defaults to &quot;false&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **dx** (Tensor) - A 4-D tensor whose dtype and shape are the same as `input_x`.</span>
<span class="sd">        - **dgrid** (Tensor) - A 4-D tensor whose dtype and shape are the same as `grid`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `grad`, `input_x` or `grid` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtypes of `grad`, `input_x` and `grid` are inconsistent.</span>
<span class="sd">        TypeError: If the dtype of `grad`, `input_x` or `grid` is not a valid type.</span>
<span class="sd">        TypeError: If `align_corners` is not a boolean value.</span>
<span class="sd">        ValueError: If the rank of `grad`, `input_x` or `grid` is not equal to 4.</span>
<span class="sd">        ValueError: If the first dimension of `grad`, `input_x` and `grid` are inconsistent.</span>
<span class="sd">        ValueError: If the last dimension of `grid` is not equal to 2.</span>
<span class="sd">        ValueError: If `interpolation_mode` is not &quot;bilinear&quot;, &quot;nearest&quot; or a string value.</span>
<span class="sd">        ValueError: If `padding_mode` is not &quot;zeros&quot;, &quot;border&quot;, &quot;reflection&quot; or a string value.</span>
<span class="sd">        ValueError: If the shape of `grad` is inconsistent with the shape of the output result of forward calculation.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">grid_sampler_2d_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">GridSampler2DGrad</span><span class="p">)(</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grid_sampler_2d_grad_op</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">grid_sampler_2d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operation samples 2d `input_x` by using interpolation based on flow field grid,</span>
<span class="sd">    which is usually gennerated by :func:`mindspore.ops.affine_grid`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.grid_sample` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **input_x** (Tensor) - A 4-D tensor with dtype of float16, float32 or float64 and shape of</span>
<span class="sd">        :math:`(N, C, H_{in}, W_{in})`.</span>

<span class="sd">        - **grid** (Tensor) - A 4-D tensor whose dtype is the same as `input_x` and whose shape is</span>
<span class="sd">        :math:`(N, H_{out}, W_{out}, 2)`.</span>
<span class="sd">        Used to specify the sampling pixel locations normalized by the input spatial</span>
<span class="sd">        dimensions.</span>

<span class="sd">        interpolation_mode (str, optional): An optional string specifying the interpolation method.</span>
<span class="sd">            The optional values are</span>
<span class="sd">            ``&quot;bilinear&quot;`` or ``&quot;nearest&quot;`` . Default: ``&quot;bilinear&quot;`` .</span>

<span class="sd">            - ``&quot;nearest&quot;``: Nearest neighbor interpolation. Each output pixel is assigned the value of the</span>
<span class="sd">            nearest input pixel. This method is simple and fast but can result in blocky or pixelated outputs.</span>
<span class="sd">            - ``&quot;bilinear&quot;``: Bilinear interpolation. Each output pixel is a weighted average of the four nearest input</span>
<span class="sd">            pixels, computed using bilinear interpolation. This method produces smoother results compared</span>
<span class="sd">            to nearest neighbor interpolation.</span>

<span class="sd">        padding_mode (str, optional): An optional string specifying the pad method.</span>
<span class="sd">            The optional values are ``&quot;zeros&quot;`` , ``&quot;border&quot;`` or ``&quot;reflection&quot;`` . Default: ``&quot;zeros&quot;`` .</span>
<span class="sd">            When the sampling grid is outside input&#39;s bounds, effects of various padding modes are as follows:</span>

<span class="sd">            - ``&quot;zeros&quot;``: Pads the input tensor with zeros.</span>
<span class="sd">            - ``&quot;border&quot;``: Pads the input tensor with the values of the pixels on the border of the tensor.</span>
<span class="sd">            - ``&quot;reflection&quot;``: Pads the input tensor by reflecting the values of the pixels at the</span>
<span class="sd">            boundary of the tensor.</span>

<span class="sd">        align_corners (bool, optional): An optional bool. When set to ``True`` ,</span>
<span class="sd">            the centers of the corner pixels of the input</span>
<span class="sd">            and output tensors are aligned. When set to ``False`` , it is not aligned. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 4-D Tensor whose dtype is the same as `input_x` and whose shape is :math:`(N, C, H_{out}, W_{out})`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(16).reshape((2, 2, 2, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grid = Tensor(np.arange(-9, 9, 0.5).reshape((2, 3, 3, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.grid_sampler_2d(input_x, grid, interpolation_mode=&#39;bilinear&#39;, padding_mode=&#39;zeros&#39;, align_corners=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 0.     0.     0.   ]</span>
<span class="sd">        [ 0.     0.     0.   ]</span>
<span class="sd">        [ 0.     0.     0.5  ]]</span>
<span class="sd">        [[ 0.     0.     0.   ]</span>
<span class="sd">        [ 0.     0.     0.   ]</span>
<span class="sd">        [ 0.     1.5    4.5  ]]]</span>
<span class="sd">        [[[10.     8.25   1.375]</span>
<span class="sd">        [ 0.     0.     0.   ]</span>
<span class="sd">        [ 0.     0.     0.   ]]</span>
<span class="sd">        [[14.    11.25   1.875]</span>
<span class="sd">        [ 0.     0.     0.   ]</span>
<span class="sd">        [ 0.     0.     0.   ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">grid_sampler_2d_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">GridSampler2D</span><span class="p">)(</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grid_sampler_2d_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">grid_sampler_3d_grad</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for GridSampler3D operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **grad** (Tensor) - A 5-D tensor whose dtype is float32 or float64 and whose shape is :math:`(N, C, D_{out},</span>
<span class="sd">        H_{out}, W_{out})`. The shape is inconsistent with the shape of the output result of forward calculation.</span>
<span class="sd">        - **input_x** (Tensor) - A 5-D tensor whose dtype is the same as `grad` and whose shape is :math:`(N, C,</span>
<span class="sd">        D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **grid** (Tensor) - A 5-D tensor whose dtype is the same as `grad` and whose shape is :math:`(N, D_{out},</span>
<span class="sd">        H_{out}, W_{out}, 3)`.</span>
<span class="sd">        interpolation_mode (str): An optional string specifying the interpolation method. The optional values are</span>
<span class="sd">            &quot;bilinear&quot; or &quot;nearest&quot;. Default: &quot;bilinear&quot;.</span>
<span class="sd">        padding_mode (str): An optional string specifying the pad method. The optional values are &quot;zeros&quot;, &quot;border&quot; or</span>
<span class="sd">            &quot;reflection&quot;. Default: &quot;zeros&quot;.</span>
<span class="sd">        align_corners (bool): An optional bool. If &quot;true&quot;, the centers of the corner pixels of the input and output</span>
<span class="sd">            tensors are aligned. Defaults to &quot;false&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **dx** (Tensor) - A 5-D tensor whose dtype and shape are the same as `input_x`.</span>
<span class="sd">        - **dgrid** (Tensor) - A 5-D tensor whose dtype and shape are the same as `grid`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `grad`, `input_x` or `grid` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtypes of `grad`, `input_x` and `grid` are inconsistent.</span>
<span class="sd">        TypeError: If the dtype of `grad`, `input_x` or `grid` is not a valid type.</span>
<span class="sd">        TypeError: If `align_corners` is not a boolean value.</span>
<span class="sd">        ValueError: If the rank of `grad`, `input_x` or `grid` is not equal to 5.</span>
<span class="sd">        ValueError: If the first dimension of `grad`, `input_x` and `grid` are inconsistent.</span>
<span class="sd">        ValueError: If the last dimension of `grid` is not equal to 3.</span>
<span class="sd">        ValueError: If `interpolation_mode` is not &quot;bilinear&quot;, &quot;nearest&quot; or a string value.</span>
<span class="sd">        ValueError: If `padding_mode` is not &quot;zeros&quot;, &quot;border&quot;, &quot;reflection&quot; or a string value.</span>
<span class="sd">        ValueError: If the shape of `grad` is inconsistent with the shape of the output result of forward calculation.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">grid_sampler_3d_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">GridSampler3DGrad</span><span class="p">)(</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grid_sampler_3d_grad_op</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">grid_sampler_3d</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given an input and a grid, the output is calculated using the input values</span>
<span class="sd">    and pixel positions in the grid. Only volume (5-D) input is supported.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.grid_sample` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **input_x** (Tensor) - A 5-D tensor with dtype of float16, float32 or float64</span>
<span class="sd">        and shape of :math:`(N, C, D_{in}, H_{in}, W_{in})`.</span>

<span class="sd">        - **grid** (Tensor) - A 5-D tensor whose dtype is the same as `input_x` and whose shape is :math:`(N, D_{out},</span>
<span class="sd">        H_{out}, W_{out}, 3)`.</span>

<span class="sd">        interpolation_mode (str, optional): An optional string specifying the interpolation method.</span>
<span class="sd">            The optional values are ``&quot;bilinear&quot;`` or ``&quot;nearest&quot;`` . Default: ``&quot;bilinear&quot;`` .</span>

<span class="sd">            - ``&quot;nearest&quot;``: Nearest neighbor interpolation. Each output pixel is assigned the value of the</span>
<span class="sd">            nearest input pixel. This method is simple and fast but can result in blocky or pixelated outputs.</span>
<span class="sd">            - ``&quot;bilinear&quot;``: Bilinear interpolation. Each output pixel is a weighted average of the four nearest input</span>
<span class="sd">            pixels, computed using bilinear interpolation. This method produces smoother results compared</span>
<span class="sd">            to nearest neighbor interpolation.</span>

<span class="sd">        padding_mode (str, optional): An optional string specifying the pad method.</span>
<span class="sd">            The optional values are ``&quot;zeros&quot;`` , ``&quot;border&quot;`` or ``&quot;reflection&quot;`` . Default: ``&quot;zeros&quot;`` .</span>
<span class="sd">            When the sampling grid is outside input&#39;s bounds, effects of various padding modes are as follows:</span>

<span class="sd">            - ``&quot;zeros&quot;``: Pads the input tensor with zeros.</span>
<span class="sd">            - ``&quot;border&quot;``: Pads the input tensor with the values of the pixels on the border of the tensor.</span>
<span class="sd">            - ``&quot;reflection&quot;``: Pads the input tensor by reflecting the values of the pixels at the</span>
<span class="sd">            boundary of the tensor.</span>

<span class="sd">        align_corners (bool, optional): An optional bool specifying alignment method. If set to ``True`` ,</span>
<span class="sd">            the extrema (-1 and 1) are considered as referring to</span>
<span class="sd">            the center points of the input&#39;s corner pixels. If set to ``False`` , they are instead considered as</span>
<span class="sd">            referring to the corner points of the input&#39;s corner pixels, making the sampling more resolution agnostic.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 5-D Tensor whose dtype is the same as `input_x` and whose shape is :math:`(N, C, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(32).reshape((2, 2, 2, 2, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grid = Tensor(np.arange(-0.2, 1, 0.1).reshape((2, 2, 1, 1, 3)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.grid_sampler_2d(input_x, grid, interpolation_mode=&#39;bilinear&#39;, padding_mode=&#39;zeros&#39;, align_corners=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[ 3.3     ]]</span>
<span class="sd">        [[ 4.35    ]]]</span>
<span class="sd">        [[[11.300001]]</span>
<span class="sd">        [[12.349999]]]]</span>
<span class="sd">        [[[[21.4     ]]</span>
<span class="sd">        [[22.449999]]]</span>
<span class="sd">        [[[29.4     ]]</span>
<span class="sd">        [[30.449999]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">grid_sampler_3d_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">GridSampler3D</span><span class="p">)(</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grid_sampler_3d_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">hshrink_grad</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for HShrinkGrad operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        Gradients (Tensor) - the gradients of loss to output of HShrink function.</span>
<span class="sd">        Currently gradients data type only support float16 and float32.</span>
<span class="sd">        Features (Tensor) - Must be the input `input_x` of the forward operator HSHrink.</span>
<span class="sd">        Currently features data type only support float16 and float32.</span>
<span class="sd">        lambd (float): the lambda value for the Hardshrink formulation. Default: 0.5</span>

<span class="sd">    Returns:</span>
<span class="sd">        backprops - Tensor, with the same shape and data type as `features`.</span>

<span class="sd">    Rasise:</span>
<span class="sd">        ValueError: If `lambd` is not a float.</span>
<span class="sd">        ValueError: If shape of `gradients` is not the same as `features`.</span>
<span class="sd">        TypeError: If dtype of `gradients` is not the same as `features`.</span>
<span class="sd">        TypeError: If dtype of `gradients` or `features` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hshrink_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">HShrinkGrad</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hshrink_grad_op</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">features</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">hshrink</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard Shrink activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.hardshrink` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor) - The input of Hard Shrink with data type of float16 or float32.</span>
<span class="sd">        lambd (float, optional): The threshold :math:`\lambda` defined by the Hard Shrink formula. Default: ``0.5`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the same shape and data type as the input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0.5,  1,  2.0], [0.0533, 0.0776, -2.1233]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.hshrink(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.      1.      2.    ]</span>
<span class="sd">        [ 0.      0.     -2.1233]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hshrink_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">HShrink</span><span class="p">)(</span><span class="n">lambd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hshrink_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">hsigmoid_grad</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the gradient of HSigmoid operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hsigmoid_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">HSigmoidGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">hsigmoid_grad_op</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">hsigmoid</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard sigmoid activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.hardsigmoid` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor) - The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.hsigmoid(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [0.3333 0.1666 0.5    0.8335 0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hsigmoid_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">HSigmoid</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">hsigmoid_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">hswish_grad</span><span class="p">(</span><span class="n">y_grad</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the gradient of HSwish operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hswish_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">HSwishGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">hswish_grad_op</span><span class="p">(</span><span class="n">y_grad</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">hardswish_</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard swish activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.hardswish` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **input_x** (Tensor) - The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.hswish(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [-0.3333  -0.3333  0  1.666  0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">hswish_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">HSwish</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">hswish_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a deepcopy of input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a deepcopy of `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[0, 1], [2, 1]], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.deepcopy(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 1]</span>
<span class="sd">        [2 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">identity_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Identity</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">identity_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">layer_norm_grad_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">d_dx</span><span class="p">,</span> <span class="n">d_dg</span><span class="p">,</span> <span class="n">d_db</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the gradient of LayerNormGrad operation.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor to be normalized, float32 or float16.</span>
<span class="sd">        - **dy** (Tensor) - The gradient of LayerNorm&#39;s output y, float32 or float16.</span>
<span class="sd">        - **variance** (Tensor) - The variance of x, float32 or float16.</span>
<span class="sd">        - **mean** (Tensor) - The mean of x, float32 or float16.</span>
<span class="sd">        - **gamma** (Tensor) - The original value of weight gamma initialized in LayerNorm, float32 or float16.</span>
<span class="sd">          Default: &#39;ones&#39;.</span>
<span class="sd">        - **d_dx** (Tensor) - The gradient of dx, where dx is the gradient of LayerNorm&#39;s input x, float32 or float16.</span>
<span class="sd">        - **d_dg** (Tensor) - The gradient of dg, where dg is the gradient of LayerNorm&#39;s weight gamma,</span>
<span class="sd">          float32 or float16.</span>
<span class="sd">        - **d_db** (Tensor) - The gradient of db, where db is the gradient of LayerNorm&#39;s weight beta,</span>
<span class="sd">          float32 or float16.</span>
<span class="sd">        - **begin_norm_axis** (int): The begin axis for the input to apply layernorm. Default: 1.</span>
<span class="sd">        - **begin_params_axis** (int): The begin axis for the parameter input to apply layernorm. Default: 1.</span>


<span class="sd">    Returns:</span>
<span class="sd">        Tuple[Tensor], tuple of 3 Tensors (the gradients of layernormgrad x, dy, gamma).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the 8 inputs don&#39;t have the same dtype.</span>
<span class="sd">        ValueError: If x, dy, d_dx don&#39;t have the same shape.</span>
<span class="sd">        ValueError: If variance, mean don&#39;t have the same shape.</span>
<span class="sd">        ValueError: If gamma, d_dg, d_db don&#39;t have the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">layer_norm_grad_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LayerNormGradGrad</span><span class="p">)(</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layer_norm_grad_grad_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">d_dx</span><span class="p">,</span> <span class="n">d_dg</span><span class="p">,</span> <span class="n">d_db</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">layer_norm_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the layer Normalization to the input array.</span>

<span class="sd">    This operator will calculate the input gradients of layernorm.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        x (Tensor): The inputs of layer norm op.</span>
<span class="sd">        dy (Tensor): The gradient of outputs of layer norm op.</span>
<span class="sd">        variance (Tensor): The variance of x.</span>
<span class="sd">        mean (Tensor): The mean of x.</span>
<span class="sd">        gamma (Tensor): The weights of normalized elements.</span>
<span class="sd">        begin_norm_axis (int): The begin axis for the input to apply layernorm. Default: 1.</span>
<span class="sd">        begin_params_axis (int): The begin axis for the parameter input to apply layernorm. Default: 1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[int], tuple of 3 values (the gradients of layernorm input,  gamma, beta).</span>

<span class="sd">        pd_x (Tensor): the gradients of layernorm input x.</span>
<span class="sd">        pd_gamma (Tensor): the gradients of gamma.</span>
<span class="sd">        pd_beta (Tensor): the gradients of beta.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">layer_norm_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LayerNormGrad</span><span class="p">)(</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layer_norm_grad_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Layer Normalization to the input tensor.</span>

<span class="sd">    This operator will normalize the input tensor on given axis. LayerNorm is described in the paper</span>
<span class="sd">    `Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        begin_norm_axis (int): The begin axis of the `input_x` to apply LayerNorm,</span>
<span class="sd">            the value must be in [-1, rank(input_x)). Default: ``1`` .</span>
<span class="sd">        begin_params_axis (int): The begin axis of the parameter input (`gamma`, `beta`) to</span>
<span class="sd">            apply LayerNorm, the value must be in [-1, rank(input_x)). Default: ``1`` .</span>
<span class="sd">        epsilon (float): A value added to the denominator for numerical stability(:math:`\epsilon`). Default: ``1e-7`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)`.</span>
<span class="sd">          The input of LayerNorm. Supported dtypes: float16, float32, float64.</span>
<span class="sd">        - **gamma** (Tensor) - Tensor of shape :math:`input_x_shape[begin_params_axis:]`. The learnable parameter</span>
<span class="sd">          :math:`\gamma` as the scale on norm. Supported dtypes: float16, float32, float64.</span>
<span class="sd">        - **beta** (Tensor) - Tensor of shape :math:`input_x_shape[begin_params_axis:]`. The learnable parameter</span>
<span class="sd">          :math:`\beta` as the scale on norm. Supported dtypes: float16, float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], tuple of 3 tensors, the normalized input and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The normalized input, has the same type and shape as the `input_x`.</span>
<span class="sd">        - **mean** (Tensor) - The first `begin_norm_axis` dimensions of `mean` shape is the same as `input_x`,</span>
<span class="sd">          and the remaining dimensions are 1. Suppose the shape of the `input_x` is :math:`(x_1, x_2, \ldots, x_R)`,</span>
<span class="sd">          the shape of the `mean` is :math:`(x_1, \ldots, x_{begin_params_axis}, 1, \ldots, 1)`</span>
<span class="sd">          (when `begin_params_axis=0`, the shape of `mean` is :math:`(1, \ldots, 1)` ).</span>
<span class="sd">        - **variance** (Tensor) - Shape is the same as `mean` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `begin_norm_axis` or `begin_params_axis` is not an int.</span>
<span class="sd">        TypeError: If `epsilon` is not a float.</span>
<span class="sd">        TypeError: If `input_x`, `gamma` or `beta` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gamma = Tensor(np.ones([3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta = Tensor(np.ones([3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mean, variance = ops.layer_norm(input_x, gamma, beta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.2247448  1.         2.2247448]</span>
<span class="sd">        [-0.2247448  1.         2.2247448]]</span>
<span class="sd">        &gt;&gt;&gt; print(mean)</span>
<span class="sd">        [[2.]</span>
<span class="sd">        [2.]]</span>
<span class="sd">        &gt;&gt;&gt; print(variance)</span>
<span class="sd">        [[0.6666667]</span>
<span class="sd">        [0.6666667]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">layer_norm_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">)(</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">layer_norm_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">less_equal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &lt;= y` element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.le` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">        a bool or a tensor whose data type is</span>
<span class="sd">        `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">        `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, which is a number.Number or</span>
<span class="sd">        a bool or a tensor whose data type is</span>
<span class="sd">        `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">        `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.less_equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">less_equal_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LessEqual</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">less_equal_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">less</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`x &lt; y` element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.less` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) -  The first input is a number or</span>
<span class="sd">        a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">        a bool or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.less(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">less_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Less</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">less_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">lin_space_</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">num</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor whose value is `num` evenly spaced in the interval `start` and `stop` (including `start` and</span>
<span class="sd">    `stop`), and the length of the output Tensor is `num`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.linspace` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **start** (Tensor) -  Start value of interval, 0-D Tensor with dtype float32 or float64.</span>
<span class="sd">        - **stop** (Tensor) - Last value of interval, 0-D Tensor with dtype float32 or float64.</span>
<span class="sd">        - **num** (Union[int, Tensor]) - Number of ticks in the interval, inclusive of `start` and `stop`.</span>
<span class="sd">          Must be a positive integer. When the input is Tensor, it must be a 0-D Tensor with dtype int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `start`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; stop = Tensor(10, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; num = 5</span>
<span class="sd">        &gt;&gt;&gt; output = ops.lin_space_(start, stop, num)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.    3.25  5.5   7.75 10.  ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lin_space_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LinSpace</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">lin_space_op</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">log1p</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of one plus the input tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = {log_e}(input_i + 1)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. The value must be greater than -1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log1p(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6931472 1.0986123 1.609438 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">log1p_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Log1p</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">log1p_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">log_matrix_determinant</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sign and the log of the absolute value of the determinant of one or more square matrices.</span>

<span class="sd">    Note:</span>
<span class="sd">        The type of output always be real-value, even `input` is complex.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A matrix to be calculated, its shape is :math:`(..., M, M)`.</span>
<span class="sd">        The matrix must be at least two dimensions, and the last two</span>
<span class="sd">        dimensions must be the same size. Data type must be float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. The signs of the log determinants. The shape is :math:`input.shape[:-2]`.</span>

<span class="sd">        Tensor. The absolute values of the log determinants. The shape is :math:`input.shape[:-2]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` not float32, float64, complex64 or complex128.</span>
<span class="sd">        ValueError: If the last two dimensions of `input` is not same size.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sign, output = ops.log_matrix_determinant(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(sign)</span>
<span class="sd">        [-1.   1.]</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.80336046e+00    3.04452229e+00]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">log_matrix_determinant_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LogMatrixDeterminant</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">log_matrix_determinant_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of a tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = \log_e(x_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the input value of operator Log is within the range (0, 0.01] or [0.95, 1.05], the output accuracy may</span>
<span class="sd">        be affacted.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor of any dimension. The value must be greater than 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.6931472 1.3862944]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">log_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Log</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">log_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">log_softmax_grad</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradient for the Log Softmax activation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">log_softmax_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LogSoftmaxGrad</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">log_softmax_grad_op</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Log Softmax function to the input tensor on the specified axis.</span>
<span class="sd">    Supposes a slice in the given axis, :math:`x` for each element :math:`x_i`,</span>
<span class="sd">    the Log Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \log \left(\frac{\exp(x_i)} {\sum_{j = 0}^{N-1}\exp(x_j)}\right),</span>

<span class="sd">    where :math:`N` is the length of the Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">        additional dimensions, with float16 or float32 data type.</span>
<span class="sd">        axis (int): The axis to perform the Log softmax operation. Default: ``-1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `axis` is not in range [-len(logits.shape), len(logits.shape)).</span>
<span class="sd">        ValueError: If dimension of `logits` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log_softmax(logits)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-4.4519143 -3.4519143 -2.4519143 -1.4519144 -0.4519144]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">log_softmax_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LogSoftmax</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">log_softmax_op</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">logical_and_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical AND&quot; of two tensors element-wise.</span>

<span class="sd">    Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one bool.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast,</span>
<span class="sd">    and the data types of them must be bool.</span>
<span class="sd">    When the inputs are one tensor and one bool, the bool object could only be a constant,</span>
<span class="sd">    and the data type of the tensor must be bool.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} \wedge other_{i}</span>

<span class="sd">    Note:</span>
<span class="sd">        LogicalAnd supports broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, bool]): The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">            converted to bool.</span>
<span class="sd">        other (Union[Tensor, bool]): The second input is a bool when the first input is a tensor or</span>
<span class="sd">            a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_and(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_and(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        False</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_and(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        False</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_and(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logical_and_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LogicalAnd</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">logical_and_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">logical_not_</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical NOT&quot; of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = \\neg input_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor, the dtype must be bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the `input`, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_not(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logical_not_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LogicalNot</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">logical_not_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">logical_or_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical OR&quot; of two tensors element-wise.</span>

<span class="sd">    Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one bool.</span>
<span class="sd">    When the inputs are two tensors, the shapes of them could be broadcast,</span>
<span class="sd">    and the data types of them must be bool.</span>
<span class="sd">    When the inputs are one tensor and one bool, the bool object could only be a constant,</span>
<span class="sd">    and the data type of the tensor must be bool.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} \\vee y_{i}</span>

<span class="sd">    Note:</span>
<span class="sd">        LogicalOr supports broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, bool]): The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">            converted to bool.</span>
<span class="sd">        other (Union[Tensor, bool]): The second input is a bool when the first input is a tensor or</span>
<span class="sd">            a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_or(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True  True]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_or(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_or(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_or(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logical_or_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LogicalOr</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">logical_or_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">logical_xor_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical XOR&quot; of two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = x_{i} \oplus y_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, bool]): The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">            converted to bool.</span>
<span class="sd">        other (Union[Tensor, bool]): The second input is a bool when the first input is a tensor or</span>
<span class="sd">            a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `input` or `other` is not bool or can not be implicitly converted to bool.</span>
<span class="sd">        ValueError: If the shape of two inputs cannot be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_xor(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True True]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_xor(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_xor(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logical_xor(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logical_xor_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LogicalXor</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">logical_xor_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">logit_grad</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">eps</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes LogitGrad of input element-wise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logit_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LogitGrad</span><span class="p">)(</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logit_grad_op</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">logit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">eps</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the logit of a tensor element-wise. When eps is not None, element in `input` is clamped to [eps, 1-eps].</span>
<span class="sd">    When eps is None, input `input` is not clamped.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{align}</span>
<span class="sd">        y_{i} &amp; = \ln(\frac{z_{i}}{1 - z_{i}}) \\</span>
<span class="sd">        z_{i} &amp; = \begin{cases}</span>
<span class="sd">        input_{i} &amp; \text{if eps is None} \\</span>
<span class="sd">        \text{eps} &amp; \text{if } input_{i} \lt \text{eps} \\</span>
<span class="sd">        input_{i} &amp; \text{if } \text{eps} \leq input_{i} \leq 1 - \text{eps} \\</span>
<span class="sd">        1 - \text{eps} &amp; \text{if } input_{i} \gt 1 - \text{eps}</span>
<span class="sd">        \end{cases}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of type float16, float32 or float64.</span>
<span class="sd">        eps (float, optional): The epsilon. If eps is not None, the input clamp bound is defined as [eps, 1-eps],</span>
<span class="sd">            otherwise, the input `input` is not clamped. Default: ``None`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.1, 0.2, 0.3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logit(x, eps=1e-5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.1972246 -1.3862944 -0.8472978]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logit_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Logit</span><span class="p">)(</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logit_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">masked_fill</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fills elements of Tensor with value where mask is True.</span>
<span class="sd">    The shapes of `input` and `mask` need to be the same or broadcastable.</span>

<span class="sd">    Note:</span>
<span class="sd">        If `value` is a floating-point number of Python, it will be converted to float32 later by default.</span>
<span class="sd">        In this case, if `input` is a float16 Tensor, it will be converted to float32 for calculation,</span>
<span class="sd">        and the result type will be converted back to float16 on the CPU and Ascend platforms, which may</span>
<span class="sd">        cause the performance penalty. A TypeError may be raised on the GPU platform. Therefore,</span>
<span class="sd">        it is recommended that &#39;value&#39; should use a Tensor with the same dtype as `input`.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **input** (Tensor): The source Tensor whose data type is one of bool, uint8, int8, int16, int32,</span>
<span class="sd">                    int64, float16, float32, float64, complex64, complex128.</span>
<span class="sd">        - **mask** (Tensor[bool]): The boolean mask.</span>
<span class="sd">        - **value** (Union[Number, Tensor]): The value to fill in with, which must be a 0-D Tensor or a Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `mask` is not bool.</span>
<span class="sd">        TypeError: If `input` or `mask` is not a Tensor.</span>
<span class="sd">        ValueError: If the shapes of `input` and `mask` could not be broadcast.</span>
<span class="sd">        TypeError: If dtype of `input` or `value` is not one of bool, uint8, int8, int16, int32,</span>
<span class="sd">                int64, float16, float32, float64, complex64, complex128.</span>
<span class="sd">        TypeError: If dtype of `value` is different from that of `input`.</span>
<span class="sd">        TypeError: If `value` is neither a Number nor a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mask = Tensor(np.array([True, True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.masked_fill(input, mask, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">    [0.5 0.5 3.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">masked_fill_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MaskedFill</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">masked_fill_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">matrix_determinant</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the determinant of one or more square matrices.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A matrix to be calculated, its shape should be :math:`[..., M, M]` who must</span>
<span class="sd">        have at least two dimensions, and the last two</span>
<span class="sd">        dimensions must be the same size. Data type must be float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. The shape is :math:`input.shape[:-2]`, and the dtype is same as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` not float32, float64, complex64 or complex128.</span>
<span class="sd">        ValueError: If the last two dimensions of `input` is not same size.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_determinant(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-16.5 21. ]</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix_determinant_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MatrixDeterminant</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">matrix_determinant_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">matrix_exp</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the exponential of a single or a batch of square matrices.</span>

<span class="sd">    .. math::</span>

<span class="sd">        matrix\_exp(x) = \sum_{k=0}^{\infty} \frac{1}{k !} x^{k} \in \mathbb{K}^{n \times n}</span>

<span class="sd">    where :math:`x` corresponds to `input` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(*, n, n)` where * is zero or more batch dimensions.</span>
<span class="sd">            Must be one of the following types: float16, float32, float64, complex64, complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` is not one of the following dtype:</span>
<span class="sd">                float16, float32, float64, complex64, complex128.</span>
<span class="sd">        ValueError: If the rank of `input` is less than 2.</span>
<span class="sd">        ValueError: If the size of last two dimensions of `input` are not equal.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[1, 2], [0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_exp(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2.7182817 5.436563 ]</span>
<span class="sd">        [0.        2.7182817]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">matrix_exp_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MatrixExp</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">matrix_exp_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">maximum_grad_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">grad_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad_y</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Grad for maximum grad.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">maximum_grad_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MaximumGradGrad</span><span class="p">)(</span><span class="n">grad_x</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">maximum_grad_grad_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">maximum_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">grad_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad_y</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Grad for maximum.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">maximum_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MaximumGrad</span><span class="p">)(</span><span class="n">grad_x</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">maximum_grad_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum of input tensors element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types</span>
<span class="sd">        consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">        dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar,</span>
<span class="sd">        the scalar could only be a constant.</span>
<span class="sd">        - Broadcasting is supported.</span>
<span class="sd">        - If one of the elements being compared is a NaN, then that element is returned.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_i = \max(input_i, other_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        ValueError: If `input` and `other` are not the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.maximum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4. 5. 6.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.maximum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">maximum_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Maximum</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">maximum_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">minimum_grad</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">grad_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad_y</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Grad for minimum.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">minimum_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">MinimumGrad</span><span class="p">)(</span><span class="n">grad_x</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">minimum_grad_op</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">minimum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.minimum` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; minimum = ops.Minimum()</span>
<span class="sd">        &gt;&gt;&gt; output = minimum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = minimum(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">minimum_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Minimum</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">minimum_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.mul` for more details.</span>

<span class="sd">    Note:</span>
<span class="sd">        - When the two inputs have different shapes,</span>
<span class="sd">          they must be able to broadcast to a common shape.</span>
<span class="sd">        - The two inputs can not be bool type at the same time,</span>
<span class="sd">          [True, Tensor(True, bool\_), Tensor(np.array([True]), bool\_)] are all considered bool type.</span>
<span class="sd">        - The two inputs comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input, which is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mul = ops.Mul()</span>
<span class="sd">        &gt;&gt;&gt; output = mul(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 4. 10. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mul_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Mul</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">mul_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">nan_to_num_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replace the `NaN`, positive infinity and negative infinity values in &#39;input&#39; with the</span>
<span class="sd">    specified values in `nan`, `posinf` and `neginf` respectively.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(input_1, input_2, ..., input_R)`.</span>
<span class="sd">            With float32 or float16 data type.</span>
<span class="sd">        nan (float): The replace value of &#39;NaN&#39;. Default value is 0.0.</span>
<span class="sd">        posinf (float): the value to replace positive infinity values with. Default: ``None``,</span>
<span class="sd">            replacing positive infinity with the maximum value supported by the data type of `input`.</span>
<span class="sd">        neginf (float): the value to replace negative infinity values with. Default: ``None``,</span>
<span class="sd">            replacing negative infinity with the minimum value supported by the data type of `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([float(&#39;nan&#39;), float(&#39;inf&#39;), -float(&#39;inf&#39;), 5.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nan_to_num(input, 1.0, 2.0, 3.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.  2.  3.  5.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nan_to_num_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NanToNum</span><span class="p">)(</span><span class="n">nan</span><span class="p">,</span> <span class="n">posinf</span><span class="p">,</span> <span class="n">neginf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nan_to_num_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">neg</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor with negative values of the input tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">    out_{i} = - input_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor with a dtype of Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, -1, 2, 0, -3.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.neg(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.  -2.   1.  -2.   0.   3.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">neg_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Neg</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">neg_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">next_after</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the next representable floating-point value after `input` towards `other` element-wise.</span>

<span class="sd">    Say there are two float32 numbers :math:`a`, :math:`b`, and let the</span>
<span class="sd">    representable delta of float32 datatype is :math:`eps`. If :math:`a &lt; b`,</span>
<span class="sd">    then the next representable of :math:`a` towards :math:`b` is :math:`a+eps`,</span>
<span class="sd">    the next representable of :math:`b` towards :math:`a` is :math:`b-eps`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor. The shape of tensor is :math:`(N,*)` where :math:`*` means,</span>
<span class="sd">            any number of additional dimensions. Must be one of the following types: float32, float64.</span>

<span class="sd">        other (Tensor): The second input tensor. The shape of tensor is :math:`(N,*)` where :math:`*` means,</span>
<span class="sd">            any number of additional dimensions. Must be one of the following types: float32, float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, it has the same shape as the `x` and `y` after broadcasting.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` and `other` is not one of: float32, float64.</span>
<span class="sd">        TypeError: If the dtypes of `input` and `other` are not same.</span>
<span class="sd">        ValueError: If `input`&#39;s shape is not the same as `other`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_ = Tensor(np.asarray([0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other_ = Tensor(np.asarray([0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_ = ops.nextafter(input_, other_)</span>
<span class="sd">        &gt;&gt;&gt; print(output_)</span>
<span class="sd">        [1.e-45]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">next_after_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NextAfter</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">next_after_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">nllloss_grad</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss_grad</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">total_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradients of `NLLLoss`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nllloss_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NLLLossGrad</span><span class="p">)(</span><span class="n">reduction</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nllloss_grad_op</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss_grad</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">total_weight</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">nll_loss_</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the negative log likelihood loss between logits and labels.</span>

<span class="sd">    The nll loss with reduction=none can be described as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top},</span>
<span class="sd">        \quad l_{n}=-w_{t_{n}} x_{n, t_{n}},</span>
<span class="sd">        \quad w_{c}=\text { weight }[c] \cdot 1</span>

<span class="sd">    where :math:`x` is the logits, :math:`t` is the labels, :math:`w` is the weight,</span>
<span class="sd">    N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">    If reduction is not ``&#39;none&#39;`` (default ``&#39;mean&#39;`` ), then</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=\left\{\begin{array}{ll}</span>
<span class="sd">        \sum_{n=1}^{N} \frac{1}{\sum_{n=1}^{N} w_{t n}} l_{n}, &amp; \text { if reduction }=\text { &#39;mean&#39;; } \\</span>
<span class="sd">        \sum_{n=1}^{N} l_{n}, &amp; \text { if reduction }=\text { &#39;sum&#39; }</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)` or `(C)`. Data type only supports float32 or float16.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N,)` or `(1)`, where each value belong to</span>
<span class="sd">        :math:`[0, C-1]`. Data type only supports int32 or int64.</span>
<span class="sd">        - **weight** (Tensor) - The rescaling weight to each class, with shape :math:`(C,)` and data type only</span>
<span class="sd">        supports float32 or float16.</span>
<span class="sd">        - **reduction** (str, optional): - Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">        - **ignore_index** (int): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. Default: ``-100`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors composed with `loss` and `total_weight`.</span>

<span class="sd">        - **loss** (Tensor) - When `reduction` is ``&#39;none&#39;`` and `logits` is a 2D tensor,</span>
<span class="sd">        the `loss` shape is :math:`(N,)`. Otherwise, the `loss` is a scalar.</span>
<span class="sd">        The data type is the same with `input&#39;s`.</span>
<span class="sd">        - **total_weight** (Tensor) - The `total_weight` is a scalar. The data type is the same with `weight&#39;s`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits` or `weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `labels` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `logits` is not a one or two dimension tensor, `labels` and `weight` are not</span>
<span class="sd">                    one dimension tensors.</span>
<span class="sd">                    When `logits` is a two dimension tensor, the first dimension of `logits` is not equal to `labels`,</span>
<span class="sd">                    and second dimension of `logits` is not equal to `weight`.</span>
<span class="sd">                    When `logits` is a one dimension tensor, the dimensions of `logits`, `labels`</span>
<span class="sd">                    and `weight` should be equal to each other.</span>
<span class="sd">        ValueError: If the value of `labels` exceed :math:`[0, C-1]`, where :math:`C` is the number of classes.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[0.5488135, 0.71518934],</span>
<span class="sd">        ...                           [0.60276335, 0.5448832],</span>
<span class="sd">        ...                           [0.4236548, 0.6458941]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0, 0, 0]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.3834415, 0.79172504]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; loss, weight = ops.nll_loss_(logits, labels, weight, &quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        -0.52507716</span>
<span class="sd">        &gt;&gt;&gt; print(weight)</span>
<span class="sd">        1.1503246</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nllloss_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NLLLoss</span><span class="p">)(</span><span class="n">reduction</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nllloss_op</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">non_zero</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a tensor of the positions of all non-zero values.</span>

<span class="sd">    Refer to :func:`mindspore.ops.nonzero` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor, its rank should be greater than or eaqual to 1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor), 2-D Tensor of data type int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import NonZero</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1,  0], [-5, 0]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.non_zero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 0]</span>
<span class="sd">        [0 1 0]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 0, 2, 0, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.non_zero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0]</span>
<span class="sd">        [2]</span>
<span class="sd">        [4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">non_zero_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NonZero</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">non_zero_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">not_equal</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the non-equivalence of two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ne` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, it has the same shape as the `x` and `y` after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.not_equal(x, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([1, 2, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.not_equal(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">not_equal_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">NotEqual</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">not_equal_op</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">one_hot_</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a one-hot tensor.</span>

<span class="sd">    The locations represented by indices in `indices` take value `on_value`, while all</span>
<span class="sd">    other locations take value `off_value`.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the input indices is rank `N`, the output will have rank `N+1`. The new axis is created at dimension `axis`. On Ascend, if `on_value` is Int64 dtype, `indices` must be Int64 dtype.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices. Tensor of shape :math:`(X_0, \ldots, X_n)`.</span>
<span class="sd">            Data type must be int32 or int64.</span>
<span class="sd">        - **depth** (int): A scalar defining the depth of the one-hot dimension.</span>
<span class="sd">        - **on_value** (Tensor): A value to fill in output when `indices[j] = i`. Data type must be int32, int64, float16 or float32.</span>
<span class="sd">        - **off_value** (Tensor): A value to fill in output when `indices[j] != i`.</span>
<span class="sd">            It has the same data type as `on_value`.</span>
<span class="sd">        - **axis** (int): Position to insert the value. e.g. If shape of `indices` is :math:`(N, C)`, and `axis` is -1,</span>
<span class="sd">            the output shape will be :math:`(N, C, D)`, If `axis` is 0, the output shape will be :math:`(D, N, C)`.</span>
<span class="sd">            Default: ``-1`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, one-hot tensor. Tensor of shape :math:`(X_0, \ldots, X_{axis}, \text{depth} ,X_{axis+1}, \ldots, X_n)`, and it has the same data type as `on_value`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` or `depth` is not an int.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If dtype of `on_value` is not int32, int64, float16 or float32.</span>
<span class="sd">        TypeError: If `indices`, `on_value` or `off_value` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is not in range [-1, len(indices_shape)].</span>
<span class="sd">        ValueError: If `depth` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; depth, on_value, off_value = 3, Tensor(1.0, mindspore.float32), Tensor(0.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.one_hot(indices, depth, on_value, off_value)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0. 0.]</span>
<span class="sd">        [0. 1. 0.]</span>
<span class="sd">        [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">one_hot_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">OneHot</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">one_hot_op</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">ones_like_</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor with a value of 1 and its shape and data type is the same as the input.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ones_like` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of any dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x` but filled with ones.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0, 1], [2, 1]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ones_like(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">        [1 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ones_like_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">OnesLike</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">ones_like_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">pow</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the `y` power of each element in `x`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.pow` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = 3.0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pow(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  8. 64.]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([2.0, 4.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pow(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 16. 64.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pow_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Pow</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">pow_op</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">prelu_grad</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradients of PReLU operation.</span>

<span class="sd">    Note:</span>
<span class="sd">        1-dimensional input_x is not supported.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **dy** (Tensor) - Representing the backprop of the next layer.</span>
<span class="sd">        - **x** (Tensor) - Must be the input `x` of forward operator PRelu.</span>
<span class="sd">        - **weight** (Tensor) - Float Tensor, w &gt; 0, must be the input `weight` of forward operator PRelu.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **dx** (Tensor), with the same type as `x`.</span>
<span class="sd">        - **dw** (Tensor), with the same type as `weight`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prelu_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">PReLUGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">prelu_grad_op</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric Rectified Linear Unit activation function.</span>

<span class="sd">    PReLU is described in the paper `Delving Deep into Rectifiers: Surpassing Human-Level Performance on</span>
<span class="sd">    ImageNet Classification &lt;https://arxiv.org/abs/1502.01852&gt;`_. Defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        prelu(x_i)= \max(0, x_i) + \min(0, w * x_i),</span>

<span class="sd">    where :math:`x_i` is an element of a channel of the input, `w` is the weight of the channel.</span>

<span class="sd">    Note:</span>
<span class="sd">        Scalar or 1-D Tensor is not supported on Ascend.</span>

<span class="sd">    PReLU Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/PReLU.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor of the activation function. The data type is float16 or float32.</span>
<span class="sd">        The shape is :math:`(N, C, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        weight (Tensor):  Weight Tensor. The data type is float16 or float32.</span>
<span class="sd">        The weight can only be a Tensor, and the length is the same as the number of channels C of the `input_x`.</span>
<span class="sd">        On GPU devices, when the input is a scalar, the shape is :math:`(1,)` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and dtype as `x`.</span>

<span class="sd">        For detailed information, please refer to :class:`mindspore.nn.PReLU`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` or `weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If the `x` or the `weight` is not a Tensor.</span>
<span class="sd">        ValueError: If the `x` is a 0-D or 1-D Tensor on Ascend.</span>
<span class="sd">        ValueError: If the `weight` is not a 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(-6, 6).reshape((2, 3, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.1, 0.6, -0.3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prelu(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[-0.60 -0.50]</span>
<span class="sd">        [-2.40 -1.80]</span>
<span class="sd">        [ 0.60  0.30]]</span>
<span class="sd">        [[ 0.00  1.00]</span>
<span class="sd">        [ 2.00  3.00]</span>
<span class="sd">        [ 4.0   5.00]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prelu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">PReLU</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">prelu_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">qr_</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the QR decomposition of one or more matrices.</span>

<span class="sd">    If `mode` is &#39;reduced&#39;(the default), compute the P columns of Q where P is minimum of the 2 innermost dimensions of</span>
<span class="sd">    input. If `mode` is &#39;complete&#39;, compute full-sized Q and R.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A matrix to be calculated. The matrix must be at least two dimensions, the supported dtype are</span>
<span class="sd">            float16, float32, float64, complex64 and complex128.</span>
<span class="sd">            Define the shape of input as :math:`(..., m, n)`, p as the</span>
<span class="sd">            minimum values of m and n.</span>
<span class="sd">        full_matrices (bool, optional): Whether compute full-sized QR decomposition. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **Q** (Tensor) - The orthonormal matrices of input. If `mode` is &#39;complete&#39;, the shape is :math:`(m, m)`,</span>
<span class="sd">        else the shape is :math:`(m, p)`. The dtype of `Q` is same as `input`.</span>
<span class="sd">        - **R** (Tensor) - The upper triangular matrices of input. If `mode` is &#39;complete&#39;, the shape is :math:`(m, n)`,</span>
<span class="sd">        else the shape is :math:`(p, n)`. The dtype of `R` is same as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `mode` is neither &#39;reduced&#39; nor &#39;complete&#39;.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[20., -31, 7], [4, 270, -90], [-8, 17, -32]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; Q, R = ops.qr_(input)</span>
<span class="sd">        &gt;&gt;&gt; print(Q)</span>
<span class="sd">        [[-0.912871    0.16366126  0.37400758]</span>
<span class="sd">        [-0.18257418 -0.9830709  -0.01544376]</span>
<span class="sd">        [ 0.36514837 -0.08238228  0.92729706]]</span>
<span class="sd">        &gt;&gt;&gt; print(R)</span>
<span class="sd">        [[ -21.908903  -14.788506  -1.6431675]</span>
<span class="sd">        [    0.       -271.9031    92.25824  ]</span>
<span class="sd">        [    0.          0.       -25.665514 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">qr_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Qr</span><span class="p">)(</span><span class="n">full_matrices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">qr_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="randperm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.randperm.html#mindspore.ops.randperm">[文档]</a><span class="k">def</span> <span class="nf">randperm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates random permutation of integers from 0 to n-1.</span>

<span class="sd">    Returns the tensor with the determined shape inferred by n, the random numbers in it drawn from the data range</span>
<span class="sd">    that a given type can represent.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        n (Union[Tensor, int]): The input n Tensor with shape: () or (1,) and with data type of int64.</span>
<span class="sd">            The value of `n` must be greater than zero.</span>
<span class="sd">        seed (int, optional): Random seed. Default: ``0`` . When seed is -1(only negative value), offset is 0,</span>
<span class="sd">            it&#39;s determined by time.</span>
<span class="sd">        offset (int, optional): Offset to generate random numbers. Priority is higher than random seed.</span>
<span class="sd">            Default: ``0`` . It must be non-negative.</span>
<span class="sd">        dtype (mindspore.dtype, optional): The type of output.</span>
<span class="sd">            Its value must be one of the following types: int32, int16, int8,</span>
<span class="sd">            uint8, int64, float64, float32, float16. Default: mstype.int64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. Its shape is specified by the required args `n`. Its type is specified by `dtype`.</span>
<span class="sd">        Otherwise is default.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dtype` is not allowed.</span>
<span class="sd">        ValueError: If `n` is a negative or 0 element.</span>
<span class="sd">        ValueError: If `seed` is a negative element.</span>
<span class="sd">        ValueError: If `n` is larger than the maximal data of the set dtype.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; n = 4</span>
<span class="sd">        &gt;&gt;&gt; seed = 0</span>
<span class="sd">        &gt;&gt;&gt; offset = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.randperm(n, seed, offset, dtype=mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 2 1 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">randperm_v2_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">RandpermV2</span><span class="p">)(</span><span class="n">seed</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">randperm_v2_op</span><span class="p">(</span><span class="n">n</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a sequence of numbers that begins at `start` and extends by increments of</span>
<span class="sd">    `delta` up to but not including `limit`.</span>

<span class="sd">    The types of all 3 inputs must be all integers or floating-point numbers.</span>

<span class="sd">    Args:</span>
<span class="sd">        start (number): The first number in the sequence. Must have</span>
<span class="sd">            type: int32 ,int64, float32 or float64.</span>
<span class="sd">        limit (number): Upper limit of the sequence, exclusive. Must</span>
<span class="sd">            have type: int32 ,int64, float32 or float64.</span>
<span class="sd">        delta (number): Number that increments `start`. Must have</span>
<span class="sd">            type: int32 ,int64, float32 or float64.</span>
<span class="sd">        maxlen (int, optional): Memory that can fit `maxlen` many elements</span>
<span class="sd">            will be allocated for the output. Optional, must be positive. Default: 1000000.</span>
<span class="sd">            If the output has more than `maxlen` elements, a runtime error will occur.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1-D Tensor. If `start`, `limit` and `delta` are all integers, the type of output is int64.</span>
<span class="sd">        If `start`, `limit` and `delta` are all floating-point numbers, the type of output is float32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If &#39;start&#39;, &#39;limit&#39;, &#39;delta&#39; have both integers and floating-point numbers.</span>
<span class="sd">        TypeError: If datatype of `start`, `limit` or `delta` is not supported.</span>
<span class="sd">        ValueError: If `delta` = 0.</span>
<span class="sd">        ValueError: If `start` &gt;= `limit` when `delta` &gt; 0.</span>
<span class="sd">        ValueError: If `start` &lt;= `limit` when `delta` &lt; 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; start = 0</span>
<span class="sd">        &gt;&gt;&gt; limit = 10</span>
<span class="sd">        &gt;&gt;&gt; delta = 4</span>
<span class="sd">        &gt;&gt;&gt; output = ops.range(start, limit, delta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 4 8]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">range_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Range</span><span class="p">)(</span><span class="n">maxlen</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">range_op</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">limit</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">real_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor in floating-point type element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.div` for more details.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  \\frac{input_i}{other_i})</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        other (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.real_div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.25 0.4 0.5 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">real_div_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">RealDiv</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">real_div_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>


<div class="viewcode-block" id="real"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.real.html#mindspore.ops.real">[文档]</a><span class="k">def</span> <span class="nf">real</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor that is the real part of the input. If input is real, it is returned unchanged.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute to.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor(np.asarray(np.complex(1.3+0.4j)), ms.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.real(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.3</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">real_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Real</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">real_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">reciprocal_grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of Reciprocal operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reciprocal_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReciprocalGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">reciprocal_grad_op</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reciprocal_</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns reciprocal of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  \frac{1}{x_{i}}</span>

<span class="sd">    Args:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>
<span class="sd">            :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor(np.array([1.0, 2.0, 4.0]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reciprocal_(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.   0.5  0.25]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reciprocal_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Reciprocal</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">reciprocal_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reduce_all</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the &quot;logicalAND&quot; of all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[bool]): The dtype of the tensor to be reduced is bool.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int), tensor]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">            dimensions. Must be in the range [-rank(x), rank(x)).</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        output(Tensor): the dtype is bool.</span>

<span class="sd">        - If axis is (), and keep_dims is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the &quot;logical and&quot; of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the &quot;logicalAND&quot; of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_all(x, (), keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[False]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_all(x, 0, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True False]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_all(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[False]</span>
<span class="sd">        [ True]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: input is a scalar.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(True)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_all(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduce_all_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReduceAll</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_all_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reduce_any</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the &quot;logical OR&quot; of all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[bool]): The dtype of the tensor to be reduced is bool.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int), tensor]): The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">            dimensions. Must be in the range [-rank(x), rank(x)).</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        output(Tensor): the dtype is bool.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the &quot;logical or&quot; of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the &quot;logical OR&quot; of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_any(x, (), keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_any(x, 0, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True True]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_any(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[True]</span>
<span class="sd">        [ True]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: input is a scalar.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(True)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_any(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduce_any_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReduceAny</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_any_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reduce_max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the maximum value in this dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int), tensor]): The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">            dimensions. Must be in the range [-r, r).</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        output(Tensor): has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">          the output is a 0-D tensor representing the maximum of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_max(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the maximum value of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_max(x, (), keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[9.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_max(x, 0, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[7. 7. 7. 7. 7. 7.]</span>
<span class="sd">          [8. 8. 8. 8. 8. 8.]</span>
<span class="sd">          [9. 9. 9. 9. 9. 9.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_max(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 3. 3. 3. 3. 3.]]</span>
<span class="sd">         [[6. 6. 6. 6. 6. 6.]]</span>
<span class="sd">         [[9. 9. 9. 9. 9. 9.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_max(x, 2, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [2.]</span>
<span class="sd">          [3.]]</span>
<span class="sd">         [[4.]</span>
<span class="sd">          [5.]</span>
<span class="sd">          [6.]]</span>
<span class="sd">         [[7.]</span>
<span class="sd">          [8.]</span>
<span class="sd">          [9.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduce_max_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReduceMax</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_max_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by averaging all elements in the dimension, by default. And also can reduce</span>
<span class="sd">    a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">      controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int), tensor]): The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">            dimensions. Must be in the range [-r, r).</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        output(Tensor): with the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the mean of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), set as (1, 2), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_maen(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by averaging all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2]],</span>
<span class="sd">        ... [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ... [[6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8], [10, 10, 10, 10, 10, 10]]]),</span>
<span class="sd">        ... mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_maen(x, (), keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[5.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along the axis 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_maen(x, 0, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4. 4. 4. 4. 4. 4.]</span>
<span class="sd">          [5. 5. 5. 5. 5. 5.]</span>
<span class="sd">          [6. 6. 6. 6. 6. 6.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along the axis 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_maen(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2. 2. 2. 2. 2.]]</span>
<span class="sd">          [[5. 5. 5. 5. 5. 5.]]</span>
<span class="sd">          [[8. 8. 8. 8. 8. 8.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along the axis 2</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_maen(x, 2, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 2.]</span>
<span class="sd">          [ 2.]</span>
<span class="sd">          [ 2.]]</span>
<span class="sd">          [[ 4.]</span>
<span class="sd">          [ 5.]</span>
<span class="sd">          [ 6.]]</span>
<span class="sd">          [[ 6.]</span>
<span class="sd">          [ 8.]</span>
<span class="sd">          [10.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduce_mean_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReduceMean</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_mean_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reduce_min</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the minimum value in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int), tensor]): The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">            dimensions. Must be in the range [-r, r).</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        output(Tensor): has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the minimum of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_min(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the minimum value of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_min(x, (), keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_min(x, 0, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3. 3. 3.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_min(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1. 1. 1.]]</span>
<span class="sd">         [[4. 4. 4. 4. 4. 4.]]</span>
<span class="sd">         [[7. 7. 7. 7. 7. 7.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_min(x, 2, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [2.]</span>
<span class="sd">          [3.]]</span>
<span class="sd">         [[4.]</span>
<span class="sd">          [5.]</span>
<span class="sd">          [6.]]</span>
<span class="sd">         [[7.]</span>
<span class="sd">          [8.]</span>
<span class="sd">          [9.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduce_min_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReduceMin</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_min_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reduce_prod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by multiplying all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int), tensor]): The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">            dimensions. Must be in the range [-r, r).</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        output(Tensor): has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_prod(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by multiplying all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_prod(x, (), keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2.2833798e+33]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_prod(x, 0, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 28.  28.  28.  28.  28.  28.]</span>
<span class="sd">          [ 80.  80.  80.  80.  80.  80.]</span>
<span class="sd">          [162. 162. 162. 162. 162. 162.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_prod(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[  6.   6.   6.   6.   6.   6.]]</span>
<span class="sd">         [[120. 120. 120. 120. 120. 120.]]</span>
<span class="sd">         [[504. 504. 504. 504. 504. 504.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_prod(x, 2, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.00000e+00]</span>
<span class="sd">          [6.40000e+01]</span>
<span class="sd">          [7.29000e+02]]</span>
<span class="sd">         [[4.09600e+03]</span>
<span class="sd">          [1.56250e+04]</span>
<span class="sd">          [4.66560e+04]]</span>
<span class="sd">         [[1.17649e+05]</span>
<span class="sd">          [2.62144e+05]</span>
<span class="sd">          [5.31441e+05]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduce_prod_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReduceProd</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_prod_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reduce_std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[],</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the standard-deviation and mean of the input Tensor along dimension(s) specified by `axis`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int), tensor]): The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">            dimensions. Let `r` be rank of `input_x`, it should be in the range :math:`[-r,r)`.</span>
<span class="sd">        unbiased(bool): Whether to use Bessel&#39;s correction.</span>
<span class="sd">            If ``True`` , will use the Bessel correction unbiased estimation.</span>
<span class="sd">            If ``False`` , will through the biased estimation to calculate the standard deviation.</span>
<span class="sd">            Default: ``True`` .</span>
<span class="sd">        keep_dims (bool): Whether the output Tensor has dim retained or not.</span>
<span class="sd">            If ``True`` , keep these reduced dimensions specified by `axis` and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions.</span>
<span class="sd">            Default: ``Fasle`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        output(tuple(Tensor)): (output_std, output_mean) containing the standard deviation and mean.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [-1, 1, 4]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reduce_std(input_x, axis=1, unbiased=True, keep_dims=False)</span>
<span class="sd">        &gt;&gt;&gt; output_std, output_mean = output[0], output[1]</span>
<span class="sd">        &gt;&gt;&gt; print(output_std)</span>
<span class="sd">        [1.        2.5166113]</span>
<span class="sd">        &gt;&gt;&gt; print(output_mean)</span>
<span class="sd">        [2.        1.3333334]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduce_std_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReduceStd</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">unbiased</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_std_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(),</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">skip_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by summing all elements in the dimension, by default. And also can reduce a</span>
<span class="sd">    dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor[Number]): The input tensor.</span>
<span class="sd">        axis (Union[int, tuple(int), list(int), tensor]): The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">            dimensions when skip_mode is ``False`` . Must be in the range [-rank(`x`), rank(`x`)).</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>
<span class="sd">        skip_mode (bool): If ``True`` and axis is empty tuple or empty list, the ReduceSum operation isn&#39;t</span>
<span class="sd">            performed, skip it. If ``True`` and axis is other values, the ReduceSum calculation is performed</span>
<span class="sd">            normally. If ``False``, do reduce. Default: ``False`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        output(Tensor): has the same dtype as the `x`.</span>

<span class="sd">        - If axis is (), keep_dims is ``False`` , and skip_mode is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the sum of all elements in the input tensor.</span>
<span class="sd">        - If axis is (), and skip_mode is ``True`` ,</span>
<span class="sd">          the ReduceSum operation is not performed, output tensor is equal to the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int) or list(int), set as (2, 3), and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `skip_mode` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is None.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.reduce_sum(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output.shape</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by summing all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.reduce_sum(x, ()), keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[270.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; op = ops.reduce_sum(x, 0, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[12. 12. 12. 12. 12. 12.]</span>
<span class="sd">          [15. 15. 15. 15. 15. 15.]</span>
<span class="sd">          [18. 18. 18. 18. 18. 18.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; op = ops.reduce_sum(x, 1, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 6.  6.  6.  6.  6.  6.]]</span>
<span class="sd">         [[15. 15. 15. 15. 15. 15.]]</span>
<span class="sd">         [[24. 24. 24. 24. 24. 24.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; op = ops.reduce_sum(x, 2, keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 6.]</span>
<span class="sd">          [12.]</span>
<span class="sd">          [18.]]</span>
<span class="sd">         [[24.]</span>
<span class="sd">          [30.]</span>
<span class="sd">          [36.]]</span>
<span class="sd">         [[42.]</span>
<span class="sd">          [48.]</span>
<span class="sd">          [54.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reduce_sum_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReduceSum</span><span class="p">)(</span><span class="n">keep_dims</span><span class="p">,</span> <span class="n">skip_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduce_sum_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_relu6_grad</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradient for the ReLU6 activation.</span>

<span class="sd">    Args:</span>
<span class="sd">        y_backprop (Tensor): Input gradients tensor, has the same dtype and shape as `x`.</span>
<span class="sd">        x (Tensor): Origin input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as `x`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relu6_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReLU6Grad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">relu6_grad_op</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit) upper bounded by 6 of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ReLU6}(x) = \min(\max(0,x), 6)</span>

<span class="sd">    It returns :math:`\min(\max(0,x), 6)` element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input Tensor of float16 or float32 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same dtype and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.relu6(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relu6_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReLU6</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">relu6_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_relu_grad</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradient for the ReLU activation.</span>

<span class="sd">    Args:</span>
<span class="sd">        y_backprop (Tensor): Input gradients tensor, has the same dtype and shape as `x`.</span>
<span class="sd">        x (Tensor): Origin input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as `x`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relu_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReluGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">relu_grad_op</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit activation function) of input tensors element-wise.</span>

<span class="sd">    It returns :math:`\max(input,\  0)` element-wise. Specially, the neurons with the negative output</span>
<span class="sd">    will be suppressed and the active neurons will stay the same.</span>

<span class="sd">    .. math::</span>

<span class="sd">        ReLU(input) = (input)^+ = \max(0, input)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input Tensor of numeric types.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not a number.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.relu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReLU</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">relu_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rearranges the input Tensor based on the given shape.</span>

<span class="sd">    The &#39;shape&#39; can only have one -1 at most, in which case it&#39;s inferred from the remaining dimensions and</span>
<span class="sd">    the number of elements in the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        shape (Union[tuple[int], Tensor[int]]): Constructed by multiple</span>
<span class="sd">            integers, i.e., :math:`(y_1, y_2, ..., y_S)`. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: Given a shape tuple, if it has several -1; or if the product</span>
<span class="sd">            of its elements is less than or equal to 0 or cannot be divided by the product</span>
<span class="sd">            of the input tensor shape; or if it does not match the input&#39;s array size.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reshape(input, (3, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.1  0.3]</span>
<span class="sd">         [ 3.6  0.4]</span>
<span class="sd">         [ 0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reshape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Reshape</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">reshape_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resize_bicubic_grad</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for ResizeBicubicGrad operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        grads (Tensor): A Tensor of type float. 4-D with shape [batch, height, width, channels]. The format must be NHWC.</span>
<span class="sd">        image (Tensor): A Tensor. Must be one of the following types: float, double.</span>
<span class="sd">            4-D with shape [batch, orig_height, orig_width, channels], The origin image tensor that was resized.</span>
<span class="sd">            The format must be NHWC.</span>
<span class="sd">        align_corners (bool): If true, the centers of the 4 corner pixels of the input and output tensors are </span>
<span class="sd">            aligned, preserving the values at the corner pixels.Default: ``False``.</span>
<span class="sd">        half_pixel_centers (bool): An optional bool. Default: ``False``.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 4-D Tensor , with the same shape and data type as `image`.</span>

<span class="sd">    Rasise:</span>
<span class="sd">        TypeError: If `grads` is not allowed.</span>
<span class="sd">        TypeError: If `image` is not allowed.</span>
<span class="sd">        ValueError: If `image` dim is not 4.</span>
<span class="sd">        ValueError: If `size` dim is not 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">resize_bicubic_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ResizeBicubicGrad</span><span class="p">)(</span><span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resize_bicubic_grad_op</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resize_bicubic</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resize image to size using bicubic interpolation.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        image (Tensor): The input image must be a 4-D tensor of shape :math:`(batch, channels, height, width)`.</span>
<span class="sd">            The format must be NCHW. Types allowed: float16, float32, float64.</span>
<span class="sd">        size (tuple[int]): - An int tuple with 2 elements: new_height, new_width.</span>
<span class="sd">        align_corners (bool, optional): If ``True`` , the centers of the 4 corner pixels of the input</span>
<span class="sd">            and output tensors are aligned, preserving the values at the corner pixels. Default: ``False`` .</span>
<span class="sd">        half_pixel_centers (bool, optional): Whether to use half-pixel center alignment. If set to ``True`` ,</span>
<span class="sd">            `align_corners` should be ``False`` . Default: ``False`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 4-D tensor with shape :math:`(batch, channels, new_height, new_width)` whose dtype is the same as `image` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the type of `image` is not allowed.</span>
<span class="sd">        TypeError: If the type of `size` is not int32.</span>
<span class="sd">        TypeError: If the type of `align_corners` is not bool.</span>
<span class="sd">        TypeError: If the type of `half_pixel_centers` is not bool.</span>
<span class="sd">        ValueError: If the dim of `image` is not 4.</span>
<span class="sd">        ValueError: If the dim of `size` is not 1.</span>
<span class="sd">        ValueError: If the number of elements in `size` is not 2.</span>
<span class="sd">        ValueError: If any value of `size` is not positive.</span>
<span class="sd">        ValueError: If the values of `align_corners` and `half_pixel_centers` are both ``True`` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; image = Tensor(np.array([1, 2, 3, 4]).reshape(1, 1, 2, 2).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.auto_generate.resize_bicubic(image, (1, 4), False, False)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[[[1. 1.5 2. 2.09375]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">resize_bicubic_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ResizeBicubic</span><span class="p">)(</span><span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resize_bicubic_op</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resize_bilinear_grad</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of ResizeBilinear operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        grads (Tensor): A 4-D Tensor with shape [batch, channel, height, width].</span>
<span class="sd">        image (Tensor): A 4-D Tensor with shape [batch, channel, height, width], The origin image tensor that was resized.</span>
<span class="sd">        align_corners (bool): If true, the centers of the 4 corner pixels of the input and output tensors are </span>
<span class="sd">            aligned, preserving the values at the corner pixels.Default: ``False``.</span>
<span class="sd">        half_pixel_centers (bool): An optional bool. Default: ``False``.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 4-D Tensor , with the same shape and data type as `image`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">resize_bilinear_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ResizeBilinearGrad</span><span class="p">)(</span><span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resize_bilinear_grad_op</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resize_bilinear</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes an image to a certain size using the bilinear interpolation.</span>

<span class="sd">    The resizing only affects the lower two dimensions which represent the height and width.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        image (Tensor): Image to be resized. Input image must be a 4-D tensor with shape</span>
<span class="sd">          :math:`(batch, channels, height, width)`, with data type of float32 or float16.</span>
<span class="sd">        size (Union[tuple[int], list[int], Tensor]): The new size of the image.</span>
<span class="sd">          A tuple or list or Tensor of 2 int elements :math:`(new_height, new_width)`.</span>
<span class="sd">        align_corners (bool, optional): If ``True`` , rescale input by :math:`(new_height - 1) / (height - 1)`,</span>
<span class="sd">                       which exactly aligns the 4 corners of image and resized image. If ``False`` ,</span>
<span class="sd">                       rescale by :math:`new_height / height`. Default: ``False`` .</span>
<span class="sd">        half_pixel_centers (bool, optional): Whether half pixel center. If set to ``True`` , `align_corners` should be</span>
<span class="sd">                           ``False`` . Default: ``False`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, resized image. 4-D with shape :math:`(batch, channels, new_height, new_width)`,</span>
<span class="sd">        with the same data type as input `image`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `align_corners` is not a bool.</span>
<span class="sd">        TypeError: If `half_pixel_centers` is not a bool.</span>
<span class="sd">        TypeError: If `align_corners` and `half_pixel_centers` are all ``True`` .</span>
<span class="sd">        ValueError: If `half_pixel_centers` is ``True`` and device_target is CPU.</span>
<span class="sd">        ValueError: If dim of `image` is not 4.</span>
<span class="sd">        ValueError: If `size` is Tensor and its dim is not 1.</span>
<span class="sd">        ValueError: If `size` contains other than 2 elements.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; image = Tensor([[[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.resize_bilinear()(image, (5, 5))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">resize_bilinear_v2_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ResizeBilinearV2</span><span class="p">)(</span><span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resize_bilinear_v2_op</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resize_linear_1d_grad</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="o">=</span><span class="s1">&#39;align_corners&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute gradient of `ResizeLinear1D` operator.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        grads (Tensor): A Tensor of type float. 3-D with shape [batch, channel, width].</span>
<span class="sd">        x (Tensor): A origin input Tensor. 3-D with shape [batch, channel, orig_width], The origin tensor that was resized.</span>
<span class="sd">        coordinate_transformation_mode (string): Default is &#39;align_corners&#39;. Describes how to transform the coordinate</span>
<span class="sd">            in the resized tensor to the coordinate in the original tensor. Other optional: &#39;half_pixel&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">resize_linear_1d_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ResizeLinear1DGrad</span><span class="p">)(</span><span class="n">coordinate_transformation_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resize_linear_1d_grad_op</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resize_linear_1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="o">=</span><span class="s1">&#39;align_corners&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using the linear interpolate method resize the input tensor &#39;x&#39;.</span>

<span class="sd">    For general resize, refer to :func:`mindspore.ops.interpolate` for more details.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - This is an experimental API that is subject to change.</span>
<span class="sd">        - Currently, the Ascend platform only supports scenarios where the input `size` is Tuple or List.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor) - A 3-D tensor which to resize, with shape [batch, channel, width]. Must be one of the</span>
<span class="sd">              following types: float16, float32, float64.</span>
<span class="sd">        size (Union[Tuple[int], List[int], Tensor[int]]): describes the new width of `x` .</span>
<span class="sd">              A tuple or list or 1-D tensor with only one int element :math:`(new_width)`.</span>
<span class="sd">        coordinate_transformation_mode (str): Default is ``&#39;align_corners&#39;`` . Describes how to transform the</span>
<span class="sd">            coordinate in the resized tensor to the coordinate in the original tensor. Other optional: &#39;half_pixel&#39;.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 3-D tensor which shape is [batch, channel, new_width] with the same type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not in the support list.</span>
<span class="sd">        TypeError: If `size` is not in Union[Tuple[int], List[int], Tensor[int]].</span>
<span class="sd">        TypeError: If `coordinate_transformation_mode` is not a string.</span>
<span class="sd">        TypeError: If `coordinate_transformation_mode` is not in the support list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[1, 2, 3], [4, 5, 6]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; size = (6,)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.resize_linear_1d(x, size, coordinate_transformation_mode=&quot;align_corners&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1.4 1.8 2.2 2.6 3.]</span>
<span class="sd">          [4. 4.4 4.8 5.2 5.6 6.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">resize_linear_1d_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ResizeLinear1D</span><span class="p">)(</span><span class="n">coordinate_transformation_mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resize_linear_1d_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resize_nearest_neighbor_grad</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute gradient of `ResizeNearestNeighbor` operator.</span>

<span class="sd">    Note:</span>
<span class="sd">        The shape of input parameter `size` must be (height, width).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **align_corners** (bool) - Whether the centers of the 4 corner pixels of the input</span>
<span class="sd">            and output tensors are aligned. Default: ``False``.</span>
<span class="sd">        - **half_pixel_centers** (bool, optional): Whether half pixel center. If set to ``True``,</span>
<span class="sd">                `align_corners` should be False. Default: ``False``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">resize_nearest_neighbor_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ResizeNearestNeighborGrad</span><span class="p">)(</span><span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resize_nearest_neighbor_grad_op</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resize_nearest_neighbor</span><span class="p">(</span><span class="n">image_in</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes the input tensor to a given size by using the nearest neighbor algorithm. The nearest</span>
<span class="sd">    neighbor algorithm selects the value of the nearest point and does not consider the</span>
<span class="sd">    values of neighboring points at all, yielding a piecewise-constant interpolant.</span>

<span class="sd">    Args:</span>
<span class="sd">        size (Union[tuple, list]): The target size. The dimension of size must be 2.</span>
<span class="sd">        align_corners (bool): Whether the centers of the 4 corner pixels of the input</span>
<span class="sd">                              and output tensors are aligned. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The shape of the tensor is :math:`(N, C, H, W)`.</span>
<span class="sd">        - **size** (Union[tuple, list]) - The target size. The dimension of size must be 2.</span>
<span class="sd">        - **align_corners** (bool) - Whether the centers of the 4 corner pixels of the input</span>
<span class="sd">                                     and output tensors are aligned. Default: ``False`` .</span>
<span class="sd">        - **half_pixel_centers** (bool, optional): Whether half pixel center. If set to ``True``,</span>
<span class="sd">                `align_corners` should be False. Default: ``False``.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is  :math:`(N, C, NEW\_H, NEW\_W)`.</span>
<span class="sd">        The data type is the same as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `size` is neither tuple nor list.</span>
<span class="sd">        TypeError: If `align_corners` is not a bool.</span>
<span class="sd">        ValueError: If length of `size` is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[[[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; size = (2, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ResizeNearestNeighbor(size=size)(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[-0.1  0.3]</span>
<span class="sd">          [ 0.4  0.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">resize_nearest_neighbor_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ResizeNearestNeighbor</span><span class="p">)(</span><span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resize_nearest_neighbor_op</span><span class="p">(</span><span class="n">image_in</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resize_nearest_neighbor_v2_grad</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute gradient of `ResizeNearestNeighborV2` operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        grads (Tensor): A 4-D Tensor with shape [batch, channel, height, width].</span>
<span class="sd">        size (Union[tuple[int], Tensor]): The size for the input image. 2 elements: [`height, width`].</span>
<span class="sd">        align_corners (bool): Whether the centers of the 4 corner pixels of the input</span>
<span class="sd">            and output tensors are aligned. Default: ``False``.</span>
<span class="sd">        half_pixel_centers (bool): Default: ``False``.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 4-D Tensor , with the same shape and data type as `image`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">resize_nearest_neighbor_v2_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ResizeNearestNeighborV2Grad</span><span class="p">)(</span><span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resize_nearest_neighbor_v2_grad_op</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">resize_nearest_neighbor_v2</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes the input tensor to specific size by using the nearest neighbor algorithm.</span>

<span class="sd">    The nearest neighbor algorithm selects the value of the nearest point and does not consider the</span>
<span class="sd">    values of neighboring points at all, yielding a piecewise-constant interpolant.</span>

<span class="sd">    Args:</span>
<span class="sd">        image (Tensor) - 4-D with shape :math:`(batch, channels, height, width)`.</span>
<span class="sd">        size (Union[tuple[int], Tensor]) - The new size for the image. 2 elements: [`new_height, new_width`].</span>
<span class="sd">        align_corners (bool, optional): If ``True`` , the centers of the 4 corner pixels of the input and output</span>
<span class="sd">            tensors are aligned, preserving the values at the corner pixels. Default: ``False``.</span>
<span class="sd">        half_pixel_centers (bool, optional): Whether half pixel center. If set to ``True``,</span>
<span class="sd">            `align_corners` should be False. Default: ``False``.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        output (Tensor) - The resized image. A 4-D with shape</span>
<span class="sd">            :math:`(batch, channels, new_height, new_width)`. It has the same dtype as `image`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `image` or `size` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type  of `size` is not int32.</span>
<span class="sd">        TypeError: If `align_corners` or `half_pixel_centers` is not bool.</span>
<span class="sd">        ValueError: If any value of `size` is not positive.</span>
<span class="sd">        ValueError: If the dimension of `image` is not 4.</span>
<span class="sd">        ValueError: If the dimension of `size` is not 1.</span>
<span class="sd">        ValueError: If the elements number of `size` is not 2.</span>
<span class="sd">        ValueError: If attr `half_pixel_centers` and `align_corners` are True at the same time.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones((1, 1, 4, 4)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; size = (2, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.resize_nearest_neighbor_v2(input_tensor, size)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1. 1.]</span>
<span class="sd">            [1. 1.]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">resize_nearest_neighbor_v2_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ResizeNearestNeighborV2</span><span class="p">)(</span><span class="n">align_corners</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">resize_nearest_neighbor_v2_op</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reverse_v2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses specific dimensions of a tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The value range of &quot;axis&quot; is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of &quot;input_x&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The target tensor.</span>
<span class="sd">            The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        axis (Union[tuple(int), list(int)]): The indices of the dimensions to reverse.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is neither list nor tuple.</span>
<span class="sd">        TypeError: If element of `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse(input_x, axis=[1])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4 3 2 1]</span>
<span class="sd">         [8 7 6 5]]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse(input_x, axis=[1, 0])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[8 7 6 5]</span>
<span class="sd">         [4 3 2 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reverse_v2_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReverseV2</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reverse_v2_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">right_shift</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shift the value of each position of Tensor `input_x` to the right by corresponding bits in Tensor `input_y`.</span>
<span class="sd">    The inputs are two tensors, dtypes of them must be consistent, and the</span>
<span class="sd">    shapes of them could be broadcast.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;out_{i} =x_{i} &gt;&gt; y_{i}</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor, will be shifted to the right</span>
<span class="sd">          by `input_y` bits element-wise. Support all int and uint types.</span>
<span class="sd">        - **input_y** (Tensor) - Number of bits shifted, the tensor must have the same type as `input_x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - The output tensor, has the same type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `input_y` is not tensor.</span>
<span class="sd">        TypeError: If `input_x` and `input_y` could not be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]).astype(np.uint8))</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([1, 1, 1]).astype(np.uint8))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.right_shift(input_x, input_y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">right_shift_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">RightShift</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">right_shift_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">roll_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls the elements of a tensor along an axis.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor.</span>
<span class="sd">        shift (Union[list(int), tuple(int), int]): Specifies the number of places by which elements are shifted</span>
<span class="sd">            positively (towards larger indices) along the specified dimension. Negative shift will roll the elements</span>
<span class="sd">            in the opposite direction.</span>
<span class="sd">        axis (Union[list(int), tuple(int), int]): Specifies the dimension indexes of shape to be rolled.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shift` is not an int, a tuple or a list.</span>
<span class="sd">        TypeError: If `axis` is not an int, a tuple or a list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0, 1, 2, 3, 4]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.roll(input_x, shift=2, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 4. 0. 1. 2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">roll_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Roll</span><span class="p">)(</span><span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">roll_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns half to even of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i \approx input_i</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.8, 1.5, 2.3, 2.5, -4.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.round(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  2.  2.  2. -4.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">round_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Round</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">round_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_rsqrt_grad</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for the Rsqrt.</span>

<span class="sd">    Args:</span>
<span class="sd">        y_backprop (Tensor): Input gradients tensor, has the same dtype and shape as `x`.</span>
<span class="sd">        x (Tensor): Origin input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as `x`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rsqrt_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">RsqrtGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">rsqrt_grad_op</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">rsqrt</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes reciprocal of square root of input tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  \frac{1}{\sqrt{input_{i}}}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of rsqrt. Its each element must be a non-negative</span>
<span class="sd">            number, if an element is negative, the calculation result is nan.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = ms.Tensor([-0.0370,  0.2970,  1.5420, -0.9105])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.rsqrt(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [       nan 1.8349396  0.80530024        nan]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rsqrt_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Rsqrt</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">rsqrt_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scalar_to_tensor</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scalar_to_tensor_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ScalarToTensor</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">scalar_to_tensor_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">scatter_nd</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scatters a tensor into a new tensor depending on the specified indices.</span>

<span class="sd">    Refer to :func:`mindspore.ops.scatter_nd` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **indices** (Tensor) - The index of scattering in the new tensor with int32 or int64 data type.</span>
<span class="sd">              The rank of indices must be at least 2 and `indices_shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        - **updates** (Tensor) - The source Tensor to be scattered.</span>
<span class="sd">              It has shape `indices_shape[:-1] + shape[indices_shape[-1]:]`.</span>
<span class="sd">        - **shape** (tuple[int]) - Define the shape of the output tensor, has the same data type as indices.</span>
<span class="sd">              The shape of `shape` is :math:`(x_1, x_2, ..., x_R)`, and the length of &#39;shape&#39; is greater than or equal to 2.</span>
<span class="sd">              In other words, the shape of `shape` is at least :math:`(x_1, x_2)`.</span>
<span class="sd">              And the value of any element in `shape` must be greater than or equal to 1.</span>
<span class="sd">              In other words, :math:`x_1` &gt;= 1, :math:`x_2` &gt;= 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the new tensor, has the same type as `update` and the same shape as `shape`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2],</span>
<span class="sd">        ...                             [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[1, 1, 1, 1], [2, 2, 2, 2],</span>
<span class="sd">        ...                             [3, 3, 3, 3], [4, 4, 4, 4]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (4, 4, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd(indices, updates, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3.]</span>
<span class="sd">          [4. 4. 4. 4.]]</span>
<span class="sd">         [[0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]]</span>
<span class="sd">         [[1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3.]</span>
<span class="sd">          [4. 4. 4. 4.]]</span>
<span class="sd">         [[0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([3.2, 1.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (3, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd(indices, updates, shape)</span>
<span class="sd">        &gt;&gt;&gt; # In order to facilitate understanding, explain the operator pseudo-operation process step by step:</span>
<span class="sd">        &gt;&gt;&gt; # Step 1: Generate an empty Tensor of the specified shape according to the shape</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # Step 2: Modify the data at the specified location according to the indicators</span>
<span class="sd">        &gt;&gt;&gt; # 0th row of indices is [0, 1], 0th row of updates is 3.2.</span>
<span class="sd">        &gt;&gt;&gt; # means that the empty tensor in the 0th row and 1st col set to 3.2</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 3.2. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # 1th row of indices is [1, 1], 1th row of updates is 1.1.</span>
<span class="sd">        &gt;&gt;&gt; # means that the empty tensor in the 1th row and 1st col set to 1.1</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 3.2. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 1.1  0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # The final result is as follows:</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 3.2 0.]</span>
<span class="sd">         [0. 1.1 0.]</span>
<span class="sd">         [0. 0.  0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scatter_nd_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ScatterNd</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">scatter_nd_op</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_sigmoid_grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the gradient of Sigmoid operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sigmoid_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">SigmoidGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">sigmoid_grad_op</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Sigmoid of input element-wise. The Sigmoid function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{sigmoid}(x_i) = \frac{1}{1 + \exp(-x_i)}</span>

<span class="sd">    where :math:`x_i` is an element of `x`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): `input` is `x` in the preceding formula. Tensor of any dimension,</span>
<span class="sd">        the data type is float16, float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32, float64, complex64 or complex128.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sigmoid(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7310586  0.880797   0.95257413 0.98201376 0.9933072 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sigmoid_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Sigmoid</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">sigmoid_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_silu_grad</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of SiLU operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">silu_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">SiLUGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">silu_grad_op</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">silu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Sigmoid Linear Unit of input element-wise. The SiLU function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{SiLU}(x) = x * \sigma(x),</span>

<span class="sd">    where :math:`x` is an element of the input, :math:`\sigma(x)` is Sigmoid function.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{sigma}(x_i) = \frac{1}{1 + \exp(-x_i)},</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): `input` is `x` in the preceding formula. Input with the data type</span>
<span class="sd">        float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-1, 2, -3, 2, -1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.silu(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.269  1.762  -0.1423  1.762  -0.269]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">silu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">SiLU</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">silu_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="sin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sin.html#mindspore.ops.sin">[文档]</a><span class="k">def</span> <span class="nf">sin</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        output_i = \sin(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`. </span>
<span class="sd">        The dtype of output is float32 when dtype of `input` is in</span>
<span class="sd">        [bool, int8, uint8, int16, int32, int64]. Otherwise output has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: On CPU or GPU: If dtype of `input` is not float16, float32 or float64, complex64, complex128.</span>
<span class="sd">                   On Ascend: If type of `input` is not bool, int8, uint8, int16, int32, int64, float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sin(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5810352 0.27635565 0.41687083 0.5810352]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sin_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Sin</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">sin_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sinc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sinc.html#mindspore.ops.sinc">[文档]</a><span class="k">def</span> <span class="nf">sinc</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the normalized sinc of input.</span>

<span class="sd">    .. math::</span>

<span class="sd">        output_i = \begin{cases} \frac{sin(\pi input_i)}{\pi input_i} &amp; input_i\neq 0\\</span>
<span class="sd">        1 &amp; input_i=0 \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`. The dtype of output is float32 when dtype of `input` is in</span>
<span class="sd">        [int, bool]. Otherwise output has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sinc(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.47735003 0.8759357  0.7224278  0.47735003]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sinc_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Sinc</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">sinc_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sinh.html#mindspore.ops.sinh">[文档]</a><span class="k">def</span> <span class="nf">sinh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        output_i = \sinh(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of hyperbolic sine function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sinh(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6604918  0.28367308 0.44337422 0.6604918 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sinh_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Sinh</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">sinh_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">split_</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_num</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor into output_num of tensors along the given axis and output numbers.</span>

<span class="sd">    Refer to :func:`mindspore.ops.split` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_0, x_1, ..., x_{R-1})`, R &gt;= 1.</span>
<span class="sd">        axis (int): Index of the split position. Default: ``0`` .</span>
<span class="sd">        output_num (int): The number of output tensors. Must be positive int. Default: ``1`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        </span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], the shape of each output tensor is the same, which is</span>
<span class="sd">        :math:`(x_0, x_1, ..., x_{axis}/{output\_num}, ..., x_{R-1})`.</span>
<span class="sd">        And the data type is the same as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 1, 1, 1], [2, 2, 2, 2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [[1 1 1 1]</span>
<span class="sd">        [2 2 2 2]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.split_(x, 1, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Int32, value=</span>
<span class="sd">        [[1, 1],</span>
<span class="sd">        [2, 2]]), Tensor(shape=[2, 2], dtype=Int32, value=</span>
<span class="sd">        [[1, 1],</span>
<span class="sd">        [2, 2]]))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.split_(x, 1, 4)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">        [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">        [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">        [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">        [2]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">split_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Split</span><span class="p">)(</span><span class="n">axis</span><span class="p">,</span> <span class="n">output_num</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">split_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sqrt_grad</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of Sqrt operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sqrt_grad_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">SqrtGrad</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">sqrt_grad_op</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">sqrt_</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns square root of a tensor element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        When there are some negative number, it will return a Tensor whose specific position is nan.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  \sqrt{x_{i}}</span>

<span class="sd">    Args:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 4.0, 9.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sqrt(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sqrt_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Sqrt</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">sqrt_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">square_</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns square of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = (x_{i})^2</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.square(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 4. 9.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">square_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Square</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">square_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">tensor_copy_slices</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Copy continues memory.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - **x** (Tensor) - The target Tensor.</span>
<span class="sd">    - **value** (Tensor) - The tensor to update x.</span>
<span class="sd">    - **begin** (tuple[int]) - A tuple which represents the location where to start. Only</span>
<span class="sd">      constant value is allowed.</span>
<span class="sd">    - **end** (tuple[int]) - A tuple or which represents the maximum location where to end.</span>
<span class="sd">      Only constant value is allowed.</span>
<span class="sd">    - **strides** (tuple[int]) - A tuple which represents the stride is continuously added</span>
<span class="sd">      before reaching the maximum location. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">    - **y** (Tensor), has the same shape and data type of x.</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">    &gt;&gt;&gt; out = ops.tensor_copy_slices(Tensor(np.zeros((5, 5))), Tensor(np.ones((2, 5))), (3, 0), (5, 5), (1, 1))</span>
<span class="sd">    &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[1., 1., 1., 1., 1.],</span>
<span class="sd">          [1., 1., 1., 1., 1.],</span>
<span class="sd">          [1., 1., 1., 1., 1.],</span>
<span class="sd">          [0., 0., 0., 0., 0.],</span>
<span class="sd">          [0., 0., 0., 0., 0.]]</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">    ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor_copy_slices_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">TensorCopySlices</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">tensor_copy_slices_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">tensor_shape</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the shape of the input tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.tensor_shape(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor_shape_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">TensorShape</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">tensor_shape_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sum of the diagonal elements in a 2-D matrix.</span>

<span class="sd">    Note:</span>
<span class="sd">        Input must be matrix, and complex number is not supported at present.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A matrix to be calculated. The matrix must be two dimensional.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, 0D Tensor with 1 element, it has the same data type as input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If the dimension of `x` is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; trace = ops.Trace()</span>
<span class="sd">        &gt;&gt;&gt; output = trace(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        15.0</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1, 13).reshape(3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; trace = ops.Trace()</span>
<span class="sd">        &gt;&gt;&gt; output = trace(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        18.0</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(12, 0, -1).reshape(4, 3), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; trace = ops.Trace()</span>
<span class="sd">        &gt;&gt;&gt; output = trace(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        24.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">trace_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Trace</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">trace_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Permutes the dimensions of the input tensor according to input permutation.</span>

<span class="sd">    For a 1-D array this has no effect, as a transposed vector is simply the same vector.</span>
<span class="sd">    To convert a 1-D array into a 2D column vector please refer the class: mindspore.ops.ExpandDims.</span>
<span class="sd">    For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given,</span>
<span class="sd">    their order indicates how the axes are permuted (see Examples).</span>
<span class="sd">    If axes are not provided and a.shape is :math:`(i[0], i[1], ... i[n-2], i[n-1])`,</span>
<span class="sd">    then a.transpose().shape is :math:`(i[n-1], i[n-2], ... i[1], i[0])`.</span>

<span class="sd">    Note:</span>
<span class="sd">        On GPU and CPU, if the value of `input_perm` is negative, its actual value is `input_perm[i] + rank(input)`.</span>
<span class="sd">        Negative value of `input_perm` is not supported on Ascend.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        perm (tuple[int]): The permutation to be converted. The elements in `input_perm` are composed of</span>
<span class="sd">            the indexes of each dimension of `input`. The length of `input_perm` and the shape of `input` must be</span>
<span class="sd">            the same. Only constant value is allowed. Must be in the range [-rank(input), rank(input)).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the type of output tensor is the same as `input` and the shape of output tensor is decided by the</span>
<span class="sd">        shape of `input` and the value of `input_perm`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_perm` is not a tuple.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to length of shape of `input_perm`.</span>
<span class="sd">        ValueError: If the same element exists in `input_perm`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_perm = (0, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.transpose(input, input_perm)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 1.  4.]</span>
<span class="sd">        [ 2.  5.]</span>
<span class="sd">        [ 3.  6.]]</span>
<span class="sd">        [[ 7. 10.]</span>
<span class="sd">        [ 8. 11.]</span>
<span class="sd">        [ 9. 12.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">transpose_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Transpose</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">transpose_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">perm</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">tuple_to_tensor</span><span class="p">(</span><span class="n">input_tuple</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tuple_to_tensor_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">TupleToTensor</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">tuple_to_tensor_op</span><span class="p">(</span><span class="n">input_tuple</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">view</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshape the tensor according to the input shape. It&#39;s the same as :func:`mindspore.Tensor.reshape`,</span>
<span class="sd">    implemented by the underlying reshape operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        shape (Union[tuple(int), int]): Dimension of the output tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, which dimension is the input shape&#39;s value.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([[1, 2, 3], [2, 3, 4]], dtype=np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = a.view((3, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2.]</span>
<span class="sd">        [3. 2.]</span>
<span class="sd">        [3. 4.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">view_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">View</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">view_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">zeros_like_</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor with a value of 0 and its shape and data type is the same as the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **input_x** (Tensor) - Input Tensor of any dimension.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x` but filled with zeros.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.zeros_like_(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0.]</span>
<span class="sd">         [0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">zeros_like_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ZerosLike</span><span class="p">)()</span>
    <span class="k">return</span> <span class="n">zeros_like_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>