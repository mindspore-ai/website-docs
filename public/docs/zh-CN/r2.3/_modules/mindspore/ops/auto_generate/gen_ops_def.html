<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.auto_generate.gen_ops_def &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script><script src="../../../../_static/js/mermaid-9.3.0.js"></script><script src="../../../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">分布式并行原生</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.hal.html">mindspore.hal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">网络搭建对比</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/operators.html">静态图语法-运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/statements.html">静态图语法-Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/python_builtin_functions.html">静态图语法-Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">模块代码</a> &raquo;</li>
      <li>mindspore.ops.auto_generate.gen_ops_def</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>mindspore.ops.auto_generate.gen_ops_def 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2023 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators definition generated by gen_ops.py, includes functions.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">.gen_ops_prim</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">.gen_inner_ops_prim</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">.pyboost_inner_prim</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.operations.manually_defined.ops_def</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._primitive_cache</span> <span class="kn">import</span> <span class="n">_get_cache_prim</span>


<div class="viewcode-block" id="abs"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.abs.html#mindspore.ops.abs">[文档]</a><span class="k">def</span> <span class="nf">abs</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns absolute value of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = |input_i|</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-1.0, 1.0, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.abs(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 1. 0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">abs_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="acos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.acos.html#mindspore.ops.acos">[文档]</a><span class="k">def</span> <span class="nf">acos</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arccosine of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \cos^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.acos(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.737726  1.5307857 1.2661036 0.9764105]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">acos_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="acosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.acosh.html#mindspore.ops.acosh">[文档]</a><span class="k">def</span> <span class="nf">acosh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic cosine of the inputs element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \cosh^{-1}(input_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Given an input tensor input, the function computes inverse hyperbolic cosine of every element.</span>
<span class="sd">        Input range is [1, inf].</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of inverse hyperbolic cosine function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.acosh(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.9624237 1.7627472 5.298292 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">acosh_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.add.html#mindspore.ops.add">[文档]</a><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds other value to input Tensor.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} + other_{i}</span>

<span class="sd">    Note:</span>
<span class="sd">        - When the two inputs have different shapes,</span>
<span class="sd">          they must be able to broadcast to a common shape.</span>
<span class="sd">        - The two inputs can not be bool type at the same time,</span>
<span class="sd">          [True, Tensor(True, bool\_), Tensor(np.array([True]), bool\_)] are all considered bool type.</span>
<span class="sd">        - The two inputs comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one of the input `input` , `other` after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, number.Number, bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: x and y are both Tensor.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 7. 9.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: x is a scalar and y is a Tensor</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4, 5, 6]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.add(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [5. 6. 7.]</span>
<span class="sd">        &gt;&gt;&gt; # the data type of x is int32, the data type of y is float32,</span>
<span class="sd">        &gt;&gt;&gt; # and the output is the data format of higher precision float32.</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">add_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="angle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.angle.html#mindspore.ops.angle">[文档]</a><span class="k">def</span> <span class="nf">angle</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the element-wise argument of a complex tensor.</span>
<span class="sd">    The elements in input are considered to be complex numbers of the form a+bj, where a is the real part and b</span>
<span class="sd">    is the imaginary part. The argument returned by this function is of the form :math:`atan2(b, a)`.</span>

<span class="sd">    Args:</span>
<span class="sd">      input (Tensor):</span>
<span class="sd">        The input tensor. types: complex64, complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tensor, has the float32 or float64 type and the same shape as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `input` is not a Tensor.</span>
<span class="sd">      TypeError:</span>
<span class="sd">        If the dtype of `input` is not one of: complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">      ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">      &gt;&gt;&gt; import mindspore</span>
<span class="sd">      &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">      &gt;&gt;&gt; input = Tensor([-1.5 + 7.8j, 3 + 5.75j], mindspore.complex64)</span>
<span class="sd">      &gt;&gt;&gt; output = ops.angle(input)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [1.7607845 1.0899091]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">angle_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="asin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.asin.html#mindspore.ops.asin">[文档]</a><span class="k">def</span> <span class="nf">asin</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes arcsine of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \sin^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32, float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.74, 0.04, 0.30, 0.56]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.asin(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.8330704  0.04001067  0.30469266  0.5943858 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">asin_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="asinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.asinh.html#mindspore.ops.asinh">[文档]</a><span class="k">def</span> <span class="nf">asinh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \sinh^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of inverse hyperbolic sine function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-5.0, 1.5, 3.0, 100.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.asinh(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.3124382  1.1947632  1.8184465  5.298342 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">asinh_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="assign_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.assign_add.html#mindspore.ops.assign_add">[文档]</a><span class="k">def</span> <span class="nf">assign_add</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates a `Parameter` by adding a value to it.</span>

<span class="sd">    Args of `variable` and `value` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>
<span class="sd">    If `value` is a number, the number is automatically converted to Tensor,</span>
<span class="sd">    and the data type is consistent with the Tensor data type involved in the operation.</span>

<span class="sd">    Note:</span>
<span class="sd">        Since `variable` is a data type Parameter, the data type cannot be changed,</span>
<span class="sd">        so only the type of `value` is allowed to be promoted to the type of `variable`.</span>
<span class="sd">        And the conversion type supported by different devices will be different,</span>
<span class="sd">        it is recommended to use the same data type when using this operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        variable (Parameter): The `Parameter`.</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        value (Tensor): The value to be added to the `variable`.</span>
<span class="sd">            It must have the same shape as `variable`.</span>
<span class="sd">            it is recommended to use the same data type when using this operator.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as original `variable`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `value` is neither Number nor Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `variable` and `value` conversion of Parameter</span>
<span class="sd">                    is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; variable = mindspore.Parameter(initializer(1, [1], mindspore.int32), name=&quot;global_step&quot;)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(np.ones([1]).astype(np.int32) * 100)</span>
<span class="sd">        &gt;&gt;&gt; ops.assign_add(variable, value)</span>
<span class="sd">        &gt;&gt;&gt; print(variable.asnumpy())</span>
<span class="sd">        [101]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">assign_add_op</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="assign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.assign.html#mindspore.ops.assign">[文档]</a><span class="k">def</span> <span class="nf">assign</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Assigns `Parameter` with a value.</span>

<span class="sd">    Args of `variable` and `value` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        variable (Parameter): The `Parameter`. :math:`(N,*)` where :math:`*` means,</span>
<span class="sd">            any number of additional dimensions.</span>
<span class="sd">        value (Tensor): The value to be assigned, has the same shape with `variable`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as original `variable`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `variable` is not a Parameter.</span>
<span class="sd">        TypeError: If `value` is not a Tensor.</span>
<span class="sd">        RuntimeError: If the data type of `variable` and `value` conversion of Parameter</span>
<span class="sd">                      is required when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([2.0], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; variable = mindspore.Parameter(Tensor([1.0], mindspore.float32), name=&quot;variable&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ops.assign(variable, value)</span>
<span class="sd">        &gt;&gt;&gt; print(variable.asnumpy())</span>
<span class="sd">        [2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">assign_op</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<div class="viewcode-block" id="atan2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atan2.html#mindspore.ops.atan2">[文档]</a><span class="k">def</span> <span class="nf">atan2</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns arctangent of input/other element-wise.</span>

<span class="sd">    It returns :math:`\theta\ \in\ [-\pi, \pi]`</span>
<span class="sd">    such that :math:`input = r*\sin(\theta), other = r*\cos(\theta)`, where :math:`r = \sqrt{input^2 + other^2}`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Arg `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">          If they have different data types, the lower precision data type will be converted to relatively the</span>
<span class="sd">          highest precision data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor, Number.number): The input tensor or scalar.</span>
<span class="sd">        other (Tensor, Number.number): The input tensor or scalar. It has the same shape with `input` or</span>
<span class="sd">            its shape is able to broadcast with `input`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is same as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` or `other` is not a Tensor or scalar.</span>
<span class="sd">        RuntimeError: If the data type of `input` and `other` conversion of Parameter is required</span>
<span class="sd">                    when data type conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atan2(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.7853982]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">atan2_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="atan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atan.html#mindspore.ops.atan">[文档]</a><span class="k">def</span> <span class="nf">atan</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the trigonometric inverse tangent of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \tan^{-1}(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            Supported dtypes: </span>

<span class="sd">            - Ascend: float16, float32.</span>
<span class="sd">            - GPU/CPU: float16, float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32, float64, complex64 or complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atan(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7853982 0.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">atan_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="atanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.atanh.html#mindspore.ops.atanh">[文档]</a><span class="k">def</span> <span class="nf">atanh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes inverse hyperbolic tangent of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \tanh^{-1}(input_{i})</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">            The data type should be one of the following types: float16, float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A Tensor, has the same type as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0, -0.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.atanh(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.         -0.54930615]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">atanh_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="bias_add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.bias_add.html#mindspore.ops.bias_add">[文档]</a><span class="k">def</span> <span class="nf">bias_add</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the sum of the input Tensor and the bias Tensor. Before adding, the bias Tensor will be broadcasted to be</span>
<span class="sd">    consistent with the shape of the input Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input tensor. The shape can be 2-5 dimensions.</span>
<span class="sd">        bias (Tensor): The bias tensor, with shape :math:`(C)`. C must be the same as channel dimension C of</span>
<span class="sd">            `input_x`.</span>
<span class="sd">        data_format (str, optional): The format of input and output data.</span>
<span class="sd">            It should be ``&quot;NHWC&quot;`` , ``&quot;NCHW&quot;`` or ``&quot;NCDHW&quot;`` .</span>
<span class="sd">            Default is ``&quot;NCHW&quot;`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `data_format` is not a str.</span>
<span class="sd">        ValueError: If value of `data_format` is not in the range of [&#39;NHWC&#39;,&#39;NCHW&#39;,&#39;NCDHW&#39;].</span>
<span class="sd">        TypeError: If `input_x` or `bias` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `bias` is inconsistent.</span>
<span class="sd">        TypeError: If dimension of `input_x` is not in the range [2, 5].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.random.random(3).reshape((3,)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.bias_add(input_x, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">bias_add_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">BiasAdd</span><span class="p">)(</span><span class="n">data_format</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bias_add_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>


<div class="viewcode-block" id="broadcast_to"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.broadcast_to.html#mindspore.ops.broadcast_to">[文档]</a><span class="k">def</span> <span class="nf">broadcast_to</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Broadcasts input tensor to a given shape. The dim of input shape must be smaller</span>
<span class="sd">    than or equal to that of target shape. Suppose input shape is :math:`(x_1, x_2, ..., x_m)`,</span>
<span class="sd">    target shape is :math:`(*, y_1, y_2, ..., y_m)`, where :math:`*` means any additional dimension.</span>
<span class="sd">    The broadcast rules are as follows:</span>

<span class="sd">    Compare the value of :math:`x_m` and :math:`y_m`, :math:`x_{m-1}` and :math:`y_{m-1}`, ...,</span>
<span class="sd">    :math:`x_1` and :math:`y_1` consecutively and</span>
<span class="sd">    decide whether these shapes are broadcastable and what the broadcast result is.</span>

<span class="sd">    If the value pairs at a specific dim are equal, then that value goes right into that dim of output shape.</span>
<span class="sd">    With an input shape :math:`(2, 3)`, target shape :math:`(2, 3)` , the inferred output shape is :math:`(2, 3)`.</span>

<span class="sd">    If the value pairs are unequal, there are three cases:</span>

<span class="sd">    Case 1: If the value of the target shape in the dimension is -1, the value of the</span>
<span class="sd">    output shape in the dimension is the value of the corresponding input shape in the dimension.</span>
<span class="sd">    With an input shape :math:`(3, 3)`, target</span>
<span class="sd">    shape :math:`(-1, 3)`, the output shape is :math:`(3, 3)`.</span>

<span class="sd">    Case 2: If the value of target shape in the dimension is not -1, but the corresponding</span>
<span class="sd">    value in the input shape is 1, then the corresponding value of the output shape</span>
<span class="sd">    is that of the target shape. With an input shape :math:`(1, 3)`, target</span>
<span class="sd">    shape :math:`(8, 3)`, the output shape is :math:`(8, 3)`.</span>

<span class="sd">    Case 3: If the corresponding values of the two shapes do not satisfy the above cases,</span>
<span class="sd">    it means that broadcasting from the input shape to the target shape is not supported.</span>

<span class="sd">    So far we got the last m dims of the outshape, now focus on the first :math:`*` dims, there are</span>
<span class="sd">    two cases:</span>

<span class="sd">    If the first :math:`*` dims of output shape does not have -1 in it, then fill the input</span>
<span class="sd">    shape with ones until their length are the same, and then refer to</span>
<span class="sd">    Case 2 mentioned above to calculate the output shape. With target shape :math:`(3, 1, 4, 1, 5, 9)`,</span>
<span class="sd">    input shape :math:`(1, 5, 9)`, the filled input shape will be :math:`(1, 1, 1, 1, 5, 9)` and thus the</span>
<span class="sd">    output shape is :math:`(3, 1, 4, 1, 5, 9)`.</span>

<span class="sd">    If the first :math:`*` dims of output shape have -1 in it, it implies this -1 is corresponding to</span>
<span class="sd">    a non-existing dim so they&#39;re not broadcastable. With target shape :math:`(3, -1, 4, 1, 5, 9)`,</span>
<span class="sd">    input shape :math:`(1, 5, 9)`, instead of operating the dim-filling process first, it raises errors directly.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>
<span class="sd">        shape (tuple): The target shape to broadcast. Can be fully specified, or have -1 in one position</span>
<span class="sd">                      where it will be substituted by the input tensor&#39;s shape in that position, see example.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the given `shape` and the same data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is not a tuple.</span>
<span class="sd">        ValueError: If the target and input shapes are incompatible, or if a - 1 in the target shape is in an invalid</span>
<span class="sd">                    location.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; shape = (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.broadcast_to(x, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">        [1. 2. 3.]]</span>
<span class="sd">        &gt;&gt;&gt; shape = (-1, 2)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1], [2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.broadcast_to(x, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 1.]</span>
<span class="sd">        [2. 2.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">broadcast_to_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">BroadcastTo</span><span class="p">)(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">broadcast_to_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>

<span class="n">cast_op</span><span class="o">=</span><span class="n">Cast</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">cast</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor with the new specified data type.</span>

<span class="sd">    Note:</span>
<span class="sd">        When converting complex numbers to boolean type, the imaginary part of the complex number is not</span>
<span class="sd">        taken into account. As long as the real part is non-zero, it returns True; otherwise, it returns False.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Union[Tensor, Number]) - The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">          The tensor to be cast.</span>
<span class="sd">        - **type** (dtype.Number) - The valid data type of the output tensor. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of tensor is the same as `input_x`, :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is neither Tensor nor Number.</span>
<span class="sd">        TypeError: If `type` is not a Number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_np = np.random.randn(2, 3, 4, 5).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(input_np)</span>
<span class="sd">        &gt;&gt;&gt; type_dst = mindspore.int32</span>
<span class="sd">        &gt;&gt;&gt; cast = ops.Cast()</span>
<span class="sd">        &gt;&gt;&gt; output = cast(input_x, type_dst)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int32</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 4, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cast_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<div class="viewcode-block" id="ceil"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ceil.html#mindspore.ops.ceil">[文档]</a><span class="k">def</span> <span class="nf">ceil</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds a tensor up to the closest integer element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor with a dtype of float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16 or float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ceil(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2.  3. -1.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ceil(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        3.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ceil_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="celu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.celu.html#mindspore.ops.celu">[文档]</a><span class="k">def</span> <span class="nf">celu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    celu activation function, computes celu (Continuously differentiable exponential</span>
<span class="sd">    linear units) of input tensors element-wise. The formula is defined as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{CeLU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</span>

<span class="sd">    For more details, please refer to `celu &lt;https://arxiv.org/abs/1704.07483&gt;`_.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    CELU Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/CELU.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input of celu with data type of float16 or float32.</span>
<span class="sd">        alpha (float, optional): The :math:`\alpha` value for the Celu formulation. Default: 1.0</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same data type and shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `alpha` is not a float.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `alpha` has the value of 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-2.0, -1.0, 1.0, 2.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.celu(x, alpha=1.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.86466473 -0.63212055  1.          2.        ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">celu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">CeLU</span><span class="p">)(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">celu_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="cat"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cat.html#mindspore.ops.cat">[文档]</a><span class="k">def</span> <span class="nf">cat</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Connect input tensors along with the given axis.</span>

<span class="sd">    The input data is a tuple or a list of tensors. These tensors have the same rank :math:`R`.</span>
<span class="sd">    Set the given axis as :math:`m`, and :math:`0 \le m &lt; R`. Set the number of input tensors as :math:`N`.</span>
<span class="sd">    For the :math:`i`-th tensor :math:`t_i`, it has the shape of :math:`(x_1, x_2, ..., x_{mi}, ..., x_R)`.</span>
<span class="sd">    :math:`x_{mi}` is the :math:`m`-th dimension of the :math:`t_i`. Then, the shape of the output tensor is</span>

<span class="sd">    .. math::</span>

<span class="sd">        (x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)</span>

<span class="sd">    Args:</span>
<span class="sd">        tensors (Union[tuple, list]): A tuple or a list of input tensors.</span>
<span class="sd">            Suppose there are two tensors in this tuple or list, namely t1 and t2.</span>
<span class="sd">            To perform `concat` in the axis 0 direction, except for the :math:`0`-th axis,</span>
<span class="sd">            all other dimensions should be equal, that is,</span>
<span class="sd">            :math:`t1.shape[1] = t2.shape[1], t1.shape[2] = t2.shape[2], ..., t1.shape[R-1] = t2.shape[R-1]`,</span>
<span class="sd">            where :math:`R` represents the rank of tensor.</span>
<span class="sd">        axis (int): The specified axis, whose value is in range :math:`[-R, R)`. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is :math:`(x_1, x_2, ..., \sum_{i=1}^Nx_{mi}, ..., x_R)`.</span>
<span class="sd">        The data type is the same with `tensors`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `tensors` have different dimension of tensor.</span>
<span class="sd">        ValueError: If `axis` not in range :math:`[-R, R)`.</span>
<span class="sd">        ValueError: If tensor&#39;s shape in `tensors` except for `axis` are different.</span>
<span class="sd">        ValueError: If `tensors` is an empty tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x1 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; input_x2 = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cat((input_x1, input_x2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1.]</span>
<span class="sd">         [2. 1.]</span>
<span class="sd">         [0. 1.]</span>
<span class="sd">         [2. 1.]]</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cat((input_x1, input_x2), 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 1. 0. 1.]</span>
<span class="sd">         [2. 1. 2. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">concat_impl</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="conj"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.conj.html#mindspore.ops.conj">[文档]</a><span class="k">def</span> <span class="nf">conj</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor of complex numbers that are the complex conjugate of each element in input.</span>
<span class="sd">    The complex numbers in input must be of the form a + bj, where a is the real part and b is the imaginary part.</span>

<span class="sd">    The complex conjugate returned by this operation is of the form a - bj.</span>

<span class="sd">    If `input` is real, it is returned unchanged.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute to. Must have numeric type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `input` is not a numeric type.</span>
<span class="sd">        TypeError: If the `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.asarray(np.complex(1.3+0.4j)), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.conj(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (1.3-0.4j)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">conj_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">contiguous</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts a Tensor into a continuous-memory Tensor that contains the same data as the original Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A contiguous in memory tensor containing the same data as self tensor.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[1, 2, 3], [4, 5, 6]], dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.transpose(x, (1, 0))</span>
<span class="sd">        &gt;&gt;&gt; y.contiguous()</span>
<span class="sd">        &gt;&gt;&gt; y[:, 1] = 1</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [[1. 2. 3.]</span>
<span class="sd">         [4. 5. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">contiguous_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">copy</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">copy_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">correlate</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Cross-correlation of two 1-dimensional sequences.</span>

<span class="sd">    This function computes the correlation as generally defined in signal processing texts:</span>

<span class="sd">    :math:`c_{av}[k] = \sum_{n}{a[n+k] * conj(v[n])}`</span>

<span class="sd">    with `a` and `v` sequences being zero-padded where necessary and conj being the conjugate.</span>

<span class="sd">    Note:</span>
<span class="sd">        - `correlate` is currently only used in `mindscience` scientific computing scenarios and</span>
<span class="sd">          dose not support other usage scenarios.</span>
<span class="sd">        - `correlate` is not supported on Windows platform yet.</span>

<span class="sd">    Args:</span>
<span class="sd">        a (Union[list, tuple, Tensor]): First input sequence.</span>
<span class="sd">        v (Union[list, tuple, Tensor]): Second input sequence.</span>
<span class="sd">        mode (str, optional): Specifies padding mode. The optional values are</span>
<span class="sd">            ``&quot;same&quot;`` , ``&quot;valid&quot;`` and ``&quot;full&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: it returns output of length :math:`max(M, N)`. Boundary</span>
<span class="sd">              effects are still visible.</span>

<span class="sd">            - ``&quot;valid&quot;``: it returns output of length :math:`max(M, N) - min(M, N) + 1`.</span>
<span class="sd">              The convolution product is only given for points where the signals overlap</span>
<span class="sd">              completely. Values outside the signal boundary have no effect.</span>

<span class="sd">            - ``&quot;full&quot;``: it returns the convolution at each point of overlap, with</span>
<span class="sd">              an output shape of :math:`(N + M - 1,)`.At the end-points of the convolution,</span>
<span class="sd">              the signals do not overlap completely, and boundary effects may be seen.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, Discrete cross-correlation of `a` and `v`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `a` or `v` is not a tensor.</span>
<span class="sd">        TypeError: If `a` and `v` is of different dtype.</span>
<span class="sd">        ValueError: If `a` and `v` are empty or have wrong dimensions</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.auto_generate import correlate</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; output = correlate(Tensor([1., 2., 3.]), Tensor([0., 1., 0.5]))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3.5]</span>
<span class="sd">        &gt;&gt;&gt; output = correlate(Tensor([1., 2., 3.]), Tensor([0., 1., 0.5]), mode=&quot;same&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.  3.5 3. ]</span>
<span class="sd">        &gt;&gt;&gt; output = correlate(Tensor([1., 2., 3., 4., 5.]), Tensor([1., 2.]), mode=&quot;full&quot;)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 2.  5.  8. 11. 14.  5.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">correlate_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Correlate</span><span class="p">)(</span><span class="n">mode</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">correlate_op</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>


<div class="viewcode-block" id="cos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cos.html#mindspore.ops.cos">[文档]</a><span class="k">def</span> <span class="nf">cos</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes cosine of input element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \cos(x_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Supported dtypes are float16 and float32, and using float64 may cause a problem of missing precision.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    :raise TypeError: If `input` is not a Tensor.</span>
<span class="sd">    :raise TypeError:</span>
<span class="sd">        * CPU/GPU: If dtype of `input` is not float16, float32 or float64, complex64, complex128.</span>
<span class="sd">        * Ascend: If dtype of `input` is not bool, int8, uint8, int16, int32, int64, float16, float32, float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cos(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.971338 0.6748758 0.95233357 0.9959527]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cos_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="cosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cosh.html#mindspore.ops.cosh">[文档]</a><span class="k">def</span> <span class="nf">cosh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic cosine of input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \cosh(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of hyperbolic cosine function, its data type</span>
<span class="sd">            must be float16, float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of `input` is not one of the following types:</span>
<span class="sd">                       float16, float32, float64, complex64, complex128.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.24, 0.83, 0.31, 0.09]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.0289385 1.364684 1.048436 1.0040528]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cosh(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        4.144313</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cosh_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="cummax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.cummax.html#mindspore.ops.cummax">[文档]</a><span class="k">def</span> <span class="nf">cummax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tuple (values,indices) where &#39;values&#39; is the cumulative maximum value of input Tensor `input`</span>
<span class="sd">    along the dimension `axis`, and `indices` is the index location of each maximum value.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        y_{i} = \max(x_{1}, x_{2}, ... , x_{i})</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor, rank of `input` &gt; 0.</span>
<span class="sd">        axis (int): The dimension to do the operation over. The value of `axis` must be in the range</span>
<span class="sd">            `[-input.ndim, input.ndim - 1]`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        tuple [Tensor], tuple of 2 Tensors, containing the cumulative maximum of elements and the index.</span>
<span class="sd">        The shape of each output tensor is the same as input `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is out the range of `[-input.ndim, input.ndim - 1]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.cummax(x, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [[ 3.  4.  6. 10.]</span>
<span class="sd">         [ 3.  6.  7. 10.]</span>
<span class="sd">         [ 4.  6.  8. 10.]</span>
<span class="sd">         [ 4.  6.  8. 10.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1])</span>
<span class="sd">        [[0 0 0 0]</span>
<span class="sd">         [0 1 1 0]</span>
<span class="sd">         [2 1 2 0]</span>
<span class="sd">         [2 1 2 0]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cummax_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Cummax</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cummax_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="diag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.diag.html#mindspore.ops.diag">[文档]</a><span class="k">def</span> <span class="nf">diag</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a diagonal tensor with a given diagonal values.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        ValueError: If rank of `input` less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([1, 2, 3, 4]).astype(&#39;int32&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.diag(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 0 0 0]</span>
<span class="sd">         [0 2 0 0]</span>
<span class="sd">         [0 0 3 0]</span>
<span class="sd">         [0 0 0 4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">diag_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="diagonal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.diagonal.html#mindspore.ops.diagonal">[文档]</a><span class="k">def</span> <span class="nf">diagonal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns specified diagonals of `input`.</span>

<span class="sd">    If `input` is 2-D, returns the diagonal of `input` with the given offset.</span>
<span class="sd">    If `input` has more than two</span>
<span class="sd">    dimensions, then the axes specified by `dim1` and `dim2` are used to determine</span>
<span class="sd">    the 2-D sub-array whose diagonal is returned. In this case, remove the `dim1` and `dim2` dimensions of `input`</span>
<span class="sd">    and insert the last dimension of `input` by the diagonal elements determined by `dim1` and `dim2`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Array from which the diagonals are taken.</span>
<span class="sd">        offset (int, optional): Offset of the diagonal from the main diagonal.</span>
<span class="sd">            Can be positive or negative. Default: ``0`` .</span>
<span class="sd">        dim1 (int, optional): Axis to be used as the first axis of the 2-D</span>
<span class="sd">            sub-arrays from which the diagonals should be taken. Defaults to</span>
<span class="sd">            first axis (0). Default: ``0`` .</span>
<span class="sd">        dim2 (int, optional): Axis to be used as the second axis of the 2-D</span>
<span class="sd">            sub-arrays from which the diagonals should be taken. Defaults to</span>
<span class="sd">            second axis (1). Default: ``1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, if `input` is 2-D, then `input` 1-D array containing the diagonal. If</span>
<span class="sd">        ``input.ndim &gt; 2``, then the dimensions specified by `dim1` and `dim2` are removed,</span>
<span class="sd">        and a new axis inserted at the end corresponding to the diagonal.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: if `dim1` or `dim2` are not an int.</span>
<span class="sd">        ValueError: if the input tensor has less than two dimensions.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[0, 1], [2, 3]], mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.diagonal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">diagonal_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Diagonal</span><span class="p">)(</span><span class="n">offset</span><span class="p">,</span> <span class="n">dim1</span><span class="p">,</span> <span class="n">dim2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">diagonal_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.elu.html#mindspore.ops.elu">[文档]</a><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exponential Linear Unit activation function.</span>

<span class="sd">    Applies the exponential linear unit function element-wise.</span>
<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ELU}(x)= \left\{</span>
<span class="sd">        \begin{array}{align}</span>
<span class="sd">            \alpha(e^{x}  - 1) &amp; \text{if } x \le 0\\</span>
<span class="sd">            x &amp; \text{if } x \gt 0\\</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Where :math:`x` is the element of input Tensor `input_x`, :math:`\alpha` is param `alpha`,</span>
<span class="sd">    it determines the smoothness of ELU.</span>
<span class="sd">    The picture about ELU looks like this `ELU &lt;https://en.wikipedia.org/wiki/</span>
<span class="sd">    Activation_function#/media/File:Activation_elu.svg&gt;`_ .</span>

<span class="sd">    ELU function graph:</span>

<span class="sd">    .. image:: ../images/ELU.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The input of ELU is a Tensor of any dimension with data type of float16 or float32.</span>
<span class="sd">        alpha (float, optional): The alpha value of ELU, the data type is float. Only support &#39;1.0&#39; currently.</span>
<span class="sd">            Default: ``1.0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `alpha` is not a float.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `alpha` is not equal to 1.0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.elu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.63212055  4.         -0.99966455]</span>
<span class="sd">         [ 2.         -0.99326205  9.        ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">elu_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Elu</span><span class="p">)(</span><span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">elu_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<div class="viewcode-block" id="equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.equal.html#mindspore.ops.equal">[文档]</a><span class="k">def</span> <span class="nf">equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the equivalence between two tensors element-wise.</span>

<span class="sd">    The second argument can be a number or a tensor whose shape is broadcastable with the first argument and vise versa.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } input_{i} = other_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } input_{i} \ne other_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    Note:</span>
<span class="sd">        - `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">        - The shapes of the inputs can be broadcasted to each other.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number]): The first input is a number or</span>
<span class="sd">            a tensor whose data type is number.</span>
<span class="sd">        other (Union[Tensor, Number]): The second input is a number or</span>
<span class="sd">            a tensor whose data type is number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor or number.Number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: The shape of two inputs are different</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([1, 2, 3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.equal(input, 2.0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: The shape of two inputs are the same</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([1, 2, 3], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor([1, 2, 4], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.equal(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">equal_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="erf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.erf.html#mindspore.ops.erf">[文档]</a><span class="k">def</span> <span class="nf">erf</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Gauss error function of `input` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        erf(x)=\frac{2} {\sqrt{\pi}} \int\limits_0^{x} e^{-t^{2}} dt</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of Gaussian error function. Its data type</span>
<span class="sd">            must be float16 float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erf(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.8427168   0.          0.8427168   0.99530876  0.99997765]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">erf_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="erfc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.erfc.html#mindspore.ops.erfc">[文档]</a><span class="k">def</span> <span class="nf">erfc</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the complementary error function of `input` element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        erfc(x) = 1 - \frac{2} {\sqrt{\pi}} \int\limits_0^{x} e^{-t^{2}} dt</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of the complementary error function, :math:`x` in the above formula. </span>
<span class="sd">            Supported dtypes:</span>

<span class="sd">            - Ascend: float16, float32.</span>
<span class="sd">            - GPU/CPU: float16, float32, float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-1, 0, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erfc(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.8427168e+00 1.0000000e+00 1.5728319e-01 4.6912432e-03 2.2351742e-05]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">erfc_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="erfinv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.erfinv.html#mindspore.ops.erfinv">[文档]</a><span class="k">def</span> <span class="nf">erfinv</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the result of the inverse error function with `input`, which is defined in the range `(-1, 1)` as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        erfinv(erf(x)) = x</span>

<span class="sd">    where :math:`x` is the `input`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute with, with data type float16, float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0, 0.5, -0.9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.erfinv(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.          0.47695306 -1.1630805 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">erfinv_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="exp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.exp.html#mindspore.ops.exp">[文档]</a><span class="k">def</span> <span class="nf">exp</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential of a tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = e^{x_i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.0, 1.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.exp(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.        2.718282 20.085537]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">exp_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="expand_dims"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.expand_dims.html#mindspore.ops.expand_dims">[文档]</a><span class="k">def</span> <span class="nf">expand_dims</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds an additional dimension to `input_x` at the given axis, the dimension</span>
<span class="sd">    of `input_x` should be greater than or equal to 1.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the specified axis is a negative number, the index is counted</span>
<span class="sd">        backward from the end and starts at 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        axis (int): Specifies the dimension index at which to expand</span>
<span class="sd">            the shape of `input_x`. The value of axis must be in the range</span>
<span class="sd">            `[-input_x.ndim-1, input_x.ndim]`. Only constant value is allowed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(1, x_1, x_2, ..., x_R)` if the</span>
<span class="sd">        value of `axis` is 0. It has the same data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is not in the valid range :math:`[-a.ndim-1, a.ndim]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[2, 2], [2, 2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.expand_dims(input_tensor, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2.]</span>
<span class="sd">          [2. 2.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">expand_dims_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="expm1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.expm1.html#mindspore.ops.expm1">[文档]</a><span class="k">def</span> <span class="nf">expm1</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns exponential then minus 1 of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">      out_i = e^{x_i} - 1</span>

<span class="sd">    Args:</span>
<span class="sd">      input (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">      TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">      ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">      &gt;&gt;&gt; import mindspore</span>
<span class="sd">      &gt;&gt;&gt; import numpy as np</span>
<span class="sd">      &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">      &gt;&gt;&gt; x = Tensor(np.array([0.0, 1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">      &gt;&gt;&gt; output = ops.expm1(x)</span>
<span class="sd">      &gt;&gt;&gt; print(output)</span>
<span class="sd">      [ 0.        1.718282  6.389056 53.598152]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">expm1_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="fast_gelu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fast_gelu.html#mindspore.ops.fast_gelu">[文档]</a><span class="k">def</span> <span class="nf">fast_gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fast Gaussian Error Linear Units activation function.</span>

<span class="sd">    FastGeLU is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output} = \frac {x} {1 + \exp(-1.702 * \left| x \right|)} * \exp(0.851 * (x - \left| x \right|)),</span>

<span class="sd">    where :math:`x` is the element of the input.</span>

<span class="sd">    FastGelu function graph:</span>

<span class="sd">    .. image:: ../images/FastGelu.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input to compute the FastGeLU with data type of float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.fast_gelu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.5418735e-01  3.9921875e+00 -9.7473649e-06]</span>
<span class="sd">        [ 1.9375000e+00 -1.0052517e-03  8.9824219e+00]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fast_gelu_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">fft</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the one dimensional discrete Fourier transform of `input`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - `fft` is currently only used in `mindscience` scientific computing scenarios and</span>
<span class="sd">          dose not support other usage scenarios.</span>
<span class="sd">        - `fft` is not supported on Windows platform yet.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">        n (int, optional): Length of the transformed `dim` of the result.</span>
<span class="sd">            If given, the input will either be zero-padded or trimmed to this length before computing `fft`.</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">        dim (int, optional): The dimension along which to take the one dimensional `fft`.</span>
<span class="sd">            Default: ``-1``, which means transform the last dimension of `input`.</span>
<span class="sd">        norm (string, optional): Normalization mode. Default: ``None`` that means ``&quot;backward&quot;``.</span>
<span class="sd">            Three modes are defined as,</span>

<span class="sd">            - ``&quot;backward&quot;``(no normalization).</span>
<span class="sd">            - ``&quot;forward&quot;`` (normalize by :math:`1/n`).</span>
<span class="sd">            - ``&quot;ortho&quot;`` (normalize by :math:`1/\sqrt{n}`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, The result of `fft()` function.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the `input` type is not Tensor.</span>
<span class="sd">        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.</span>
<span class="sd">        TypeError: If `n` or `dim` type is not int.</span>
<span class="sd">        ValueError: If `dim` is not in the range of &quot;[ `-input.ndim` , `input.ndim` )&quot;.</span>
<span class="sd">        ValueError: If `n` is less than 1.</span>
<span class="sd">        ValueError: If `norm` is none of ``&quot;backward&quot;`` , ``&quot;forward&quot;`` or ``&quot;ortho&quot;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([ 1.6243454+0.j, -0.6117564+0.j, -0.5281718+0.j, -1.0729686+0.j])</span>
<span class="sd">        &gt;&gt;&gt; y = ops.fft(input)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [-0.5885514+0.j          2.1525172-0.46121222j  2.7808986+0.j</span>
<span class="sd">        2.1525172+0.46121222j]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fft_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>


<div class="viewcode-block" id="fftshift"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.fftshift.html#mindspore.ops.fftshift">[文档]</a><span class="k">def</span> <span class="nf">fftshift</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shift the zero-frequency component to the center of the spectrum.</span>

<span class="sd">    Note:</span>
<span class="sd">        - `fftshift` is currently only used in `mindscience` scientific computing scenarios and</span>
<span class="sd">          dose not support other usage scenarios.</span>
<span class="sd">        - `fftshift` is not supported on Windows platform yet.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor.</span>
<span class="sd">        dim (Union[int, list(int), tuple(int)], optional): The dimensions which to shift.</span>
<span class="sd">            Default is ``None``, which shifts all dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor), the shifted tensor with the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a tensor.</span>
<span class="sd">        TypeError: If the type/dtype of `dim` is not int.</span>
<span class="sd">        ValueError: If `dim` is out of the range of `[-input.ndim, input.ndim)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import fftshift</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([0, 1, 2, 3, 4, -5, -4, -3, -2, -1], dtype=mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; fftshift(input)</span>
<span class="sd">        Tensor(shape=[10], dtype=Int32, value= [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">fftshift_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor_divide"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.floor_divide.html#mindspore.ops.floor_divide">[文档]</a><span class="k">def</span> <span class="nf">floor_divide</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor element-wise and round down to the closest integer.</span>

<span class="sd">    Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = \\text{floor}( \\frac{x_i}{y_i})</span>

<span class="sd">    where the :math:`floor` indicates the Floor operator, for more details,</span>
<span class="sd">    please refer to the :class:`mindspore.ops.Floor` operator.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        other (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` nor `other` is a Tensor, number.Number or bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor_divide(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0  1 -1]</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(2.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(2.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor_divide(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">floor_div_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor_mod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.floor_mod.html#mindspore.ops.floor_mod">[文档]</a><span class="k">def</span> <span class="nf">floor_mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the remainder of division element-wise. It&#39;s a flooring divide.</span>
<span class="sd">    E.g. :math:`floor(x / y) * y + mod(x, y) = x`.</span>

<span class="sd">    Inputs of `x` and `y` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    When the inputs are two tensors,</span>
<span class="sd">    dtypes of them cannot be both bool, and the shapes of them could be broadcast.</span>
<span class="sd">    When the inputs are one tensor and one scalar,</span>
<span class="sd">    the scalar could only be a constant.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =\text{floor}(x_{i} // y_{i})</span>

<span class="sd">    where the :math:`floor` indicates the Floor operator, for more details,</span>
<span class="sd">    please refer to the :class:`mindspore.ops.Floor` operator.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - Data of input `y` should not be 0, or the maximum value of its dtype will be returned.</span>
<span class="sd">        - When the elements of input exceeds 2048 , the accuracy of operator cannot guarantee the requirement of</span>
<span class="sd">          double thousandths in the mini form.</span>
<span class="sd">        - Due to different architectures, the calculation results of this operator on NPU and CPU may be inconsistent.</span>
<span class="sd">        - If shape is expressed as :math:`(D1, D2 ..., Dn)`, then D1\*D2... \*DN&lt;=1000000,n&lt;=8.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        y (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision of the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `x` nor `y` is a Tensor or number.Number.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([2, 4, -1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3, 3, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor_mod(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">floor_mod_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="floor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.floor.html#mindspore.ops.floor">[文档]</a><span class="k">def</span> <span class="nf">floor</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rounds a tensor down to the closest integer element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i = \lfloor x_i \rfloor</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor, its data type must be float16,</span>
<span class="sd">            float32 or float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not in [float16, float32, float64].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.1, 2.5, -1.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.floor(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  2. -2.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">floor_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather_d"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather_d.html#mindspore.ops.gather_d">[文档]</a><span class="k">def</span> <span class="nf">gather_d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers elements along an axis specified by dim.</span>

<span class="sd">    Refer to :func:`mindspore.ops.gather_elements` for more detail.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2], [3, 4]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; index = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dim = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather_d(x, dim, index)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">         [4 3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gather_d_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather_nd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather_nd.html#mindspore.ops.gather_nd">[文档]</a><span class="k">def</span> <span class="nf">gather_nd</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gathers slices from a tensor by indices.</span>

<span class="sd">    Using given indices to gather slices from a tensor with a specified shape.</span>

<span class="sd">    `indices` is an K-dimensional integer tensor.</span>
<span class="sd">    Supposes it as a (K-1)-dimensional tensor and each element of it defines a slice of `input_x`:</span>

<span class="sd">    .. math::</span>

<span class="sd">        output[(i_0, ..., i_{K-2})] = input\_x[indices[(i_0, ..., i_{K-2})]]</span>

<span class="sd">    The last dimension of `indices` can not more than the rank of `input_x`:</span>

<span class="sd">    :math:`indices.shape[-1] &lt;= input\_x.rank`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The target tensor to gather values.</span>
<span class="sd">        indices (Tensor): The index tensor, with int32 or int64 data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as `input_x` and the shape is</span>
<span class="sd">        :math:`indices\_shape[:-1] + input\_x\_shape[indices\_shape[-1]:]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If length of shape of `input_x` is less than the last dimension of `indices`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 0], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather_nd(input_x, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.1  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gather_nd_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span></div>


<div class="viewcode-block" id="gather"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gather.html#mindspore.ops.gather">[文档]</a><span class="k">def</span> <span class="nf">gather</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the slice of the input tensor corresponding to the elements of `input_indices` on the specified `axis`.</span>

<span class="sd">    The following figure shows the calculation process of Gather commonly:</span>

<span class="sd">    .. image:: Gather.png</span>

<span class="sd">    where params represents the input `input_params`, and indices represents the index to be sliced `input_indices`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        1. The value of input_indices must be in the range of `[0, input_param.shape[axis])`.</span>
<span class="sd">           On CPU and GPU, an error is raised if an out of bound indice is found. On Ascend, the results may be</span>
<span class="sd">           undefined.</span>
<span class="sd">        2. The data type of input_params cannot be</span>
<span class="sd">           `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ on Ascend</span>
<span class="sd">           platform currently.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_params (Tensor): The original Tensor. The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        input_indices (Tensor): Index tensor to be sliced, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>
<span class="sd">            Specifies the indices of elements of the original Tensor. The data type can be int32 or int64.</span>
<span class="sd">        axis (Union(int, Tensor[int])): Specifies the dimension index to gather indices.</span>
<span class="sd">            It must be greater than or equal to `batch_dims`.</span>
<span class="sd">            When `axis` is a Tensor, the size must be 1.</span>
<span class="sd">        batch_dims (int): Specifies the number of batch dimensions. It must be less than or euqal to the rank</span>
<span class="sd">            of `input_indices`. Default: ``0`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is</span>
<span class="sd">        :math:`input\_params.shape[:axis] + input\_indices.shape[batch\_dims:] + input\_params.shape[axis + 1:]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError:  If `axis` is not an int or Tensor.</span>
<span class="sd">        ValueError: If `axis` is a Tensor and its size is not 1.</span>
<span class="sd">        TypeError:  If `input_params` is not a tensor.</span>
<span class="sd">        TypeError:  If `input_indices` is not a tensor of type int.</span>
<span class="sd">        RuntimeError: If `input_indices` is out of range `[0, input_param.shape[axis])` on CPU or GPU.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case1: input_indices is a Tensor with shape (5, ).</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([1, 2, 3, 4, 5, 6, 7]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2, 4, 2, 6]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 3. 5. 3. 7.]</span>
<span class="sd">        &gt;&gt;&gt; # case2: input_indices is a Tensor with shape (2, 2). When the input_params has one dimension,</span>
<span class="sd">        &gt;&gt;&gt; # the output shape is equal to the input_indices shape.</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([[0, 2], [2, 6]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 3.]</span>
<span class="sd">         [3. 7.]]</span>
<span class="sd">        &gt;&gt;&gt; # case3: input_indices is a Tensor with shape (2, ) and</span>
<span class="sd">        &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 0.</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 1.  2.  3.  4.]</span>
<span class="sd">         [ 9. 10. 11. 12.]]</span>
<span class="sd">        &gt;&gt;&gt; # case4: input_indices is a Tensor with shape (2, ) and</span>
<span class="sd">        &gt;&gt;&gt; # input_params is a Tensor with shape (3, 4) and axis is 1, batch_dims is 1.</span>
<span class="sd">        &gt;&gt;&gt; input_params = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_indices = Tensor(np.array([0, 2, 1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 1</span>
<span class="sd">        &gt;&gt;&gt; batch_dims = 1</span>
<span class="sd">        &gt;&gt;&gt; output = ops.gather(input_params, input_indices, axis, batch_dims)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  7. 10.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gather_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Gather</span><span class="p">)(</span><span class="n">batch_dims</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gather_op</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="gcd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.gcd.html#mindspore.ops.gcd">[文档]</a><span class="k">def</span> <span class="nf">gcd</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes greatest common divisor of input tensors element-wise.</span>
<span class="sd">    The shape of two inputs should be broadcastable, and data type of them should be</span>
<span class="sd">    one of: int32, int64.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor.</span>
<span class="sd">        other (Tensor): The second input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is one</span>
<span class="sd">        with higher precision in the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type `input` or `other` is not int32 or int64.</span>
<span class="sd">        ValueError: If shape of two inputs are not broadcastable.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([7, 8, 9]))</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([14, 6, 12]))</span>
<span class="sd">        &gt;&gt;&gt; y = ops.gcd(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [7 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">gcd_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="geqrf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.geqrf.html#mindspore.ops.geqrf">[文档]</a><span class="k">def</span> <span class="nf">geqrf</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Decomposes a matrix into the product of an orthogonal matrix `Q` and an upper triangular matrix `R`.</span>
<span class="sd">    The process is called QR decomposition: :math:`A = QR`.</span>

<span class="sd">    Both `Q` and `R` matrices are stored in the same output tensor `y`.</span>
<span class="sd">    The elements of `R` are stored on and above the diagonal, whereas elementary reflectors</span>
<span class="sd">    (or Householder vectors) implicitly defining matrix `Q` are stored below the diagonal.</span>

<span class="sd">    This function returns two tensors (`y`, `tau`).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Tensor of shape :math:`(*, m, n)`, input must be a matrix greater than or equal to 2D,</span>
<span class="sd">            with dtype of float32, float64, complex64, complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **y** (Tensor) - Tensor of shape :math:`(*, m, n)`, has the same dtype as the `input`.</span>
<span class="sd">        - **tau** (Tensor) - Tensor of shape :math:`(*, p)` and :math:`p = min(m, n)`, has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` is not float32, float64, complex64 or complex128.</span>
<span class="sd">        ValueError: If dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[-2.0, -1.0], [1.0, 2.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; y, tau = ops.geqrf(input)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 2.236068   1.7888544]</span>
<span class="sd">        [-0.236068   1.3416407]]</span>
<span class="sd">        &gt;&gt;&gt; print(tau)</span>
<span class="sd">        [1.8944271 0.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">geqrf_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="greater_equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.greater_equal.html#mindspore.ops.greater_equal">[文档]</a><span class="k">def</span> <span class="nf">greater_equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given two Tensors, compares them element-wise to check if each element in the first</span>
<span class="sd">    Tensor is greater than or equal to the corresponding element in the second Tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ge` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        other (Union[Tensor, Number]): The second input is a number</span>
<span class="sd">            or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; greater_equal = ops.greater_equal()</span>
<span class="sd">        &gt;&gt;&gt; output = greater_equal(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">greater_equal_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="greater"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.greater.html#mindspore.ops.greater">[文档]</a><span class="k">def</span> <span class="nf">greater</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare the value of the input parameters :math:`input &gt; other` element-wise, and the output result is a bool value.</span>

<span class="sd">    Refer to :func:`mindspore.ops.gt` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number]): The first input is a Number or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ .</span>
<span class="sd">        other (Union[Tensor, Number]): The second input, which is a Number or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; greater = ops.greater()</span>
<span class="sd">        &gt;&gt;&gt; output = greater(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">greater_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="deepcopy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.deepcopy.html#mindspore.ops.deepcopy">[文档]</a><span class="k">def</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">input_x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a deepcopy of input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a deepcopy of `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([[0, 1], [2, 1]], dtype=mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.deepcopy(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 1]</span>
<span class="sd">        [2 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">identity_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">ifft</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the inverse of `fft()`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - `ifft` is currently only used in `mindscience` scientific computing scenarios and</span>
<span class="sd">          dose not support other usage scenarios.</span>
<span class="sd">        - `ifft` is not supported on Windows platform yet.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>
<span class="sd">        n (int, optional): Length of the transformed `dim` of the result.</span>
<span class="sd">            If given, the input will either be zero-padded or trimmed to this length before computing the `ifft`.</span>
<span class="sd">            Default: ``None``.</span>
<span class="sd">        dim (int, optional): The dimension along which to take the one dimensional `ifft`.</span>
<span class="sd">            Default: ``-1``, which means transform the last dimension of `input`.</span>
<span class="sd">        norm (string, optional): Normalization mode. Default: ``None`` that means ``&quot;backward&quot;``.</span>
<span class="sd">            Three modes are defined as,</span>

<span class="sd">            - ``&quot;backward&quot;``(no normalization).</span>
<span class="sd">            - ``&quot;forward&quot;`` (normalize by :math:`1/n`).</span>
<span class="sd">            - ``&quot;ortho&quot;`` (normalize by :math:`1/\sqrt{n}`).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, The result of `ifft()` function.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the `input` type is not Tensor.</span>
<span class="sd">        TypeError: If the `input` data type is not one of: int32, int64, float32, float64, complex64, complex128.</span>
<span class="sd">        TypeError: If `n` or `dim` type is not int.</span>
<span class="sd">        ValueError: If `dim` is not in the range of &quot;[ `-input.ndim` , `input.ndim` )&quot;.</span>
<span class="sd">        ValueError: If `n` is less than 1.</span>
<span class="sd">        ValueError: If `norm` is none of ``&quot;backward&quot;`` , ``&quot;forward&quot;`` or ``&quot;ortho&quot;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([ 1.6243454+0.j, -0.6117564+0.j, -0.5281718+0.j, -1.0729686+0.j])</span>
<span class="sd">        &gt;&gt;&gt; y = ops.ifft(input)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [-0.14713785+0.j          0.5381293 +0.11530305j  0.69522465+0.j</span>
<span class="sd">        0.5381293 -0.11530305j]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ifft_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>


<div class="viewcode-block" id="ifftshift"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ifftshift.html#mindspore.ops.ifftshift">[文档]</a><span class="k">def</span> <span class="nf">ifftshift</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The inverse of :func:`mindspore.ops.fftshift` .</span>

<span class="sd">    Note:</span>
<span class="sd">        - `ifftshift` is currently only used in `mindscience` scientific computing scenarios and</span>
<span class="sd">          dose not support other usage scenarios.</span>
<span class="sd">        - `ifftshift` is not supported on Windows platform yet.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input tensor.</span>
<span class="sd">        dim (Union[int, list(int), tuple(int)], optional): The dimensions which to shift.</span>
<span class="sd">            Default is ``None``, which shifts all dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor), the shifted tensor with the same shape and dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a tensor.</span>
<span class="sd">        TypeError: If the type/dtype of `dim` is not int.</span>
<span class="sd">        ValueError: If `dim` is out of the range of `[-input.ndim, input.ndim)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import fftshift, ifftshift</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor([0, 1, 2, 3, 4, -5, -4, -3, -2, -1], dtype=mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; ifftshift(fftshift(input))</span>
<span class="sd">        Tensor(shape=[10], dtype=Int32, value= [ 0, 1, 2, 3, 4, -5, -4, -3, -2, -1])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">ifftshift_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">is_finite</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine which elements are finite for each position.</span>

<span class="sd">    Args:</span>
<span class="sd">      - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape of input, and the dtype is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If x is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; is_finite = ops.IsFinite()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([np.log(-1), 1, np.log(0)]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = is_finite(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True False]</span>
<span class="sd">        </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">is_finite_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<div class="viewcode-block" id="less_equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.less_equal.html#mindspore.ops.less_equal">[文档]</a><span class="k">def</span> <span class="nf">less_equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`input &lt;= other` element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_{i} =\begin{cases}</span>
<span class="sd">            &amp; \text{True,    if } input_{i}&lt;=other_{i} \\</span>
<span class="sd">            &amp; \text{False,   if } input_{i}&gt;other_{i}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number, bool]): The first input is a Number or a bool or a tensor whose data type is </span>
<span class="sd">            number or bool_.</span>
<span class="sd">        other (Union[Tensor, Number, bool]): The second input is a Number or a bool or a tensor whose data type is </span>
<span class="sd">            number or bool_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting, and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.less_equal(x, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False  True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">less_equal_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="less"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.less.html#mindspore.ops.less">[文档]</a><span class="k">def</span> <span class="nf">less</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the boolean value of :math:`input &lt; other` element-wise.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        other (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, Number, bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([1, 1, 4]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.less(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">less_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="log1p"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log1p.html#mindspore.ops.log1p">[文档]</a><span class="k">def</span> <span class="nf">log1p</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of one plus the input tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = {log_e}(input_i + 1)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor. The value must be greater than -1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log1p(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6931472 1.0986123 1.609438 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">log1p_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="log"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log.html#mindspore.ops.log">[文档]</a><span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the natural logarithm of a tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = \log_e(x_i)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        If the input value of operator Log is within the range (0, 0.01] or [0.95, 1.05], the output accuracy may</span>
<span class="sd">        be affacted.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): Input Tensor of any dimension. The value must be greater than 0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.        0.6931472 1.3862944]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">log_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.log_softmax.html#mindspore.ops.log_softmax">[文档]</a><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Log Softmax function to the input tensor on the specified axis.</span>
<span class="sd">    Supposes a slice in the given axis, :math:`x` for each element :math:`x_i`,</span>
<span class="sd">    the Log Softmax function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(x_i) = \log \left(\frac{\exp(x_i)} {\sum_{j = 0}^{N-1}\exp(x_j)}\right),</span>

<span class="sd">    where :math:`N` is the length of the Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        logits (Tensor): The input Tensor, which is the :math:`x` in the formula above, it&#39;s shape is :math:`(N, *)`, </span>
<span class="sd">            where :math:`*` means, any number of additional dimensions, with float16 or float32 data type.</span>
<span class="sd">        axis (int): The axis to perform the Log softmax operation. Default: ``-1`` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the logits.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `axis` is not in range [-len(logits.shape), len(logits.shape)).</span>
<span class="sd">        ValueError: If dimension of `logits` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.log_softmax(logits)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-4.4519143 -3.4519143 -2.4519143 -1.4519144 -0.4519144]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">log_softmax_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">LogSoftmax</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">log_softmax_op</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span></div>


<div class="viewcode-block" id="logit"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.logit.html#mindspore.ops.logit">[文档]</a><span class="k">def</span> <span class="nf">logit</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">eps</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the logit of a tensor element-wise. </span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{align}</span>
<span class="sd">        y_{i} &amp; = \ln(\frac{z_{i}}{1 - z_{i}}) \\</span>
<span class="sd">        z_{i} &amp; = \begin{cases}</span>
<span class="sd">        input_{i} &amp; \text{if eps is None} \\</span>
<span class="sd">        \text{eps} &amp; \text{if } input_{i} \lt \text{eps} \\</span>
<span class="sd">        input_{i} &amp; \text{if } \text{eps} \leq input_{i} \leq 1 - \text{eps} \\</span>
<span class="sd">        1 - \text{eps} &amp; \text{if } input_{i} \gt 1 - \text{eps}</span>
<span class="sd">        \end{cases}</span>
<span class="sd">        \end{align}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of type float16, float32 or float64.</span>
<span class="sd">        eps (float, optional): The epsilon, the input clamp bound is defined as [eps, 1-eps]. Default: ``-1.0``.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.1, 0.2, 0.3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.logit(x, eps=1e-5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.1972246 -1.3862944 -0.8472978]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logit_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Logit</span><span class="p">)(</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logit_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="masked_fill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.masked_fill.html#mindspore.ops.masked_fill">[文档]</a><span class="k">def</span> <span class="nf">masked_fill</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fills elements of Tensor with value where mask is True.</span>
<span class="sd">    The shapes of `input_x` and `mask` need to be the same or broadcastable.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): The source Tensor whose data type is one of bool, uint8, int8, int16, int32,</span>
<span class="sd">            int64, float16, float32, float64, complex64, complex128.</span>
<span class="sd">        mask (Tensor[bool]): The boolean mask.</span>
<span class="sd">        value (Union[Number, Tensor]): The value to fill in with, which dtype is the same as `input_x`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type and shape as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `mask` is not bool.</span>
<span class="sd">        TypeError: If `input_x` or `mask` is not a Tensor.</span>
<span class="sd">        ValueError: If the shapes of `input_x` and `mask` could not be broadcast.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `value` is not one of bool, uint8, int8, int16, int32,</span>
<span class="sd">            int64, float16, float32, float64, complex64, complex128.</span>
<span class="sd">        TypeError: If dtype of `value` is different from that of `input_x`.</span>
<span class="sd">        TypeError: If `value` is neither float number nor Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1., 2., 3., 4.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mask = Tensor(np.array([True, True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.masked_fill(input_x, mask, 0.5)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5 0.5 3.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">masked_fill_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">matrix_exp</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the exponential of a single or a batch of square matrices.</span>

<span class="sd">    .. math::</span>

<span class="sd">        matrix\_exp(x) = \sum_{k=0}^{\infty} \frac{1}{k !} x^{k} \in \mathbb{K}^{n \times n}</span>

<span class="sd">    where :math:`x` corresponds to `input` .</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(*, n, n)` where * is zero or more batch dimensions.</span>
<span class="sd">            Must be one of the following types: float16, float32, float64, complex64, complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` is not one of the following dtype:</span>
<span class="sd">                float16, float32, float64, complex64, complex128.</span>
<span class="sd">        ValueError: If the rank of `input` is less than 2.</span>
<span class="sd">        ValueError: If the size of last two dimensions of `input` are not equal.</span>

<span class="sd">    Supported Platforms:</span>


<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[1, 2], [0, 1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.matrix_exp(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2.7182817 5.436563 ]</span>
<span class="sd">        [0.        2.7182817]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">matrix_exp_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<div class="viewcode-block" id="maximum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.maximum.html#mindspore.ops.maximum">[文档]</a><span class="k">def</span> <span class="nf">maximum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the maximum of input tensors element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>
<span class="sd">        - The inputs must be two tensors or one tensor and one scalar.</span>
<span class="sd">        - When the inputs are two tensors,</span>
<span class="sd">          dtypes of them cannot be bool at the same time, and the shapes of them could be broadcast.</span>
<span class="sd">        - When the inputs are one tensor and one scalar,</span>
<span class="sd">          the scalar could only be a constant.</span>
<span class="sd">        - Broadcasting is supported.</span>
<span class="sd">        - If one of the elements being compared is a NaN, then that element is returned.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_i = \max(input_i, other_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        other (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        ValueError: If `input` and `other` are not the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.maximum(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [4. 5. 6.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.maximum(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">maximum_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="minimum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.minimum.html#mindspore.ops.minimum">[文档]</a><span class="k">def</span> <span class="nf">minimum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the minimum of input tensors element-wise.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Inputs of `input` and `other` comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>
<span class="sd">        - When the inputs are two tensors, dtypes of them cannot be bool at the same time.</span>
<span class="sd">        - When the inputs are one tensor and one scalar, the scalar could only be a constant.</span>
<span class="sd">        - Shapes of them are supposed to be broadcast.</span>
<span class="sd">        - If one of the elements being compared is a NaN, then that element is returned.</span>

<span class="sd">    .. math::</span>
<span class="sd">        output_i = \min(input_i, other_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number, bool]): The first input is a number or</span>
<span class="sd">            a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        other (Union[Tensor, Number, bool]): The second input is a number or</span>
<span class="sd">            a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, Number, bool.</span>
<span class="sd">        ValueError: If `input` and `other` are not the same shape after broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 : same data type</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.minimum(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 5.0, 3.0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; other = Tensor(np.array([4.0, 2.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.minimum(input, other)</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">minimum_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.mul.html#mindspore.ops.mul">[文档]</a><span class="k">def</span> <span class="nf">mul</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multiplies two tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = input_{i} * other_{i}</span>

<span class="sd">    Note:</span>
<span class="sd">        - One of the two inputs must be a Tensor, when the two inputs have different shapes,</span>
<span class="sd">          they must be able to broadcast to a common shape.</span>
<span class="sd">        - The two inputs can not be bool type at the same time,</span>
<span class="sd">          [True, Tensor(True, bool\_), Tensor(np.array([True]), bool\_)] are all considered bool type.</span>
<span class="sd">        - The two inputs comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, number.Number, bool]): The first input is a number.Number or</span>
<span class="sd">            a bool or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        other (Union[Tensor, number.Number, bool]): The second input, when the first input is a Tensor,</span>
<span class="sd">            the second input should be a number.Number or bool value, or a Tensor whose data type is number or bool.</span>
<span class="sd">            When the first input is Scalar, the second input must be a Tensor whose data type is number or bool.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` and `other` is not one of the following: Tensor, number.Number, bool.</span>
<span class="sd">        ValueError: If `input` and `other` are not the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.mul(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 4. 10. 18.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">mul_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="neg"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.neg.html#mindspore.ops.neg">[文档]</a><span class="k">def</span> <span class="nf">neg</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a tensor with negative values of the input tensor element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_{i} = - input_{i}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor with a dtype of Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, -1, 2, 0, -3.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.neg(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.  -2.   1.  -2.   0.   3.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">neg_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="nextafter"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.nextafter.html#mindspore.ops.nextafter">[文档]</a><span class="k">def</span> <span class="nf">nextafter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the next representable floating-point value after `input` towards `other` element-wise.</span>

<span class="sd">    Say there are two float32 numbers :math:`a`, :math:`b`, and let the</span>
<span class="sd">    representable delta of float32 datatype is :math:`eps`. If :math:`a &lt; b`,</span>
<span class="sd">    then the next representable of :math:`a` towards :math:`b` is :math:`a+eps`,</span>
<span class="sd">    the next representable of :math:`b` towards :math:`a` is :math:`b-eps`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  nextafter({input_{i}, other_{i}})</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The first input tensor. The shape of tensor is :math:`(N,*)` where :math:`*` means,</span>
<span class="sd">            any number of additional dimensions. Must be one of the following types: float32, float64.</span>

<span class="sd">        other (Tensor): The second input tensor. The shape of tensor is :math:`(N,*)` where :math:`*` means,</span>
<span class="sd">            any number of additional dimensions. Must be one of the following types: float32, float64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and data type as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `input` nor `other` is a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input` and `other` is not one of: float32, float64.</span>
<span class="sd">        TypeError: If the dtypes of `input` and `other` are not same.</span>
<span class="sd">        ValueError: If `input`&#39;s shape is not the same as `other`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_ = Tensor(np.asarray([0.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; other_ = Tensor(np.asarray([0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_ = ops.nextafter(input_, other_)</span>
<span class="sd">        &gt;&gt;&gt; print(output_)</span>
<span class="sd">        [1.e-45]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">next_after_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="nonzero"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.nonzero.html#mindspore.ops.nonzero">[文档]</a><span class="k">def</span> <span class="nf">nonzero</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return a Tensor of the positions of all non-zero values.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor, its rank should be greater than or eaqual to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, a 2-D Tensor whose data type is int64, containing the positions of all non-zero values of the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not Tensor.</span>
<span class="sd">        ValueError: If dim of `input` equals to 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1,  0], [-5, 0]]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nonzero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0 0 0]</span>
<span class="sd">         [0 1 0]]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 0, 2, 0, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.nonzero(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0]</span>
<span class="sd">         [2]</span>
<span class="sd">         [4]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">non_zero_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="not_equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.not_equal.html#mindspore.ops.not_equal">[文档]</a><span class="k">def</span> <span class="nf">not_equal</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Alias for :func:`mindspore.ops.ne` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">not_equal_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<div class="viewcode-block" id="pow"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.pow.html#mindspore.ops.pow">[文档]</a><span class="k">def</span> <span class="nf">pow</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">exponent</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the `exponent` power of each element in `input`.</span>

<span class="sd">    When `exponent` is a Tensor, the shapes of `input` and `exponent` must be broadcastable.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Union[Tensor, Number]): The first input is a Number or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        exponent (Union[Tensor, Number]): The second input is a Number or a tensor whose data type is</span>
<span class="sd">            `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">            `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; exponent = 3.0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pow(input, exponent)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  8. 64.]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; exponent = Tensor(np.array([2.0, 4.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.pow(input, exponent)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1. 16. 64.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">pow_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">exponent</span><span class="p">)</span></div>


<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.prelu.html#mindspore.ops.prelu">[文档]</a><span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parametric Rectified Linear Unit activation function.</span>

<span class="sd">    PReLU is described in the paper `Delving Deep into Rectifiers: Surpassing Human-Level Performance on</span>
<span class="sd">    ImageNet Classification &lt;https://arxiv.org/abs/1502.01852&gt;`_. Defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        prelu(x_i)= \max(0, x_i) + \min(0, w * x_i),</span>

<span class="sd">    where :math:`x_i` is an element of a channel of the input, `w` is the weight of the channel.</span>

<span class="sd">    Note:</span>
<span class="sd">        Scalar or 1-D Tensor is not supported on Ascend.</span>

<span class="sd">    PReLU Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/PReLU.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input Tensor of the activation function. The data type is float16 or float32.</span>
<span class="sd">            The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        weight (Tensor):  Weight Tensor. The data type is float16 or float32.</span>
<span class="sd">            The weight can only be a Tensor, and the length is the same as the number of channels C of the `input_x`.</span>
<span class="sd">            On GPU devices, when the input is a scalar, the shape is :math:`(1,)` .</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same shape and dtype as `x`.</span>
<span class="sd">        For detailed information, please refer to :class:`mindspore.nn.PReLU`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` or `weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If the `x` or the `weight` is not a Tensor.</span>
<span class="sd">        ValueError: If the `x` is a 0-D or 1-D Tensor on Ascend.</span>
<span class="sd">        ValueError: If the `weight` is not a 1-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(-6, 6).reshape((2, 3, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.1, 0.6, -0.3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.prelu(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[-0.60 -0.50]</span>
<span class="sd">          [-2.40 -1.80]</span>
<span class="sd">          [ 0.60  0.30]]</span>
<span class="sd">         [[ 0.00  1.00]</span>
<span class="sd">          [ 2.00  3.00]</span>
<span class="sd">          [ 4.0   5.00]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">prelu_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="randperm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.randperm.html#mindspore.ops.randperm">[文档]</a><span class="k">def</span> <span class="nf">randperm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates random permutation of integers from 0 to n-1.</span>

<span class="sd">    Returns the tensor with the determined shape inferred by n, the random numbers in it drawn from the data range</span>
<span class="sd">    that a given type can represent.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        n (Union[Tensor, int]): The input n Tensor with shape: () or (1,) and with data type of int64.</span>
<span class="sd">            The value of `n` must be greater than zero.</span>
<span class="sd">        seed (int, optional): Random seed. Default: ``0`` . When seed is -1(only negative value), offset is 0,</span>
<span class="sd">            it&#39;s determined by time.</span>
<span class="sd">        offset (int, optional): Offset to generate random numbers. Priority is higher than random seed.</span>
<span class="sd">            Default: ``0`` . It must be non-negative.</span>
<span class="sd">        dtype (mindspore.dtype, optional): The type of output.</span>
<span class="sd">            Its value must be one of the following types: int32, int16, int8,</span>
<span class="sd">            uint8, int64, float64, float32, float16. Default: mstype.int64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. Its shape is specified by the required args `n`. Its type is specified by `dtype`.</span>
<span class="sd">        Otherwise is default.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dtype` is not allowed.</span>
<span class="sd">        ValueError: If `n` is a negative or 0 element.</span>
<span class="sd">        ValueError: If `seed` is a negative element.</span>
<span class="sd">        ValueError: If `n` is larger than the maximal data of the set dtype.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; n = 4</span>
<span class="sd">        &gt;&gt;&gt; seed = 0</span>
<span class="sd">        &gt;&gt;&gt; offset = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.randperm(n, seed, offset, dtype=mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 2 1 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">randperm_v2_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">RandpermV2</span><span class="p">)(</span><span class="n">seed</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">randperm_v2_op</span><span class="p">(</span><span class="n">n</span><span class="p">)</span></div>


<div class="viewcode-block" id="range"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.range.html#mindspore.ops.range">[文档]</a><span class="k">def</span> <span class="nf">range</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a sequence of numbers that begins at `start` and extends by increments of</span>
<span class="sd">    `step` up to but not including `end`.</span>

<span class="sd">    The types of all 3 inputs must be all integers or floating-point numbers.</span>

<span class="sd">    Args:</span>
<span class="sd">        start (number): The first number in the sequence. Must have</span>
<span class="sd">            type: int32 ,int64, float32 or float64.</span>
<span class="sd">        end (number): Upper end of the sequence, exclusive. Must</span>
<span class="sd">            have type: int32 ,int64, float32 or float64.</span>
<span class="sd">        step (number): Number that increments `start`. Must have</span>
<span class="sd">            type: int32 ,int64, float32 or float64.</span>
<span class="sd">        maxlen (int, optional): Memory that can fit `maxlen` many elements</span>
<span class="sd">            will be allocated for the output. Optional, must be positive. Default: 1000000.</span>
<span class="sd">            If the output has more than `maxlen` elements, a runtime error will occur.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A 1-D Tensor. If `start`, `end` and `step` are all integers, the type of output is int64.</span>
<span class="sd">        If `start`, `end` and `step` are all floating-point numbers, the type of output is float32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `start`, `end`, `step` have both integers and floating-point numbers.</span>
<span class="sd">        TypeError: If datatype of `start`, `end` or `step` is not supported.</span>
<span class="sd">        ValueError: If `step` = 0.</span>
<span class="sd">        ValueError: If `start` &gt;= `end` when `step` &gt; 0.</span>
<span class="sd">        ValueError: If `start` &lt;= `end` when `step` &lt; 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; start = 0</span>
<span class="sd">        &gt;&gt;&gt; end = 10</span>
<span class="sd">        &gt;&gt;&gt; step = 4</span>
<span class="sd">        &gt;&gt;&gt; output = ops.range(start, end, step)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 4 8]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">range_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">Range</span><span class="p">)(</span><span class="n">maxlen</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">range_op</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span></div>


<div class="viewcode-block" id="real"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.real.html#mindspore.ops.real">[文档]</a><span class="k">def</span> <span class="nf">real</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor that is the real part of the input. If input is real, it is returned unchanged.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor to compute to.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is the same as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If input is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor(np.asarray(np.complex(1.3+0.4j)), ms.complex64)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.real(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        1.3</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">real_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="relu6"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.relu6.html#mindspore.ops.relu6">[文档]</a><span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLU (Rectified Linear Unit) upper bounded by 6 of input tensors element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{ReLU6}(x) = \min(\max(0,x), 6)</span>

<span class="sd">    It returns :math:`\min(\max(0,x), 6)` element-wise.</span>

<span class="sd">    ReLU6 Activation Function Graph:</span>

<span class="sd">    .. image:: ../images/ReLU6.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Tensor of shape :math:`(N, *)`, where :math:`*` means any number of additional dimensions.</span>
<span class="sd">            Data type must be float16, float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same dtype and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.relu6(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 6.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">relu6_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="reshape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.reshape.html#mindspore.ops.reshape">[文档]</a><span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rearranges the input Tensor based on the given shape.</span>

<span class="sd">    The &#39;shape&#39; can only have one -1 at most, in which case it&#39;s inferred from the remaining dimensions and</span>
<span class="sd">    the number of elements in the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        shape (Union[tuple[int], list[int], Tensor[int]]): Constructed by multiple</span>
<span class="sd">            integers, i.e., :math:`(y_1, y_2, ..., y_S)`. Only constant value is allowed.</span>
<span class="sd">            Compatible with support for list and Tensor type inputs, but not recommended for use.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape of tensor is :math:`(y_1, y_2, ..., y_S)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: The given &#39;shape&#39; contains more than one -1.</span>
<span class="sd">        ValueError: The given &#39;shape&#39; contains elements less than -1.</span>
<span class="sd">        ValueError: For scenarios where the given &#39;shape&#39; does not contain -1, the product of elements of the given</span>
<span class="sd">            &#39;shape&#39; is not equal to the product of the input&#39;s &#39;shape&#39;,</span>
<span class="sd">            :math:`\prod_{i=1}^{R}x_{i}\ne \prod_{i=1}^{S}y_{i}`, (Namely, it does not match the input&#39;s array size).</span>
<span class="sd">            And for scenarios where the given &#39;shape&#39; contains -1, the product of elements other than -1 of the given</span>
<span class="sd">            &#39;shape&#39; cannot be divided by the product of the input&#39;s &#39;shape&#39; :math:`\prod_{i=1}^{R}x_{i}`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reshape(input, (3, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.1  0.3]</span>
<span class="sd">         [ 3.6  0.4]</span>
<span class="sd">         [ 0.5 -3.2]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">reshape_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="reverse"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.reverse.html#mindspore.ops.reverse">[文档]</a><span class="k">def</span> <span class="nf">reverse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reverses specific dimensions of a tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        The value range of &quot;axis&quot; is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of &quot;x&quot;.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The target tensor.</span>
<span class="sd">            The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        axis (Union[tuple(int), list(int)]): The indices of the dimensions to reverse.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is neither list nor tuple.</span>
<span class="sd">        TypeError: If element of `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse(input_x, axis=[1])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4 3 2 1]</span>
<span class="sd">         [8 7 6 5]]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3, 4], [5, 6, 7, 8]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.reverse(input_x, axis=[1, 0])</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[8 7 6 5]</span>
<span class="sd">         [4 3 2 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">reverse_v2_op</span> <span class="o">=</span> <span class="n">_get_cache_prim</span><span class="p">(</span><span class="n">ReverseV2</span><span class="p">)(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reverse_v2_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="round"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.round.html#mindspore.ops.round">[文档]</a><span class="k">def</span> <span class="nf">round</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns half to even of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_i \approx input_i</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and type as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.8, 1.5, 2.3, 2.5, -4.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.round(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.  2.  2.  2. -4.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">round_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="rsqrt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.rsqrt.html#mindspore.ops.rsqrt">[文档]</a><span class="k">def</span> <span class="nf">rsqrt</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes reciprocal of square root of input tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  \frac{1}{\sqrt{input_{i}}}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input of rsqrt. Its each element must be a non-negative</span>
<span class="sd">            number, if an element is negative, the calculation result is nan.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; input = ms.Tensor([-0.0370,  0.2970,  1.5420, -0.9105])</span>
<span class="sd">        &gt;&gt;&gt; output = ops.rsqrt(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [       nan 1.8349396  0.80530024        nan]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">rsqrt_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>

<span class="n">scalar_to_tensor_op</span><span class="o">=</span><span class="n">ScalarToTensor</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">scalar_to_tensor</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scalar_to_tensor_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<div class="viewcode-block" id="scatter_nd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.scatter_nd.html#mindspore.ops.scatter_nd">[文档]</a><span class="k">def</span> <span class="nf">scatter_nd</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Scatters a tensor into a new tensor depending on the specified indices.</span>

<span class="sd">    Creates an empty tensor with the given `shape`, and set values by scattering the update tensor</span>
<span class="sd">    depending on indices. The empty tensor has rank :math:`P` and `indices` has rank :math:`Q`.</span>

<span class="sd">    The `shape` is :math:`(s_0, s_1, ..., s_{P-1})`, where :math:`P \ge 1`.</span>

<span class="sd">    `indices` has shape :math:`(i_0, i_1, ..., i_{Q-2}, N)`, where :math:`Q \ge 2` and :math:`N \le P`.</span>

<span class="sd">    The last dimension of `indices` (with length :math:`N` ) indicates slices along the :math:`N` th dimension of the</span>
<span class="sd">    empty tensor.</span>

<span class="sd">    `updates` is a tensor of rank :math:`Q-1+P-N`, and</span>
<span class="sd">    its shape is :math:`(i_0, i_1, ..., i_{Q-2}, s_N, s_{N+1}, ..., s_{P-1})`.</span>

<span class="sd">    If `indices` contains duplicates, the duplicate `updates` are summed.</span>

<span class="sd">    The following figure shows the calculation process of inserting two new value matrices into the first dimension</span>
<span class="sd">    with rank-3:</span>

<span class="sd">    .. image:: ScatterNd.png</span>

<span class="sd">    Args:</span>
<span class="sd">        indices (Tensor): Define the index of scattering in the new tensor with int32 or int64 data type.</span>
<span class="sd">            The rank of `indices` must be at least 2 and `indices.shape[-1] &lt;= len(shape)`.</span>
<span class="sd">        updates (Tensor): Define the source Tensor to be updated.</span>
<span class="sd">            It has shape `indices.shape[:-1] + shape[indices.shape[-1]:]`.</span>
<span class="sd">        shape (tuple[int]): Define the shape of the output tensor, has the same data type as indices.</span>
<span class="sd">            `shape` can not be empty, and the elements in `shape` must be greater than or equal to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the new tensor, has the same type as `update` and the same shape as `shape`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `shape` is not a tuple.</span>
<span class="sd">        ValueError: If any element of `shape` is less than 1.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0], [2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([[[1, 1, 1, 1], [2, 2, 2, 2],</span>
<span class="sd">        ...                             [3, 3, 3, 3], [4, 4, 4, 4]],</span>
<span class="sd">        ...                            [[1, 1, 1, 1], [2, 2, 2, 2],</span>
<span class="sd">        ...                             [3, 3, 3, 3], [4, 4, 4, 4]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (4, 4, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd(indices, updates, shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3.]</span>
<span class="sd">          [4. 4. 4. 4.]]</span>
<span class="sd">         [[0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]]</span>
<span class="sd">         [[1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3.]</span>
<span class="sd">          [4. 4. 4. 4.]]</span>
<span class="sd">         [[0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]</span>
<span class="sd">          [0. 0. 0. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([[0, 1], [1, 1]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; updates = Tensor(np.array([3.2, 1.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; shape = (3, 3)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.scatter_nd(indices, updates, shape)</span>
<span class="sd">        &gt;&gt;&gt; # In order to facilitate understanding, explain the operator pseudo-operation process step by step:</span>
<span class="sd">        &gt;&gt;&gt; # Step 1: Generate an empty Tensor of the specified shape according to the shape</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0. 0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # Step 2: Modify the data at the specified location according to the indicators</span>
<span class="sd">        &gt;&gt;&gt; # 0th row of indices is [0, 1], 0th row of updates is 3.2.</span>
<span class="sd">        &gt;&gt;&gt; # means that the empty tensor in the 0th row and 1st col set to 3.2</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 3.2. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # 1th row of indices is [1, 1], 1th row of updates is 1.1.</span>
<span class="sd">        &gt;&gt;&gt; # means that the empty tensor in the 1th row and 1st col set to 1.1</span>
<span class="sd">        &gt;&gt;&gt; # [</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 3.2. 0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 1.1  0.]</span>
<span class="sd">        &gt;&gt;&gt; #     [0. 0.   0.]</span>
<span class="sd">        &gt;&gt;&gt; # ]</span>
<span class="sd">        &gt;&gt;&gt; # The final result is as follows:</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 3.2 0.]</span>
<span class="sd">         [0. 1.1 0.]</span>
<span class="sd">         [0. 0.  0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scatter_nd_op</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>


<div class="viewcode-block" id="select"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.select.html#mindspore.ops.select">[文档]</a><span class="k">def</span> <span class="nf">select</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The conditional tensor determines whether the corresponding element in the output must be</span>
<span class="sd">    selected from `x` (if True) or `y` (if False) based on the value of each</span>
<span class="sd">    element.</span>

<span class="sd">    It can be defined as:</span>

<span class="sd">    ..  math::</span>
<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">        x_i, &amp; \text{if } cond_i \\</span>
<span class="sd">        y_i, &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">      cond (Tensor[bool]): The condition tensor, decides which element is chosen.</span>
<span class="sd">        The shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">      x (Union[Tensor, int, float]): The first Tensor or number to be selected.</span>
<span class="sd">        If x is a Tensor, the shape is or can be broadcadt to :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">        If x is an int or a float, it will be cast to the type of int32 or float32,</span>
<span class="sd">        and broadcast to the same shape as y. One of x and y must be a Tensor.</span>
<span class="sd">      y (Union[Tensor, int, float]): The second Tensor or number to be selected.</span>
<span class="sd">        If y is a Tensor, The shape is or can be broadcadt to :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">        If y is an int or a float, it will be cast to the type of int32 or float32,</span>
<span class="sd">        and broadcast to the same shape as x. One of x and y must be a Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `cond`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # 1) Both inputs are Tensor</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; cond = Tensor([True, False])</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor([1,2], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.select(cond, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 2.]</span>
<span class="sd">        &gt;&gt;&gt; # 2) y is a float</span>
<span class="sd">        &gt;&gt;&gt; cond = Tensor([True, False])</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = 2.0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.select(cond, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 2.]</span>
<span class="sd">        </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">select_op</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sigmoid.html#mindspore.ops.sigmoid">[文档]</a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Sigmoid of input element-wise. The Sigmoid function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{sigmoid}(x_i) = \frac{1}{1 + \exp(-x_i)}</span>

<span class="sd">    where :math:`x_i` is an element of `x`.</span>

<span class="sd">    Sigmoid Function Graph:</span>

<span class="sd">    .. image:: ../images/Sigmoid.png</span>
<span class="sd">        :align: center</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): `input` is `x` in the preceding formula. Tensor of any dimension,</span>
<span class="sd">            the data type is float16, float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is not float16, float32, float64, complex64 or complex128.</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sigmoid(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7310586  0.880797   0.95257413 0.98201376 0.9933072 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sigmoid_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="silu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.silu.html#mindspore.ops.silu">[文档]</a><span class="k">def</span> <span class="nf">silu</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Sigmoid Linear Unit of input element-wise. The SiLU function is defined as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{SiLU}(x) = x * \sigma(x),</span>

<span class="sd">    where :math:`x` is an element of the input, :math:`\sigma(x)` is Sigmoid function.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{sigma}(x_i) = \frac{1}{1 + \exp(-x_i)},</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): `input` is `x` in the preceding formula. Input with the data type</span>
<span class="sd">            float16 or float32.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same type and shape as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([-1, 2, -3, 2, -1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.silu(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-0.269  1.762  -0.1423  1.762  -0.269]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">silu_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sin.html#mindspore.ops.sin">[文档]</a><span class="k">def</span> <span class="nf">sin</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        output_i = \sin(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is</span>
<span class="sd">            :math:`(N,*)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`. </span>
<span class="sd">        The dtype of output is float32 when dtype of `input` is in</span>
<span class="sd">        [bool, int8, uint8, int16, int32, int64]. Otherwise output has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: On CPU or GPU: If dtype of `input` is not float16, float32 or float64, complex64, complex128.</span>
<span class="sd">                   On Ascend: If type of `input` is not bool, int8, uint8, int16, int32, int64, float16, float32 or float64, complex64, complex128.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sin(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.5810352 0.27635565 0.41687083 0.5810352]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sin_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sinc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sinc.html#mindspore.ops.sinc">[文档]</a><span class="k">def</span> <span class="nf">sinc</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the normalized sinc of input.</span>

<span class="sd">    .. math::</span>

<span class="sd">        output_i = \begin{cases} \frac{sin(\pi input_i)}{\pi input_i} &amp; input_i\neq 0\\</span>
<span class="sd">        1 &amp; input_i=0 \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as the `input`. The dtype of output is float32 when dtype of `input` is in</span>
<span class="sd">        [int, bool]. Otherwise output has the same dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sinc(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.47735003 0.8759357  0.7224278  0.47735003]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sinc_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sinh.html#mindspore.ops.sinh">[文档]</a><span class="k">def</span> <span class="nf">sinh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic sine of the input element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        output_i = \sinh(input_i)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor of hyperbolic sine function.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([0.62, 0.28, 0.43, 0.62]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sinh(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.6604918  0.28367308 0.44337422 0.6604918 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sinh_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="sqrt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.sqrt.html#mindspore.ops.sqrt">[文档]</a><span class="k">def</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns sqrt of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} = \sqrt{x_{i}}</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor with a dtype of number.Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 4.0, 9.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.sqrt(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 2. 3.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">sqrt_op</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="square"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.square.html#mindspore.ops.square">[文档]</a><span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns square of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = input_i ^ 2</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The input tensor with a dtype of Number.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.square(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1. 4. 9.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">square_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="trace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.trace.html#mindspore.ops.trace">[文档]</a><span class="k">def</span> <span class="nf">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a new tensor that is the sum of the `input` main trace.</span>

<span class="sd">    Note:</span>
<span class="sd">        Input must be matrix, and complex number is not supported at present.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A matrix to be calculated. The matrix must be two dimensional.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, with the same data type as input `input`, and size equals to 1.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        ValueError: If the dimension of `input` is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[10, 11, 12], [13, 14, 15], [16, 17, 18]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.trace(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        42.0</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.arange(1, 13).reshape(3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.trace(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        18.0</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.arange(12, 0, -1).reshape(4, 3), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.trace(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        24.0</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">trace_op</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="transpose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.transpose.html#mindspore.ops.transpose">[文档]</a><span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Permutes the dimensions of the input tensor according to input permutation.</span>

<span class="sd">    For a 1-D array this has no effect, as a transposed vector is simply the same vector.</span>
<span class="sd">    To convert a 1-D array into a 2D column vector please refer to :func:`mindspore.ops.expand_dims`.</span>
<span class="sd">    For a 2-D array, this is a standard matrix transpose. For an n-D array, if axes are given,</span>
<span class="sd">    their order indicates how the axes are permuted (see Examples).</span>
<span class="sd">    If axes are not provided and a.shape is :math:`(i[0], i[1], ... i[n-2], i[n-1])`,</span>
<span class="sd">    then a.transpose().shape is :math:`(i[n-1], i[n-2], ... i[1], i[0])`.</span>

<span class="sd">    Note:</span>
<span class="sd">        On GPU and CPU, if the value of `input_perm` is negative, its actual value is `input_perm[i] + rank(input)`.</span>
<span class="sd">        Negative value of `input_perm` is not supported on Ascend.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): The shape of tensor is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        input_perm (tuple[int]): The permutation to be converted. The elements in `input_perm` are composed of</span>
<span class="sd">            the indexes of each dimension of `input`. The length of `input_perm` and the shape of `input` must be</span>
<span class="sd">            the same. Only constant value is allowed. Must be in the range [-rank(input), rank(input)).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the type of output tensor is the same as `input` and the shape of output tensor is decided by the</span>
<span class="sd">        shape of `input` and the value of `input_perm`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_perm` is not a tuple.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to length of shape of `input_perm`.</span>
<span class="sd">        ValueError: If the same element exists in `input_perm`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; input_perm = (0, 2, 1)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.transpose(input, input_perm)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 1.  4.]</span>
<span class="sd">          [ 2.  5.]</span>
<span class="sd">          [ 3.  6.]]</span>
<span class="sd">         [[ 7. 10.]</span>
<span class="sd">          [ 8. 11.]</span>
<span class="sd">          [ 9. 12.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">transpose_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">tuple_to_tensor</span><span class="p">(</span><span class="n">input_tuple</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">tuple_to_tensor_op</span><span class="p">(</span><span class="n">input_tuple</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>


<div class="viewcode-block" id="unsorted_segment_sum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.unsorted_segment_sum.html#mindspore.ops.unsorted_segment_sum">[文档]</a><span class="k">def</span> <span class="nf">unsorted_segment_sum</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sum of a tensor along segments.</span>

<span class="sd">    Calculates a tensor such that :math:`\text{output}[i] = \sum_{segment\_ids[j] == i} \text{data}[j, \ldots]`, where</span>
<span class="sd">    :math:`j,...` is a tuple describing the index of element in data.</span>
<span class="sd">    `segment_ids` selects which elements in data to sum</span>
<span class="sd">    up. Segment_ids does not need to be sorted, and it does not need to cover all values in the entire valid value</span>
<span class="sd">    range.</span>

<span class="sd">    The following figure shows the calculation process of unsorted_segment_sum:</span>

<span class="sd">    .. image:: UnsortedSegmentSum.png</span>

<span class="sd">    Note:</span>
<span class="sd">        - If the segment_id i is absent in the segment_ids, then output[i] will be filled with 0.</span>
<span class="sd">        - On Ascend, if the value of segment_id is less than 0 or greater than the length of the input data shape, an</span>
<span class="sd">          execution error will occur.</span>

<span class="sd">    If the sum of the given segment_ids :math:`i` is empty, then :math:`\text{output}[i] = 0`. If the given segment_ids</span>
<span class="sd">    is negative, the value will be ignored. &#39;num_segments&#39; must be equal to the number of different segment_ids.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_x (Tensor): Input Tensor contains the data to be summed.</span>
<span class="sd">            The shape is :math:`(x_1, x_2, ..., x_R)`.</span>
<span class="sd">        segment_ids (Tensor): The label indicates the segment to which each element belongs.</span>
<span class="sd">            Set the shape as :math:`(x_1, x_2, ..., x_N)`, where 0 &lt; N &lt;= R.</span>
<span class="sd">        num_segments (Union[int, Tensor], optional): Set :math:`z` as num_segments, it can be an int or 0-D Tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, the shape is :math:`(z, x_{N+1}, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `num_segments` is not an int or 0-D Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 1, 2], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 4</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_sum(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 3. 4. 0.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([1, 2, 3, 4, 2, 5], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; segment_ids = Tensor([0, 0, 1, 2, 3, 4], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; num_segments = 6</span>
<span class="sd">        &gt;&gt;&gt; output = ops.unsorted_segment_sum(input_x, segment_ids, num_segments)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 3. 4. 2. 5. 0.]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">unsorted_segment_sum_op</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">view</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reshape the tensor according to the input shape. It&#39;s the same as :func:`mindspore.Tensor.reshape`,</span>
<span class="sd">    implemented by the underlying reshape operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        shape (Union[tuple(int), int]): Dimension of the output tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, which dimension is the input shape&#39;s value.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([[1, 2, 3], [2, 3, 4]], dtype=np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = a.view((3, 2))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 2.]</span>
<span class="sd">        [3. 2.]</span>
<span class="sd">        [3. 4.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">view_op</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>