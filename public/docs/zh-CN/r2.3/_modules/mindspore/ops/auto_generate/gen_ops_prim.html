<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.auto_generate.gen_ops_prim &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script><script src="../../../../_static/js/mermaid-9.3.0.js"></script><script src="../../../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">分布式并行原生</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.extend.html">mindspore.ops.extend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.hal.html">mindspore.hal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">网络搭建对比</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/operators.html">静态图语法-运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/statements.html">静态图语法-Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/python_builtin_functions.html">静态图语法-Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">模块代码</a> &raquo;</li>
      <li>mindspore.ops.auto_generate.gen_ops_prim</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>mindspore.ops.auto_generate.gen_ops_prim 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2023 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators definition generated by gen_ops.py, includes primitive classes.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">Primitive</span><span class="p">,</span> <span class="n">prim_arg_register</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">signature</span> <span class="k">as</span> <span class="n">sig</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common._decorator</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">mindspore.ops._primitive_cache</span> <span class="kn">import</span> <span class="n">_get_cache_prim</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.auto_generate.gen_arg_dtype_cast</span> <span class="kn">import</span> <span class="n">type_it</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.auto_generate.gen_arg_handler</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">OpDtype</span>
<span class="kn">from</span> <span class="nn">mindspore.common._stub_tensor</span> <span class="kn">import</span> <span class="n">_convert_stub</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_add_ext</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_argmax_with_value</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_argmin_with_value</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_broadcast_to</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_cast</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_concat</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_contiguous</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_copy</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_cos</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_div</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_equal</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_erfinv</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_exp</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_gather_d_grad_v2</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_gather_d</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_gelu_grad</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_gelu</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_greater</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_less_equal</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_log</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_masked_fill</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_mul</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_neg</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_reciprocal</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_reduce_any</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_relu_grad</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_relu</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_scatter</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_sigmoid_grad</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_sigmoid</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_silu_grad</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_silu</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_sin</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_softmax_backward</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_softmax</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_sqrt</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_square</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_sub_ext</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_tile</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">pyboost_transpose</span>


<span class="k">class</span> <span class="nc">ACosGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ACosGrad of input element-wise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="n">a_cos_grad_op</span><span class="o">=</span><span class="n">ACosGrad</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">AbsGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for abs operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="n">abs_grad_op</span><span class="o">=</span><span class="n">AbsGrad</span><span class="p">()</span>


<div class="viewcode-block" id="Abs"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Abs.html#mindspore.ops.Abs">[文档]</a><span class="k">class</span> <span class="nc">Abs</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Abs()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.abs(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.abs` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">abs_op</span><span class="o">=</span><span class="n">Abs</span><span class="p">()</span>


<div class="viewcode-block" id="ACos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ACos.html#mindspore.ops.ACos">[文档]</a><span class="k">class</span> <span class="nc">ACos</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ACos()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.acos(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.acos` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">acos_op</span><span class="o">=</span><span class="n">ACos</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">AcoshGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of Acosh operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="n">acosh_grad_op</span><span class="o">=</span><span class="n">AcoshGrad</span><span class="p">()</span>


<div class="viewcode-block" id="Acosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Acosh.html#mindspore.ops.Acosh">[文档]</a><span class="k">class</span> <span class="nc">Acosh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Acosh()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.acosh(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.acosh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">acosh_op</span><span class="o">=</span><span class="n">Acosh</span><span class="p">()</span>


<div class="viewcode-block" id="AdamWeightDecay"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AdamWeightDecay.html#mindspore.ops.AdamWeightDecay">[文档]</a><span class="k">class</span> <span class="nc">AdamWeightDecay</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates gradients by the Adaptive Moment Estimation algorithm with weight decay (AdamWeightDecay).</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>
<span class="sd">    The AdamWeightDecay variant was proposed in `Decoupled Weight Decay Regularization</span>
<span class="sd">    &lt;https://arxiv.org/abs/1711.05101&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            update = \frac{m}{\sqrt{v} + \epsilon} \\</span>
<span class="sd">            update =</span>
<span class="sd">            \begin{cases}</span>
<span class="sd">                update + weight\_decay * w</span>
<span class="sd">                    &amp; \text{ if } weight\_decay &gt; 0 \\</span>
<span class="sd">                update</span>
<span class="sd">                    &amp; \text{ otherwise }</span>
<span class="sd">            \end{cases} \\</span>
<span class="sd">            w  = w - lr * update</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`lr` represents `learning_rate`, :math:`w` represents `var`, :math:`decay` represents `weight_decay`,</span>
<span class="sd">    :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If ``True`` , updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If ``False`` , the result is unpredictable. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated. The shape is :math:`(N, *)` where :math:`*` means,</span>
<span class="sd">          any number of additional dimensions. The data type can be float16 or float32.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula,</span>
<span class="sd">          it should have the the shape as `var`. The data type can be float16 or float32.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula,</span>
<span class="sd">          it should have the same shape as `m`.</span>
<span class="sd">        - **lr** (float) - :math:`lr` in the updating formula. The paper suggested value is :math:`10^{-8}`,</span>
<span class="sd">          the data type should be float32.</span>
<span class="sd">        - **beta1** (float) - The exponential decay rate for the 1st moment estimations,</span>
<span class="sd">          the data type should be float32. The paper suggested value is :math:`0.9`</span>
<span class="sd">        - **beta2** (float) - The exponential decay rate for the 2nd moment estimations,</span>
<span class="sd">          the data type should be float32. The paper suggested value is :math:`0.999`</span>
<span class="sd">        - **epsilon** (float) - Term added to the denominator to improve numerical stability,</span>
<span class="sd">          the data type should be float32.</span>
<span class="sd">        - **decay** (float) - The weight decay value, must be a scalar tensor with float32 data type.</span>
<span class="sd">          Default: ``0.0`` .</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same shape as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `lr`, `beta1`, `beta2`, `epsilon` or `decay` is not a float32.</span>
<span class="sd">        TypeError: If `var`, `m` or `v` is not a Parameter with dtype float16 or float32.</span>
<span class="sd">        TypeError: If `gradient` is not a Tensor.</span>
<span class="sd">        ValueError: If `epsilon` &lt;= 0.</span>
<span class="sd">        ValueError: If `beta1`, `beta2` is not in range (0.0,1.0).</span>
<span class="sd">        ValueError: If `decay` &lt; 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter, ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.adam_weight_decay = ops.AdamWeightDecay()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, lr, beta1, beta2, epsilon, decay, grad):</span>
<span class="sd">        ...         out = self.adam_weight_decay(self.var, self.m, self.v, lr, beta1, beta2,</span>
<span class="sd">        ...                               epsilon, decay, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(0.001, 0.9, 0.999, 1e-8, 0.0, gradient)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.999 0.999]</span>
<span class="sd">        [0.999 0.999]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;decay&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">gradient</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">decay</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">AddExt</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AddExt()</span>
<span class="sd">        out = prim(input, other, alpha)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.add_ext(input, other, alpha)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.add_ext` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;other&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_add_ext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">alpha</span><span class="p">]))</span>

<span class="n">add_ext_op</span><span class="o">=</span><span class="n">AddExt</span><span class="p">()</span>


<div class="viewcode-block" id="Add"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Add.html#mindspore.ops.Add">[文档]</a><span class="k">class</span> <span class="nc">Add</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Add()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.add(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.add` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">add_op</span><span class="o">=</span><span class="n">Add</span><span class="p">()</span>


<div class="viewcode-block" id="Addcdiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Addcdiv.html#mindspore.ops.Addcdiv">[文档]</a><span class="k">class</span> <span class="nc">Addcdiv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds the element-wise division of `x1` by `x2`, multiplied by `value` to `input_data`.</span>
<span class="sd">    It computes the following operation:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y[i] = input\_data[i] + value[i] * (x1[i] / x2[i])</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - The tensor to be added.</span>
<span class="sd">        - **x1** (Tensor) - The numerator tensor.</span>
<span class="sd">        - **x2** (Tensor) - The denominator tensor.</span>
<span class="sd">        - **value** (Tensor) - The multiplier for tensor x1/x2.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as x1/x2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x1`, `x2`, `value`, `input_data` is not tensor.</span>
<span class="sd">        TypeError: If dtype of `x1`, `x2`, `value`, `input_data` are not the same.</span>
<span class="sd">        ValueError: If `x1` could not be broadcast to `x2`.</span>
<span class="sd">        ValueError: If `value` could not be broadcast to `x1/x2`.</span>
<span class="sd">        ValueError: If `input_data` could not be broadcast to `value*(x1/x2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([4, 3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; addcdiv = ops.Addcdiv()</span>
<span class="sd">        &gt;&gt;&gt; y = addcdiv(input_data, x1, x2, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1.25      1.6666667 2.5       5.       ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<span class="n">addcdiv_op</span><span class="o">=</span><span class="n">Addcdiv</span><span class="p">()</span>


<div class="viewcode-block" id="Addcmul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Addcmul.html#mindspore.ops.Addcmul">[文档]</a><span class="k">class</span> <span class="nc">Addcmul</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds the element-wise product of `x1` by `x2`, multiplied by `value` to `input_data`.</span>
<span class="sd">    It computes the following operation:</span>

<span class="sd">    .. math::</span>
<span class="sd">        output[i] = input\_data[i] + value[i] * (x1[i] * x2[i])</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_data** (Tensor) - The tensor to be added.</span>
<span class="sd">        - **x1** (Tensor) - The tensor to be multiplied.</span>
<span class="sd">        - **x2** (Tensor) - The tensor to be multiplied.</span>
<span class="sd">        - **value** (Tensor) - The multiplier for tensor x1*x2.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as x1*x2.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x1`, `x2`, `value`, `input_data` is not tensor.</span>
<span class="sd">        TypeError: If dtype of `x1`, `x2`, `value`, `input_data` are not the same.</span>
<span class="sd">        ValueError: If `x1` could not be broadcast to `x2`.</span>
<span class="sd">        ValueError: If `value` could not be broadcast to `x1` * `x2`.</span>
<span class="sd">        ValueError: If `input_data` could not be broadcast to `value*(x1*x2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_data = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1], [2], [3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([[1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor([1], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; addcmul = ops.Addcmul()</span>
<span class="sd">        &gt;&gt;&gt; y = addcmul(input_data, x1, x2, value)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 2.  3.  4.]</span>
<span class="sd">        [ 3.  5.  7.]</span>
<span class="sd">        [ 4.  7. 10.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">tensor1</span><span class="p">,</span> <span class="n">tensor2</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<span class="n">addcmul_op</span><span class="o">=</span><span class="n">Addcmul</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">AddN</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AddN()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.addn(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.addn` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">addn_op</span><span class="o">=</span><span class="n">AddN</span><span class="p">()</span>


<div class="viewcode-block" id="Angle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Angle.html#mindspore.ops.Angle">[文档]</a><span class="k">class</span> <span class="nc">Angle</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Angle()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.angle(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.angle` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">angle_op</span><span class="o">=</span><span class="n">Angle</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ApplyCamePart1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Part 1 of the CAME Optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **grad** (Tensor) - The shape = 2D :math:`(..., n, m)`.</span>
<span class="sd">          A Tensor of types: float16, float32, bfloat16.</span>
<span class="sd">        - **eps** (float) - data type must be float.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **sum_grad_r** (Tensor) - A Tensor of shape :math:`(..., n)`</span>
<span class="sd">        - **sum_grad_c** (Tensor) - A Tensor of shape :math:`(..., m)`</span>
<span class="sd">        - **sum_grad_rc** (Tensor) - A Tensor of of shape:math:`(..., m)`</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` </span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as P</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.ones([1024, 64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; apply_came_part1 = P.ApplyCamePart1()</span>
<span class="sd">        &gt;&gt;&gt; output = apply_came_part1(grad, 1.1)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].asnumpy())</span>
<span class="sd">        (1024,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>


<span class="n">apply_came_part1_op</span><span class="o">=</span><span class="n">ApplyCamePart1</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ApplyCamePart2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Part 2 of the CAME Optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **grad** (Tensor) - The shape = 2D :math:`(..., n, m)`.</span>
<span class="sd">          A Tensor of types: float16, float32, bfloat16.</span>
<span class="sd">        - **sum_grad_r** (Tensor) - The shape = 1D :math:`(..., n)`.</span>
<span class="sd">          A Tensor of types: float32.</span>
<span class="sd">        - **sum_grad_c** (Tensor) - The shape = 1D :math:`(..., m)`.</span>
<span class="sd">          A Tensor of types: float32.</span>
<span class="sd">        - **sum_grad_rc** (Tensor) - The shape = 1D :math:`(...)`.</span>
<span class="sd">          A Tensor of types: float32.</span>
<span class="sd">        - **r** (Tensor) - The shape = 1D :math:`(..., n)`.</span>
<span class="sd">          The Tensor has the same data type as `grad`.</span>
<span class="sd">        - **c** (Tensor) - The shape = 1D :math:`(..., m)`.</span>
<span class="sd">          The Tensor has the same data type as `grad`.</span>
<span class="sd">        - **beta2** (float) - data type must be float.</span>
<span class="sd">        - **sum_r** (Tensor) - The shape = 1D :math:`(..., 1)`.</span>
<span class="sd">          &#39;None&#39; is currently supported. A Tensor of types: float32.</span>
<span class="sd">        - **global_shape** (Tensor) - the shape = 1D :math:`(2)`.</span>
<span class="sd">          &#39;None&#39; is currently supported. A Tensor of types: int64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **r** (Tensor) - A Tensor of shape :math:`(..., n)`</span>
<span class="sd">        - **c** (Tensor) - A Tensor of shape :math:`(..., m)`</span>
<span class="sd">        - **u** (Tensor) - A Tensor of of shape:math:`(..., n, m)`</span>
<span class="sd">        - **sum_square_u** (Tensor) - A Tensor of of shape:math:`(1)`</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` </span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as P</span>
<span class="sd">        &gt;&gt;&gt; apply_came_part2 = P.ApplyCamePart2()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.ones([1024, 64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; sum_grad_r = Tensor(np.ones([1024]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; sum_grad_c = Tensor(np.ones([64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; sum_grad_rc = Tensor(np.array([64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; r = Tensor(np.ones([1024]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; c = Tensor(np.ones([64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = 0.5</span>
<span class="sd">        &gt;&gt;&gt; sum_r = Tensor(np.array([128]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; global_shape = (1024, 64)</span>
<span class="sd">        &gt;&gt;&gt; output = apply_came_part2(grad, sum_grad_r, sum_grad_c, sum_grad_rc, r, c, beta2, sum_r, global_shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].shape)</span>
<span class="sd">        (1024,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_grad_r&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_grad_c&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_grad_rc&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_r&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;global_shape&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">sum_grad_r</span><span class="p">,</span> <span class="n">sum_grad_c</span><span class="p">,</span> <span class="n">sum_grad_rc</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">sum_r</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">global_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">sum_grad_r</span><span class="p">,</span> <span class="n">sum_grad_c</span><span class="p">,</span> <span class="n">sum_grad_rc</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="n">sum_r</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">)</span>


<span class="n">apply_came_part2_op</span><span class="o">=</span><span class="n">ApplyCamePart2</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ApplyCamePart3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Part 3 of the CAME Optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **u** (Parameter) - The shape = 2D :math:`(..., n, m)`.</span>
<span class="sd">          A Tensor of types: float16, float32, bfloat16.</span>
<span class="sd">        - **m** (Parameter) - The shape = 2D :math:`(..., n, m)`.</span>
<span class="sd">          A Tensor of types: float16, float32, bfloat16.</span>
<span class="sd">        - **eps** (float) - data type must be float.</span>
<span class="sd">        - **beta1** (float) - data type must be float.</span>
<span class="sd">        - **clip_threshold** (float) - data type must be float.</span>
<span class="sd">        - **sum_square_u** (Tensor) - The shape = 1D :math:`(1)`.</span>
<span class="sd">          A Tensor of types: float32.</span>
<span class="sd">        - **global_shape** (Tensor) - the shape = 1D :math:`(2)`.</span>
<span class="sd">          &#39;None&#39; is currently supported. A Tensor of types: int64.</span>
<span class="sd">        - **use_first_moment** (bool).</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **m** (Tensor) - A Tensor of shape :math:`(..., n, m)`</span>
<span class="sd">        - **sum_u_r** (Tensor) - A Tensor of shape :math:`(..., n)`</span>
<span class="sd">        - **sum_u_c** (Tensor) - A Tensor of of shape:math:`(..., m)`</span>
<span class="sd">        - **sum_u_rc** (Tensor) - A Tensor of of shape:math:`(...)`</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `u` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` </span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as P</span>
<span class="sd">        &gt;&gt;&gt; apply_came_part3 = P.ApplyCamePart3()</span>
<span class="sd">        &gt;&gt;&gt; u = Tensor(np.ones([1024, 64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; m = Tensor(np.ones([1024, 64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; eps = 0.8</span>
<span class="sd">        &gt;&gt;&gt; beta1 = 0.5</span>
<span class="sd">        &gt;&gt;&gt; clip_threshold = 0.5</span>
<span class="sd">        &gt;&gt;&gt; sum_square_u = Tensor(np.array([128]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; global_shape = (1024, 64)</span>
<span class="sd">        &gt;&gt;&gt; use_first_moment = False</span>
<span class="sd">        &gt;&gt;&gt; output = apply_came_part3(u, m, eps, beta1, clip_threshold, sum_square_u, global_shape, use_first_moment)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].shape)</span>
<span class="sd">        (1024, 64)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;u&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;eps&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;clip_threshold&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_square_u&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;global_shape&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;use_first_moment&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">clip_threshold</span><span class="p">,</span> <span class="n">sum_square_u</span><span class="p">,</span> <span class="n">global_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_first_moment</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="n">clip_threshold</span><span class="p">,</span> <span class="n">sum_square_u</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">,</span> <span class="n">use_first_moment</span><span class="p">)</span>


<span class="n">apply_came_part3_op</span><span class="o">=</span><span class="n">ApplyCamePart3</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ApplyCamePart4</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes Part 4 of the CAME Optimizer.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **param** (Parameter) - The shape = 2D :math:`(..., n, m)`.</span>
<span class="sd">          A Tensor of types: float16, float32, bfloat16.</span>
<span class="sd">        - **m** (Parameter) - The shape = 2D :math:`(..., n, m)`.</span>
<span class="sd">          The Tensor has the same data type as `param`.</span>
<span class="sd">        - **r** (Tensor) - The shape = 1D :math:`(..., n)`.</span>
<span class="sd">          The Tensor has the same data type as `param`.</span>
<span class="sd">        - **c** (Tensor) - The shape = 1D :math:`(..., m)`.</span>
<span class="sd">          The Tensor has the same data type as `param`.</span>
<span class="sd">        - **weight_decay** (Tensor) - The shape = 1D :math:`(1)`.</span>
<span class="sd">          A Tensor of types: float32.</span>
<span class="sd">        - **lr** (Tensor) - The shape = 1D :math:`(1)`.</span>
<span class="sd">          A Tensor of types: float32.</span>
<span class="sd">        - **beta3** (float) - data type must be float.</span>
<span class="sd">        - **sum_r** (Tensor) - The shape = 1D :math:`(..., 1)`.</span>
<span class="sd">          &#39;None&#39; is currently supported. A Tensor of types: float32.</span>
<span class="sd">        - **sum_u_r** (Tensor) - The shape = 1D :math:`(..., n)`.</span>
<span class="sd">          A Tensor of types: float32.</span>
<span class="sd">        - **sum_u_c** (Tensor) - The shape = 1D :math:`(..., m)`.</span>
<span class="sd">          A Tensor of types: float32.</span>
<span class="sd">        - **sum_u_rc** (Tensor) - The shape = 1D :math:`(...)`.</span>
<span class="sd">          A Tensor of types: float32.</span>
<span class="sd">        - **global_shape** (Tensor) - the shape = 1D :math:`(2)`.</span>
<span class="sd">          &#39;None&#39; is currently supported. A Tensor of types: int64.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **param** (Tensor) - A Tensor of shape :math:`(..., n, m)`</span>
<span class="sd">        - **r** (Tensor) - A Tensor of shape :math:`(..., n)`</span>
<span class="sd">        - **c** (Tensor) - A Tensor of of shape:math:`(..., m)`</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `param` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` </span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops as P</span>
<span class="sd">        &gt;&gt;&gt; apply_came_part4 = P.ApplyCamePart4()</span>
<span class="sd">        &gt;&gt;&gt; param = Tensor(np.ones([1024, 64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; m = Tensor(np.ones([1024, 64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; r = Tensor(np.ones([1024]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; c = Tensor(np.ones([64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight_decay = Tensor([0.8])</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor([0.5])</span>
<span class="sd">        &gt;&gt;&gt; beta3 = 0.5</span>
<span class="sd">        &gt;&gt;&gt; sum_r = Tensor(np.array([128.]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; sum_u_r = Tensor(np.ones([1024]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; sum_u_c = Tensor(np.ones([64]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; sum_u_rc = Tensor(np.array([128.]), dtype=ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; global_shape = (1024, 64)</span>
<span class="sd">        &gt;&gt;&gt; output = apply_came_part4(param, m, r, c, weight_decay, lr, beta3, \</span>
<span class="sd">        ... sum_r, sum_u_r, sum_u_c, sum_u_rc, global_shape)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].shape)</span>
<span class="sd">        (1024, 64)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;param&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;r&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;c&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta3&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_r&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_u_r&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_u_c&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sum_u_rc&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;global_shape&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta3</span><span class="p">,</span> <span class="n">sum_r</span><span class="p">,</span> <span class="n">sum_u_r</span><span class="p">,</span> <span class="n">sum_u_c</span><span class="p">,</span> <span class="n">sum_u_rc</span><span class="p">,</span> <span class="n">global_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">beta3</span><span class="p">,</span> <span class="n">sum_r</span><span class="p">,</span> <span class="n">sum_u_r</span><span class="p">,</span> <span class="n">sum_u_c</span><span class="p">,</span> <span class="n">sum_u_rc</span><span class="p">,</span> <span class="n">global_shape</span><span class="p">)</span>


<span class="n">apply_came_part4_op</span><span class="o">=</span><span class="n">ApplyCamePart4</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ApplyRotaryPosEmb</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ApplyRotaryPosEmb(cos_format)</span>
<span class="sd">        out = prim(query, key, cos, sin, position_ids)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.apply_rotary_pos_emb_(query, key, cos, sin, position_ids, cos_format)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.apply_rotary_pos_emb_` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cos_format</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;cos_format&quot;</span><span class="p">,</span> <span class="n">cos_format</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cos_format</span><span class="p">)</span>


<div class="viewcode-block" id="Argmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Argmax.html#mindspore.ops.Argmax">[文档]</a><span class="k">class</span> <span class="nc">Argmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the maximum value along a specified `axis` of a Tensor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.argmax` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Axis where the Argmax operation applies to. Default: ``-1`` .</span>
<span class="sd">        output_type (:class:`mindspore.dtype`): Output data type.</span>
<span class="sd">            Supported types: ``mstype.int32`` , ``mstype.int64`` . Default: ``mstype.int32`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. :math:`(N, *)` where :math:`*` means, any number of additional</span>
<span class="sd">          dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, indices of the max value of input tensor across the axis.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 20, 5], [67, 8, 9], [130, 24, 15]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Argmax(output_type=mindspore.int32)(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 0 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;output_type&quot;</span><span class="p">,</span> <span class="n">output_type</span><span class="p">,</span> <span class="n">dtype_to_type_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="ArgMaxWithValue"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ArgMaxWithValue.html#mindspore.ops.ArgMaxWithValue">[文档]</a><span class="k">class</span> <span class="nc">ArgMaxWithValue</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the maximum value along with the given axis for the input tensor, and returns the maximum values and</span>
<span class="sd">    indices.</span>

<span class="sd">    Note:</span>
<span class="sd">        In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If there are multiple maximum values, the index of the first maximum value is used.</span>
<span class="sd">        - The value range of `axis` is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of `input`.</span>

<span class="sd">    Also see :func:`mindspore.ops.max`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The dimension to reduce. Default: ``0`` .</span>
<span class="sd">        keep_dims (bool): Whether to reduce dimension, if ``True`` , the output will keep same dimension with the</span>
<span class="sd">            input, the output will reduce dimension if ``false`` . Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor, can be any dimension. Set the shape of input tensor as</span>
<span class="sd">          :math:`(input_1, input_2, ..., input_N)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the maximum value of the input</span>
<span class="sd">        tensor.</span>

<span class="sd">        - **index** (Tensor) - The index for the maximum value of the input tensor, with dtype int64. If `keep_dims`</span>
<span class="sd">          is ``True`` , the shape of output tensors is :math:`(input_1, input_2, ..., input_{axis-1}, 1, input_{axis+1}, ..., input_N)`.</span>
<span class="sd">          Otherwise, the shape is :math:`(input_1, input_2, ..., input_{axis-1}, input_{axis+1}, ..., input_N)` .</span>
<span class="sd">        - **values** (Tensor) - The maximum value of input tensor, with the same shape as `index`, and same dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.ArgMaxWithValue()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        3 0.7</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.ArgMaxWithValue(keep_dims=True)(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        [3] [0.7]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_argmax_with_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">]))</span></div>

<div class="viewcode-block" id="Argmin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Argmin.html#mindspore.ops.Argmin">[文档]</a><span class="k">class</span> <span class="nc">Argmin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the indices of the minimum value along a specified `axis` of a Tensor.</span>

<span class="sd">    If the shape of input tensor is :math:`(x_1, ..., x_N)`, the shape of the output tensor is</span>
<span class="sd">    :math:`(x_1, ..., x_{axis-1}, x_{axis+1}, ..., x_N)`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Axis where the Argmin operation applies to. Default: ``-1`` .</span>
<span class="sd">        output_type (:class:`mindspore.dtype`): Output data type.</span>
<span class="sd">            Supported types: ``mstype.int32`` , ``mstype.int64`` . Default: ``mstype.int32`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input tensor.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, which is the minimum index in the specified axis of input Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        TypeError: If `output_type` is neither int32 nor int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([2.0, 3.1, 1.2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index = ops.Argmin()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(index)</span>
<span class="sd">        2</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;output_type&quot;</span><span class="p">,</span> <span class="n">output_type</span><span class="p">,</span> <span class="n">dtype_to_type_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="ArgMinWithValue"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ArgMinWithValue.html#mindspore.ops.ArgMinWithValue">[文档]</a><span class="k">class</span> <span class="nc">ArgMinWithValue</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the minimum value along with the given axis for the input tensor, and returns the minimum values and</span>
<span class="sd">    indices.</span>

<span class="sd">    Note:</span>
<span class="sd">        In auto_parallel and semi_auto_parallel mode, the first output index can not be used.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - If there are multiple minimum values, the index of the first minimum value is used.</span>
<span class="sd">        - The value range of `axis` is [-dims, dims - 1]. &quot;dims&quot; is the dimension length of `input`.</span>

<span class="sd">    Also see :func:`mindspore.ops.min`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The dimension to reduce. Default: ``0`` .</span>
<span class="sd">        keep_dims (bool): Whether to reduce dimension, if ``True`` the output will keep the same dimension as the</span>
<span class="sd">            input, the output will reduce dimension if ``false`` . Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor, can be any dimension. Set the shape of input tensor as</span>
<span class="sd">          :math:`(input_1, input_2, ..., input_N)` .Complex tensor is not supported.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple (Tensor), tuple of 2 tensors, containing the corresponding index and the minimum value of the input</span>
<span class="sd">        tensor.</span>

<span class="sd">        - **index** (Tensor) - The index for the minimum value of the input tensor, with dtype int64. If `keep_dims`</span>
<span class="sd">          is ``True`` , the shape of output tensors is :math:`(input_1, input_2, ..., input_{axis-1}, 1, input_{axis+1}, ..., input_N)`.</span>
<span class="sd">          Otherwise, the shape is :math:`(input_1, input_2, ..., input_{axis-1}, input_{axis+1}, ..., input_N)` .</span>
<span class="sd">        - **values** (Tensor) - The minimum value of input tensor, with the same</span>
<span class="sd">          shape as `index`, and same dtype as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not Tensor.</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.0, 0.4, 0.6, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.ArgMinWithValue()(x)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        0 0.0</span>
<span class="sd">        &gt;&gt;&gt; index, output = ops.ArgMinWithValue(keep_dims=True)(x)</span>
<span class="sd">        &gt;&gt;&gt; print(index, output)</span>
<span class="sd">        [0] [0.0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_argmin_with_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">]))</span></div>

<span class="k">class</span> <span class="nc">AsinGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes AsinGrad of input element-wise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="n">asin_grad_op</span><span class="o">=</span><span class="n">AsinGrad</span><span class="p">()</span>


<div class="viewcode-block" id="Asin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Asin.html#mindspore.ops.Asin">[文档]</a><span class="k">class</span> <span class="nc">Asin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Asin()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.asin(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.asin` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">asin_op</span><span class="o">=</span><span class="n">Asin</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">AsinhGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of Asinh operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="n">asinh_grad_op</span><span class="o">=</span><span class="n">AsinhGrad</span><span class="p">()</span>


<div class="viewcode-block" id="Asinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Asinh.html#mindspore.ops.Asinh">[文档]</a><span class="k">class</span> <span class="nc">Asinh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Asinh()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.asinh(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.asinh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">asinh_op</span><span class="o">=</span><span class="n">Asinh</span><span class="p">()</span>


<div class="viewcode-block" id="AssignAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AssignAdd.html#mindspore.ops.AssignAdd">[文档]</a><span class="k">class</span> <span class="nc">AssignAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.AssignAdd()</span>
<span class="sd">        out = prim(variable, value)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.assign_add(variable, value)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.assign_add` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<span class="n">assign_add_op</span><span class="o">=</span><span class="n">AssignAdd</span><span class="p">()</span>


<div class="viewcode-block" id="Assign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Assign.html#mindspore.ops.Assign">[文档]</a><span class="k">class</span> <span class="nc">Assign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Assign()</span>
<span class="sd">        out = prim(variable, value)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.assign(variable, value)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.assign` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">variable</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span></div>


<span class="n">assign_op</span><span class="o">=</span><span class="n">Assign</span><span class="p">()</span>


<div class="viewcode-block" id="Atan2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Atan2.html#mindspore.ops.Atan2">[文档]</a><span class="k">class</span> <span class="nc">Atan2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Atan2()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.atan2(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.atan2` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">atan2_op</span><span class="o">=</span><span class="n">Atan2</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">AtanGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes AtanGrad of input element-wise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>


<span class="n">atan_grad_op</span><span class="o">=</span><span class="n">AtanGrad</span><span class="p">()</span>


<div class="viewcode-block" id="Atan"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Atan.html#mindspore.ops.Atan">[文档]</a><span class="k">class</span> <span class="nc">Atan</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Atan()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.atan(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.atan` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">atan_op</span><span class="o">=</span><span class="n">Atan</span><span class="p">()</span>


<div class="viewcode-block" id="Atanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Atanh.html#mindspore.ops.Atanh">[文档]</a><span class="k">class</span> <span class="nc">Atanh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Atanh()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.atanh(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.atanh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">atanh_op</span><span class="o">=</span><span class="n">Atanh</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">AvgPoolGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradients of the avg pool operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">to_kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">to_strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>


<div class="viewcode-block" id="AvgPool"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AvgPool.html#mindspore.ops.AvgPool">[文档]</a><span class="k">class</span> <span class="nc">AvgPool</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average pooling operation.</span>

<span class="sd">    Refer to :func:`mindspore.ops.avg_pool2d` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the average value,</span>
<span class="sd">            is an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively. Default: ``1`` .</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: ``1`` .</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;SAME&quot;`` or ``&quot;VALID&quot;`` . Default: ``&quot;VALID&quot;`` .</span>

<span class="sd">            - ``&quot;SAME&quot;``: Pad the input around its edges so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally, If the amount is even, it is</span>
<span class="sd">              uniformly distributed around the input, if it is odd, the excess amount goes to the right/bottom side.</span>
<span class="sd">            - ``&quot;valid&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded.</span>

<span class="sd">        data_format (str, optional): The format of input and output data. It should be ``&#39;NHWC&#39;`` or ``&#39;NCHW&#39;`` .</span>
<span class="sd">            Default: ``&#39;NCHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Supported dtypes: float16, float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        TypeError: If dtype of `x` is not  float16, float32 or float64.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; nor &#39;NHWC&#39;.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, nn</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.avgpool_op = ops.AvgPool(pad_mode=&#39;VALID&#39;, kernel_size=2, strides=1)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         result = self.avgpool_op(x)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape(1, 3, 3, 4), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 2.5   3.5   4.5]</span>
<span class="sd">           [ 6.5   7.5   8.5]]</span>
<span class="sd">          [[14.5  15.5  16.5]</span>
<span class="sd">           [18.5  19.5  20.5]]</span>
<span class="sd">          [[26.5  27.5  28.5]</span>
<span class="sd">           [30.5  31.5  32.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">to_kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">to_strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">BatchNormGradGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of BatchNormGrad operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">saved_mean</span><span class="p">,</span> <span class="n">saved_variance</span><span class="p">,</span> <span class="n">dout_dx</span><span class="p">,</span> <span class="n">dout_dscale</span><span class="p">,</span> <span class="n">dout_dbias</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">saved_mean</span><span class="p">,</span> <span class="n">saved_variance</span><span class="p">,</span> <span class="n">dout_dx</span><span class="p">,</span> <span class="n">dout_dscale</span><span class="p">,</span> <span class="n">dout_dbias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BatchNormGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of BatchNorm operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">saved_mean</span><span class="p">,</span> <span class="n">saved_variance</span><span class="p">,</span> <span class="n">reserve</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">saved_mean</span><span class="p">,</span> <span class="n">saved_variance</span><span class="p">,</span> <span class="n">reserve</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Betainc</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the regularized incomplete beta function</span>
<span class="sd">    :math:`I_{x}(a, b)`. It is defined as the ratio of the incomplete beta function</span>
<span class="sd">    to the complete beta function:</span>

<span class="sd">    .. math::</span>

<span class="sd">    I_{x}(a, b)=\frac{B(x ; a, b)}{B(a, b)}</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>

<span class="sd">        B(x ; a, b)=\int_{0}^{x} t^{a-1}(1-t)^{b-1} dt</span>

<span class="sd">    is the incomplete beta function and</span>

<span class="sd">    .. math::</span>

<span class="sd">        B(a, b) = \int_0^1 t^{a-1} (1-t)^{b-1} dt</span>

<span class="sd">    is the complete beta function.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **a** (Tensor) - Peak location of beta distribution.</span>
<span class="sd">          A Tensor of types: float32, float64.</span>
<span class="sd">        - **b** (Tensor) - Spread of the beta distribution.</span>
<span class="sd">          A Tensor, must have the same dtype and shape as `a` .</span>
<span class="sd">        - **x** (Tensor) - Upper limit of integration of the incomplete beta function.</span>
<span class="sd">          A Tensor, must have the same dtype and shape as `a` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor, has the same dtype and shape as `a` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `a` is not float32 nor float64.</span>
<span class="sd">        TypeError: If either dtype of `b` and `x` is not the same as the `a`.</span>
<span class="sd">        ValueError: If either shape of `b` and `x` is not the same as the `a`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor(np.array([0.3, 0.1, 0.4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.array([0.4, 0.5, 0.9]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.2, 0.6, 0.5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; betainc = ops.Betainc()</span>
<span class="sd">        &gt;&gt;&gt; print(betainc(a, b, x))</span>
<span class="sd">        [0.41462693 0.8706035  0.7298298 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="n">betainc_op</span><span class="o">=</span><span class="n">Betainc</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">BiasAddGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients of BiasAdd.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>


<div class="viewcode-block" id="BiasAdd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BiasAdd.html#mindspore.ops.BiasAdd">[文档]</a><span class="k">class</span> <span class="nc">BiasAdd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the sum of the input Tensor and the bias Tensor. Before adding, the bias Tensor will be broadcasted to be</span>
<span class="sd">    consistent with the shape of the input Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        data_format (str, optional): The format of input and output data.</span>
<span class="sd">            It should be ``&quot;NHWC&quot;`` , ``&quot;NCHW&quot;`` or ``&quot;NCDHW&quot;`` .</span>
<span class="sd">            Default is ``&quot;NCHW&quot;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The shape can be 2-5 dimensions. Supported dtypes:</span>

<span class="sd">          - Ascend/CPU: all Number type.</span>
<span class="sd">          - GPU: float16, float32, int8.</span>

<span class="sd">        - **bias** (Tensor) - The bias tensor, with shape :math:`(C)`. C must be the same as channel dimension C of</span>
<span class="sd">          `input_x`. It has the same type as `input_x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `data_format` is not a str.</span>
<span class="sd">        ValueError: If value of `data_format` is not in the range of [&#39;NHWC&#39;,&#39;NCHW&#39;,&#39;NCDHW&#39;].</span>
<span class="sd">        TypeError: If `input_x` or `bias` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` or `bias` is inconsistent.</span>
<span class="sd">        TypeError: If dimension of `input_x` is not in the range [2, 5].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(6).reshape((2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.random.random(3).reshape((3,)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; bias_add = ops.BiasAdd()</span>
<span class="sd">        &gt;&gt;&gt; output = bias_add(input_x, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;data_format&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">BoolNot</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns bool_not `not` of bool input.</span>

<span class="sd">    .. note::</span>
<span class="sd">        The inputs can be constant/variable value. Usage is the same as &#39;not&#39; in Python.</span>
<span class="sd">        This primitive only have &#39;CPU&#39; implementation, for other platform, it runs using heterogeneous.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Scalar) - A constant or variable scalar, the type can be bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Scalar, the type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` are not bool scalar.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">bool_not_op</span><span class="o">=</span><span class="n">BoolNot</span><span class="p">()</span>


<div class="viewcode-block" id="BroadcastTo"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BroadcastTo.html#mindspore.ops.BroadcastTo">[文档]</a><span class="k">class</span> <span class="nc">BroadcastTo</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.BroadcastTo(shape)</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.broadcast_to(input, shape)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.broadcast_to` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;shape&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;BroadcastTo&#39;</span><span class="p">,</span> <span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TENSOR</span><span class="p">),</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_broadcast_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">]))</span></div>

<div class="viewcode-block" id="Ceil"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Ceil.html#mindspore.ops.Ceil">[文档]</a><span class="k">class</span> <span class="nc">Ceil</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Ceil()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.ceil(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.ceil` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">ceil_op</span><span class="o">=</span><span class="n">Ceil</span><span class="p">()</span>


<div class="viewcode-block" id="CeLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.CeLU.html#mindspore.ops.CeLU">[文档]</a><span class="k">class</span> <span class="nc">CeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.CeLU(alpha)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.celu(x, alpha)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.celu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">CholeskyGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the reverse mode backpropgated gradient of the Cholesky algorithm.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **x** (Tensor) - A tensor with float32 or float64 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor with float32 or float64 data type. `grad` should have</span>
<span class="sd">          the same dtype with `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `a` and `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If x is not Tensor.</span>
<span class="sd">        TypeError: If grad is not Tensor.</span>
<span class="sd">        TypeError: If dtype of input x and grad is not float64 nor float32,</span>
<span class="sd">        TypeError: If x has different dtype with grad.</span>
<span class="sd">        ValueError: If input tensor&#39;s last two dims are not equal,</span>
<span class="sd">        ValueError: If the shape of x and grad mismatch.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>


<span class="n">cholesky_grad_op</span><span class="o">=</span><span class="n">CholeskyGrad</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">CholeskyInverse</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the inverse of the positive definite matrix using cholesky matrix factorization given its Cholesky factor.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cholesky_inverse` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        upper(bool, optional): Whether to return a lower or upper triangular matrix. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor whose rank is 2. Supported dtypes: float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 1], [1, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = ops.CholeskyInverse()(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 5.0  -3.0 ]</span>
<span class="sd">        [-3.0   2.0 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;upper&quot;</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>


<div class="viewcode-block" id="Cholesky"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cholesky.html#mindspore.ops.Cholesky">[文档]</a><span class="k">class</span> <span class="nc">Cholesky</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the Cholesky decomposition on a single or a batch of symmetric positive-definite matrices.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cholesky` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        upper (bool, optional): Flag that indicates whether to return a upper or lower triangular matrix.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x**  (Tensor) - Tensor of shape :math:`(*, N, N)`, where :math:`*` is zero or more batch dimensions</span>
<span class="sd">          consisting of symmetric positive-definite matrices, with float32 or float64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1.0, 1.0], [1.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Cholesky()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0.]</span>
<span class="sd">         [1. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;upper&quot;</span><span class="p">,</span> <span class="n">upper</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span></div>


<div class="viewcode-block" id="Complex"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Complex.html#mindspore.ops.Complex">[文档]</a><span class="k">class</span> <span class="nc">Complex</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a complex Tensor from the real part and the imag part.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **real** (Tensor) - The real input tensor. types: float32, float64.</span>
<span class="sd">        - **imag** (Tensor) - The imag input tensor. types: float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the complex type.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the dtype of input is not one of: float32, float64.</span>
<span class="sd">        TypeError: If the dtypes of two inputs are not same.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; real = Tensor(np.array([1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; imag = Tensor(np.array([2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; complex = ops.Complex()</span>
<span class="sd">        &gt;&gt;&gt; output = complex(real, imag)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.+2.j]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">imag</span><span class="p">)</span></div>


<span class="n">complex_op</span><span class="o">=</span><span class="n">Complex</span><span class="p">()</span>


<div class="viewcode-block" id="Concat"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Concat.html#mindspore.ops.Concat">[文档]</a><span class="k">class</span> <span class="nc">Concat</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Concat(axis)</span>
<span class="sd">        out = prim(tensors)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cat(tensors, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cat` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensors</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_concat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">tensors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]))</span></div>

<div class="viewcode-block" id="Conj"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Conj.html#mindspore.ops.Conj">[文档]</a><span class="k">class</span> <span class="nc">Conj</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Conj()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.conj(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.conj` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">conj_op</span><span class="o">=</span><span class="n">Conj</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">Contiguous</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Contiguous()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.contiguous(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.contiguous` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_contiguous</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span>

<span class="n">contiguous_op</span><span class="o">=</span><span class="n">Contiguous</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">Copy</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Copy()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.copy(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.copy` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_copy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span>

<span class="n">copy_op</span><span class="o">=</span><span class="n">Copy</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">Correlate</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Correlate(mode)</span>
<span class="sd">        out = prim(a, v)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.correlate(a, v, mode)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.correlate` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>


<div class="viewcode-block" id="Cos"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cos.html#mindspore.ops.Cos">[文档]</a><span class="k">class</span> <span class="nc">Cos</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Cos()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cos(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cos` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_cos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span></div>

<span class="n">cos_op</span><span class="o">=</span><span class="n">Cos</span><span class="p">()</span>


<div class="viewcode-block" id="Cosh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cosh.html#mindspore.ops.Cosh">[文档]</a><span class="k">class</span> <span class="nc">Cosh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Cosh()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cosh(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cosh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">cosh_op</span><span class="o">=</span><span class="n">Cosh</span><span class="p">()</span>


<div class="viewcode-block" id="CumProd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.CumProd.html#mindspore.ops.CumProd">[文档]</a><span class="k">class</span> <span class="nc">CumProd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative product of the tensor x along axis.</span>
<span class="sd">    For example, if input is a vector of size N, the result will also be a vector of size N, with elements.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y_i = x_1 * x_2 * x_3 * ... * x_i</span>

<span class="sd">    Args:</span>
<span class="sd">        exclusive (bool): If ``True`` , perform exclusive cumulative product. Default: ``False`` .</span>
<span class="sd">        reverse (bool): If ``True`` , reverse the result along axis. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[Number]) - The input Tensor with shape</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means any number of additional dimensions.</span>
<span class="sd">        - **axis** (int) - The dimensions to compute the cumulative product.</span>
<span class="sd">          Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `exclusive` or `reverse` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>
<span class="sd">        ValueError: If `axis` is None.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; a, b, c, = 1, 2, 3</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([a, b, c]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op0 = ops.CumProd()</span>
<span class="sd">        &gt;&gt;&gt; output0 = op0(x, 0) # output=[a, a * b, a * b * c]</span>
<span class="sd">        &gt;&gt;&gt; op1 = ops.CumProd(exclusive=True)</span>
<span class="sd">        &gt;&gt;&gt; output1 = op1(x, 0) # output=[1, a, a * b]</span>
<span class="sd">        &gt;&gt;&gt; op2 = ops.CumProd(reverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output2 = op2(x, 0) # output=[a * b * c, b * c, c]</span>
<span class="sd">        &gt;&gt;&gt; op3 = ops.CumProd(exclusive=True, reverse=True)</span>
<span class="sd">        &gt;&gt;&gt; output3 = op3(x, 0) # output=[b * c, c, 1]</span>
<span class="sd">        &gt;&gt;&gt; print(output0)</span>
<span class="sd">        [1. 2. 6.]</span>
<span class="sd">        &gt;&gt;&gt; print(output1)</span>
<span class="sd">        [1. 1. 2.]</span>
<span class="sd">        &gt;&gt;&gt; print(output2)</span>
<span class="sd">        [6. 6. 3.]</span>
<span class="sd">        &gt;&gt;&gt; print(output3)</span>
<span class="sd">        [6. 3. 1.]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 2, 3], [4, 5, 6], [5, 3, 5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output4 = op0(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; output5 = op0(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output4)</span>
<span class="sd">        [[ 1.  2.  3.]</span>
<span class="sd">         [ 4. 10. 18.]</span>
<span class="sd">         [20. 30. 90.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output5)</span>
<span class="sd">        [[  1.   2.   6.]</span>
<span class="sd">         [  4.  20. 120.]</span>
<span class="sd">         [  5.  15.  75.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;exclusive&quot;</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">exclusive</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse</span><span class="p">)</span></div>


<div class="viewcode-block" id="CumSum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.CumSum.html#mindspore.ops.CumSum">[文档]</a><span class="k">class</span> <span class="nc">CumSum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cumulative sum of input tensor along axis.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y_i = x_1 + x_2 + x_3 + ... + x_i</span>

<span class="sd">    Args:</span>
<span class="sd">        exclusive (bool): By default, this op performs an inclusive cumsum, which means that the first</span>
<span class="sd">            element of the input is identical to the first element of the output. Default: ``False`` .</span>
<span class="sd">        reverse (bool): If ``True`` , perform inverse cumulative sum. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input Tensor with shape</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means any number of additional dimensions.</span>
<span class="sd">        - **axis**  (int) - The axis to accumulate the tensor&#39;s value. Only constant value is allowed.</span>
<span class="sd">          Must be in the range [-rank(input), rank(input)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is consistent with the input tensor&#39;s.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `exclusive` or `reverse` is not a bool.</span>
<span class="sd">        TypeError: If `axis` is not an int.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[3, 4, 6, 10], [1, 6, 7, 9], [4, 3, 8, 7], [1, 3, 7, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; cumsum = ops.CumSum()</span>
<span class="sd">        &gt;&gt;&gt; # case 1: along the axis 0</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 3.  4.  6. 10.]</span>
<span class="sd">         [ 4. 10. 13. 19.]</span>
<span class="sd">         [ 8. 13. 21. 26.]</span>
<span class="sd">         [ 9. 16. 28. 35.]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: along the axis 1</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 3.  7. 13. 23.]</span>
<span class="sd">         [ 1.  7. 14. 23.]</span>
<span class="sd">         [ 4.  7. 15. 22.]</span>
<span class="sd">         [ 1.  4. 11. 20.]]</span>
<span class="sd">        &gt;&gt;&gt; # Next demonstrate exclusive and reverse, along axis 1</span>
<span class="sd">        &gt;&gt;&gt; # case 3: exclusive = True</span>
<span class="sd">        &gt;&gt;&gt; cumsum = ops.CumSum(exclusive=True)</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[ 0.  3.  7. 13.]</span>
<span class="sd">         [ 0.  1.  7. 14.]</span>
<span class="sd">         [ 0.  4.  7. 15.]</span>
<span class="sd">         [ 0.  1.  4. 11.]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: reverse = True</span>
<span class="sd">        &gt;&gt;&gt; cumsum = ops.CumSum(reverse=True)</span>
<span class="sd">        &gt;&gt;&gt; y = cumsum(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[23. 20. 16. 10.]</span>
<span class="sd">         [23. 22. 16.  9.]</span>
<span class="sd">         [22. 18. 15.  7.]</span>
<span class="sd">         [20. 19. 16.  9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exclusive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;exclusive&quot;</span><span class="p">,</span> <span class="n">exclusive</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">exclusive</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse</span><span class="p">)</span></div>


<div class="viewcode-block" id="Cummax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cummax.html#mindspore.ops.Cummax">[文档]</a><span class="k">class</span> <span class="nc">Cummax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Cummax(axis)</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.cummax(input, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.cummax` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="Cummin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Cummin.html#mindspore.ops.Cummin">[文档]</a><span class="k">class</span> <span class="nc">Cummin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the cumulative minimum of elements and the index.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.cummin` for more detail.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): The axis to accumulate the tensor&#39;s value. Must be in the range [-rank(input), rank(input)).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tuple of 2 Tensors(values, indices), containing the cumulative minimum of elements and the index,</span>
<span class="sd">        the shape of each output tensor is the same as input `input`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; a = Tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; output = ops.Cummin(axis)(a)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [-0.2284 -0.6628 -0.6628 -0.6628 -1.3298 -1.3298]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1])</span>
<span class="sd">        [0 1 1 1 4 4]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">DCT</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;type&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;BACKWARD&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;forward&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="nb">type</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;BACKWARD&#39;</span><span class="p">,</span> <span class="n">forward</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">type</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;DCT&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">),</span> <span class="n">forward</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>


<span class="n">dct_op</span><span class="o">=</span><span class="n">DCT</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">DecoderKVCache</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.DecoderKVCache()</span>
<span class="sd">        out = prim(cache, update, valid_seq_len, batch_index, seq_len_axis, new_max_seq_len, cur_max_seq_len)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.decoder_k_v_cache(cache, update, valid_seq_len, batch_index, seq_len_axis, new_max_seq_len, cur_max_seq_len)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.decoder_k_v_cache` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="n">valid_seq_len</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">,</span> <span class="n">seq_len_axis</span><span class="p">,</span> <span class="n">new_max_seq_len</span><span class="p">,</span> <span class="n">cur_max_seq_len</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="n">valid_seq_len</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">,</span> <span class="n">seq_len_axis</span><span class="p">,</span> <span class="n">new_max_seq_len</span><span class="p">,</span> <span class="n">cur_max_seq_len</span><span class="p">)</span>


<span class="n">decoder_k_v_cache_op</span><span class="o">=</span><span class="n">DecoderKVCache</span><span class="p">()</span>


<div class="viewcode-block" id="Diag"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Diag.html#mindspore.ops.Diag">[文档]</a><span class="k">class</span> <span class="nc">Diag</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Diag()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.diag(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.diag` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">diag_op</span><span class="o">=</span><span class="n">Diag</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">Diagonal</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Diagonal(offset, dim1, dim2)</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.diagonal(input, offset, dim1, dim2)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.diagonal` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim2</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="n">offset</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;dim1&quot;</span><span class="p">,</span> <span class="n">dim1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;dim2&quot;</span><span class="p">,</span> <span class="n">dim2</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim2</span><span class="p">)</span>


<div class="viewcode-block" id="Div"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Div.html#mindspore.ops.Div">[文档]</a><span class="k">class</span> <span class="nc">Div</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the quotient of dividing the first input tensor by the second input tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.div` for more details.</span>

<span class="sd">    Note:</span>
<span class="sd">        - One of the two inputs must be a Tensor, when the two inputs have different shapes,</span>
<span class="sd">          they must be able to broadcast to a common shape.</span>
<span class="sd">        - The two inputs can not be bool type at the same time,</span>
<span class="sd">          [True, Tensor(True, bool\_), Tensor(np.array([True]), bool\_)] are all considered bool type.</span>
<span class="sd">        - The two inputs comply with the implicit type conversion rules to make the data types</span>
<span class="sd">          consistent.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, number.Number, bool]) - The first input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>
<span class="sd">        - **y** (Union[Tensor, number.Number, bool]) - The second input is a number.Number or</span>
<span class="sd">          a bool or a tensor whose data type is </span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_ or</span>
<span class="sd">          `bool_ &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one of the input `x` , `y` after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1 :has same data type and shape of the two inputs</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([3.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; div = ops.Div()</span>
<span class="sd">        &gt;&gt;&gt; output = div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-1.3333334  2.5        2.        ]</span>
<span class="sd">        &gt;&gt;&gt; # case 2 : different data type and shape of the two inputs</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([-4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(2, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = div(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.  2.5  3.]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_div</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]))</span></div>

<span class="n">div_op</span><span class="o">=</span><span class="n">Div</span><span class="p">()</span>


<div class="viewcode-block" id="Dropout"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Dropout.html#mindspore.ops.Dropout">[文档]</a><span class="k">class</span> <span class="nc">Dropout</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the elements of the input tensor</span>
<span class="sd">    with probability :math:`1 - keep\_prob` from a Bernoulli distribution. It plays the</span>
<span class="sd">    role of reducing neuron correlation and avoid overfitting.</span>

<span class="sd">    Refer to :func:`mindspore.ops.dropout` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_prob (float, optional): The keep rate, between 0 and 1, e.g. keep_prob = 0.9,</span>
<span class="sd">            means dropping out 10% of input units. Default: ``0.5`` .</span>
<span class="sd">        Seed0 (int, optional): Seed0 value for random generating. Default: ``0`` .</span>
<span class="sd">        Seed1 (int, optional): Seed1 value for random generating. Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor of shape :math:`(*, N)`, with data type of float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - With the same shape and data type as `x`.</span>
<span class="sd">        - **mask** (Tensor) - The mask applied to `x`.</span>

<span class="sd">          - On GPU and CPU, `mask` has the same shape and data type as `x`.</span>
<span class="sd">          - On Ascend, to achieve a better performance, it is denoted as a 1-D Tensor</span>
<span class="sd">            with Uint8 data type. It has shape :math:`(byte\_counts, )` where :math:`byte\_counts` is the</span>
<span class="sd">            number of bytes needed to mask the input `x`, :math:`byte\_counts` is calculated using the</span>
<span class="sd">            following formula:</span>

<span class="sd">            .. math::</span>

<span class="sd">                byte\_counts = \text{ceil}(\text{cumprod}(x.shape) / 128) * 16</span>

<span class="sd">            If shape of `x` is :math:`(2, 3, 4, 5, 6)`, the shape of `mask` will be :math:`(96, )`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; dropout = ops.Dropout(keep_prob=0.5)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape, mask.shape, mask.dtype)</span>
<span class="sd">        (1, 2, 3, 4, 5) (16,) UInt8</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">Seed0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">Seed1</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;Seed0&quot;</span><span class="p">,</span> <span class="n">Seed0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;Seed1&quot;</span><span class="p">,</span> <span class="n">Seed1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_hidden&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Seed0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Seed1</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Eig</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the eigenvalues and eigenvectors of a square matrix(batch square matrices).</span>

<span class="sd">    Args:</span>
<span class="sd">      compute_v (bool, optional): If ``True`` , compute both eigenvalues and eigenvectors;</span>
<span class="sd">          If `False`, just eigenvalues will be computed. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">      - **x** (Tensor) - Square matrices of shape :math:`(*, N, N)`, with float32, float64, complex64 or</span>
<span class="sd">      complex128 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">      - **eigen_values** (Tensor) - Shape :math:`(*, N)`. Each inner most vector represents eigenvalues of</span>
<span class="sd">          the corresponding matrix. The eigenvalues may not have an order.</span>
<span class="sd">      - **eigen_vectors** (Tensor) - If `compute_v` is `False`, it&#39;s an empty tensor. Otherwise, this tensor has</span>
<span class="sd">          shape :math:`(*, N, N)`, whose columns represent normalized (unit length) eigenvectors of corresponding</span>
<span class="sd">          eigenvalues.</span>

<span class="sd">    Raises:</span>
<span class="sd">       TypeError: If `compute_v` is not a bool.</span>
<span class="sd">       TypeError: If dtype of `x` is not one of: float64, float32, complex64 or complex128.</span>
<span class="sd">       TypeError: If `x` is not a Tensor.</span>
<span class="sd">       ValueError: If `x` is not a square(batch squares).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">       ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 0.0], [0.0, 2.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; u, v = ops.Eig(True)(x)</span>
<span class="sd">        &gt;&gt;&gt; print(u)</span>
<span class="sd">        [1.+0.j 2.+0.j]</span>
<span class="sd">        &gt;&gt;&gt; print(v)</span>
<span class="sd">        [[1.+0.j 0.+0.j]</span>
<span class="sd">         [0.+0.j 1.+0.j]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">compute_v</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;compute_v&quot;</span><span class="p">,</span> <span class="n">compute_v</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_v</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">EluGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradients of Elu operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>


<span class="n">elu_grad_op</span><span class="o">=</span><span class="n">EluGrad</span><span class="p">()</span>


<div class="viewcode-block" id="Elu"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Elu.html#mindspore.ops.Elu">[文档]</a><span class="k">class</span> <span class="nc">Elu</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Elu(alpha)</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.elu(input_x, alpha)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.elu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span></div>


<div class="viewcode-block" id="Equal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Equal.html#mindspore.ops.Equal">[文档]</a><span class="k">class</span> <span class="nc">Equal</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Equal()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.equal(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.equal` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_equal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">]))</span></div>

<span class="n">equal_op</span><span class="o">=</span><span class="n">Equal</span><span class="p">()</span>


<div class="viewcode-block" id="Erf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Erf.html#mindspore.ops.Erf">[文档]</a><span class="k">class</span> <span class="nc">Erf</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Erf()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.erf(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.erf` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">erf_op</span><span class="o">=</span><span class="n">Erf</span><span class="p">()</span>


<div class="viewcode-block" id="Erfc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Erfc.html#mindspore.ops.Erfc">[文档]</a><span class="k">class</span> <span class="nc">Erfc</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Erfc()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.erfc(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.erfc` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">erfc_op</span><span class="o">=</span><span class="n">Erfc</span><span class="p">()</span>


<div class="viewcode-block" id="Erfinv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Erfinv.html#mindspore.ops.Erfinv">[文档]</a><span class="k">class</span> <span class="nc">Erfinv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Erfinv()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.erfinv(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.erfinv` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_erfinv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span></div>

<span class="n">erfinv_op</span><span class="o">=</span><span class="n">Erfinv</span><span class="p">()</span>


<div class="viewcode-block" id="Exp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Exp.html#mindspore.ops.Exp">[文档]</a><span class="k">class</span> <span class="nc">Exp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Exp()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.exp(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.exp` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_exp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span></div>

<span class="n">exp_op</span><span class="o">=</span><span class="n">Exp</span><span class="p">()</span>


<div class="viewcode-block" id="ExpandDims"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ExpandDims.html#mindspore.ops.ExpandDims">[文档]</a><span class="k">class</span> <span class="nc">ExpandDims</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ExpandDims()</span>
<span class="sd">        out = prim(input_x, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.expand_dims(input_x, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.expand_dims` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span></div>


<span class="n">expand_dims_op</span><span class="o">=</span><span class="n">ExpandDims</span><span class="p">()</span>


<div class="viewcode-block" id="Expm1"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Expm1.html#mindspore.ops.Expm1">[文档]</a><span class="k">class</span> <span class="nc">Expm1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Expm1()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.expm1(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.expm1` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">expm1_op</span><span class="o">=</span><span class="n">Expm1</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ExtractImagePatches</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ExtractImagePatches(ksizes, strides, rates, padding)</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.extract_image_patches(input_x, ksizes, strides, rates, padding)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.extract_image_patches` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksizes</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">rates</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;VALID&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;ksizes&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;ExtractImagePatches&#39;</span><span class="p">,</span> <span class="s1">&#39;ksizes&#39;</span><span class="p">,</span> <span class="n">ksizes</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">),</span> <span class="n">to_kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;ExtractImagePatches&#39;</span><span class="p">,</span> <span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">),</span> <span class="n">to_strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;rates&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;ExtractImagePatches&#39;</span><span class="p">,</span> <span class="s1">&#39;rates&#39;</span><span class="p">,</span> <span class="n">rates</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">),</span> <span class="n">to_rates</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;padding&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksizes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">)</span>


<div class="viewcode-block" id="Eye"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Eye.html#mindspore.ops.Eye">[文档]</a><span class="k">class</span> <span class="nc">Eye</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a tensor with ones on the diagonal and zeros in the rest.</span>

<span class="sd">    Refer to :func:`mindspore.ops.eye` for more details.</span>

<span class="sd">    Note:</span>
<span class="sd">        The data type of returned tensor can be float16, float32, int8, int16, int32, int64, uint8 or bool on Ascend platforms.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **n** (int) - The number of rows of returned tensor. Constant value only.</span>
<span class="sd">        - **m** (int) - The number of columns of returned tensor. Constant value only.</span>
<span class="sd">        - **t** (mindspore.dtype) - MindSpore&#39;s dtype, the data type of the returned tensor.</span>
<span class="sd">          Default: ``None`` , the data type of the returned tensor is mindspore.float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, a tensor with ones on the diagonal and the rest of elements are zero. The shape of `output` depends on</span>
<span class="sd">        the user&#39;s Inputs `n` and `m`. And the data type depends on Inputs `t`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; eye = ops.Eye()</span>
<span class="sd">        &gt;&gt;&gt; output = eye(2, 2, mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 0]</span>
<span class="sd">         [0 1]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Int32</span>
<span class="sd">        &gt;&gt;&gt; output = eye(1, 2, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0.]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">dtype_to_type_id</span><span class="p">(</span><span class="s1">&#39;Eye&#39;</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span></div>


<span class="n">eye_op</span><span class="o">=</span><span class="n">Eye</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">FastGeLUGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradients of FastGeLU operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="n">fast_gelu_grad_op</span><span class="o">=</span><span class="n">FastGeLUGrad</span><span class="p">()</span>


<div class="viewcode-block" id="FastGeLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FastGeLU.html#mindspore.ops.FastGeLU">[文档]</a><span class="k">class</span> <span class="nc">FastGeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FastGeLU()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.fast_gelu(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.fast_gelu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="n">fast_gelu_op</span><span class="o">=</span><span class="n">FastGeLU</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">FFT2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FFT2()</span>
<span class="sd">        out = prim(input, s, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.fft2(input, s, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.fft2` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span> <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;FFT2&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>


<span class="n">fft2_op</span><span class="o">=</span><span class="n">FFT2</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">FFT</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FFT()</span>
<span class="sd">        out = prim(input, n, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.fft(input, n, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.fft` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span> <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;FFT&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>


<span class="n">fft_op</span><span class="o">=</span><span class="n">FFT</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">FFTShapeCopy</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Truncate or zero-fill the gradient of an fft operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>


<span class="n">fft_shapecopy_op</span><span class="o">=</span><span class="n">FFTShapeCopy</span><span class="p">()</span>


<div class="viewcode-block" id="FFTWithSize"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FFTWithSize.html#mindspore.ops.FFTWithSize">[文档]</a><span class="k">class</span> <span class="nc">FFTWithSize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fourier transform, can be adjusted by parameters to achieve FFT/IFFT/RFFT/IRFFT.</span>

<span class="sd">    For fft, it computes the following expression:</span>

<span class="sd">    .. math::</span>
<span class="sd">        X[\omega_1, \dots, \omega_d] =</span>
<span class="sd">            \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]</span>
<span class="sd">             e^{-j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</span>

<span class="sd">    where :math:`d` = `signal_ndim` is number of dimensions for the</span>
<span class="sd">    signal, and :math:`N_i` is the size of signal dimension :math:`i`.</span>

<span class="sd">    For ifft, it computes the following expression:</span>

<span class="sd">    .. math::</span>
<span class="sd">        X[\omega_1, \dots, \omega_d] =</span>
<span class="sd">            \frac{1}{\prod_{i=1}^d N_i} \sum_{n_1=0}^{N_1-1} \dots \sum_{n_d=0}^{N_d-1} x[n_1, \dots, n_d]</span>
<span class="sd">             e^{\ j\ 2 \pi \sum_{i=0}^d \frac{\omega_i n_i}{N_i}},</span>

<span class="sd">    where :math:`d` = `signal_ndim` is number of dimensions for the</span>
<span class="sd">    signal, and :math:`N_i` is the size of signal dimension :math:`i`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - FFT/IFFT requires complex64 or complex128 inputs, return complex64 or complex128 outputs.</span>
<span class="sd">        - RFFT requires bool, uint8, int8, int16, int32, int64, float32 and float64 inputs,</span>
<span class="sd">          return complex64 or complex128 outputs.</span>
<span class="sd">        - IRFFT requires complex64 or complex128 inputs, return float32 or float64 outputs.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        signal_ndim (int): The number of dimensions in each signal, this controls how many dimensions</span>
<span class="sd">            of the fourier transform are realized, can only be 1, 2 or 3.</span>
<span class="sd">        inverse (bool): Whether it is the inverse transformation, used to select from FFT and RFFT or IFFT and IRFFT.</span>

<span class="sd">            - when set to ``True``: IFFT and IRFFT.</span>
<span class="sd">            - when set to ``False``: FFT and RFFT.</span>

<span class="sd">        real (bool): Whether it is the real transformation, combines with `inverse` to select a specific</span>
<span class="sd">            transformation mode:</span>

<span class="sd">            - `inverse` is ``False`` ,  `real` is ``False`` : corresponds to FFT.</span>
<span class="sd">            - `inverse` is ``True`` , `real` is ``False`` : corresponds to IFFT.</span>
<span class="sd">            - `inverse` is ``False`` , `real` is ``True`` : corresponds to RFFT.</span>
<span class="sd">            - `inverse` is ``True`` , `real` is ``True``  : corresponds to IRFFT.</span>

<span class="sd">        norm (str, optional): The normalization, optional values: [ ``&quot;backward&quot;`` , ``&quot;forward&quot;`` , ``&quot;ortho&quot;`` ].</span>
<span class="sd">            Default value: ``&quot;backward&quot;`` .</span>

<span class="sd">            - ``&quot;backward&quot;`` has the direct transforms unscaled and the inverse transforms scaled by :math:`1/n`,</span>
<span class="sd">              where n is the input x&#39;s element numbers.</span>
<span class="sd">            - ``&quot;ortho&quot;`` has both direct and inverse transforms are scaled by :math:`1/\sqrt n`.</span>
<span class="sd">            - ``&quot;forward&quot;`` has the direct transforms scaled by :math:`1/n` and the inverse transforms unscaled.</span>

<span class="sd">        onesided (bool, optional): Controls whether the input is halved to avoid redundancy. Default: ``True`` .</span>
<span class="sd">        signal_sizes (tuple, optional): Size of the original signal (the signal before rfft, no batch dimension),</span>
<span class="sd">            only in IRFFT mode and set `onesided` to ``True`` requires the parameter, the following conditions must be</span>
<span class="sd">            satisfied. Default: ``()`` .</span>

<span class="sd">            - The length of `signal_sizes` is equal to the signal_ndim of the IRFFT:</span>
<span class="sd">              :math:`len(signal\_sizes)=signal\_ndim`.</span>
<span class="sd">            - The last dimension of `signal_sizes` divided by 2 is equal to</span>
<span class="sd">              the last dimension of the IRFFT input: :math:`signal\_size[-1]/2+1=x.shape[-1]`.</span>
<span class="sd">            - `signal_sizes` has exactly the same dimensions as the input shape</span>
<span class="sd">              except for the last dimension: :math:`signal\_sizes[:-1]=x.shape[:-1]`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The dimension of the input tensor must be greater than or equal to signal_ndim.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A tensor containing the complex-to-complex, real-to-complex or complex-to-real Fourier transform result.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the input type of FFT/IFFT/IRFFT is not one of: complex64, complex128.</span>
<span class="sd">        TypeError: If the input type is not Tensor.</span>
<span class="sd">        ValueError: If `x` dimension is less than signal_ndim.</span>
<span class="sd">        ValueError: If signal_ndim is greater than 3 or less than 1.</span>
<span class="sd">        ValueError: If norm is none of &quot;backward&quot;, &quot;forward&quot; or &quot;ortho&quot;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case FFT: signal_ndim: 1, inverse: False, real: False.</span>
<span class="sd">        &gt;&gt;&gt; fft_in = Tensor(np.array([2, 1, 2]), mindspore.complex64)</span>
<span class="sd">        &gt;&gt;&gt; fft_net = ops.FFTWithSize(signal_ndim=1, inverse=False, real=False)</span>
<span class="sd">        &gt;&gt;&gt; fft_output = fft_net(fft_in)</span>
<span class="sd">        &gt;&gt;&gt; print(fft_output)</span>
<span class="sd">        [5.        +0.j         0.5       +0.86602545j 0.50000006-0.8660255j ]</span>
<span class="sd">        &gt;&gt;&gt; # case IFFT: signal_ndim: 1, inverse: True, real: False.</span>
<span class="sd">        &gt;&gt;&gt; ifft_in = fft_output</span>
<span class="sd">        &gt;&gt;&gt; ifft_net = ops.FFTWithSize(signal_ndim=1, inverse=True, real=False)</span>
<span class="sd">        &gt;&gt;&gt; ifft_output = ifft_net(ifft_in)</span>
<span class="sd">        &gt;&gt;&gt; print(ifft_output)</span>
<span class="sd">        [2.        -1.9868216e-08j 0.99999994+0.0000000e+00j</span>
<span class="sd">         1.9999999 +7.9472862e-08j]</span>
<span class="sd">        &gt;&gt;&gt; # case RFFT2D: signal_ndim: 2, inverse: False, real: True.</span>
<span class="sd">        &gt;&gt;&gt; rfft_in = Tensor(np.array([[2, 1, 2], [3, 1, 6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rfft_net = ops.FFTWithSize(signal_ndim=2, inverse=False, real=True)</span>
<span class="sd">        &gt;&gt;&gt; rfft_output = rfft_net(rfft_in)</span>
<span class="sd">        &gt;&gt;&gt; print(rfft_output)</span>
<span class="sd">        [[ 1.5000000e+01+1.1920929e-07j -2.3841858e-07+5.1961522e+00j]</span>
<span class="sd">         [-5.0000000e+00-2.9802322e-08j  9.9999988e-01-3.4641016e+00j]]</span>
<span class="sd">        &gt;&gt;&gt; # case IRFFT2D: signal_ndim: 2, inverse: True, real: True.</span>
<span class="sd">        &gt;&gt;&gt; irfft_in = rfft_output</span>
<span class="sd">        &gt;&gt;&gt; irfft_net = ops.FFTWithSize(signal_ndim=2, inverse=True, real=True, signal_sizes=rfft_in.shape)</span>
<span class="sd">        &gt;&gt;&gt; irfft_output = irfft_net(irfft_in)</span>
<span class="sd">        &gt;&gt;&gt; print(irfft_output)</span>
<span class="sd">        [[2.         1.         2.        ]</span>
<span class="sd">         [3.         0.99999994 5.9999995 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">,</span> <span class="n">inverse</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s1">&#39;backward&#39;</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">signal_sizes</span><span class="o">=</span><span class="p">()):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;signal_ndim&quot;</span><span class="p">,</span> <span class="n">signal_ndim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;inverse&quot;</span><span class="p">,</span> <span class="n">inverse</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;real&quot;</span><span class="p">,</span> <span class="n">real</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;norm&quot;</span><span class="p">,</span> <span class="n">norm</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;onesided&quot;</span><span class="p">,</span> <span class="n">onesided</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;signal_sizes&quot;</span><span class="p">,</span> <span class="n">signal_sizes</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">signal_ndim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inverse</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">real</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">onesided</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">signal_sizes</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">FFTN</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FFTN()</span>
<span class="sd">        out = prim(input, s, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.fftn(input, s, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.fftn` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span> <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;FFTN&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>


<span class="n">fftn_op</span><span class="o">=</span><span class="n">FFTN</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">FFTShift</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FFTShift()</span>
<span class="sd">        out = prim(input, dim)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.fftshift(input, dim)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.fftshift` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>


<span class="n">fftshift_op</span><span class="o">=</span><span class="n">FFTShift</span><span class="p">()</span>


<div class="viewcode-block" id="Flatten"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Flatten.html#mindspore.ops.Flatten">[文档]</a><span class="k">class</span> <span class="nc">Flatten</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Flattens a tensor without changing its batch size on the 0-th axis.</span>

<span class="sd">    Refer to :func:`mindspore.ops.flatten` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)` to be flattened, where :math:`N` is batch size.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is :math:`(N, X)`, where :math:`X` is</span>
<span class="sd">        the product of the remaining dimension.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[1, 2, 3, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; flatten = ops.Flatten()</span>
<span class="sd">        &gt;&gt;&gt; output = flatten(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 24)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="n">flatten_op</span><span class="o">=</span><span class="n">Flatten</span><span class="p">()</span>


<div class="viewcode-block" id="FloorDiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FloorDiv.html#mindspore.ops.FloorDiv">[文档]</a><span class="k">class</span> <span class="nc">FloorDiv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FloorDiv()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.floor_divide(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.floor_divide` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">floor_div_op</span><span class="o">=</span><span class="n">FloorDiv</span><span class="p">()</span>


<div class="viewcode-block" id="FloorMod"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FloorMod.html#mindspore.ops.FloorMod">[文档]</a><span class="k">class</span> <span class="nc">FloorMod</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.FloorMod()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.floor_mod(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.floor_mod` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<span class="n">floor_mod_op</span><span class="o">=</span><span class="n">FloorMod</span><span class="p">()</span>


<div class="viewcode-block" id="Floor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Floor.html#mindspore.ops.Floor">[文档]</a><span class="k">class</span> <span class="nc">Floor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Floor()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.floor(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.floor` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">floor_op</span><span class="o">=</span><span class="n">Floor</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">GatherDGradV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradient for the GatherD operation. Note that the operator &quot;GatherDGrad&quot; has been abandoned.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_gather_d_grad_v2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">dout</span><span class="p">]))</span>

<span class="n">gather_d_grad_v2_op</span><span class="o">=</span><span class="n">GatherDGradV2</span><span class="p">()</span>


<div class="viewcode-block" id="GatherD"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GatherD.html#mindspore.ops.GatherD">[文档]</a><span class="k">class</span> <span class="nc">GatherD</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GatherD()</span>
<span class="sd">        out = prim(x, dim, index)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gather_d(x, dim, index)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gather_d` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_gather_d</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">]))</span></div>

<span class="n">gather_d_op</span><span class="o">=</span><span class="n">GatherD</span><span class="p">()</span>


<div class="viewcode-block" id="GatherNd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GatherNd.html#mindspore.ops.GatherNd">[文档]</a><span class="k">class</span> <span class="nc">GatherNd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GatherNd()</span>
<span class="sd">        out = prim(input_x, indices)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gather_nd(input_x, indices)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gather_nd` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span></div>


<span class="n">gather_nd_op</span><span class="o">=</span><span class="n">GatherNd</span><span class="p">()</span>


<div class="viewcode-block" id="Gather"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Gather.html#mindspore.ops.Gather">[文档]</a><span class="k">class</span> <span class="nc">Gather</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Gather(batch_dims)</span>
<span class="sd">        out = prim(input_params, input_indices, axis)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gather(input_params, input_indices, axis, batch_dims)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gather` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_dims</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;batch_dims&quot;</span><span class="p">,</span> <span class="n">batch_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_params</span><span class="p">,</span> <span class="n">input_indices</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="Gcd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Gcd.html#mindspore.ops.Gcd">[文档]</a><span class="k">class</span> <span class="nc">Gcd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Gcd()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.gcd(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.gcd` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">gcd_op</span><span class="o">=</span><span class="n">Gcd</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">GeLUGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradients of GeLU operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_gelu_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]))</span>

<span class="n">gelu_grad_op</span><span class="o">=</span><span class="n">GeLUGrad</span><span class="p">()</span>


<div class="viewcode-block" id="GeLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GeLU.html#mindspore.ops.GeLU">[文档]</a><span class="k">class</span> <span class="nc">GeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gaussian Error Linear Units activation function.</span>

<span class="sd">    GeLU is described in the paper `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_.</span>
<span class="sd">    And also please refer to `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>
<span class="sd">    &lt;https://arxiv.org/abs/1810.04805&gt;`_.</span>

<span class="sd">    GeLU is defined as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        GELU(x_i) = x_i*P(X &lt; x_i)</span>

<span class="sd">    where :math:`P` is the cumulative distribution function of the standard Gaussian distribution,</span>
<span class="sd">    :math:`x_i` is the input element.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input of the activation function GeLU, the data type is float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; result = ops.GeLU()(x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [0.841192  1.9545976  2.9963627]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_gelu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span></div>

<span class="n">gelu_op</span><span class="o">=</span><span class="n">GeLU</span><span class="p">()</span>


<div class="viewcode-block" id="Geqrf"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Geqrf.html#mindspore.ops.Geqrf">[文档]</a><span class="k">class</span> <span class="nc">Geqrf</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Geqrf()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.geqrf(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.geqrf` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">geqrf_op</span><span class="o">=</span><span class="n">Geqrf</span><span class="p">()</span>


<div class="viewcode-block" id="GreaterEqual"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GreaterEqual.html#mindspore.ops.GreaterEqual">[文档]</a><span class="k">class</span> <span class="nc">GreaterEqual</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.GreaterEqual()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.greater_equal(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.greater_equal` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">greater_equal_op</span><span class="o">=</span><span class="n">GreaterEqual</span><span class="p">()</span>


<div class="viewcode-block" id="Greater"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Greater.html#mindspore.ops.Greater">[文档]</a><span class="k">class</span> <span class="nc">Greater</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Greater()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.greater(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.greater` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_greater</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">]))</span></div>

<span class="n">greater_op</span><span class="o">=</span><span class="n">Greater</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">GridSampler2DGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for GridSampler2D operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **grad** (Tensor) - A 4-D tensor whose dtype is float16 or float32 and whose shape is :math:`(N, C,</span>
<span class="sd">        H_{out}, W_{out})`. The shape is inconsistent with the shape of the output result of forward calculation.</span>
<span class="sd">        - **input_x** (Tensor) - A 4-D tensor whose dtype is the same as `grad` and whose shape is :math:`(N, C,</span>
<span class="sd">        H_{in}, W_{in})`.</span>
<span class="sd">        - **grid** (Tensor) - A 4-D tensor whose dtype is the same as `grad` and whose</span>
<span class="sd">        shape is :math:`(N, H_{out}, W_{out}, 2)`.</span>
<span class="sd">        interpolation_mode (str): An optional string specifying the interpolation method. The optional values are</span>
<span class="sd">            &quot;bilinear&quot; or &quot;nearest&quot;. Default: &quot;bilinear&quot;.</span>
<span class="sd">        padding_mode (str): An optional string specifying the pad method. The optional values are &quot;zeros&quot;, &quot;border&quot; or</span>
<span class="sd">            &quot;reflection&quot;. Default: &quot;zeros&quot;.</span>
<span class="sd">        align_corners (bool): An optional bool. If &quot;true&quot;, the centers of the corner pixels of the input and output</span>
<span class="sd">            tensors are aligned. Defaults to &quot;false&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **dx** (Tensor) - A 4-D tensor whose dtype and shape are the same as `input_x`.</span>
<span class="sd">        - **dgrid** (Tensor) - A 4-D tensor whose dtype and shape are the same as `grid`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `grad`, `input_x` or `grid` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtypes of `grad`, `input_x` and `grid` are inconsistent.</span>
<span class="sd">        TypeError: If the dtype of `grad`, `input_x` or `grid` is not a valid type.</span>
<span class="sd">        TypeError: If `align_corners` is not a boolean value.</span>
<span class="sd">        ValueError: If the rank of `grad`, `input_x` or `grid` is not equal to 4.</span>
<span class="sd">        ValueError: If the first dimension of `grad`, `input_x` and `grid` are inconsistent.</span>
<span class="sd">        ValueError: If the last dimension of `grid` is not equal to 2.</span>
<span class="sd">        ValueError: If `interpolation_mode` is not &quot;bilinear&quot;, &quot;nearest&quot; or a string value.</span>
<span class="sd">        ValueError: If `padding_mode` is not &quot;zeros&quot;, &quot;border&quot;, &quot;reflection&quot; or a string value.</span>
<span class="sd">        ValueError: If the shape of `grad` is inconsistent with the shape of the output result of forward calculation.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;interpolation_mode&quot;</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;padding_mode&quot;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span>


<div class="viewcode-block" id="GridSampler2D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GridSampler2D.html#mindspore.ops.GridSampler2D">[文档]</a><span class="k">class</span> <span class="nc">GridSampler2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operation samples 2d `input_x` by using interpolation based on flow field grid,</span>
<span class="sd">    which is usually gennerated by :func:`mindspore.ops.affine_grid`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.grid_sample` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        interpolation_mode (str, optional): An optional string specifying the interpolation method.</span>
<span class="sd">            The optional values are</span>
<span class="sd">            ``&quot;bilinear&quot;`` or ``&quot;nearest&quot;`` . Default: ``&quot;bilinear&quot;`` .</span>

<span class="sd">            - ``&quot;nearest&quot;``: Nearest neighbor interpolation. Each output pixel is assigned the value of the</span>
<span class="sd">              nearest input pixel. This method is simple and fast but can result in blocky or pixelated outputs.</span>
<span class="sd">            - ``&quot;bilinear&quot;``: Bilinear interpolation. Each output pixel is a weighted average of the four nearest input</span>
<span class="sd">              pixels, computed using bilinear interpolation. This method produces smoother results compared</span>
<span class="sd">              to nearest neighbor interpolation.</span>

<span class="sd">        padding_mode (str, optional): An optional string specifying the pad method.</span>
<span class="sd">            The optional values are ``&quot;zeros&quot;`` , ``&quot;border&quot;`` or ``&quot;reflection&quot;`` . Default: ``&quot;zeros&quot;`` .</span>
<span class="sd">            When the sampling grid is outside input&#39;s bounds, effects of various padding modes are as follows:</span>

<span class="sd">            - ``&quot;zeros&quot;``: Pads the input tensor with zeros.</span>
<span class="sd">            - ``&quot;border&quot;``: Pads the input tensor with the values of the pixels on the border of the tensor.</span>
<span class="sd">            - ``&quot;reflection&quot;``: Pads the input tensor by reflecting the values of the pixels at the</span>
<span class="sd">              boundary of the tensor.</span>

<span class="sd">        align_corners (bool, optional): An optional bool. When set to ``True`` ,</span>
<span class="sd">            the centers of the corner pixels of the input</span>
<span class="sd">            and output tensors are aligned. When set to ``False`` , it is not aligned. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A 4-D tensor with shape</span>
<span class="sd">          :math:`(N, C, H_{in}, W_{in})`. Supported dtypes:</span>

<span class="sd">          - Ascend: float16, float32.</span>
<span class="sd">          - GPU/CPU: float16, float32, float64.</span>

<span class="sd">        - **grid** (Tensor) - A 4-D tensor whose dtype is the same as `input_x` and whose shape is</span>
<span class="sd">          :math:`(N, H_{out}, W_{out}, 2)`.</span>
<span class="sd">          Used to specify the sampling pixel locations normalized by the input spatial</span>
<span class="sd">          dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 4-D Tensor whose dtype is the same as `input_x` and whose shape is :math:`(N, C, H_{out}, W_{out})`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; gridsampler = ops.GridSampler2D(interpolation_mode=&#39;bilinear&#39;, padding_mode=&#39;zeros&#39;, align_corners=True)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(16).reshape((2, 2, 2, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grid = Tensor(np.arange(-9, 9, 0.5).reshape((2, 3, 3, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = gridsampler(input_x, grid)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     0.     0.5  ]]</span>
<span class="sd">          [[ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     1.5    4.5  ]]]</span>
<span class="sd">         [[[10.     8.25   1.375]</span>
<span class="sd">           [ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     0.     0.   ]]</span>
<span class="sd">          [[14.    11.25   1.875]</span>
<span class="sd">           [ 0.     0.     0.   ]</span>
<span class="sd">           [ 0.     0.     0.   ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;interpolation_mode&quot;</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;padding_mode&quot;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">GridSampler3DGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for GridSampler3D operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        - **grad** (Tensor) - A 5-D tensor whose dtype is float32 or float64 and whose shape is :math:`(N, C, D_{out},</span>
<span class="sd">        H_{out}, W_{out})`. The shape is inconsistent with the shape of the output result of forward calculation.</span>
<span class="sd">        - **input_x** (Tensor) - A 5-D tensor whose dtype is the same as `grad` and whose shape is :math:`(N, C,</span>
<span class="sd">        D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **grid** (Tensor) - A 5-D tensor whose dtype is the same as `grad` and whose shape is :math:`(N, D_{out},</span>
<span class="sd">        H_{out}, W_{out}, 3)`.</span>
<span class="sd">        interpolation_mode (str): An optional string specifying the interpolation method. The optional values are</span>
<span class="sd">            &quot;bilinear&quot; or &quot;nearest&quot;. Default: &quot;bilinear&quot;.</span>
<span class="sd">        padding_mode (str): An optional string specifying the pad method. The optional values are &quot;zeros&quot;, &quot;border&quot; or</span>
<span class="sd">            &quot;reflection&quot;. Default: &quot;zeros&quot;.</span>
<span class="sd">        align_corners (bool): An optional bool. If &quot;true&quot;, the centers of the corner pixels of the input and output</span>
<span class="sd">            tensors are aligned. Defaults to &quot;false&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        - **dx** (Tensor) - A 5-D tensor whose dtype and shape are the same as `input_x`.</span>
<span class="sd">        - **dgrid** (Tensor) - A 5-D tensor whose dtype and shape are the same as `grid`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `grad`, `input_x` or `grid` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtypes of `grad`, `input_x` and `grid` are inconsistent.</span>
<span class="sd">        TypeError: If the dtype of `grad`, `input_x` or `grid` is not a valid type.</span>
<span class="sd">        TypeError: If `align_corners` is not a boolean value.</span>
<span class="sd">        ValueError: If the rank of `grad`, `input_x` or `grid` is not equal to 5.</span>
<span class="sd">        ValueError: If the first dimension of `grad`, `input_x` and `grid` are inconsistent.</span>
<span class="sd">        ValueError: If the last dimension of `grid` is not equal to 3.</span>
<span class="sd">        ValueError: If `interpolation_mode` is not &quot;bilinear&quot;, &quot;nearest&quot; or a string value.</span>
<span class="sd">        ValueError: If `padding_mode` is not &quot;zeros&quot;, &quot;border&quot;, &quot;reflection&quot; or a string value.</span>
<span class="sd">        ValueError: If the shape of `grad` is inconsistent with the shape of the output result of forward calculation.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;interpolation_mode&quot;</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;padding_mode&quot;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span>


<div class="viewcode-block" id="GridSampler3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GridSampler3D.html#mindspore.ops.GridSampler3D">[文档]</a><span class="k">class</span> <span class="nc">GridSampler3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given an input and a grid, the output is calculated using the input values</span>
<span class="sd">    and pixel positions in the grid. Only volume (5-D) input is supported.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.grid_sample` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        interpolation_mode (str, optional): An optional string specifying the interpolation method.</span>
<span class="sd">            The optional values are ``&quot;bilinear&quot;`` or ``&quot;nearest&quot;`` . Default: ``&quot;bilinear&quot;`` .</span>

<span class="sd">            - ``&quot;nearest&quot;``: Nearest neighbor interpolation. Each output pixel is assigned the value of the</span>
<span class="sd">              nearest input pixel. This method is simple and fast but can result in blocky or pixelated outputs.</span>
<span class="sd">            - ``&quot;bilinear&quot;``: Bilinear interpolation. Each output pixel is a weighted average of the four nearest input</span>
<span class="sd">              pixels, computed using bilinear interpolation. This method produces smoother results compared</span>
<span class="sd">              to nearest neighbor interpolation.</span>

<span class="sd">        padding_mode (str, optional): An optional string specifying the pad method.</span>
<span class="sd">            The optional values are ``&quot;zeros&quot;`` , ``&quot;border&quot;`` or ``&quot;reflection&quot;`` . Default: ``&quot;zeros&quot;`` .</span>
<span class="sd">            When the sampling grid is outside input&#39;s bounds, effects of various padding modes are as follows:</span>

<span class="sd">            - ``&quot;zeros&quot;``: Pads the input tensor with zeros.</span>
<span class="sd">            - ``&quot;border&quot;``: Pads the input tensor with the values of the pixels on the border of the tensor.</span>
<span class="sd">            - ``&quot;reflection&quot;``: Pads the input tensor by reflecting the values of the pixels at the</span>
<span class="sd">              boundary of the tensor.</span>

<span class="sd">        align_corners (bool, optional): An optional bool specifying alignment method. If set to ``True`` ,</span>
<span class="sd">            the extrema (-1 and 1) are considered as referring to</span>
<span class="sd">            the center points of the input&#39;s corner pixels. If set to ``False`` , they are instead considered as</span>
<span class="sd">            referring to the corner points of the input&#39;s corner pixels, making the sampling more resolution agnostic.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A 5-D tensor with dtype of float16, float32 or float64</span>
<span class="sd">          and shape of :math:`(N, C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **grid** (Tensor) - A 5-D tensor whose dtype is the same as `input_x` and whose shape is :math:`(N, D_{out},</span>
<span class="sd">          H_{out}, W_{out}, 3)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 5-D Tensor whose dtype is the same as `input_x` and whose shape is :math:`(N, C, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; gridsampler = ops.GridSampler3D(interpolation_mode=&#39;bilinear&#39;, padding_mode=&#39;zeros&#39;, align_corners=True)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.arange(32).reshape((2, 2, 2, 2, 2)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grid = Tensor(np.arange(-0.2, 1, 0.1).reshape((2, 2, 1, 1, 3)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = gridsampler(input_x, grid)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[ 3.3     ]]</span>
<span class="sd">           [[ 4.35    ]]]</span>
<span class="sd">          [[[11.300001]]</span>
<span class="sd">           [[12.349999]]]]</span>
<span class="sd">         [[[[21.4     ]]</span>
<span class="sd">           [[22.449999]]]</span>
<span class="sd">          [[[29.4     ]]</span>
<span class="sd">           [[30.449999]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;interpolation_mode&quot;</span><span class="p">,</span> <span class="n">interpolation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;padding_mode&quot;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">HShrinkGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for HShrinkGrad operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        Gradients (Tensor) - the gradients of loss to output of HShrink function.</span>
<span class="sd">        Currently gradients data type only support float16 and float32.</span>
<span class="sd">        Features (Tensor) - Must be the input `input_x` of the forward operator HSHrink.</span>
<span class="sd">        Currently features data type only support float16 and float32.</span>
<span class="sd">        lambd (float): the lambda value for the Hardshrink formulation. Default: 0.5</span>

<span class="sd">    Returns:</span>
<span class="sd">        backprops - Tensor, with the same shape and data type as `features`.</span>

<span class="sd">    Rasise:</span>
<span class="sd">        ValueError: If `lambd` is not a float.</span>
<span class="sd">        ValueError: If shape of `gradients` is not the same as `features`.</span>
<span class="sd">        TypeError: If dtype of `gradients` is not the same as `features`.</span>
<span class="sd">        TypeError: If dtype of `gradients` or `features` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;lambd&quot;</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span><span class="p">)</span>


<div class="viewcode-block" id="HShrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.HShrink.html#mindspore.ops.HShrink">[文档]</a><span class="k">class</span> <span class="nc">HShrink</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard Shrink activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.hardshrink` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        lambd (float, optional): The threshold :math:`\lambda` defined by the Hard Shrink formula. Default: ``0.5`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of Hard Shrink with data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the same shape and data type as the input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0.5,  1,  2.0], [0.0533, 0.0776, -2.1233]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; hshrink = ops.HShrink()</span>
<span class="sd">        &gt;&gt;&gt; output = hshrink(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.      1.      2.    ]</span>
<span class="sd">        [ 0.      0.     -2.1233]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;lambd&quot;</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambd</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">HSigmoidGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the gradient of HSigmoid operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">input_x</span><span class="p">)</span>


<span class="n">hsigmoid_grad_op</span><span class="o">=</span><span class="n">HSigmoidGrad</span><span class="p">()</span>


<div class="viewcode-block" id="HSigmoid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.HSigmoid.html#mindspore.ops.HSigmoid">[文档]</a><span class="k">class</span> <span class="nc">HSigmoid</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard sigmoid activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.hardsigmoid` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; hsigmoid = ops.HSigmoid()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; result = hsigmoid(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [0.3333 0.1666 0.5    0.8335 0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="n">hsigmoid_op</span><span class="o">=</span><span class="n">HSigmoid</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">HSwishGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the gradient of HSwish operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_grad</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">y_grad</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="n">hswish_grad_op</span><span class="o">=</span><span class="n">HSwishGrad</span><span class="p">()</span>


<div class="viewcode-block" id="HSwish"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.HSwish.html#mindspore.ops.HSwish">[文档]</a><span class="k">class</span> <span class="nc">HSwish</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hard swish activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.hardswish` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input Tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; hswish = ops.HSwish()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([-1, -2, 0, 2, 1]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; result = hswish(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [-0.3333  -0.3333  0  1.666  0.6665]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="n">hswish_op</span><span class="o">=</span><span class="n">HSwish</span><span class="p">()</span>


<div class="viewcode-block" id="Identity"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Identity.html#mindspore.ops.Identity">[文档]</a><span class="k">class</span> <span class="nc">Identity</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Identity()</span>
<span class="sd">        out = prim(input_x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.deepcopy(input_x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.deepcopy` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="n">identity_op</span><span class="o">=</span><span class="n">Identity</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">IFFT2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.IFFT2()</span>
<span class="sd">        out = prim(input, s, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.ifft2(input, s, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.ifft2` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span> <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;IFFT2&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>


<span class="n">ifft2_op</span><span class="o">=</span><span class="n">IFFT2</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">IFFT</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.IFFT()</span>
<span class="sd">        out = prim(input, n, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.ifft(input, n, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.ifft` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span> <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;IFFT&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>


<span class="n">ifft_op</span><span class="o">=</span><span class="n">IFFT</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">IFFTN</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.IFFTN()</span>
<span class="sd">        out = prim(input, s, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.ifftn(input, s, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.ifftn` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span> <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;IFFTN&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>


<span class="n">ifftn_op</span><span class="o">=</span><span class="n">IFFTN</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">IFFTShift</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.IFFTShift()</span>
<span class="sd">        out = prim(input, dim)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.ifftshift(input, dim)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.ifftshift` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>


<span class="n">ifftshift_op</span><span class="o">=</span><span class="n">IFFTShift</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">IRFFTGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input1&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input2&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span> <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;IRFFTGrad&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>


<span class="n">irfft_grad_op</span><span class="o">=</span><span class="n">IRFFTGrad</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">IRFFT</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.IRFFT()</span>
<span class="sd">        out = prim(input, n, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.irfft(input, n, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.irfft` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span> <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;IRFFT&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>


<span class="n">irfft_op</span><span class="o">=</span><span class="n">IRFFT</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">IsFinite</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.IsFinite()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.is_finite(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.is_finite` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="n">is_finite_op</span><span class="o">=</span><span class="n">IsFinite</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">LayerNormGradGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the gradient of LayerNormGrad operation.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor to be normalized, float32 or float16.</span>
<span class="sd">        - **dy** (Tensor) - The gradient of LayerNorm&#39;s output y, float32 or float16.</span>
<span class="sd">        - **variance** (Tensor) - The variance of x, float32 or float16.</span>
<span class="sd">        - **mean** (Tensor) - The mean of x, float32 or float16.</span>
<span class="sd">        - **gamma** (Tensor) - The original value of weight gamma initialized in LayerNorm, float32 or float16.</span>
<span class="sd">          Default: &#39;ones&#39;.</span>
<span class="sd">        - **d_dx** (Tensor) - The gradient of dx, where dx is the gradient of LayerNorm&#39;s input x, float32 or float16.</span>
<span class="sd">        - **d_dg** (Tensor) - The gradient of dg, where dg is the gradient of LayerNorm&#39;s weight gamma,</span>
<span class="sd">          float32 or float16.</span>
<span class="sd">        - **d_db** (Tensor) - The gradient of db, where db is the gradient of LayerNorm&#39;s weight beta,</span>
<span class="sd">          float32 or float16.</span>
<span class="sd">        - **begin_norm_axis** (int) - The begin axis for the input to apply layernorm. Default: 1.</span>
<span class="sd">        - **begin_params_axis** (int) - The begin axis for the parameter input to apply layernorm. Default: 1.</span>


<span class="sd">    Outputs:</span>
<span class="sd">        Tuple[Tensor], tuple of 3 Tensors (the gradients of layernormgrad x, dy, gamma).</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the 8 inputs don&#39;t have the same dtype.</span>
<span class="sd">        ValueError: If x, dy, d_dx don&#39;t have the same shape.</span>
<span class="sd">        ValueError: If variance, mean don&#39;t have the same shape.</span>
<span class="sd">        ValueError: If gamma, d_dg, d_db don&#39;t have the same shape.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_norm_axis&quot;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_params_axis&quot;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">d_dx</span><span class="p">,</span> <span class="n">d_dg</span><span class="p">,</span> <span class="n">d_db</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">d_dx</span><span class="p">,</span> <span class="n">d_dg</span><span class="p">,</span> <span class="n">d_db</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_params_axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LayerNormGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the layer Normalization to the input array.</span>

<span class="sd">    This operator will calculate the input gradients of layernorm.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        x (Tensor): The inputs of layer norm op.</span>
<span class="sd">        dy (Tensor): The gradient of outputs of layer norm op.</span>
<span class="sd">        variance (Tensor): The variance of x.</span>
<span class="sd">        mean (Tensor): The mean of x.</span>
<span class="sd">        gamma (Tensor): The weights of normalized elements.</span>
<span class="sd">        begin_norm_axis (int): The begin axis for the input to apply layernorm. Default: 1.</span>
<span class="sd">        begin_params_axis (int): The begin axis for the parameter input to apply layernorm. Default: 1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[int], tuple of 3 values (the gradients of layernorm input,  gamma, beta).</span>

<span class="sd">        pd_x (Tensor): the gradients of layernorm input x.</span>
<span class="sd">        pd_gamma (Tensor): the gradients of gamma.</span>
<span class="sd">        pd_beta (Tensor): the gradients of beta.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_norm_axis&quot;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_params_axis&quot;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_params_axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">LayerNormGradV3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_norm_axis&quot;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_params_axis&quot;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">variance</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_params_axis</span><span class="p">)</span>


<div class="viewcode-block" id="LayerNorm"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LayerNorm.html#mindspore.ops.LayerNorm">[文档]</a><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Layer Normalization to the input tensor.</span>

<span class="sd">    This operator will normalize the input tensor on given axis. LayerNorm is described in the paper</span>
<span class="sd">    `Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        begin_norm_axis (int): The begin axis of the `input_x` to apply LayerNorm,</span>
<span class="sd">            the value must be in [-1, rank(input_x)). Default: ``1`` .</span>
<span class="sd">        begin_params_axis (int): The begin axis of the parameter input (`gamma`, `beta`) to</span>
<span class="sd">            apply LayerNorm, the value must be in [-1, rank(input_x)). Default: ``1`` .</span>
<span class="sd">        epsilon (float): A value added to the denominator for numerical stability(:math:`\epsilon`). Default: ``1e-7`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, \ldots)`.</span>
<span class="sd">          The input of LayerNorm. Supported dtypes: float16, float32, float64.</span>
<span class="sd">        - **gamma** (Tensor) - Learnable parameter :math:`\gamma` . Tensor of shape `input_x_shape[begin_params_axis:]`. Supported dtypes: float16, float32, float64.</span>
<span class="sd">        - **beta** (Tensor) - Learnable parameter :math:`\beta` . Tensor of shape `input_x_shape[begin_params_axis:]`. Supported dtypes: float16, float32, float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], tuple of 3 tensors, the normalized input and the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The normalized input, has the same type and shape as the `input_x`.</span>
<span class="sd">        - **mean** (Tensor) - The first `begin_norm_axis` dimensions of `mean` shape is the same as `input_x`,</span>
<span class="sd">          and the remaining dimensions are 1. Suppose the shape of the `input_x` is :math:`(x_1, x_2, \ldots, x_R)`,</span>
<span class="sd">          the shape of the `mean` is :math:`(x_1, \ldots, x_{begin\_params\_axis}, 1, \ldots, 1)`</span>
<span class="sd">          (when `begin_params_axis=0`, the shape of `mean` is :math:`(1, \ldots, 1)` ).</span>
<span class="sd">        - **variance** (Tensor) - Shape is the same as `mean` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `begin_norm_axis` or `begin_params_axis` is not an int.</span>
<span class="sd">        TypeError: If `epsilon` is not a float.</span>
<span class="sd">        TypeError: If `input_x`, `gamma` or `beta` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [1, 2, 3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gamma = Tensor(np.ones([3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta = Tensor(np.ones([3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; layer_norm = ops.LayerNorm()</span>
<span class="sd">        &gt;&gt;&gt; output, mean, variance = layer_norm(input_x, gamma, beta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-0.2247448  1.         2.2247448]</span>
<span class="sd">         [-0.2247448  1.         2.2247448]]</span>
<span class="sd">        &gt;&gt;&gt; print(mean)</span>
<span class="sd">        [[2.]</span>
<span class="sd">         [2.]]</span>
<span class="sd">        &gt;&gt;&gt; print(variance)</span>
<span class="sd">        [[0.6666667]</span>
<span class="sd">         [0.6666667]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_norm_axis&quot;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_params_axis&quot;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_params_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">LayerNormV3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_norm_axis&quot;</span><span class="p">,</span> <span class="n">begin_norm_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_params_axis&quot;</span><span class="p">,</span> <span class="n">begin_params_axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_norm_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_params_axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span>


<div class="viewcode-block" id="LessEqual"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LessEqual.html#mindspore.ops.LessEqual">[文档]</a><span class="k">class</span> <span class="nc">LessEqual</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LessEqual()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.less_equal(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.less_equal` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_less_equal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">]))</span></div>

<span class="n">less_equal_op</span><span class="o">=</span><span class="n">LessEqual</span><span class="p">()</span>


<div class="viewcode-block" id="Less"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Less.html#mindspore.ops.Less">[文档]</a><span class="k">class</span> <span class="nc">Less</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Less()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.less(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.less` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">less_op</span><span class="o">=</span><span class="n">Less</span><span class="p">()</span>


<div class="viewcode-block" id="LinSpace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LinSpace.html#mindspore.ops.LinSpace">[文档]</a><span class="k">class</span> <span class="nc">LinSpace</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor whose value is `num` evenly spaced in the interval `start` and `stop` (including `start` and</span>
<span class="sd">    `stop`), and the length of the output Tensor is `num`.</span>

<span class="sd">    Refer to :func:`mindspore.ops.linspace` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **start** (Tensor) -  Start value of interval, 0-D Tensor with dtype float32 or float64.</span>
<span class="sd">        - **stop** (Tensor) - Last value of interval, 0-D Tensor with dtype float32 or float64.</span>
<span class="sd">        - **num** (Union[int, Tensor]) - Number of ticks in the interval, inclusive of `start` and `stop`.</span>
<span class="sd">          Must be a positive integer. When the input is Tensor, it must be a 0-D Tensor with dtype int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as `start`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; start = Tensor(1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; stop = Tensor(10, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; num = 5</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LinSpace()(start, stop, num)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 1.    3.25  5.5   7.75 10.  ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">num</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span></div>


<span class="n">lin_space_op</span><span class="o">=</span><span class="n">LinSpace</span><span class="p">()</span>


<div class="viewcode-block" id="Log1p"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Log1p.html#mindspore.ops.Log1p">[文档]</a><span class="k">class</span> <span class="nc">Log1p</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Log1p()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.log1p(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.log1p` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">log1p_op</span><span class="o">=</span><span class="n">Log1p</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">LogMatrixDeterminant</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the sign and the log of the absolute value of the determinant of one or more square matrices.</span>

<span class="sd">    Note:</span>
<span class="sd">        The type of output always be real-value, even `input` is complex.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): A matrix to be calculated, its shape is :math:`(..., M, M)`.</span>
<span class="sd">        The matrix must be at least two dimensions, and the last two</span>
<span class="sd">        dimensions must be the same size. Data type must be float32, float64, complex64 or complex128.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor. The signs of the log determinants. The shape is :math:`input.shape[:-2]`.</span>

<span class="sd">        Tensor. The absolute values of the log determinants. The shape is :math:`input.shape[:-2]`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input` not float32, float64, complex64 or complex128.</span>
<span class="sd">        ValueError: If the last two dimensions of `input` is not same size.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sign, output = ops.LogMatrixDeterminant()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(sign)</span>
<span class="sd">        [-1.   1.]</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2.80336046e+00    3.04452229e+00]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="n">log_matrix_determinant_op</span><span class="o">=</span><span class="n">LogMatrixDeterminant</span><span class="p">()</span>


<div class="viewcode-block" id="Log"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Log.html#mindspore.ops.Log">[文档]</a><span class="k">class</span> <span class="nc">Log</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Log()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.log(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.log` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;cust_aicpu&quot;</span><span class="p">,</span> <span class="s1">&#39;Log&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;base&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;scale&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;shift&quot;</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span></div>

<span class="n">log_op</span><span class="o">=</span><span class="n">Log</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">LogSoftmaxGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradient for the Log Softmax activation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="LogSoftmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogSoftmax.html#mindspore.ops.LogSoftmax">[文档]</a><span class="k">class</span> <span class="nc">LogSoftmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.LogSoftmax(axis)</span>
<span class="sd">        out = prim(logits)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.log_softmax(logits, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.log_softmax` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="LogicalAnd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogicalAnd.html#mindspore.ops.LogicalAnd">[文档]</a><span class="k">class</span> <span class="nc">LogicalAnd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical AND&quot; of two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.logical_and` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, bool]) - The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">          converted to bool.</span>
<span class="sd">        - **y** (Union[Tensor, bool]) - The second input is a bool when the first input is a tensor or</span>
<span class="sd">          a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the `x` and `y` after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_and = ops.LogicalAnd()</span>
<span class="sd">        &gt;&gt;&gt; output = logical_and(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True False False]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalAnd()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        False</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalAnd()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        False</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalAnd()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<span class="n">logical_and_op</span><span class="o">=</span><span class="n">LogicalAnd</span><span class="p">()</span>


<div class="viewcode-block" id="LogicalNot"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogicalNot.html#mindspore.ops.LogicalNot">[文档]</a><span class="k">class</span> <span class="nc">LogicalNot</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical NOT&quot; of a tensor element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.logical_not` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor, the dtype must be bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the `x`, and the dtype is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_not = ops.LogicalNot()</span>
<span class="sd">        &gt;&gt;&gt; output = logical_not(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False  True False]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">logical_not_op</span><span class="o">=</span><span class="n">LogicalNot</span><span class="p">()</span>


<div class="viewcode-block" id="LogicalOr"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogicalOr.html#mindspore.ops.LogicalOr">[文档]</a><span class="k">class</span> <span class="nc">LogicalOr</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical OR&quot; of two tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.logical_or` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, bool]) - The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">          converted to bool.</span>
<span class="sd">        - **y** (Union[Tensor, bool]) - The second input is a bool when the first input is a tensor or</span>
<span class="sd">          a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the `x` and `y` after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_or = ops.LogicalOr()</span>
<span class="sd">        &gt;&gt;&gt; output = logical_or(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  True  True]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalOr()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalOr()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalOr()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [True True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<span class="n">logical_or_op</span><span class="o">=</span><span class="n">LogicalOr</span><span class="p">()</span>


<div class="viewcode-block" id="LogicalXor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LogicalXor.html#mindspore.ops.LogicalXor">[文档]</a><span class="k">class</span> <span class="nc">LogicalXor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the &quot;logical XOR&quot; of two tensors element-wise.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.logical_xor` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, bool]) - The first input is a bool or a tensor whose data type can be implicitly</span>
<span class="sd">          converted to bool.</span>
<span class="sd">        - **y** (Union[Tensor, bool]) - The second input is a bool when the first input is a tensor or</span>
<span class="sd">          a tensor whose data type can be implicitly converted to bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the `x` and `y` after broadcasting, and the data type is bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([True, False, True]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; logical_xor = ops.LogicalXor()</span>
<span class="sd">        &gt;&gt;&gt; output = logical_xor(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ False True True]</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(1, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalXor()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(0, mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalXor()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; x = True</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([True, False]), mindspore.bool_)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.LogicalXor()(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [False True]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">logical_xor_op</span><span class="o">=</span><span class="n">LogicalXor</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">LogitGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes LogitGrad of input element-wise.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same type as input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>


<div class="viewcode-block" id="Logit"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Logit.html#mindspore.ops.Logit">[文档]</a><span class="k">class</span> <span class="nc">Logit</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the logit of a tensor element-wise. Element in `x` is clamped to [eps, 1-eps].</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.logit` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        eps (float, optional): The epsilon. The input clamp bound is defined as [eps, 1-eps]. Default: ``-1.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor of type float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and dtype as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.1, 0.2, 0.3]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Logit(eps=1e-5)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-2.1972246 -1.3862944 -0.8472978]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaskedFill"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaskedFill.html#mindspore.ops.MaskedFill">[文档]</a><span class="k">class</span> <span class="nc">MaskedFill</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.MaskedFill()</span>
<span class="sd">        out = prim(input_x, mask, value)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.masked_fill(input_x, mask, value)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.masked_fill` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_masked_fill</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">input_x</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">]))</span></div>

<span class="n">masked_fill_op</span><span class="o">=</span><span class="n">MaskedFill</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">MatrixDeterminant</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the value of the determinant for one or more square matrices.</span>

<span class="sd">    Refer to :func:`mindspore.ops.det` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A matrix to be calculated. The matrix must be at least two dimensions, and the last two</span>
<span class="sd">        dimensions must be the same size.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is `x_shape[:-2]`, the dtype is same as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[-4.5, -1.5], [7.0, 6.0]], [[2.5, 0.5], [3.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.MatrixDeterminant()</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [-16.5 21. ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="n">matrix_determinant_op</span><span class="o">=</span><span class="n">MatrixDeterminant</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">MatrixExp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.MatrixExp()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.matrix_exp(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.matrix_exp` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="n">matrix_exp_op</span><span class="o">=</span><span class="n">MatrixExp</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">MaximumGradGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Grad for maximum grad.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad_y</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_x&quot;</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_y&quot;</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_y</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MaximumGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Grad for maximum.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad_y</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_x&quot;</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_y&quot;</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_y</span><span class="p">)</span>


<div class="viewcode-block" id="Maximum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Maximum.html#mindspore.ops.Maximum">[文档]</a><span class="k">class</span> <span class="nc">Maximum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Maximum()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.maximum(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.maximum` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">maximum_op</span><span class="o">=</span><span class="n">Maximum</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">MinimumGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Grad for minimum.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grad_x</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grad_y</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_x&quot;</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;grad_y&quot;</span><span class="p">,</span> <span class="n">grad_y</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">grads</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_y</span><span class="p">)</span>


<div class="viewcode-block" id="Minimum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Minimum.html#mindspore.ops.Minimum">[文档]</a><span class="k">class</span> <span class="nc">Minimum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Minimum()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.minimum(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.minimum` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">minimum_op</span><span class="o">=</span><span class="n">Minimum</span><span class="p">()</span>


<div class="viewcode-block" id="Mul"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Mul.html#mindspore.ops.Mul">[文档]</a><span class="k">class</span> <span class="nc">Mul</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Mul()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.mul(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.mul` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_READ</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;other&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_READ</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_mul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">]))</span></div>

<span class="n">mul_op</span><span class="o">=</span><span class="n">Mul</span><span class="p">()</span>


<div class="viewcode-block" id="NanToNum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NanToNum.html#mindspore.ops.NanToNum">[文档]</a><span class="k">class</span> <span class="nc">NanToNum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Replaces `NaN`, positive infinity and negative infinity values in the input Tensor with the values</span>
<span class="sd">    specified by `nan`, `posinf` and `neginf` respectively.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.nan_to_num` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        nan (float, optional): The value to replace `NaN`. Default value is ``None`` .</span>
<span class="sd">        posinf (float, optional): If a Number, the value to replace positive infinity values with. If None, positive</span>
<span class="sd">            infinity values are replaced with the greatest finite value representable by `x`&#39;s dtype.</span>
<span class="sd">            Default value is ``None`` .</span>
<span class="sd">        neginf (float, optional): if a Number, the value to replace negative infinity values with. If None, negative</span>
<span class="sd">            infinity values are replaced with the lowest finite value representable by `x`&#39;s dtype.</span>
<span class="sd">            Default value is ``None`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input Tensor of any dimensions. Supported data types: float32 or float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and dtype as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; nan_to_num = ops.NanToNum()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([float(&#39;nan&#39;), float(&#39;inf&#39;), -float(&#39;inf&#39;), 3.14]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = nan_to_num(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.0000000e+00  3.4028235e+38 -3.4028235e+38  3.1400001e+00]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nan</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">posinf</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">neginf</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">,</span> <span class="n">nan</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;posinf&quot;</span><span class="p">,</span> <span class="n">posinf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;neginf&quot;</span><span class="p">,</span> <span class="n">neginf</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">posinf</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">neginf</span><span class="p">)</span></div>


<div class="viewcode-block" id="Neg"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Neg.html#mindspore.ops.Neg">[文档]</a><span class="k">class</span> <span class="nc">Neg</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Neg()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.neg(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.neg` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_neg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span></div>

<span class="n">neg_op</span><span class="o">=</span><span class="n">Neg</span><span class="p">()</span>


<div class="viewcode-block" id="NextAfter"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NextAfter.html#mindspore.ops.NextAfter">[文档]</a><span class="k">class</span> <span class="nc">NextAfter</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.NextAfter()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.nextafter(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.nextafter` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">next_after_op</span><span class="o">=</span><span class="n">NextAfter</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">NLLLossGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradients of `NLLLoss`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;ignore_index&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">loss_grad</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">total_weight</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">loss_grad</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">total_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">)</span>


<div class="viewcode-block" id="NLLLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NLLLoss.html#mindspore.ops.NLLLoss">[文档]</a><span class="k">class</span> <span class="nc">NLLLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the negative log likelihood loss between logits and labels.</span>

<span class="sd">    The nll loss with :math:`reduction = none` can be described as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=L=\left\{l_{1}, \ldots, l_{N}\right\}^{\top},</span>
<span class="sd">        \quad l_{n}=-w_{t_{n}} x_{n, t_{n}},</span>
<span class="sd">        \quad w_{c}=\text { weight }[c] \cdot 1</span>

<span class="sd">    where :math:`x` is the logits, :math:`t` is the labels, :math:`w` is the weight,</span>
<span class="sd">    N is the batch size, :math:`c` belonging to [0, C-1] is class index, where :math:`C` is the number of classes.</span>

<span class="sd">    If :math:`reduction \neq none` (default ``&#39;mean&#39;`` ), then</span>

<span class="sd">    .. math::</span>

<span class="sd">        \ell(x, t)=\left\{\begin{array}{ll}</span>
<span class="sd">        \sum_{n=1}^{N} \frac{1}{\sum_{n=1}^{N} w_{t n}} l_{n}, &amp; \text { if reduction }=\text { &#39;mean&#39;; } \\</span>
<span class="sd">        \sum_{n=1}^{N} l_{n}, &amp; \text { if reduction }=\text { &#39;sum&#39; }</span>
<span class="sd">        \end{array}\right.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">        ignore_index (int): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. Default: ``-100`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type only supports float32 or float16.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N,)`, where each value belong to</span>
<span class="sd">          :math:`[0, C-1]`. Data type only supports int32 or int64.</span>
<span class="sd">        - **weight** (Tensor) - The rescaling weight to each class, with shape :math:`(C,)` and data type only</span>
<span class="sd">          supports float32 or float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors composed with `loss` and `total_weight`.</span>

<span class="sd">        - **loss** (Tensor) - When `reduction` is ``&#39;none&#39;`` and `logits` is a 2D tensor,</span>
<span class="sd">          the `loss` shape is :math:`(N,)`. Otherwise, the `loss` is a scalar.</span>
<span class="sd">          The data type is the same with `input&#39;s`.</span>
<span class="sd">        - **total_weight** (Tensor) - The `total_weight` is a scalar. The data type is the same with `weight&#39;s`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits` or `weight` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `labels` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `logits` is not a one or two dimension tensor, `labels` and `weight` are not</span>
<span class="sd">                    one dimension tensors.</span>
<span class="sd">                    When `logits` is a two dimension tensor, the first dimension of `logits` is not equal to `labels`,</span>
<span class="sd">                    and second dimension of `logits` is not equal to `weight`.</span>
<span class="sd">                    When `logits` is a one dimension tensor, the dimensions of `logits`, `labels`</span>
<span class="sd">                    and `weight` should be equal to each other.</span>
<span class="sd">        ValueError: If the value of `labels` exceed :math:`[0, C-1]`, where :math:`C` is the number of classes.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[0.5488135, 0.71518934],</span>
<span class="sd">        ...                           [0.60276335, 0.5448832],</span>
<span class="sd">        ...                           [0.4236548, 0.6458941]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0, 0, 0]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([0.3834415, 0.79172504]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; nll_loss = ops.NLLLoss(reduction=&quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt; loss, weight = nll_loss(logits, labels, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        -0.52507716</span>
<span class="sd">        &gt;&gt;&gt; print(weight)</span>
<span class="sd">        1.1503246</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;ignore_index&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">)</span></div>


<div class="viewcode-block" id="NonZero"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NonZero.html#mindspore.ops.NonZero">[文档]</a><span class="k">class</span> <span class="nc">NonZero</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.NonZero()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.nonzero(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.nonzero` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">non_zero_op</span><span class="o">=</span><span class="n">NonZero</span><span class="p">()</span>


<div class="viewcode-block" id="NotEqual"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.NotEqual.html#mindspore.ops.NotEqual">[文档]</a><span class="k">class</span> <span class="nc">NotEqual</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.NotEqual()</span>
<span class="sd">        out = prim(input, other)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.not_equal(input, other)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.not_equal` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span></div>


<span class="n">not_equal_op</span><span class="o">=</span><span class="n">NotEqual</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">NPUClearFloatStatusV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare to NPUClearStatus</span>
<span class="sd">    Clear the flag for storage overflow status. This flag is located in a register at a</span>
<span class="sd">    fixed address on the `Ascend` device, and overflow information is automatically</span>
<span class="sd">    written to this register.</span>
<span class="sd">    The flag is a one-dimensional Tensor with shape :math:`(8,)` and data type `mindspore.dtype.int32`.</span>
<span class="sd">    If the value of flag is zero, no overflow has occurred, otherwise, overflow.</span>
<span class="sd">    When performing overflow detection on the network, you should first call `NPUClearFloatStatusV2` to</span>
<span class="sd">    reset the register before the detection, and then call `NPUGetFloatStatusV2` to get the register</span>
<span class="sd">    status after the network execution is completed.</span>

<span class="sd">    Note:</span>
<span class="sd">        - In order to avoid mis-optimization by the compiler, additional input and output are added to</span>
<span class="sd">        this operator. The input and output are defined as a shape of: math:`(8,)` and data type of</span>
<span class="sd">        `mindspore.dtype.int32` Tensor, meaningless.</span>
<span class="sd">        - Since this op lacks contextual dependencies with parameters in the network,</span>
<span class="sd">        :class:`mindspore.ops.Depend` needs to be used to ensure order of execution.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** Tensor, an additional input created to avoid compiler optimization, is specified as shape :math:`(8,)`,</span>
<span class="sd">    data type is `mindspore.dtype.int32`, and has no actual meaning..</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** Tensor, shape and data type are the same as input, meaningless.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, nn, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import NPUGetFloatStatusV2, NPUClearFloatStatusV2</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.clear_status = NPUClearFloatStatusV2()</span>
<span class="sd">        ...         self.get_status = NPUGetFloatStatusV2()</span>
<span class="sd">        ...         self.sub = ops.Sub()</span>
<span class="sd">        ...         self.neg = ops.Neg()</span>
<span class="sd">        ...         self.equal = ops.Equal()</span>
<span class="sd">        ...         self.reduce_all = ops.ReduceAll(keep_dims=False)</span>
<span class="sd">        ...         self.base = Tensor([0], dtype=ms.int32)</span>
<span class="sd">        ...         self.logic_not = ops.LogicalNot()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         init = Tensor([0]*8, dtype=ms.int32)</span>
<span class="sd">        ...         clear_status = self.clear_status(init)</span>
<span class="sd">        ...         x = ops.depend(x, clear_status)</span>
<span class="sd">        ...         res = self.sub(x, self.neg(x))</span>
<span class="sd">        ...         init = ops.depend(init, res)</span>
<span class="sd">        ...         get_status = self.get_status(init)</span>
<span class="sd">        ...         flag = self.equal(self.base, get_status)</span>
<span class="sd">        ...         overall_finite = self.reduce_all(flag)</span>
<span class="sd">        ...         overflow = self.logic_not(overall_finite)</span>
<span class="sd">        ...         return overflow</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; value = 65504</span>
<span class="sd">        &gt;&gt;&gt; data = np.full((2, 3), value, dtype=np.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(data, dtype=ms.float16)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; value = 10</span>
<span class="sd">        &gt;&gt;&gt; data = np.full((2, 3), value, dtype=np.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(data, dtype=ms.float16)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="n">npu_clear_float_status_v2_op</span><span class="o">=</span><span class="n">NPUClearFloatStatusV2</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">NPUGetFloatStatusV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the flag for storage overflow status. This flag is located in a register at a</span>
<span class="sd">    fixed address on the `Ascend` device, and overflow information is automatically</span>
<span class="sd">    written to this register.</span>
<span class="sd">    The flag is a one-dimensional Tensor with shape :math:`(8,)` and data type `mindspore.dtype.int32`.</span>
<span class="sd">    If the value of flag is zero, no overflow has occurred, otherwise, overflow.</span>
<span class="sd">    When performing overflow detection on the network, you should first call `NPUClearFloatStatusV2` to</span>
<span class="sd">    reset the register before the detection, and then call `NPUGetFloatStatusV2` to get the register</span>
<span class="sd">    status after the network execution is completed.</span>

<span class="sd">    Note:</span>
<span class="sd">        - In order to avoid mis-optimization by the compiler, additional input is added to</span>
<span class="sd">        this operator. The input is defined as a shape of: math:`(8,)` and data type of</span>
<span class="sd">        `mindspore.dtype.int32` Tensor, meaningless.</span>
<span class="sd">        - Since this op lacks contextual dependencies with parameters in the network,</span>
<span class="sd">        :class:`mindspore.ops.Depend` needs to be used to ensure order of execution.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** Tensor, an additional input created to avoid compiler optimization, is specified as shape :math:`(8,)`,</span>
<span class="sd">        data type is `mindspore.dtype.int32`, and has no actual meaning.</span>
<span class="sd">        Usually use the output of `NPUClearFloatStatusV2`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** Tensor, shape and data type are the same as input. If all are zero, it means no overflow, otherwise, overflow.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, nn, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import NPUGetFloatStatusV2, NPUClearFloatStatusV2</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.clear_status = NPUClearFloatStatusV2()</span>
<span class="sd">        ...         self.get_status = NPUGetFloatStatusV2()</span>
<span class="sd">        ...         self.sub = ops.Sub()</span>
<span class="sd">        ...         self.neg = ops.Neg()</span>
<span class="sd">        ...         self.equal = ops.Equal()</span>
<span class="sd">        ...         self.reduce_all = ops.ReduceAll(keep_dims=False)</span>
<span class="sd">        ...         self.base = Tensor([0], dtype=ms.int32)</span>
<span class="sd">        ...         self.logic_not = ops.LogicalNot()</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         init = Tensor([0]*8, dtype=ms.int32)</span>
<span class="sd">        ...         clear_status = self.clear_status(init)</span>
<span class="sd">        ...         x = ops.depend(x, clear_status)</span>
<span class="sd">        ...         res = self.sub(x, self.neg(x))</span>
<span class="sd">        ...         init = ops.depend(init, res)</span>
<span class="sd">        ...         get_status = self.get_status(init)</span>
<span class="sd">        ...         flag = self.equal(self.base, get_status)</span>
<span class="sd">        ...         overall_finite = self.reduce_all(flag)</span>
<span class="sd">        ...         overflow = self.logic_not(overall_finite)</span>
<span class="sd">        ...         return overflow</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; value = 65504</span>
<span class="sd">        &gt;&gt;&gt; data = np.full((2, 3), value, dtype=np.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(data, dtype=ms.float16)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; value = 10</span>
<span class="sd">        &gt;&gt;&gt; data = np.full((2, 3), value, dtype=np.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(data, dtype=ms.float16)</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; res = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(res)</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>


<span class="n">npu_get_float_status_v2_op</span><span class="o">=</span><span class="n">NPUGetFloatStatusV2</span><span class="p">()</span>


<div class="viewcode-block" id="OneHot"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.OneHot.html#mindspore.ops.OneHot">[文档]</a><span class="k">class</span> <span class="nc">OneHot</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a one-hot tensor.</span>

<span class="sd">    The locations represented by indices in `indices` take value `on_value`, while all</span>
<span class="sd">    other locations take value `off_value`.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the input indices is rank `N`, the output will have rank `N+1`. The new axis is created at dimension `axis`. </span>
<span class="sd">        On Ascend, if `on_value` is Int64 dtype, `indices` must be Int64 dtype, and the value for `on_value` and </span>
<span class="sd">        `off_value` can only be 1 and 0.</span>
<span class="sd">        </span>
<span class="sd">    Args:</span>
<span class="sd">        axis (int): Position to insert the value. e.g. If shape of `indices` is :math:`(N, C)`, and `axis` is -1,</span>
<span class="sd">            the output shape will be :math:`(N, C, D)`, If `axis` is 0, the output shape will be :math:`(D, N, C)`.</span>
<span class="sd">            Default: ``-1`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices. Tensor of shape :math:`(X_0, \ldots, X_n)`.</span>
<span class="sd">          Data type must be int32 or int64.</span>
<span class="sd">        - **depth** (Union[int, Tensor]) - A scalar defining the depth of the one-hot dimension.</span>
<span class="sd">        - **on_value** (Tensor) - A value to fill in output when `indices[j] = i`.</span>
<span class="sd">        - **off_value** (Tensor) - A value to fill in output when `indices[j] != i`.</span>
<span class="sd">          It has the same data type as `on_value`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, one-hot tensor. Tensor of shape :math:`(X_0, \ldots, X_{axis}, \text{depth} ,X_{axis+1}, \ldots, X_n)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` or `depth` is not an int.</span>
<span class="sd">        TypeError: If dtype of `on_value` is not int32, int64, float16 or float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32 or int64.</span>
<span class="sd">        TypeError: If `indices`, `on_value` or `off_value` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is not in range [-1, len(indices_shape)].</span>
<span class="sd">        ValueError: If `depth` is less than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; depth, on_value, off_value = 3, Tensor(1.0, mindspore.float32), Tensor(0.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; onehot = ops.OneHot()</span>
<span class="sd">        &gt;&gt;&gt; output = onehot(indices, depth, on_value, off_value)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1. 0. 0.]</span>
<span class="sd">         [0. 1. 0.]</span>
<span class="sd">         [0. 0. 1.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">on_value</span><span class="p">,</span> <span class="n">off_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span></div>


<div class="viewcode-block" id="OnesLike"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.OnesLike.html#mindspore.ops.OnesLike">[文档]</a><span class="k">class</span> <span class="nc">OnesLike</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor with a value of 1 and its shape and data type is the same as the input.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ones_like` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of any dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x` but filled with ones.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0, 1], [2, 1]]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.OnesLike()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1 1]</span>
<span class="sd">        [1 1]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="n">ones_like_op</span><span class="o">=</span><span class="n">OnesLike</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">PagedAttentionMask</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.PagedAttentionMask(head_num, scale_value, kv_head_num)</span>
<span class="sd">        out = prim(query, key_cache, value_cache, block_tables, context_lens, alibi_mask)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.paged_attention_mask(query, key_cache, value_cache, block_tables, context_lens, alibi_mask, head_num, scale_value, kv_head_num)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.paged_attention_mask` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">head_num</span><span class="p">,</span> <span class="n">scale_value</span><span class="p">,</span> <span class="n">kv_head_num</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;head_num&quot;</span><span class="p">,</span> <span class="n">head_num</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;scale_value&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;kv_head_num&quot;</span><span class="p">,</span> <span class="n">kv_head_num</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">block_tables</span><span class="p">,</span> <span class="n">context_lens</span><span class="p">,</span> <span class="n">alibi_mask</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">block_tables</span><span class="p">,</span> <span class="n">context_lens</span><span class="p">,</span> <span class="n">alibi_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_head_num</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PagedAttention</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.PagedAttention(head_num, scale_value, kv_head_num)</span>
<span class="sd">        out = prim(query, key_cache, value_cache, block_tables, context_lens)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.paged_attention(query, key_cache, value_cache, block_tables, context_lens, head_num, scale_value, kv_head_num)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.paged_attention` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">head_num</span><span class="p">,</span> <span class="n">scale_value</span><span class="p">,</span> <span class="n">kv_head_num</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;head_num&quot;</span><span class="p">,</span> <span class="n">head_num</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;scale_value&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;kv_head_num&quot;</span><span class="p">,</span> <span class="n">kv_head_num</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">block_tables</span><span class="p">,</span> <span class="n">context_lens</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">block_tables</span><span class="p">,</span> <span class="n">context_lens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_head_num</span><span class="p">)</span>


<div class="viewcode-block" id="Pow"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Pow.html#mindspore.ops.Pow">[文档]</a><span class="k">class</span> <span class="nc">Pow</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Pow()</span>
<span class="sd">        out = prim(input, exponent)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.pow(input, exponent)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.pow` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">exponent</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">exponent</span><span class="p">)</span></div>


<span class="n">pow_op</span><span class="o">=</span><span class="n">Pow</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">PReLUGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gradients of PReLU operation.</span>

<span class="sd">    Note:</span>
<span class="sd">        1-dimensional input_x is not supported.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **dy** (Tensor) - Representing the backprop of the next layer.</span>
<span class="sd">        - **x** (Tensor) - Must be the input `x` of forward operator PRelu.</span>
<span class="sd">        - **weight** (Tensor) - Float Tensor, w &gt; 0, must be the input `weight` of forward operator PRelu.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **dx** (Tensor), with the same type as `x`.</span>
<span class="sd">        - **dw** (Tensor), with the same type as `weight`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="n">prelu_grad_op</span><span class="o">=</span><span class="n">PReLUGrad</span><span class="p">()</span>


<div class="viewcode-block" id="PReLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.PReLU.html#mindspore.ops.PReLU">[文档]</a><span class="k">class</span> <span class="nc">PReLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.PReLU()</span>
<span class="sd">        out = prim(x, weight)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.prelu(x, weight)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.prelu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<span class="n">prelu_op</span><span class="o">=</span><span class="n">PReLU</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">PromptKVCache</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.PromptKVCache(align_mode)</span>
<span class="sd">        out = prim(cache, update, valid_seq_len, batch_index, seq_len_axis, new_max_seq_len, cur_max_seq_len)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.prompt_k_v_cache(cache, update, valid_seq_len, batch_index, seq_len_axis, new_max_seq_len, cur_max_seq_len, align_mode)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.prompt_k_v_cache` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_mode</span><span class="o">=</span><span class="s1">&#39;LEFT&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;align_mode&quot;</span><span class="p">,</span> <span class="n">align_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="n">valid_seq_len</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">,</span> <span class="n">seq_len_axis</span><span class="p">,</span> <span class="n">new_max_seq_len</span><span class="p">,</span> <span class="n">cur_max_seq_len</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">update</span><span class="p">,</span> <span class="n">valid_seq_len</span><span class="p">,</span> <span class="n">batch_index</span><span class="p">,</span> <span class="n">seq_len_axis</span><span class="p">,</span> <span class="n">new_max_seq_len</span><span class="p">,</span> <span class="n">cur_max_seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_mode</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Qr</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the QR decomposition of one or more matrices.</span>

<span class="sd">    If `mode` is &#39;reduced&#39;(the default), compute the P columns of Q where P is minimum of the 2 innermost dimensions of</span>
<span class="sd">    input. If `mode` is &#39;complete&#39;, compute full-sized Q and R.</span>

<span class="sd">    Args:</span>
<span class="sd">        full_matrices (bool, optional): Whether compute full-sized QR decomposition. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A matrix to be calculated. The matrix must be at least two dimensions, the supported dtype are</span>
<span class="sd">          float16, float32, float64, complex64 and complex128.</span>
<span class="sd">          Define the shape of input as :math:`(..., m, n)`, p as the</span>
<span class="sd">          minimum values of m and n.    </span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **Q** (Tensor) - The orthonormal matrices of input. If `mode` is &#39;complete&#39;, the shape is :math:`(m, m)`,</span>
<span class="sd">          else the shape is :math:`(m, p)`. The dtype of `Q` is same as `input`.</span>
<span class="sd">        - **R** (Tensor) - The upper triangular matrices of input. If `mode` is &#39;complete&#39;, the shape is :math:`(m, n)`,</span>
<span class="sd">          else the shape is :math:`(p, n)`. The dtype of `R` is same as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input` is not a Tensor.</span>
<span class="sd">        TypeError: If `mode` is neither &#39;reduced&#39; nor &#39;complete&#39;.</span>
<span class="sd">        ValueError: If the dimension of `input` is less than 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore as ms</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[20., -31, 7], [4, 270, -90], [-8, 17, -32]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; Q, R = ops.Qr()(x)</span>
<span class="sd">        &gt;&gt;&gt; print(Q)</span>
<span class="sd">        [[-0.912871    0.16366126  0.37400758]</span>
<span class="sd">        [-0.18257418 -0.9830709  -0.01544376]</span>
<span class="sd">        [ 0.36514837 -0.08238228  0.92729706]]</span>
<span class="sd">        &gt;&gt;&gt; print(R)</span>
<span class="sd">        [[ -21.908903  -14.788506  -1.6431675]</span>
<span class="sd">        [    0.       -271.9031    92.25824  ]</span>
<span class="sd">        [    0.          0.       -25.665514 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;full_matrices&quot;</span><span class="p">,</span> <span class="n">full_matrices</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">full_matrices</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">QuantBatchMatmul</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.QuantBatchMatmul(transpose_x1, transpose_x2, dtype)</span>
<span class="sd">        out = prim(x1, x2, scale, offset, bias)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.quant_batch_matmul(x1, x2, scale, offset, bias, transpose_x1, transpose_x2, dtype)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.quant_batch_matmul` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;offset&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transpose_x1</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_x2</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;transpose_x1&quot;</span><span class="p">,</span> <span class="n">transpose_x1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;transpose_x2&quot;</span><span class="p">,</span> <span class="n">transpose_x2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">dtype_to_type_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">scale</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_x1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_x2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


<div class="viewcode-block" id="RandpermV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.RandpermV2.html#mindspore.ops.RandpermV2">[文档]</a><span class="k">class</span> <span class="nc">RandpermV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.RandpermV2(seed, offset, dtype)</span>
<span class="sd">        out = prim(n)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.randperm(n, seed, offset, dtype)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.randperm` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;RandpermV2&#39;</span><span class="p">,</span> <span class="s1">&#39;seed&#39;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TENSOR</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;RandpermV2&#39;</span><span class="p">,</span> <span class="s1">&#39;offset&#39;</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TENSOR</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">dtype_to_type_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span></div>


<div class="viewcode-block" id="Range"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Range.html#mindspore.ops.Range">[文档]</a><span class="k">class</span> <span class="nc">Range</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Range(maxlen)</span>
<span class="sd">        out = prim(start, end, step)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.range(start, end, step, maxlen)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.range` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">1000000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;maxlen&quot;</span><span class="p">,</span> <span class="n">maxlen</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">maxlen</span><span class="p">)</span></div>


<div class="viewcode-block" id="RealDiv"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.RealDiv.html#mindspore.ops.RealDiv">[文档]</a><span class="k">class</span> <span class="nc">RealDiv</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divides the first input tensor by the second input tensor in floating-point type element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.div` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Union[Tensor, Number, bool]) - The first input is a number or</span>
<span class="sd">          a bool or a tensor whose data type is number or bool.</span>
<span class="sd">        - **y** (Union[Tensor, Number, bool]) - The second input is a number or</span>
<span class="sd">          a bool when the first input is a tensor or a tensor whose data type is number or bool.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is the same as the one after broadcasting,</span>
<span class="sd">        and the data type is the one with higher precision or higher digits among the two inputs.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 3.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor(np.array([4.0, 5.0, 6.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; realdiv = ops.RealDiv()</span>
<span class="sd">        &gt;&gt;&gt; output = realdiv(x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.25 0.4  0.5 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<span class="n">real_div_op</span><span class="o">=</span><span class="n">RealDiv</span><span class="p">()</span>


<div class="viewcode-block" id="Real"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Real.html#mindspore.ops.Real">[文档]</a><span class="k">class</span> <span class="nc">Real</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Real()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.real(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.real` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">real_op</span><span class="o">=</span><span class="n">Real</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ReciprocalGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of Reciprocal operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>


<span class="n">reciprocal_grad_op</span><span class="o">=</span><span class="n">ReciprocalGrad</span><span class="p">()</span>


<div class="viewcode-block" id="Reciprocal"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Reciprocal.html#mindspore.ops.Reciprocal">[文档]</a><span class="k">class</span> <span class="nc">Reciprocal</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns reciprocal of a tensor element-wise.</span>

<span class="sd">    .. math::</span>

<span class="sd">        out_{i} =  \frac{1}{x_{i}}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1.0, 2.0, 4.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; reciprocal = ops.Reciprocal()</span>
<span class="sd">        &gt;&gt;&gt; output = reciprocal(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.   0.5  0.25]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_reciprocal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]))</span></div>

<span class="n">reciprocal_op</span><span class="o">=</span><span class="n">Reciprocal</span><span class="p">()</span>


<div class="viewcode-block" id="ReduceAll"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceAll.html#mindspore.ops.ReduceAll">[文档]</a><span class="k">class</span> <span class="nc">ReduceAll</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the &quot;logicalAND&quot; of all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[bool]) - The input tensor. The dtype of the tensor to be reduced is bool.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Only constant value is allowed. Must be in the range [-rank(x), rank(x)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the dtype is bool.</span>

<span class="sd">        - If axis is (), and keep_dims is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the &quot;logical and&quot; of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceAll(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the &quot;logicalAND&quot; of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[False]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True False]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[False]</span>
<span class="sd">        [ True]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: input is a scalar.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(True)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceAll()</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReduceAny"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceAny.html#mindspore.ops.ReduceAny">[文档]</a><span class="k">class</span> <span class="nc">ReduceAny</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the &quot;logical OR&quot; of all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">       keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">                         If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[bool]) - The input tensor. The dtype of the tensor to be reduced is bool.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Only constant value is allowed. Must be in the range [-rank(x), rank(x)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the dtype is bool.</span>

<span class="sd">        - If axis is (), and keep_dims is False,</span>
<span class="sd">          the output is a 0-D tensor representing the &quot;logical or&quot; of all elements in the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int), set as (2, 3), and keep_dims is False,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[True, False], [True, True]]))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceAny(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the &quot;logical OR&quot; of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ True True]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[True]</span>
<span class="sd">        [ True]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: input is a scalar.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(True)</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceAny()</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_reduce_any</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">]))</span></div>

<div class="viewcode-block" id="ReduceMax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceMax.html#mindspore.ops.ReduceMax">[文档]</a><span class="k">class</span> <span class="nc">ReduceMax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the maximum value in this dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int), tensor]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Must be in the range [-r, r).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        output(Tensor): has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is False,</span>
<span class="sd">          the output is a 0-D tensor representing the maximum of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), set as (1, 2), and `keep_dims` is False,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ReduceMax(keep_dims=True)(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the maximum value of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ReduceMax(keep_dims=True)(x, ())</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[9.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ReduceMax(keep_dims=True)(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[7. 7. 7. 7. 7. 7.]</span>
<span class="sd">          [8. 8. 8. 8. 8. 8.]</span>
<span class="sd">          [9. 9. 9. 9. 9. 9.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ReduceMax(keep_dims=True)(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 3. 3. 3. 3. 3.]]</span>
<span class="sd">         [[6. 6. 6. 6. 6. 6.]]</span>
<span class="sd">         [[9. 9. 9. 9. 9. 9.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ReduceMax(keep_dims=True)(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [2.]</span>
<span class="sd">          [3.]]</span>
<span class="sd">         [[4.]</span>
<span class="sd">          [5.]</span>
<span class="sd">          [6.]]</span>
<span class="sd">         [[7.]</span>
<span class="sd">          [8.]</span>
<span class="sd">          [9.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReduceMean"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceMean.html#mindspore.ops.ReduceMean">[文档]</a><span class="k">class</span> <span class="nc">ReduceMean</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by averaging all elements in the dimension, by default. And also can reduce</span>
<span class="sd">    a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Only constant value is allowed. Must be in the range [-r, r).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the mean of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int) or list(int), set as (1, 2), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceMean(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by averaging all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2]],</span>
<span class="sd">        ... [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ... [[6, 6, 6, 6, 6, 6], [8, 8, 8, 8, 8, 8], [10, 10, 10, 10, 10, 10]]]),</span>
<span class="sd">        ... mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[5.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along the axis 0</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4. 4. 4. 4. 4. 4.]</span>
<span class="sd">          [5. 5. 5. 5. 5. 5.]</span>
<span class="sd">          [6. 6. 6. 6. 6. 6.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along the axis 1</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2. 2. 2. 2. 2. 2.]]</span>
<span class="sd">         [[5. 5. 5. 5. 5. 5.]]</span>
<span class="sd">         [[8. 8. 8. 8. 8. 8.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along the axis 2</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 2.]</span>
<span class="sd">          [ 2.]</span>
<span class="sd">          [ 2.]]</span>
<span class="sd">         [[ 4.]</span>
<span class="sd">          [ 5.]</span>
<span class="sd">          [ 6.]]</span>
<span class="sd">         [[ 6.]</span>
<span class="sd">          [ 8.]</span>
<span class="sd">          [10.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReduceMin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceMin.html#mindspore.ops.ReduceMin">[文档]</a><span class="k">class</span> <span class="nc">ReduceMin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by the minimum value in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Only constant value is allowed. Must be in the range [-r, r).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the minimum of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceMin(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by the minimum value of all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1. 1. 1.]</span>
<span class="sd">          [2. 2. 2. 2. 2. 2.]</span>
<span class="sd">          [3. 3. 3. 3. 3. 3.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1. 1. 1. 1. 1.]]</span>
<span class="sd">         [[4. 4. 4. 4. 4. 4.]]</span>
<span class="sd">         [[7. 7. 7. 7. 7. 7.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.]</span>
<span class="sd">          [2.]</span>
<span class="sd">          [3.]]</span>
<span class="sd">         [[4.]</span>
<span class="sd">          [5.]</span>
<span class="sd">          [6.]]</span>
<span class="sd">         [[7.]</span>
<span class="sd">          [8.]</span>
<span class="sd">          [9.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span></div>


<div class="viewcode-block" id="ReduceProd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceProd.html#mindspore.ops.ReduceProd">[文档]</a><span class="k">class</span> <span class="nc">ReduceProd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by multiplying all elements in the dimension, by default. And also can</span>
<span class="sd">    reduce a dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">        - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">          dimensions. Only constant value is allowed. Must be in the range [-r, r).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">        - If `axis` is (), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the product of all elements in the input tensor.</span>
<span class="sd">        - If `axis` is int, set as 1, and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_2, ..., x_R)`.</span>
<span class="sd">        - If `axis` is tuple(int), set as (1, 2), and `keep_dims` is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_0, x_3, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If `axis` is not one of the following: int, tuple or list.</span>
<span class="sd">        ValueError: If `axis` is out of range.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceProd(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; result = output.shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by multiplying all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[2.2833798e+33]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 28.  28.  28.  28.  28.  28.]</span>
<span class="sd">          [ 80.  80.  80.  80.  80.  80.]</span>
<span class="sd">          [162. 162. 162. 162. 162. 162.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[  6.   6.   6.   6.   6.   6.]]</span>
<span class="sd">         [[120. 120. 120. 120. 120. 120.]]</span>
<span class="sd">         [[504. 504. 504. 504. 504. 504.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.00000e+00]</span>
<span class="sd">          [6.40000e+01]</span>
<span class="sd">          [7.29000e+02]]</span>
<span class="sd">         [[4.09600e+03]</span>
<span class="sd">          [1.56250e+04]</span>
<span class="sd">          [4.66560e+04]]</span>
<span class="sd">         [[1.17649e+05]</span>
<span class="sd">          [2.62144e+05]</span>
<span class="sd">          [5.31441e+05]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ReduceStd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the standard-deviation and mean of the input Tensor along</span>
<span class="sd">    dimension(s) specified by `axis`.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple(int), list(int)], optional): The dimensions to reduce.</span>
<span class="sd">            Default: ``()`` , reduce all dimensions. Only constant value is allowed.</span>
<span class="sd">            Let `r` be rank of `input_x`, it should be in the range :math:`[-r,r)`.</span>
<span class="sd">        unbiased (bool, optional):  Whether to use Bessel&#39;s correction.</span>
<span class="sd">            If ``True`` , will use the Bessel correction unbiased estimation.</span>
<span class="sd">            If ``False`` , will through the biased estimation to calculate the standard deviation.</span>
<span class="sd">            Default: ``True`` .</span>
<span class="sd">        keep_dims (bool, optional): Whether the output Tensor has dim retained or not.</span>
<span class="sd">            If ``True`` , keep these reduced dimensions specified by `axis` and the length is 1.</span>
<span class="sd">            If ``False`` , don&#39;t keep these dimensions.</span>
<span class="sd">            Default: ``Fasle`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor[Number]) - The input Tensor with shape</span>
<span class="sd">          :math:`(N, *)` where :math:`*` means any number of additional dimensions.</span>
<span class="sd">          Supported dtypes: float16, float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple(output_std, output_mean) containing the standard deviation and mean.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is not one of the following: int, tuple or list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[1, 2, 3], [-1, 1, 4]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceStd(axis=1, unbiased=True, keep_dims=False)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; output_std, output_mean = output[0], output[1]</span>
<span class="sd">        &gt;&gt;&gt; print(output_std)</span>
<span class="sd">        [1.        2.5166113]</span>
<span class="sd">        &gt;&gt;&gt; print(output_mean)</span>
<span class="sd">        [2.        1.3333334]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[],</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;ReduceStd&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TENSOR</span><span class="p">),</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;unbiased&quot;</span><span class="p">,</span> <span class="n">unbiased</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unbiased</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">)</span>


<div class="viewcode-block" id="ReduceSum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReduceSum.html#mindspore.ops.ReduceSum">[文档]</a><span class="k">class</span> <span class="nc">ReduceSum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reduces a dimension of a tensor by summing all elements in the dimension, by default. And also can reduce a</span>
<span class="sd">    dimension of `x` along the axis. Determine whether the dimensions of the output and input are the same by</span>
<span class="sd">    controlling `keep_dims`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_dims (bool): If ``True`` , keep these reduced dimensions and the length is 1.</span>
<span class="sd">                          If ``False`` , don&#39;t keep these dimensions. Default: ``False`` .</span>
<span class="sd">        skip_mode (bool): If ``True`` and axis is empty tuple or empty list, the ReduceSum operation isn&#39;t performed,</span>
<span class="sd">                          skip it.</span>
<span class="sd">                          If ``True`` and axis is other values, the ReduceSum calculation is performed normally.</span>
<span class="sd">                          If ``False`` , do reduce. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">         - **x** (Tensor[Number]) - The input tensor.</span>
<span class="sd">         - **axis** (Union[int, tuple(int), list(int)]) - The dimensions to reduce. Default: ``()`` , reduce all</span>
<span class="sd">           dimensions when skip_mode is ``False`` . Only constant value is allowed. Must be in the range [-rank(`x`),</span>
<span class="sd">           rank(`x`)).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as the `x`.</span>

<span class="sd">        - If axis is (), keep_dims is ``False`` , and skip_mode is ``False`` ,</span>
<span class="sd">          the output is a 0-D tensor representing the sum of all elements in the input tensor.</span>
<span class="sd">        - If axis is (), and skip_mode is ``True`` ,</span>
<span class="sd">          the ReduceSum operation is not performed, output tensor is equal to the input tensor.</span>
<span class="sd">        - If axis is int, set as 2, and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_3, ..., x_R)`.</span>
<span class="sd">        - If axis is tuple(int) or list(int), set as (2, 3), and keep_dims is ``False`` ,</span>
<span class="sd">          the shape of output is :math:`(x_1, x_4, ..., x_R)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `keep_dims` is not a bool.</span>
<span class="sd">        TypeError: If `skip_mode` is not a bool.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If `axis` is None.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randn(3, 4, 5, 6).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.ReduceSum(keep_dims=True)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; output.shape</span>
<span class="sd">        (3, 1, 5, 6)</span>
<span class="sd">        &gt;&gt;&gt; # case 1: Reduces a dimension by summing all elements in the dimension.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3]],</span>
<span class="sd">        ...                      [[4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5], [6, 6, 6, 6, 6, 6]],</span>
<span class="sd">        ...                      [[7, 7, 7, 7, 7, 7], [8, 8, 8, 8, 8, 8], [9, 9, 9, 9, 9, 9]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[270.]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: Reduces a dimension along axis 0.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[12. 12. 12. 12. 12. 12.]</span>
<span class="sd">          [15. 15. 15. 15. 15. 15.]</span>
<span class="sd">          [18. 18. 18. 18. 18. 18.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: Reduces a dimension along axis 1.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 1)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 6.  6.  6.  6.  6.  6.]]</span>
<span class="sd">         [[15. 15. 15. 15. 15. 15.]]</span>
<span class="sd">         [[24. 24. 24. 24. 24. 24.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 4: Reduces a dimension along axis 2.</span>
<span class="sd">        &gt;&gt;&gt; output = op(x, 2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[ 6.]</span>
<span class="sd">          [12.]</span>
<span class="sd">          [18.]]</span>
<span class="sd">         [[24.]</span>
<span class="sd">          [30.]</span>
<span class="sd">          [36.]]</span>
<span class="sd">         [[42.]</span>
<span class="sd">          [48.]</span>
<span class="sd">          [54.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">()),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">skip_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;keep_dims&quot;</span><span class="p">,</span> <span class="n">keep_dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;skip_mode&quot;</span><span class="p">,</span> <span class="n">skip_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">()):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_dims</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip_mode</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ReLU6Grad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradient for the ReLU6 activation.</span>

<span class="sd">    Args:</span>
<span class="sd">        y_backprop (Tensor): Input gradients tensor, has the same dtype and shape as `x`.</span>
<span class="sd">        x (Tensor): Origin input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as `x`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="n">relu6_grad_op</span><span class="o">=</span><span class="n">ReLU6Grad</span><span class="p">()</span>


<div class="viewcode-block" id="ReLU6"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReLU6.html#mindspore.ops.ReLU6">[文档]</a><span class="k">class</span> <span class="nc">ReLU6</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReLU6()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.relu6(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.relu6` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="n">relu6_op</span><span class="o">=</span><span class="n">ReLU6</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ReluGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradient for the ReLU activation.</span>

<span class="sd">    Args:</span>
<span class="sd">        y_backprop (Tensor): Input gradients tensor, has the same dtype and shape as `x`.</span>
<span class="sd">        x (Tensor): Origin input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as `x`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_relu_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">]))</span>

<span class="n">relu_grad_op</span><span class="o">=</span><span class="n">ReluGrad</span><span class="p">()</span>


<div class="viewcode-block" id="ReLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReLU.html#mindspore.ops.ReLU">[文档]</a><span class="k">class</span> <span class="nc">ReLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReLU()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.relu(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.relu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_relu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span></div>

<span class="n">relu_op</span><span class="o">=</span><span class="n">ReLU</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ReshapeAndCache</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReshapeAndCache()</span>
<span class="sd">        out = prim(key, value, key_cache, value_cache, slot_mapping)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reshape_and_cache(key, value, key_cache, value_cache, slot_mapping)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reshape_and_cache` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;key&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;key_cache&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;value_cache&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;slot_mapping&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">slot_mapping</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">key_cache</span><span class="p">,</span> <span class="n">value_cache</span><span class="p">,</span> <span class="n">slot_mapping</span><span class="p">)</span>


<span class="n">reshape_and_cache_op</span><span class="o">=</span><span class="n">ReshapeAndCache</span><span class="p">()</span>


<div class="viewcode-block" id="Reshape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Reshape.html#mindspore.ops.Reshape">[文档]</a><span class="k">class</span> <span class="nc">Reshape</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Reshape()</span>
<span class="sd">        out = prim(input, shape)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reshape(input, shape)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reshape` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>


<span class="n">reshape_op</span><span class="o">=</span><span class="n">Reshape</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ResizeBicubicGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for ResizeBicubicGrad operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        grads (Tensor): A Tensor of type float. 4-D with shape [batch, height, width, channels]. The format must be NHWC.</span>
<span class="sd">        image (Tensor): A Tensor. Must be one of the following types: float, double.</span>
<span class="sd">            4-D with shape [batch, orig_height, orig_width, channels], The origin image tensor that was resized.</span>
<span class="sd">            The format must be NHWC.</span>
<span class="sd">        align_corners (bool): If true, the centers of the 4 corner pixels of the input and output tensors are </span>
<span class="sd">            aligned, preserving the values at the corner pixels.Default: ``False``.</span>
<span class="sd">        half_pixel_centers (bool): An optional bool. Default: ``False``.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 4-D Tensor , with the same shape and data type as `image`.</span>

<span class="sd">    Rasise:</span>
<span class="sd">        TypeError: If `grads` is not allowed.</span>
<span class="sd">        TypeError: If `image` is not allowed.</span>
<span class="sd">        ValueError: If `image` dim is not 4.</span>
<span class="sd">        ValueError: If `size` dim is not 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<div class="viewcode-block" id="ResizeBicubic"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ResizeBicubic.html#mindspore.ops.ResizeBicubic">[文档]</a><span class="k">class</span> <span class="nc">ResizeBicubic</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resize images to size using bicubic interpolation.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        align_corners (bool, optional): If ``True`` , the centers of the 4 corner pixels of the input</span>
<span class="sd">            and output tensors are aligned, preserving the values at the corner pixels. Default: ``False`` .</span>
<span class="sd">        half_pixel_centers (bool, optional): Whether to use half-pixel center alignment. If set to ``True`` ,</span>
<span class="sd">                                                 `align_corners` should be ``False`` . Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **images** (Tensor) - The input image must be a 4-D tensor of shape :math:`(batch, channels, height, width)`.</span>
<span class="sd">          The format must be NCHW. Types allowed: float16, float32, float64.</span>
<span class="sd">        - **size** (Union[tuple[int], Tensor[int]]) - A 1-D tensor or tuple with 2 elements: new_height, new_width. Besides, tuple[int] is recommended.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 4-D tensor with shape :math:`(batch, channels, new\_height, new\_width)` whose dtype is the same as `images` .</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the type of `images` is not allowed.</span>
<span class="sd">        TypeError: If the type of `align_corners` is not bool.</span>
<span class="sd">        TypeError: If the type of `half_pixel_centers` is not bool.</span>
<span class="sd">        ValueError: If the dim of `images` is not 4.</span>
<span class="sd">        ValueError: If the dim of `size` is not 1 when `size` is a tensor.</span>
<span class="sd">        ValueError: If the number of elements in `size` is not 2.</span>
<span class="sd">        ValueError: If any value of `size` is not positive.</span>
<span class="sd">        ValueError: If the values of `align_corners` and `half_pixel_centers` are both ``True`` .</span>


<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops, nn</span>
<span class="sd">        &gt;&gt;&gt; class NetResizeBicubic(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(NetResizeBicubic, self).__init__()</span>
<span class="sd">        ...         align_corners = False</span>
<span class="sd">        ...         half_pixel_centers = False</span>
<span class="sd">        ...         self.resize = ops.ResizeBicubic(align_corners, half_pixel_centers)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, images, size):</span>
<span class="sd">        ...         return self.resize(images, size)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; images = Tensor(np.array([1, 2, 3, 4]).reshape(1, 1, 2, 2).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; size = Tensor([1, 4], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; resizebicubic = NetResizeBicubic()</span>
<span class="sd">        &gt;&gt;&gt; output = resizebicubic(images, size)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">            [[[[1. 1.5 2. 2.09375]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ResizeBilinearGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of ResizeBilinear operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        grads (Tensor): A 4-D Tensor with shape [batch, channel, height, width].</span>
<span class="sd">        image (Tensor): A 4-D Tensor with shape [batch, channel, height, width], The origin image tensor that was resized.</span>
<span class="sd">        align_corners (bool): If true, the centers of the 4 corner pixels of the input and output tensors are </span>
<span class="sd">            aligned, preserving the values at the corner pixels.Default: ``False``.</span>
<span class="sd">        half_pixel_centers (bool): An optional bool. Default: ``False``.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 4-D Tensor , with the same shape and data type as `image`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<div class="viewcode-block" id="ResizeBilinearV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ResizeBilinearV2.html#mindspore.ops.ResizeBilinearV2">[文档]</a><span class="k">class</span> <span class="nc">ResizeBilinearV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes an image to a certain size using the bilinear interpolation.</span>

<span class="sd">    The resizing only affects the lower two dimensions which represent the height and width.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        align_corners (bool, optional): If ``True`` , rescale input by :math:`(new\_height - 1) / (height - 1)`,</span>
<span class="sd">                       which exactly aligns the 4 corners of images and resized images. If ``False`` ,</span>
<span class="sd">                       rescale by :math:`new\_height / height`. Default: ``False`` .</span>
<span class="sd">        half_pixel_centers (bool, optional): Whether half pixel center. If set to ``True`` , `align_corners` should be</span>
<span class="sd">                           ``False`` . Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Image to be resized. Input images must be a 4-D tensor with shape</span>
<span class="sd">          :math:`(batch, channels, height, width)`, with data type of float32 or float16.</span>
<span class="sd">        - **size** (Union[tuple[int], list[int], Tensor]) - The new size of the images.</span>
<span class="sd">          A tuple or list or Tensor of 2 int elements :math:`(new\_height, new\_width)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, resized image. 4-D with shape :math:`(batch, channels, new\_height, new\_width)`,</span>
<span class="sd">        with the same data type as input `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `align_corners` is not a bool.</span>
<span class="sd">        TypeError: If `half_pixel_centers` is not a bool.</span>
<span class="sd">        TypeError: If `align_corners` and `half_pixel_centers` are all ``True`` .</span>
<span class="sd">        ValueError: If `half_pixel_centers` is ``True`` and device_target is CPU.</span>
<span class="sd">        ValueError: If dim of `x` is not 4.</span>
<span class="sd">        ValueError: If `size` is Tensor and its dim is not 1.</span>
<span class="sd">        ValueError: If `size` contains other than 2 elements.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ResizeBilinearV2()(x, (5, 5))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]</span>
<span class="sd">           [1. 2. 3. 4. 5.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ResizeLinear1DGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute gradient of `ResizeLinear1D` operator.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change.</span>

<span class="sd">    Args:</span>
<span class="sd">        grads (Tensor): A Tensor of type float. 3-D with shape [batch, channel, width].</span>
<span class="sd">        x (Tensor): A origin input Tensor. 3-D with shape [batch, channel, orig_width], The origin tensor that was resized.</span>
<span class="sd">        coordinate_transformation_mode (string): Default is &#39;align_corners&#39;. Describes how to transform the coordinate</span>
<span class="sd">            in the resized tensor to the coordinate in the original tensor. Other optional: &#39;half_pixel&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="o">=</span><span class="s1">&#39;align_corners&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;coordinate_transformation_mode&quot;</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coordinate_transformation_mode</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeLinear1D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using the linear interpolate method resize the input tensor &#39;x&#39;.</span>

<span class="sd">    For general resize, refer to :func:`mindspore.ops.interpolate` for more details.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - This is an experimental API that is subject to change.</span>
<span class="sd">        - Currently, the Ascend platform only supports scenarios where the input `size` is Tuple or List.</span>

<span class="sd">    Args:</span>
<span class="sd">        coordinate_transformation_mode (str): Default is ``&#39;align_corners&#39;`` . Describes how to transform the</span>
<span class="sd">            coordinate in the resized tensor to the coordinate in the original tensor. Other optional: &#39;half_pixel&#39;.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 3-D tensor which to resize, with shape [batch, channel, width]. Must be one of the</span>
<span class="sd">          following types: float16, float32, float64.</span>
<span class="sd">        - **size** (Union[Tuple[int], List[int], Tensor[int]]) - describes the new width of `x` .</span>
<span class="sd">          A tuple or list or 1-D tensor with only one int element :math:`(new\_width)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 3-D tensor which shape is [batch, channel, new_width] with the same type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `x` is not in the support list.</span>
<span class="sd">        TypeError: If `size` is not in Union[Tuple[int], List[int], Tensor[int]].</span>
<span class="sd">        TypeError: If `coordinate_transformation_mode` is not a string.</span>
<span class="sd">        TypeError: If `coordinate_transformation_mode` is not in the support list.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[1, 2, 3], [4, 5, 6]]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; size = (6,)</span>
<span class="sd">        &gt;&gt;&gt; resize_linear_1d = ops.ResizeLinear1D(coordinate_transformation_mode=&quot;align_corners&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = resize_linear_1d(x, size)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 1.4 1.8 2.2 2.6 3.]</span>
<span class="sd">          [4. 4.4 4.8 5.2 5.6 6.]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="o">=</span><span class="s1">&#39;align_corners&#39;</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg_with_handler</span><span class="p">(</span><span class="s2">&quot;coordinate_transformation_mode&quot;</span><span class="p">,</span> <span class="n">coordinate_transformation_mode</span><span class="p">,</span> <span class="n">str_to_enum</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coordinate_transformation_mode</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeNearestNeighborGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute gradient of `ResizeNearestNeighbor` operator.</span>

<span class="sd">    Note:</span>
<span class="sd">        The shape of input parameter `size` must be (height, width).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **align_corners** (bool) - Whether the centers of the 4 corner pixels of the input</span>
<span class="sd">          and output tensors are aligned. Default: ``False``.</span>
<span class="sd">        - **half_pixel_centers** (bool, optional) - Whether half pixel center. If set to ``True``,</span>
<span class="sd">          `align_corners` should be False. Default: ``False``.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<div class="viewcode-block" id="ResizeNearestNeighbor"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ResizeNearestNeighbor.html#mindspore.ops.ResizeNearestNeighbor">[文档]</a><span class="k">class</span> <span class="nc">ResizeNearestNeighbor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes the input tensor to a given size by using the nearest neighbor algorithm. The nearest</span>
<span class="sd">    neighbor algorithm selects the value of the nearest point and does not consider the</span>
<span class="sd">    values of neighboring points at all, yielding a piecewise-constant interpolant.</span>

<span class="sd">    Args:</span>
<span class="sd">        size (Union[tuple, list]): The target size. The dimension of size must be 2.</span>
<span class="sd">        align_corners (bool): Whether the centers of the 4 corner pixels of the input and output tensors are aligned. </span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        half_pixel_centers (bool, optional): Whether half pixel center. If set to ``True`` ,</span>
<span class="sd">            `align_corners` should be False. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input tensor. The shape of the tensor is :math:`(N, C, H, W)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape of the output tensor is  :math:`(N, C, NEW\_H, NEW\_W)`.</span>
<span class="sd">        The data type is the same as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `size` is neither tuple nor list.</span>
<span class="sd">        TypeError: If `align_corners` is not a bool.</span>
<span class="sd">        ValueError: If length of `size` is not equal to 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.array([[[[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; size = (2, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.ResizeNearestNeighbor(size=size)(input_tensor)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[-0.1  0.3]</span>
<span class="sd">           [ 0.4  0.5]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;ResizeNearestNeighbor&#39;</span><span class="p">,</span> <span class="s1">&#39;size&#39;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ResizeNearestNeighborV2Grad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute gradient of `ResizeNearestNeighborV2` operator.</span>

<span class="sd">    Args:</span>
<span class="sd">        grads (Tensor): A 4-D Tensor with shape [batch, channel, height, width].</span>
<span class="sd">        size (Union[tuple[int], Tensor]): The size for the input image. 2 elements: [`height, width`].</span>
<span class="sd">        align_corners (bool): Whether the centers of the 4 corner pixels of the input</span>
<span class="sd">            and output tensors are aligned. Default: ``False``.</span>
<span class="sd">        half_pixel_centers (bool): Default: ``False``.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A 4-D Tensor , with the same shape and data type as `image`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ResizeNearestNeighborV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resizes the input tensor to specific size by using the nearest neighbor algorithm.</span>

<span class="sd">    The nearest neighbor algorithm selects the value of the nearest point and does not consider the</span>
<span class="sd">    values of neighboring points at all, yielding a piecewise-constant interpolant.</span>

<span class="sd">    Args:</span>
<span class="sd">        align_corners (bool, optional): If ``True`` , the centers of the 4 corner pixels of the input and output</span>
<span class="sd">            tensors are aligned, preserving the values at the corner pixels. Default: ``False`` .</span>
<span class="sd">        half_pixel_centers (bool, optional): Whether half pixel center. If set to ``True`` ,</span>
<span class="sd">            `align_corners` should be False. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - 4-D with shape :math:`(batch, channels, height, width)` .</span>
<span class="sd">        - **size** (Tensor) - The new size for the images. A 1-D int32 Tensor</span>
<span class="sd">          of 2 elements: [`new_height, new_width`].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - The resized images. A 4-D with shape</span>
<span class="sd">          :math:`(batch, channels, new\_height, new\_width)`. It has the same dtype as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `size` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type  of `size` is not int32.</span>
<span class="sd">        TypeError: If `align_corners` or `half_pixel_centers` is not bool.</span>
<span class="sd">        ValueError: If any value of `size` is non positive.</span>
<span class="sd">        ValueError: If the dimension of `x` is not 4.</span>
<span class="sd">        ValueError: If the dimension of `size` is not 1.</span>
<span class="sd">        ValueError: If the elements number of `size` is not 2.</span>
<span class="sd">        ValueError: If attr `half_pixel_centers` and `align_corners` are True at the same time.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones((1, 1, 4, 4)), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; size = Tensor([2, 2], mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; resize = ops.ResizeNearestNeighborV2()</span>
<span class="sd">        &gt;&gt;&gt; output = resize(input_tensor, size)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1. 1.]</span>
<span class="sd">           [1. 1.]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (1, 1, 2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;half_pixel_centers&quot;</span><span class="p">,</span> <span class="n">half_pixel_centers</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_pixel_centers</span><span class="p">)</span>


<div class="viewcode-block" id="ReverseV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ReverseV2.html#mindspore.ops.ReverseV2">[文档]</a><span class="k">class</span> <span class="nc">ReverseV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ReverseV2(axis)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.reverse(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.reverse` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;ReverseV2&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">RFFTGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input1&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input2&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span> <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;RFFTGrad&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>


<span class="n">rfft_grad_op</span><span class="o">=</span><span class="n">RFFTGrad</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">RFFT</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.RFFT()</span>
<span class="sd">        out = prim(input, n, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.rfft(input, n, dim, norm)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.rfft` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">norm</span> <span class="k">if</span> <span class="n">norm</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">str_to_enum</span><span class="p">(</span><span class="s1">&#39;RFFT&#39;</span><span class="p">,</span> <span class="s1">&#39;norm&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">))</span>


<span class="n">rfft_op</span><span class="o">=</span><span class="n">RFFT</span><span class="p">()</span>


<div class="viewcode-block" id="RightShift"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.RightShift.html#mindspore.ops.RightShift">[文档]</a><span class="k">class</span> <span class="nc">RightShift</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Shift the value of each position of Tensor `input_x` to the right by corresponding bits in Tensor `input_y`.</span>
<span class="sd">    The inputs are two tensors, dtypes of them must be consistent, and the</span>
<span class="sd">    shapes of them could be broadcast.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{aligned}</span>
<span class="sd">        &amp;out_{i} =x_{i} &gt;&gt; y_{i}</span>
<span class="sd">        \end{aligned}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The target tensor, will be shifted to the right</span>
<span class="sd">          by `input_y` bits element-wise. Support all int and uint types.</span>
<span class="sd">        - **input_y** (Tensor) - Number of bits shifted, the tensor must have the same type as `input_x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - The output tensor, has the same type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `input_y` is not tensor.</span>
<span class="sd">        TypeError: If `input_x` and `input_y` could not be broadcast.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]).astype(np.uint8))</span>
<span class="sd">        &gt;&gt;&gt; input_y = Tensor(np.array([1, 1, 1]).astype(np.uint8))</span>
<span class="sd">        &gt;&gt;&gt; output = ops.RightShift()(input_x, input_y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 1 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">)</span></div>


<span class="n">right_shift_op</span><span class="o">=</span><span class="n">RightShift</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">Roll</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Rolls the elements of a tensor along an axis.</span>

<span class="sd">    Refer to :func:`mindspore.ops.roll` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        shift (Union[list(int), tuple(int), int]): Specifies the number of places by which elements are shifted</span>
<span class="sd">            positively (towards larger indices) along the specified dimension. Negative shifts will roll the elements</span>
<span class="sd">            in the opposite direction.</span>
<span class="sd">        axis (Union[list(int), tuple(int), int]): Specifies the dimension indexes of shape to be rolled.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0, 1, 2, 3, 4]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Roll(shift=2, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3. 4. 0. 1. 2.]</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Roll(shift=-1, axis=0)</span>
<span class="sd">        &gt;&gt;&gt; output = op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[5. 6. 7. 8. 9.]</span>
<span class="sd">        [0. 1. 2. 3. 4.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="n">axis</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;shift&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;Roll&#39;</span><span class="p">,</span> <span class="s1">&#39;shift&#39;</span><span class="p">,</span> <span class="n">shift</span><span class="p">,</span> <span class="p">(</span><span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">),</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;Roll&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">(</span><span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_LIST_INT</span><span class="p">),</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shift</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<div class="viewcode-block" id="Round"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Round.html#mindspore.ops.Round">[文档]</a><span class="k">class</span> <span class="nc">Round</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Round()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.round(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.round` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">round_op</span><span class="o">=</span><span class="n">Round</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">RsqrtGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradients for the Rsqrt.</span>

<span class="sd">    Args:</span>
<span class="sd">        y_backprop (Tensor): Input gradients tensor, has the same dtype and shape as `x`.</span>
<span class="sd">        x (Tensor): Origin input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, has the same dtype and shape as `x`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">y_backprop</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="n">rsqrt_grad_op</span><span class="o">=</span><span class="n">RsqrtGrad</span><span class="p">()</span>


<div class="viewcode-block" id="Rsqrt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Rsqrt.html#mindspore.ops.Rsqrt">[文档]</a><span class="k">class</span> <span class="nc">Rsqrt</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Rsqrt()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.rsqrt(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.rsqrt` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">rsqrt_op</span><span class="o">=</span><span class="n">Rsqrt</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ScalarCast</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ScalarCast()</span>
<span class="sd">        out = prim(input_x, input_y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.scalar_cast(input_x, input_y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.scalar_cast` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">input_y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">dtype_to_type_id</span><span class="p">(</span><span class="s1">&#39;ScalarCast&#39;</span><span class="p">,</span> <span class="s1">&#39;input_y&#39;</span><span class="p">,</span> <span class="n">input_y</span><span class="p">))</span>


<span class="n">scalar_cast_op</span><span class="o">=</span><span class="n">ScalarCast</span><span class="p">()</span>


<div class="viewcode-block" id="ScatterNd"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ScatterNd.html#mindspore.ops.ScatterNd">[文档]</a><span class="k">class</span> <span class="nc">ScatterNd</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.ScatterNd()</span>
<span class="sd">        out = prim(indices, updates, shape)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.scatter_nd(indices, updates, shape)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.scatter_nd` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">updates</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span></div>


<span class="n">scatter_nd_op</span><span class="o">=</span><span class="n">ScatterNd</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">Scatter</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    reverse operation of gather</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">reduce</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_scatter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">reduce</span><span class="p">]))</span>

<span class="n">scatter_op</span><span class="o">=</span><span class="n">Scatter</span><span class="p">()</span>


<div class="viewcode-block" id="Select"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Select.html#mindspore.ops.Select">[文档]</a><span class="k">class</span> <span class="nc">Select</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The conditional tensor determines whether the corresponding element in the output must be</span>
<span class="sd">    selected from `x` (if True) or `y` (if False) based on the value of each</span>
<span class="sd">    element.</span>

<span class="sd">    It can be defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        out_i = \begin{cases}</span>
<span class="sd">        x_i, &amp; \text{if } cond_i \\</span>
<span class="sd">        y_i, &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Inputs:</span>
<span class="sd">      - **cond** (Tensor[bool]): The condition tensor, decides which element is chosen.</span>
<span class="sd">        The shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">      - **x** (Tensor): The first Tensor to be selected.</span>
<span class="sd">        The shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>
<span class="sd">      - **y** (Tensor): The second Tensor to be selected.</span>
<span class="sd">        The shape is :math:`(x_1, x_2, ..., x_N, ..., x_R)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape as `cond`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If x or y is not a Tensor.</span>
<span class="sd">        ValueError: The shape of inputs are different.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # 1) Both inputs are Tensor</span>
<span class="sd">        &gt;&gt;&gt; select = ops.Select()</span>
<span class="sd">        &gt;&gt;&gt; cond = Tensor([True, False])</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = Tensor([1,2], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = select(cond, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 2.]</span>
<span class="sd">        &gt;&gt;&gt; # 2) y is a float</span>
<span class="sd">        &gt;&gt;&gt; cond = Tensor([True, False])</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([2,3], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; y = 2.0</span>
<span class="sd">        &gt;&gt;&gt; output = select(cond, x, y)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [2. 2.]</span>
<span class="sd">        </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">cond</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></div>


<span class="n">select_op</span><span class="o">=</span><span class="n">Select</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">SequenceConcat</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.SequenceConcat(axis)</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sequence_concat(x, axis)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sequence_concat` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SigmoidGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the gradient of Sigmoid operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_sigmoid_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">dy</span><span class="p">]))</span>

<span class="n">sigmoid_grad_op</span><span class="o">=</span><span class="n">SigmoidGrad</span><span class="p">()</span>


<div class="viewcode-block" id="Sigmoid"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sigmoid.html#mindspore.ops.Sigmoid">[文档]</a><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sigmoid()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sigmoid(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sigmoid` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span></div>

<span class="n">sigmoid_op</span><span class="o">=</span><span class="n">Sigmoid</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">SiLUGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of SiLU operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_silu_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">dout</span><span class="p">,</span> <span class="n">x</span><span class="p">]))</span>

<span class="n">silu_grad_op</span><span class="o">=</span><span class="n">SiLUGrad</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">SiLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.SiLU()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.silu(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.silu` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_silu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span>

<span class="n">silu_op</span><span class="o">=</span><span class="n">SiLU</span><span class="p">()</span>


<div class="viewcode-block" id="Sin"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sin.html#mindspore.ops.Sin">[文档]</a><span class="k">class</span> <span class="nc">Sin</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sin()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sin(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sin` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_sin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span></div>

<span class="n">sin_op</span><span class="o">=</span><span class="n">Sin</span><span class="p">()</span>


<div class="viewcode-block" id="Sinc"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sinc.html#mindspore.ops.Sinc">[文档]</a><span class="k">class</span> <span class="nc">Sinc</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sinc()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sinc(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sinc` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">sinc_op</span><span class="o">=</span><span class="n">Sinc</span><span class="p">()</span>


<div class="viewcode-block" id="Sinh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sinh.html#mindspore.ops.Sinh">[文档]</a><span class="k">class</span> <span class="nc">Sinh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sinh()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sinh(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sinh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">sinh_op</span><span class="o">=</span><span class="n">Sinh</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">SoftmaxBackward</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dout&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;out&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_softmax_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">dout</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="p">]))</span>

<span class="n">softmax_backward_op</span><span class="o">=</span><span class="n">SoftmaxBackward</span><span class="p">()</span>


<div class="viewcode-block" id="Softmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Softmax.html#mindspore.ops.Softmax">[文档]</a><span class="k">class</span> <span class="nc">Softmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the Softmax operation to the input tensor on the specified axis.</span>

<span class="sd">    Refer to :func:`mindspore.ops.softmax` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[int, tuple], optional): The axis to perform the Softmax operation. Default: ``-1`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions. </span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the input.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softmax = ops.Softmax()</span>
<span class="sd">        &gt;&gt;&gt; output = softmax(input)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.01165623 0.03168492 0.08612854 0.23412167 0.6364086 ]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">type_it</span><span class="p">(</span><span class="s1">&#39;Softmax&#39;</span><span class="p">,</span> <span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_INT</span><span class="p">,</span> <span class="n">OpDtype</span><span class="o">.</span><span class="n">DT_TUPLE_INT</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]))</span></div>

<span class="k">class</span> <span class="nc">SolveTriangular</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.SolveTriangular()</span>
<span class="sd">        out = prim(a, b, trans, lower, unit_diagonal)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.solve_triangular(a, b, trans, lower, unit_diagonal)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.solve_triangular` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;trans&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;unit_diagonal&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">trans</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">unit_diagonal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">trans</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">unit_diagonal</span><span class="p">)</span>


<span class="n">solve_triangular_op</span><span class="o">=</span><span class="n">SolveTriangular</span><span class="p">()</span>


<div class="viewcode-block" id="Split"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Split.html#mindspore.ops.Split">[文档]</a><span class="k">class</span> <span class="nc">Split</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Splits the input tensor into output_num of tensors along the given axis and output numbers.</span>

<span class="sd">    Refer to :func:`mindspore.ops.split` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): Index of the split position. Default: ``0`` .</span>
<span class="sd">        output_num (int): The number of output tensors. Must be positive int. Default: ``1`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The shape of tensor is :math:`(x_0, x_1, ..., x_{R-1})`, R &gt;= 1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], the shape of each output tensor is the same, which is</span>
<span class="sd">        :math:`(x_0, x_1, ..., x_{axis}/{output\_num}, ..., x_{R-1})`.</span>
<span class="sd">        And the data type is the same as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; split = ops.Split(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1, 1, 1, 1], [2, 2, 2, 2]]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; print(x)</span>
<span class="sd">        [[1 1 1 1]</span>
<span class="sd">         [2 2 2 2]]</span>
<span class="sd">        &gt;&gt;&gt; output = split(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Int32, value=</span>
<span class="sd">        [[1, 1],</span>
<span class="sd">         [2, 2]]), Tensor(shape=[2, 2], dtype=Int32, value=</span>
<span class="sd">        [[1, 1],</span>
<span class="sd">         [2, 2]]))</span>
<span class="sd">        &gt;&gt;&gt; split = ops.Split(1, 4)</span>
<span class="sd">        &gt;&gt;&gt; output = split(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]), Tensor(shape=[2, 1], dtype=Int32, value=</span>
<span class="sd">        [[1],</span>
<span class="sd">         [2]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_num</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;output_num&quot;</span><span class="p">,</span> <span class="n">output_num</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_num</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">SqrtGrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs grad of Sqrt operation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="n">sqrt_grad_op</span><span class="o">=</span><span class="n">SqrtGrad</span><span class="p">()</span>


<div class="viewcode-block" id="Sqrt"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Sqrt.html#mindspore.ops.Sqrt">[文档]</a><span class="k">class</span> <span class="nc">Sqrt</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sqrt()</span>
<span class="sd">        out = prim(x)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sqrt(x)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sqrt` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_sqrt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">]))</span></div>

<span class="n">sqrt_op</span><span class="o">=</span><span class="n">Sqrt</span><span class="p">()</span>


<div class="viewcode-block" id="Square"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Square.html#mindspore.ops.Square">[文档]</a><span class="k">class</span> <span class="nc">Square</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Square()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.square(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.square` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_square</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">]))</span></div>

<span class="n">square_op</span><span class="o">=</span><span class="n">Square</span><span class="p">()</span>


<div class="viewcode-block" id="StridedSlice"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.StridedSlice.html#mindspore.ops.StridedSlice">[文档]</a><span class="k">class</span> <span class="nc">StridedSlice</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.StridedSlice(begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask)</span>
<span class="sd">        out = prim(input_x, begin, end, strides)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.strided_slice(input_x, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.strided_slice` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">begin_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ellipsis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">new_axis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shrink_axis_mask</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;begin_mask&quot;</span><span class="p">,</span> <span class="n">begin_mask</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;end_mask&quot;</span><span class="p">,</span> <span class="n">end_mask</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;ellipsis_mask&quot;</span><span class="p">,</span> <span class="n">ellipsis_mask</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;new_axis_mask&quot;</span><span class="p">,</span> <span class="n">new_axis_mask</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;shrink_axis_mask&quot;</span><span class="p">,</span> <span class="n">shrink_axis_mask</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">begin_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ellipsis_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_axis_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shrink_axis_mask</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">SubExt</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.SubExt()</span>
<span class="sd">        out = prim(input, other, alpha)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sub_ext(input, other, alpha)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sub_ext` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;other&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_sub_ext</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="n">other</span><span class="p">,</span> <span class="n">alpha</span><span class="p">]))</span>

<span class="n">sub_ext_op</span><span class="o">=</span><span class="n">SubExt</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">Sub</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Sub()</span>
<span class="sd">        out = prim(x, y)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.sub(x, y)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.sub` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>


<span class="n">sub_op</span><span class="o">=</span><span class="n">Sub</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">TensorCopySlices</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Copy continues memory.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The target Tensor.</span>
<span class="sd">        - **value** (Tensor) - The tensor to update x.</span>
<span class="sd">        - **begin** (tuple[int]) - A tuple which represents the location where to start. Only</span>
<span class="sd">          constant value is allowed.</span>
<span class="sd">        - **end** (tuple[int]) - A tuple or which represents the maximum location where to end.</span>
<span class="sd">          Only constant value is allowed.</span>
<span class="sd">        - **strides** (tuple[int]) - A tuple which represents the stride is continuously added</span>
<span class="sd">          before reaching the maximum location. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor), has the same shape and data type of x.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import _inner_ops</span>
<span class="sd">        &gt;&gt;&gt; copy_slices = _inner_ops.TensorCopySlices()</span>
<span class="sd">        &gt;&gt;&gt; out = copy_slices(Tensor(np.zeros((5, 5))), Tensor(np.ones((2, 5))), (3, 0), (5, 5), (1, 1))</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">            [[1., 1., 1., 1., 1.],</span>
<span class="sd">             [1., 1., 1., 1., 1.],</span>
<span class="sd">             [1., 1., 1., 1., 1.],</span>
<span class="sd">             [0., 0., 0., 0., 0.],</span>
<span class="sd">             [0., 0., 0., 0., 0.]]</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">begin</span><span class="p">,</span> <span class="n">end</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span>


<span class="n">tensor_copy_slices_op</span><span class="o">=</span><span class="n">TensorCopySlices</span><span class="p">()</span>


<div class="viewcode-block" id="TensorShape"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TensorShape.html#mindspore.ops.TensorShape">[文档]</a><span class="k">class</span> <span class="nc">TensorShape</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the shape of the input tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones(shape=[3, 2, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = ops.TensorShape()(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [3 2 1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">)</span></div>


<span class="n">tensor_shape_op</span><span class="o">=</span><span class="n">TensorShape</span><span class="p">()</span>


<div class="viewcode-block" id="Trace"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Trace.html#mindspore.ops.Trace">[文档]</a><span class="k">class</span> <span class="nc">Trace</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Trace()</span>
<span class="sd">        out = prim(input)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.trace(input)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.trace` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="n">trace_op</span><span class="o">=</span><span class="n">Trace</span><span class="p">()</span>


<div class="viewcode-block" id="Transpose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Transpose.html#mindspore.ops.Transpose">[文档]</a><span class="k">class</span> <span class="nc">Transpose</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.Transpose()</span>
<span class="sd">        out = prim(input, input_perm)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.transpose(input, input_perm)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.transpose` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">):</span>
          <span class="k">return</span> <span class="n">_convert_stub</span><span class="p">(</span><span class="n">pyboost_transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="nb">input</span><span class="p">,</span> <span class="n">input_perm</span><span class="p">]))</span></div>

<span class="n">transpose_op</span><span class="o">=</span><span class="n">Transpose</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">TupleToTensor</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.TupleToTensor()</span>
<span class="sd">        out = prim(input_tuple, dtype)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.tuple_to_tensor(input_tuple, dtype)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.tuple_to_tensor` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_tuple&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tuple</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_tuple</span><span class="p">,</span> <span class="n">dtype</span> <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dtype_to_type_id</span><span class="p">(</span><span class="s1">&#39;TupleToTensor&#39;</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">))</span>


<span class="n">tuple_to_tensor_op</span><span class="o">=</span><span class="n">TupleToTensor</span><span class="p">()</span>


<div class="viewcode-block" id="UnsortedSegmentSum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.UnsortedSegmentSum.html#mindspore.ops.UnsortedSegmentSum">[文档]</a><span class="k">class</span> <span class="nc">UnsortedSegmentSum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.UnsortedSegmentSum()</span>
<span class="sd">        out = prim(input_x, segment_ids, num_segments)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.unsorted_segment_sum(input_x, segment_ids, num_segments)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.unsorted_segment_sum` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">input_x</span><span class="p">,</span> <span class="n">segment_ids</span><span class="p">,</span> <span class="n">num_segments</span><span class="p">)</span></div>


<span class="n">unsorted_segment_sum_op</span><span class="o">=</span><span class="n">UnsortedSegmentSum</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">View</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.View()</span>
<span class="sd">        out = prim(input, shape)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.view(input, shape)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.view` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>


<span class="n">view_op</span><span class="o">=</span><span class="n">View</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">WeightQuantBatchMatmul</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    .. code-block::</span>
<span class="sd">        </span>
<span class="sd">        prim = ops.WeightQuantBatchMatmul(transpose_x, transpose_weight, antiquant_group_size)</span>
<span class="sd">        out = prim(x, weight, antiquant_scale, antiquant_offset, quant_scale, quant_offset, bias)</span>
<span class="sd">        </span>
<span class="sd">    is equivalent to</span>
<span class="sd">    </span>
<span class="sd">    .. code-block::</span>
<span class="sd">    </span>
<span class="sd">        ops.weight_quant_batch_matmul(x, weight, antiquant_scale, antiquant_offset, quant_scale, quant_offset, bias, transpose_x, transpose_weight, antiquant_group_size)</span>
<span class="sd">        </span>
<span class="sd">    Refer to :func:`mindspore.ops.weight_quant_batch_matmul` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span>  <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;antiquant_scale&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;antiquant_offset&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;quant_scale&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;quant_offset&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transpose_x</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transpose_weight</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">antiquant_group_size</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;transpose_x&quot;</span><span class="p">,</span> <span class="n">transpose_x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;transpose_weight&quot;</span><span class="p">,</span> <span class="n">transpose_weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_prim_arg</span><span class="p">(</span><span class="s2">&quot;antiquant_group_size&quot;</span><span class="p">,</span> <span class="n">antiquant_group_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">antiquant_scale</span><span class="p">,</span> <span class="n">antiquant_offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">quant_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">quant_offset</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">antiquant_scale</span><span class="p">,</span> <span class="n">antiquant_offset</span><span class="p">,</span> <span class="n">quant_scale</span><span class="p">,</span> <span class="n">quant_offset</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">antiquant_group_size</span><span class="p">)</span>


<div class="viewcode-block" id="ZerosLike"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ZerosLike.html#mindspore.ops.ZerosLike">[文档]</a><span class="k">class</span> <span class="nc">ZerosLike</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns a Tensor with a value of 0 and its shape and data type is the same as the input.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input Tensor of any dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x` but filled with zeros.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; zeroslike = ops.ZerosLike()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0, 1], [2, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = zeroslike(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 0.]</span>
<span class="sd">         [0. 0.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@prim_arg_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
          <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<span class="n">zeros_like_op</span><span class="o">=</span><span class="n">ZerosLike</span><span class="p">()</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>