<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.ops.operations.nn_ops &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script><script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/js/theme.js"></script><script src="../../../../_static/underscore.js"></script><script src="../../../../_static/doctools.js"></script><script src="../../../../_static/js/mermaid-9.3.0.js"></script><script src="../../../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/distributed_training_design.html">分布式并行原生</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.hal.html">mindspore.hal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/enveriment_preparation.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/model_development/model_development.html">网络搭建对比</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/operators.html">静态图语法-运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/statements.html">静态图语法-Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/static_graph_syntax/python_builtin_functions.html">静态图语法-Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../index.html">模块代码</a> &raquo;</li>
      <li>mindspore.ops.operations.nn_ops</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>mindspore.ops.operations.nn_ops 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2023 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>

<span class="sd">&quot;&quot;&quot;Operators for nn.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">_check_3d_int_or_tuple</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindspore.ops</span> <span class="kn">import</span> <span class="n">signature</span> <span class="k">as</span> <span class="n">sig</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">_checkparam</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore.common</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">mstype</span>
<span class="kn">from</span> <span class="nn">mindspore.common._decorator</span> <span class="kn">import</span> <span class="n">deprecated</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">Primitive</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">PrimitiveWithInfer</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">PrimitiveWithCheck</span>
<span class="kn">from</span> <span class="nn">mindspore.ops.primitive</span> <span class="kn">import</span> <span class="n">prim_attr_register</span>
<span class="kn">from</span> <span class="nn">..auto_generate</span> <span class="kn">import</span> <span class="p">(</span><span class="n">CeLU</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">LogSoftmax</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">,</span> <span class="n">ReLU6</span><span class="p">,</span>
                             <span class="n">Elu</span><span class="p">,</span> <span class="n">Sigmoid</span><span class="p">,</span> <span class="n">Softmax</span><span class="p">,</span> <span class="n">HSwish</span><span class="p">,</span> <span class="n">HSigmoid</span><span class="p">,</span> <span class="n">AvgPool</span><span class="p">,</span> <span class="n">BiasAdd</span><span class="p">,</span>
                             <span class="n">NLLLoss</span><span class="p">,</span> <span class="n">OneHot</span><span class="p">,</span> <span class="n">GeLU</span><span class="p">,</span> <span class="n">FastGeLU</span><span class="p">,</span> <span class="n">PReLU</span><span class="p">,</span>
                             <span class="n">GridSampler3D</span><span class="p">,</span> <span class="n">GridSampler2D</span><span class="p">,</span> <span class="n">LayerNorm</span><span class="p">,</span> <span class="n">HShrink</span><span class="p">,</span> <span class="n">AdamWeightDecay</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">.manually_defined</span> <span class="kn">import</span> <span class="n">BatchNorm</span>


<span class="k">def</span> <span class="nf">_check_positive_int_or_tuple</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                 <span class="n">ret_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strict_positive</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether an argument is a positive int or tuple with 2 or 4(when allow_four is True) positive int elements.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_raise_message</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; must be an positive int number or a tuple of two &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;or four &#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">allow_four</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">positive int numbers, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_return_value</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">)</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="n">arg_value</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">allow_four</span><span class="p">:</span>
                <span class="n">_raise_message</span><span class="p">()</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">arg_value</span> <span class="k">if</span> <span class="n">ret_four</span> <span class="k">else</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_raise_message</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="n">ret_value</span> <span class="o">=</span> <span class="n">_get_return_value</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ret_value</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">strict_positive</span> <span class="ow">and</span> <span class="n">item</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>
        <span class="n">_raise_message</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ret_value</span>


<span class="k">def</span> <span class="nf">_check_shape</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks whether an shape dims is a positive int elements.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_raise_message</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; dims elements must be positive int numbers, &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="n">prim_name</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">arg_value</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">_raise_message</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">arg_value</span>


<span class="k">def</span> <span class="nf">_update_attr_by_format</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_format</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    If the format is NHWC, should modify the strides or dilation shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">arg_value</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">arg_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>

    <span class="k">return</span> <span class="n">ret</span>


<div class="viewcode-block" id="AdaptiveAvgPool3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AdaptiveAvgPool3D.html#mindspore.ops.AdaptiveAvgPool3D">[文档]</a><span class="k">class</span> <span class="nc">AdaptiveAvgPool3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AdaptiveAvgPool3D operation.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.adaptive_avg_pool3d` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size (Union[int, tuple]): Specify the size of output tensor. It</span>
<span class="sd">            can be a single int or a tuple of three ints.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input of AdaptiveAvgPool3D, which is a 5D or 4D tensor.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import nn, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops import AdaptiveAvgPool3D</span>
<span class="sd">        &gt;&gt;&gt; class AdaptiveAvgPool3DNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, output_size):</span>
<span class="sd">        ...         super(AdaptiveAvgPool3DNet, self).__init__()</span>
<span class="sd">        ...         self.output_size_ = output_size</span>
<span class="sd">        ...         self.adaptive_avg_pool_3d = AdaptiveAvgPool3D(self.output_size_)</span>
<span class="sd">        ...     def construct(self, x_):</span>
<span class="sd">        ...         return self.adaptive_avg_pool_3d(x_)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; output_size=(1,1,1)</span>
<span class="sd">        &gt;&gt;&gt; input_x_val = np.zeros((1,1,2,2,2))</span>
<span class="sd">        &gt;&gt;&gt; input_x_val[:,:,0,:,:]  += 1</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(input_x_val, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; adaptive_avg_pool_3d = AdaptiveAvgPool3DNet(output_size)</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_avg_pool_3d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[0.5]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">3</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">output_size</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output_size[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output_size[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">val</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="AdaptiveAvgPool2D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AdaptiveAvgPool2D.html#mindspore.ops.AdaptiveAvgPool2D">[文档]</a><span class="k">class</span> <span class="nc">AdaptiveAvgPool2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    AdaptiveAvgPool2D operation.</span>

<span class="sd">    Refer to :func:`mindspore.ops.adaptive_avg_pool2d` for more details.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size. `output_size` can be a tuple :math:`(H, W)`,</span>
<span class="sd">            or an int H for :math:`(H, H)`. :math:`H` and :math:`W` can be int or None.</span>
<span class="sd">            If it is None, it means the output size is the same as the input size.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of AdaptiveAvgPool2D, which is a 3D or 4D tensor,</span>
<span class="sd">          with float16 ,float32 or float64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(None, 2)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                            [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; adaptive_avg_pool_2d = ops.AdaptiveAvgPool2D((None, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_avg_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]</span>
<span class="sd">         [[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]</span>
<span class="sd">         [[1.5 2.5]</span>
<span class="sd">          [4.5 5.5]</span>
<span class="sd">          [7.5 8.5]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=2</span>
<span class="sd">        &gt;&gt;&gt; adaptive_avg_pool_2d = ops.AdaptiveAvgPool2D(2)</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_avg_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[3. 4.]</span>
<span class="sd">          [6. 7.]]</span>
<span class="sd">         [[3. 4.]</span>
<span class="sd">          [6. 7.]]</span>
<span class="sd">         [[3. 4.]</span>
<span class="sd">          [6. 7.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; adaptive_avg_pool_2d = ops.AdaptiveAvgPool2D((1, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_avg_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[4.5 5.5]]</span>
<span class="sd">         [[4.5 5.5]]</span>
<span class="sd">         [[4.5 5.5]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AdaptiveAvgPool2D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;length of output_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">output_size</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output_size[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output_size[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">val</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">AdaptiveMaxPool2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs 2D adaptive max pooling on a multi-plane input signal.</span>

<span class="sd">    Refer to :func:`mindspore.ops.adaptive_max_pool2d` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size (Union[int, tuple]): The target output size. `output_size` can be a tuple :math:`(H, W)`,</span>
<span class="sd">            or an int H for :math:`(H, H)`. :math:`H` and :math:`W` can be int or None.</span>
<span class="sd">            If it is None, it means the output size is the same as the input size.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of AdaptiveMaxPool2D, which is a 3D or 4D tensor,</span>
<span class="sd">          with float16, float32 or float64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case 1: output_size=(None, 2)</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],</span>
<span class="sd">        ...                             [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; adaptive_max_pool_2d = ops.AdaptiveMaxPool2D((None, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_max_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [[[[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[2. 3.]</span>
<span class="sd">           [5. 6.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 2: output_size=2</span>
<span class="sd">        &gt;&gt;&gt; adaptive_max_pool_2d = ops.AdaptiveMaxPool2D(2)</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_max_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [[[[5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[5. 6.]</span>
<span class="sd">           [8. 9.]]</span>
<span class="sd">          [[5. 6.]</span>
<span class="sd">           [8. 9.]]]]</span>
<span class="sd">        &gt;&gt;&gt; # case 3: output_size=(1, 2)</span>
<span class="sd">        &gt;&gt;&gt; adaptive_max_pool_2d = ops.AdaptiveMaxPool2D((1, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = adaptive_max_pool_2d(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0])</span>
<span class="sd">        [[[[8. 9.]]</span>
<span class="sd">          [[8. 9.]]</span>
<span class="sd">          [[8. 9.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AdaptiveMaxPool2D.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span>
                                <span class="s1">&#39;length of output_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;output_size&quot;</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AdaptiveMaxPool3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs 3D adaptive max pooling on a multi-plane input signal.</span>

<span class="sd">    Refer to :func:`mindspore.ops.adaptive_max_pool3d` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor, with shape :math:`(C, D, H, W)` or :math:`(N, C, D, H, W)`.</span>
<span class="sd">        - **output_size** (Union[int, tuple]) - The specified output size, which is an integer that represents depth,</span>
<span class="sd">          height and width, or a tuple of three int numbers that represent depth, height and width respectively.</span>
<span class="sd">          The value must be a positive integer. If it is None, the output size and input size of the corresponding</span>
<span class="sd">          dimension are the same.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Tensor, with the same number of dims and data type as the `input`.</span>
<span class="sd">        - **argmax** (Tensor) - Tensor, the indices of max value, which has the same shape as the</span>
<span class="sd">          `y` and it&#39;s data type is int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class AdaptiveMaxPool3DNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(AdaptiveMaxPool3DNet, self).__init__()</span>
<span class="sd">        ...         self.adaptive_max_pool_3d = ops.AdaptiveMaxPool3D()</span>
<span class="sd">        ...     def construct(self, x_, output_size_):</span>
<span class="sd">        ...         return self.adaptive_max_pool_3d(x_, output_size_)</span>
<span class="sd">        &gt;&gt;&gt; x = np.arange(0,36).reshape((1, 3, 3, 4)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_size = np.array([1, 1, 2], dtype=np.int32)</span>
<span class="sd">        &gt;&gt;&gt; net = AdaptiveMaxPool3DNet()</span>
<span class="sd">        &gt;&gt;&gt; output = net(Tensor(x), Tensor(output_size))</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].asnumpy())</span>
<span class="sd">        [[[[33. 35.]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(output[1].asnumpy())</span>
<span class="sd">        [[[[33 35]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Softplus"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Softplus.html#mindspore.ops.Softplus">[文档]</a><span class="k">class</span> <span class="nc">Softplus</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softplus activation function.</span>

<span class="sd">    Softplus is a smooth approximation to the ReLU function.</span>
<span class="sd">    It can be used to constrain the output of a machine to always be positive.</span>
<span class="sd">    The function is shown as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{output} = \log(1 + \exp(\text{x}))</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of any dimension.</span>
<span class="sd">          Supported dtypes:</span>

<span class="sd">          - GPU/CPU: float16, float32, float64.</span>
<span class="sd">          - Ascend: float16, float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If the dtype of `input_x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``  ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softplus = ops.Softplus()</span>
<span class="sd">        &gt;&gt;&gt; output = softplus(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1.3132615 2.126928  3.0485873 4.01815   5.0067153]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Softplus&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Softsign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Softsign.html#mindspore.ops.Softsign">[文档]</a><span class="k">class</span> <span class="nc">Softsign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Softsign activation function.</span>

<span class="sd">    Refer to :func:`mindspore.ops.softsign` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0, -1, 2, 30, -30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softsign = ops.Softsign()</span>
<span class="sd">        &gt;&gt;&gt; output = softsign(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ 0.        -0.5         0.6666667  0.9677419 -0.9677419]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Softsign&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">ReLUV3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes ReLUV3 (Rectified Linear Unit activation function) of input tensors element-wise.</span>

<span class="sd">    It returns max(x, 0) element-wise. Specially, the neurons with the negative output</span>
<span class="sd">    will be suppressed and the active neurons will stay the same.</span>

<span class="sd">    .. math::</span>

<span class="sd">        ReLUV3(x) = (x)^+ = max(0, x)</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions, data type is</span>
<span class="sd">          `number &lt;https://www.mindspore.cn/docs/en/r2.3/api_python/mindspore.html#mindspore.dtype&gt;`_.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(N, *)`, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; relu_v3 = ops.ReLUV3()</span>
<span class="sd">        &gt;&gt;&gt; output = relu_v3(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0. 4. 0.]</span>
<span class="sd">         [2. 0. 9.]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ReLUV3&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="Mish"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Mish.html#mindspore.ops.Mish">[文档]</a><span class="k">class</span> <span class="nc">Mish</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes MISH(A Self Regularized Non-Monotonic Neural Activation Function) of input tensors element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.mish` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor.</span>
<span class="sd">          Supported dtypes:</span>

<span class="sd">          - GPU/CPU: float16, float32, float64.</span>
<span class="sd">          - Ascend: float16, float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; mish = ops.Mish()</span>
<span class="sd">        &gt;&gt;&gt; output = mish(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(2.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = mish(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        2.050599</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Mish&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="SeLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SeLU.html#mindspore.ops.SeLU">[文档]</a><span class="k">class</span> <span class="nc">SeLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Activation function SeLU (Scaled exponential Linear Unit).</span>

<span class="sd">    The activation function is defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        E_{i} =</span>
<span class="sd">        scale *</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        x_{i}, &amp;\text{if } x_{i} \geq 0; \cr</span>
<span class="sd">        \text{alpha} * (\exp(x_i) - 1), &amp;\text{otherwise.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`alpha` and :math:`scale` are pre-defined constants(:math:`alpha=1.67326324`</span>
<span class="sd">    and :math:`scale=1.05070098`).</span>

<span class="sd">    See more details in `Self-Normalizing Neural Networks &lt;https://arxiv.org/abs/1706.02515&gt;`_.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of any dimension.</span>
<span class="sd">          The data type is int8, int32, float16, float32, float64(only CPU, GPU).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `input_x` is not int8, int32, float16, float32, float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-1.0, 4.0, -8.0], [2.0, -5.0, 9.0]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; selu = ops.SeLU()</span>
<span class="sd">        &gt;&gt;&gt; output = selu(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.1113307 4.202804 -1.7575096]</span>
<span class="sd">        [ 2.101402 -1.7462534 9.456309 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SeLU&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="Tanh"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Tanh.html#mindspore.ops.Tanh">[文档]</a><span class="k">class</span> <span class="nc">Tanh</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes hyperbolic tangent of input element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.tanh` for more details.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input Tensor of any dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``  ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3, 4, 5]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; tanh = ops.Tanh()</span>
<span class="sd">        &gt;&gt;&gt; output = tanh(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.7615941 0.9640276 0.9950547 0.9993293 0.9999092]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Tanh&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">FusedBatchNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The FusedBatchNorm interface is deprecated, please use the BatchNorm interface.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The FusedBatchNorm interface is deprecated, please use the BatchNorm interface.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FusedBatchNormEx</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The FusedBatchNormEx interface is deprecated, please use the BatchNorm interface.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;FusedBatchnormEx interface is deprecated, please use BatchNorm interface.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">InstanceNorm</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Instance Normalization over a 4D input.</span>

<span class="sd">    This operator applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with</span>
<span class="sd">    additional channel dimension) as described in the paper `Instance Normalization: The Missing Ingredient for</span>
<span class="sd">    Fast Stylization &lt;https://arxiv.org/abs/1607.08022&gt;`_. It rescales and recenters the feature using a mini-batch</span>
<span class="sd">    of data and the learned parameters which can be described in the following formula.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale, :math:`\beta` is bias, :math:`\epsilon` is epsilon.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Default: ``1e-5`` .</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for running_mean and running_var</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = momentum * running\_mean + (1 - momentum) * current\_mean`).</span>
<span class="sd">            Momentum value must be [0, 1]. Default: ``0.1`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of InstanceNorm, Tensor of shape :math:`(N, C)`,</span>
<span class="sd">          data type: float16 or float32.</span>
<span class="sd">        - **gamma** (Parameter) - Scale, Tensor of shape :math:`(C,)`,</span>
<span class="sd">          data type: float32.</span>
<span class="sd">        - **beta** (Parameter) - Bias, Tensor of shape :math:`(C,)`,</span>
<span class="sd">          data type: float32.</span>
<span class="sd">        - **mean** (Parameter) - Mean value, Tensor of shape :math:`(C,)`, data type: float32.</span>
<span class="sd">        - **variance** (Parameter) - Variance value, Tensor of shape :math:`(C,)`, data type: float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, the normalized input, the updated parameters.</span>

<span class="sd">        - **output_x** (Tensor) - The output of InstanceNorm, same type and shape as the `input_x`.</span>
<span class="sd">        - **updated_moving_mean** (Tensor) - Updated mean value, Tensor of shape :math:`(NC,)`, data type: float32.</span>
<span class="sd">        - **updated_moving_variance** (Tensor) - Updated variance value, Tensor of shape :math:`(NC,)`,</span>
<span class="sd">          data type: float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `epsilon` or `momentum` is not a float.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `gamma`, `beta` or `mean` is not float32.</span>
<span class="sd">        ValueError: If `epsilon` is not in the range of [0, 1).</span>
<span class="sd">        ValueError: If `momentum` is not in the range of [0, 1].</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class InstanceNormNet(nn.Cell):</span>
<span class="sd">        &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">        &gt;&gt;&gt;         super(InstanceNormNet, self).__init__()</span>
<span class="sd">        &gt;&gt;&gt;         self.instance_norm = ops.InstanceNorm()</span>
<span class="sd">        &gt;&gt;&gt;         self.gamma = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;gamma&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.beta = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;beta&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.mean = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;mean&quot;)</span>
<span class="sd">        &gt;&gt;&gt;         self.variance = Parameter(Tensor(np.ones([64]), mindspore.float32), name=&quot;variance&quot;)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;     def construct(self, input_x):</span>
<span class="sd">        &gt;&gt;&gt;         out = self.instance_norm(input_x, self.gamma, self.beta, self.mean, self.variance)</span>
<span class="sd">        &gt;&gt;&gt;         return out</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.ones([128, 64, 32, 64]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = InstanceNormNet()</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x)</span>
<span class="sd">        &gt;&gt;&gt; result = output[0].shape</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        (128, 64, 32, 64)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InstanceNorm.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;save_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;save_variance&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">momentum</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_parameter</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">InstanceNormV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Instance Normalization over a 4D or 5D input.</span>

<span class="sd">    This operator applies Instance Normalization over a 4D or 5D input (a mini-batch of 2D inputs with</span>
<span class="sd">    additional channel dimension) as described in the paper `Instance Normalization: The Missing Ingredient for</span>
<span class="sd">    Fast Stylization &lt;https://arxiv.org/abs/1607.08022&gt;`_. It rescales and recenters the feature using a mini-batch</span>
<span class="sd">    of data and the learned parameters which can be described in the following formula.</span>

<span class="sd">    .. math::</span>

<span class="sd">        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta</span>

<span class="sd">    where :math:`\gamma` is scale(gamma), :math:`\beta` is bias(beta), :math:`\epsilon` is epsilon.</span>

<span class="sd">    Note:</span>
<span class="sd">        The format of input `x` support ``NCHW`` and ``NC1HWC0`` in platform ``CPU`` and ``Ascend``.</span>
<span class="sd">        When attr `is_training` is `False`, this module does not tracks the running mean and variance.</span>
<span class="sd">        The output `batch_mean` and `batch_variance` would be all zero.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_training(bool): An optional boolean value. Default: ``True``.</span>
<span class="sd">            When set to ``True``, this module tracks the running mean and variance.</span>
<span class="sd">            When set to ``False``, this module does not track such statistics and always uses batch</span>
<span class="sd">            statistics in both training and eval modes.</span>
<span class="sd">        momentum (float): The hyper parameter to compute moving average for running_mean and running_var</span>
<span class="sd">            (e.g. :math:`new\_running\_mean = momentum * running\_mean + (1 - momentum) * current\_mean`).</span>
<span class="sd">            Momentum value must be [0, 1]. Default: ``0.1`` .</span>
<span class="sd">        epsilon (float): A small value added to the denominator for numerical stability.</span>
<span class="sd">            Epsilon value must be [0, 1). Default: ``1e-5`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input of InstanceNormV2, Tensor of shape :math:`(N, C, H, W)`</span>
<span class="sd">          or :math:`(N, C1, H, W, C0)`, data type: float16 or float32.</span>
<span class="sd">        - **gamma** (Tensor) - Scale, Shape depends on the shape of input `x`, data type: float32.</span>
<span class="sd">          If `x` shape is :math:`(N, C, H, W)`, shape of `gamma` is :math:`(N, C, 1, 1)`.</span>
<span class="sd">          If `x` shape is :math:`(N, C1, H, W, C0)`, shape of `gamma` is :math:`(N, C1, 1, 1, C0)`.</span>
<span class="sd">        - **beta** (Tensor) - Bias, has the same shape and data type as `gamma`.</span>
<span class="sd">        - **mean** (Tensor) - Mean value, has the same shape and data type as `gamma`.</span>
<span class="sd">        - **variance** (Tensor) - Variance value, has the same shape and data type as `gamma`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, the normalized input, the mean and variance of batch input.</span>

<span class="sd">        - **y** (Tensor) - The output of InstanceNormV2, same type and shape as the `x`.</span>
<span class="sd">        - **batch_mean** (Tensor) - The mean value of batch input, same type and shape as the input `mean`.</span>
<span class="sd">        - **batch_variance** (Tensor) - The variance value of batch input, same type and shape as the input `variance`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If either item in the inputs is not Tensor.</span>
<span class="sd">        TypeError: If data type of `x` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If data type of `gamma` is not a Tensor of float32.</span>
<span class="sd">        TypeError: If data type of `beta` is not a Tensor of float32.</span>
<span class="sd">        TypeError: If data type of `mean` is not a Tensor of float32.</span>
<span class="sd">        TypeError: If data type of `variance` is not a Tensor of float32.</span>
<span class="sd">        TypeError: If data type of attr `is_training` is not bool.</span>
<span class="sd">        TypeError: If data type of attr `momentum` is not float.</span>
<span class="sd">        TypeError: If data type of attr `epsilon` is not float.</span>
<span class="sd">        ValueError: If :math:`H * W &lt;= 1` in input `x`.</span>
<span class="sd">        ValueError: If the shape of either item in the inputs is neither 4D nor 5D.</span>
<span class="sd">        ValueError: If `epsilon` is not in the range of [0, 1).</span>
<span class="sd">        ValueError: If `momentum` is not in the range of [0, 1].</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(input_data=np.random.randn(128, 48, 32, 64, 12), dtype=mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gamma = Tensor(input_data=np.random.randn(128, 48, 1, 1, 12), dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta = Tensor(input_data=np.random.randn(128, 48, 1, 1, 12), dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; mean = Tensor(input_data=np.random.randn(128, 48, 1, 1, 12), dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; var = Tensor(input_data=np.random.randn(128, 48, 1, 1, 12), dtype=mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; ops = P.InstanceNormV2()</span>
<span class="sd">        &gt;&gt;&gt; output = ops(x, gamma, beta, mean, var)</span>
<span class="sd">        &gt;&gt;&gt; y_shape = output[0].shape</span>
<span class="sd">        &gt;&gt;&gt; print(y_shape)</span>
<span class="sd">        (128, 48, 32, 64, 12)</span>
<span class="sd">        &gt;&gt;&gt; batch_mean_shape = output[1].shape</span>
<span class="sd">        &gt;&gt;&gt; print(batch_mean_shape)</span>
<span class="sd">        (128, 48, 1, 1, 12)</span>
<span class="sd">        &gt;&gt;&gt; batch_var_shape = output[2].shape</span>
<span class="sd">        &gt;&gt;&gt; print(batch_var_shape)</span>
<span class="sd">        (128, 48, 1, 1, 12)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variance&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InstanceNormV2.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">,</span> <span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;variance&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_mean&#39;</span><span class="p">,</span> <span class="s1">&#39;batch_variance&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_float</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_float</span><span class="p">(</span><span class="n">momentum</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_RIGHT</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">momentum</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">is_training</span><span class="p">,</span> <span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="Conv2D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Conv2D.html#mindspore.ops.Conv2D">[文档]</a><span class="k">class</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    2D convolution layer.</span>

<span class="sd">    Applies a 2D convolution over an input tensor which is typically of shape :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size, :math:`C` is channel number, :math:`H` is feature height, :math:`W` is feature width.</span>

<span class="sd">    The output is calculated based on formula:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{in} - 1} \text{ccor}({\text{weight}(C_{\text{out}_j}, k), \text{X}(N_i, k)})</span>

<span class="sd">    where :math:`bias` is the output channel bias, :math:`ccor` is</span>
<span class="sd">    the `cross-correlation &lt;https://en.wikipedia.org/wiki/Cross-correlation&gt;`_,</span>
<span class="sd">    , :math:`weight` is the convolution kernel value and :math:`X` represents the input feature map.</span>

<span class="sd">    Here are the indices&#39; meanings:</span>

<span class="sd">    - :math:`i` corresponds to the batch number, the range is :math:`[0, N-1]`,</span>
<span class="sd">      where :math:`N` is the batch size of the input.</span>

<span class="sd">    - :math:`j` corresponds to the output channel, the range is :math:`[0, C_{out}-1]`,</span>
<span class="sd">      where :math:`C_{out}` is the number of</span>
<span class="sd">      output channels, which is also equal to the number of kernels.</span>

<span class="sd">    - :math:`k` corresponds to the input channel, the range is :math:`[0, C_{in}-1]`,</span>
<span class="sd">      where :math:`C_{in}` is the number of</span>
<span class="sd">      input channels, which is also equal to the number of channels in the convolutional kernels.</span>

<span class="sd">    Therefore, in the above formula, :math:`{bias}(C_{\text{out}_j})` represents the bias of the :math:`j`-th</span>
<span class="sd">    output channel, :math:`{weight}(C_{\text{out}_j}, k)` represents the slice of the :math:`j`-th convolutional</span>
<span class="sd">    kernel in the :math:`k`-th channel, and :math:`{X}(N_i, k)` represents the slice of the :math:`k`-th input</span>
<span class="sd">    channel in the :math:`i`-th batch of the input feature map.</span>

<span class="sd">    The shape of the convolutional kernel is given by :math:`(\text{kernel_size[0]},\text{kernel_size[1]})`,</span>
<span class="sd">    where :math:`\text{kernel_size[0]}`</span>
<span class="sd">    and :math:`\text{kernel_size[1]}` are the height and width of the kernel, respectively.</span>
<span class="sd">    If we consider the input and output channels as well as the `group` parameter, the complete kernel shape</span>
<span class="sd">    will be :math:`(C_{out}, C_{in} / \text{group}, \text{kernel_size[0]}, \text{kernel_size[1]})`,</span>
<span class="sd">    where `group` is the number of groups dividing `x`&#39;s input channel when applying group convolution.</span>

<span class="sd">    For more details about convolution layer, please refer to `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_.</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend platform, only group convolution in depthwise convolution scenarios is supported.</span>
<span class="sd">        That is, when `group&gt;1`, condition `in\_channels` = `out\_channels` = `group` must be satisfied.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): Specifies output channel :math:`C_{out}`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): Specifies the height and width of the 2D convolution kernel.</span>
<span class="sd">            It can be a single int or a tuple of 2 integers. A single int means the value is for both the height</span>
<span class="sd">            and the width. A tuple of 2 ints means the first value is for the height and the other is for the width.</span>
<span class="sd">        mode (int, optional): Modes for different convolutions. The value is currently not used. Default: ``1`` .</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;same&quot;`` , ``&quot;valid&quot;`` or ``&quot;pad&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Pad the input around its edges so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally, If the amount is even, it is</span>
<span class="sd">              uniformly distributed around the input, if it is odd, the excess amount goes to the right/bottom side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;valid&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded. If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;pad&quot;``: Pad the input with a specified amount. In this mode, the amount of padding</span>
<span class="sd">              in the height and width directions is determined by the `pad` parameter.</span>
<span class="sd">              If this mode is set, `pad` must be greater than or equal to 0.</span>

<span class="sd">        pad (Union(int, tuple[int]), optional): Specifies the amount of padding to apply on input</span>
<span class="sd">            when `pad_mode` is set to ``&quot;pad&quot;``. It can be a single int or a tuple of 4 ints.</span>
<span class="sd">            If `pad` is one integer, the paddings of top, bottom, left and right are the same, equal to `pad`.</span>
<span class="sd">            If `pad` is a tuple with four integers, the paddings of top, bottom, left and right will be equal to pad[0],</span>
<span class="sd">            pad[1], pad[2], and pad[3] accordingly. Default: ``0`` .</span>
<span class="sd">        stride (Union(int, tuple[int]), optional): Specifies the stride of the convolution kernel&#39;s movement.</span>
<span class="sd">            It can be a single int or a tuple of two or four ints. A single int means the stride is the same in</span>
<span class="sd">            both the height and width directions. A tuple of two ints indicates the strides in the height and</span>
<span class="sd">            width directions, respectively. For a tuple of four ints, the two ints correspond to (N, C) dimension</span>
<span class="sd">            are treated as 1, and the two correspond to (H, W) dimensions is the step size in the height</span>
<span class="sd">            and width directions respectively. Default: ``1`` .</span>
<span class="sd">        dilation (Union(int, tuple[int]), optional): Specifies the dilation rate to use for dilated convolution.</span>
<span class="sd">            It can be a single int or a tuple of 2 or 4 integers. A single int means the dilation size is the same</span>
<span class="sd">            in both the height and width directions. A tuple of two ints represents the dilation size in</span>
<span class="sd">            the height and width directions, respectively. For a tuple of four ints, the two ints correspond</span>
<span class="sd">            to (N, C) dimension are treated as 1, and the two correspond to (H, W) dimensions is the</span>
<span class="sd">            dilation size in the height and width directions respectively.</span>
<span class="sd">            Assuming :math:`dilation=(d0, d1)`, the convolutional kernel samples the input with a</span>
<span class="sd">            spacing of :math:`d0-1` elements in the height direction and :math:`d1-1` elements in the width direction.</span>
<span class="sd">            The values in the height and width dimensions are in the ranges [1, H] and [1, W], respectively.</span>
<span class="sd">            Default: ``1`` .</span>
<span class="sd">        group (int, optional): Specifies the number of groups dividing `x`&#39;s input channel when applying</span>
<span class="sd">            group convolution. Default: ``1`` .</span>
<span class="sd">        data_format (str, optional): The optional value for data format, is ``&#39;NHWC&#39;`` or ``&#39;NCHW&#39;`` .</span>
<span class="sd">            Default: ``&quot;NCHW&quot;``. (NHWC is only supported in GPU now.)</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})` or</span>
<span class="sd">          :math:`(N, H_{in}, W_{in}, C_{in}, )` depending on `data_format` .</span>
<span class="sd">        - **weight** (Tensor) - The convolutional kernel value, it should has shape</span>
<span class="sd">          :math:`(C_{out}, C_{in} / \text{group}, \text{kernel_size[0]}, \text{kernel_size[1]})` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied 2D convolution. The shape is :math:`(N, C_{out}, H_{out}, W_{out})`</span>
<span class="sd">        or :math:`(N, H_{out}, W_{out}, C_{out}, )`.</span>
<span class="sd">        To see how different pad modes affect the output shape, please refer to</span>
<span class="sd">        :class:`mindspore.nn.Conv2d` for more details.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        ValueError: If `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of ``&#39;same&#39;``, ``&#39;valid&#39;`` or ``&#39;pad&#39;``.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 4.</span>
<span class="sd">        ValueError: If `pad_mode` it not equal to ``&#39;pad&#39;`` and `pad` is not equal to ``(0, 0, 0, 0)``.</span>
<span class="sd">        ValueError: If `data_format` is neither ``&#39;NHWC&#39;`` nor ``&#39;NCHW&#39;`` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: All parameters use default values.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv2d = ops.Conv2D(out_channel=32, kernel_size=3)</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 30, 30)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: pad_mode=&quot;pad&quot;, other parameters being default.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv2d = ops.Conv2D(out_channel=32, kernel_size=3, pad_mode=&quot;pad&quot;, pad=(4, 10, 4, 10))</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 44, 44)</span>
<span class="sd">        &gt;&gt;&gt; # case 3: stride=(2, 4), other parameters being default.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv2d = ops.Conv2D(out_channel=32, kernel_size=3, stride=(2, 4))</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 15, 8)</span>
<span class="sd">        &gt;&gt;&gt; # case 4: dilation=2, other parameters being default.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv2d = ops.Conv2D(out_channel=32, kernel_size=3, dilation=2)</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 28, 28)</span>
<span class="sd">        &gt;&gt;&gt; # case 5: group=2, other parameters being default.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 64, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv2d = ops.Conv2D(out_channel=32, kernel_size=3, group=2)</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 30, 30)</span>
<span class="sd">        &gt;&gt;&gt; # case 6: All parameters are specified.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 64, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv2d = ops.Conv2D(out_channel=32, kernel_size=3, pad_mode=&quot;pad&quot;,</span>
<span class="sd">        ...                     pad=(4, 10, 4, 10), stride=(2, 4), dilation=2,  group=2)</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 21, 11)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2D&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero when &#39;pad_mode&#39; is not &#39;pad&#39;, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got &#39;pad&#39;: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39;: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;and platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">DataFormatVecPermute</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Converts the input tensor from the `src_format` to the `dst_format` by permuting its dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        src_format (str, optional): the source data format, which can be ``&#39;NHWC&#39;`` and ``&#39;NCHW&#39;`` .</span>
<span class="sd">          Default: ``&#39;NHWC&#39;`` .</span>
<span class="sd">        dst_format (str, optional): the target data format, which can be ``&#39;NHWC&#39;`` and ``&#39;NCHW&#39;`` .</span>
<span class="sd">          Default: ``&#39;NCHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A Tensor of shape :math:`(4, )` or :math:`(4, 2)` in source data format.</span>
<span class="sd">          Supports int32 and int64 datatype.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same data type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `src_format` or `dst_format` is not a str in [&#39;NHWC&#39;, &#39;NCHW&#39;].</span>
<span class="sd">        ValueError: If `input_x` shape is not :math:`(4, )` or :math:`(4, 2)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, src_format=&quot;NHWC&quot;, dst_format=&quot;NCHW&quot;):</span>
<span class="sd">        ...         super().__init__()</span>
<span class="sd">        ...         self.op = ops.DataFormatVecPermute(src_format, dst_format)</span>
<span class="sd">        ...     def construct(self, x):</span>
<span class="sd">        ...         return self.op(x)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [1 4 2 3]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="n">dst_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DataFormatVecPermute.&quot;&quot;&quot;</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;NCHW&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">src_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;src_format&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">dst_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;dst_format&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">DepthwiseConv2dNative</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DepthwiseConv2dNative will be deprecated in the future. Please use :class:`mindspore.nn.Conv2d` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">channel_multiplier</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DepthwiseConv2dNative&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;WARN_DEPRECATED: The usage of DepthwiseConv2dNative is deprecated.&quot;</span>
                       <span class="s2">&quot; Please use nn.Conv2D.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The height and width of &#39;stride&#39; must be equal,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got height:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">,  width:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The height and width of &#39;dilation&#39; must be equal,&quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got height:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">,  width:</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero or (0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">channel_multiplier</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">channel_multiplier</span><span class="p">,</span> <span class="s2">&quot;channel_multiplier&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s2">&quot;group&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;offset_a&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;weight rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;w_shape[2:4]&#39;</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">w_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]),</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">kernel_size_n</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kernel_size_h</span><span class="p">,</span> <span class="n">kernel_size_w</span> <span class="o">=</span> <span class="n">w_shape</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">dilation_h</span><span class="p">,</span> <span class="n">dilation_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span>
        <span class="k">if</span> <span class="n">kernel_size_n</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the batch of &#39;weight&#39; must be 1, but got </span><span class="si">{</span><span class="n">kernel_size_n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;valid&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>

            <span class="n">pad_needed_h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">h_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_h</span> <span class="o">+</span> <span class="n">dilation_h</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">pad_top</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_h</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_bottom</span> <span class="o">=</span> <span class="n">pad_needed_h</span> <span class="o">-</span> <span class="n">pad_top</span>

            <span class="n">pad_needed_w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">w_out</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride_w</span> <span class="o">+</span> <span class="n">dilation_w</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">pad_left</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">pad_needed_w</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">pad_right</span> <span class="o">=</span> <span class="n">pad_needed_w</span> <span class="o">-</span> <span class="n">pad_left</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span>

            <span class="n">h_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_top</span> <span class="o">+</span> <span class="n">pad_bottom</span> <span class="o">-</span> <span class="n">kernel_size_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                    <span class="o">/</span> <span class="n">stride_h</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">pad_left</span> <span class="o">+</span> <span class="n">pad_right</span> <span class="o">-</span> <span class="n">kernel_size_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_size_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> \
                    <span class="o">/</span> <span class="n">stride_w</span>
            <span class="n">h_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">h_out</span><span class="p">)</span>
            <span class="n">w_out</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">w_out</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_top</span><span class="p">,</span> <span class="n">pad_bottom</span><span class="p">,</span> <span class="n">pad_left</span><span class="p">,</span> <span class="n">pad_right</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>

        <span class="n">out_channel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">channel_multiplier</span> <span class="o">*</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">h_out</span><span class="p">,</span> <span class="n">w_out</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="n">w_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">number_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x_dtype</span><span class="o">.</span><span class="n">element_type</span><span class="p">()</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mstype</span><span class="o">.</span><span class="n">TensorType</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>


<span class="k">class</span> <span class="nc">_Pool</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs max/avg pooling operation.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the kernel, that must be a tuple</span>
<span class="sd">           of two `int` for height and width. Default: ``1`` .</span>
<span class="sd">        strides (Union[int, tuple[int]]): The stride of the window, that must be</span>
<span class="sd">            a tuple of two `int` for height and width. Default: ``1`` .</span>
<span class="sd">        pad_mode (str): The optional value for pad mode, is ``&quot;same&quot;`` or ``&quot;valid&quot;`` .</span>
<span class="sd">            Default: ``&quot;valid&quot;`` .</span>
<span class="sd">        data_format (str): The optional value for data format, is ``&#39;NHWC&#39;`` or ``&#39;NCHW&#39;`` .</span>
<span class="sd">            Default: ``&quot;NCHW&quot;`` .</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize _Pool.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;MaxPoolWithArgmax&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
        <span class="n">x_shape_norm</span> <span class="o">=</span> <span class="n">x_shape</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">[</span><span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]]</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape_norm</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">input_h</span><span class="p">,</span> <span class="n">input_w</span> <span class="o">=</span> <span class="n">x_shape_norm</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_maxpoolwithargmax</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">kernel_h</span><span class="p">,</span> <span class="n">kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">stride_h</span><span class="p">,</span> <span class="n">stride_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;VALID&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_h</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">out_h</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_h</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">input_w</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">out_w</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="n">input_w</span> <span class="o">-</span> <span class="p">(</span><span class="n">kernel_w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s2">&quot;SAME&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">input_h</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">out_h</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_h</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_h</span> <span class="o">/</span> <span class="n">stride_h</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">input_w</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">out_w</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">out_w</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">input_w</span> <span class="o">/</span> <span class="n">stride_w</span><span class="p">)</span>
        <span class="n">out_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">out_h</span><span class="p">,</span> <span class="n">out_w</span><span class="p">,</span> <span class="n">channel</span><span class="p">]</span>

        <span class="n">is_dynamic_shape</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">in_shape_val</span> <span class="ow">in</span> <span class="n">x_shape_norm</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">in_shape_val</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">is_dynamic_shape</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">for</span> <span class="n">out_shape_val</span> <span class="ow">in</span> <span class="n">out_shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">out_shape_val</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_dynamic_shape</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the each element of the output shape must be larger than 0, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got output shape: </span><span class="si">{</span><span class="n">out_shape</span><span class="si">}</span><span class="s2">. The input shape: </span><span class="si">{</span><span class="n">x_shape</span><span class="si">}</span><span class="s2">, &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;kernel size: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="si">}</span><span class="s2">, strides: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="si">}</span><span class="s2">.&quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;Please check the official api documents for &quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;more information about the output.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_subclass</span><span class="p">(</span><span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">tensor_type</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x_dtype</span>


<div class="viewcode-block" id="MaxPool"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaxPool.html#mindspore.ops.MaxPool">[文档]</a><span class="k">class</span> <span class="nc">MaxPool</span><span class="p">(</span><span class="n">_Pool</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Max pooling operation.</span>

<span class="sd">    Applies a 2D max pooling over an input Tensor which can be regarded as a composition of 2D planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1)`, the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively. Default: ``1`` .</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            not only the height of movement but also the width of movement, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: ``1`` .</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&#39;same&#39;`` or ``&#39;valid&#39;`` . Default: ``&#39;valid&#39;`` .</span>

<span class="sd">            - ``&#39;same&#39;``: Pad the input around its edges so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally, If the amount is even, it is</span>
<span class="sd">              uniformly distributed around the input, if it is odd, the excess amount goes to the right/bottom side.</span>
<span class="sd">            - ``&#39;valid&#39;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded.</span>

<span class="sd">        data_format (str) : The optional value for data format, is ``&#39;NHWC&#39;`` or ``&#39;NCHW&#39;`` .</span>
<span class="sd">            Default: ``&#39;NCHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Supported dtypes:</span>

<span class="sd">          - CPU: float16, float32, float64.</span>
<span class="sd">          - GPU/Ascend: float16, float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `pad_mode` is neither ``&#39;valid&#39;`` nor ``&#39;same&#39;`` with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither ``&#39;NCHW&#39;`` nor ``&#39;NHWC&#39;``.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `input` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape((1, 3, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maxpool_op = ops.MaxPool(pad_mode=&quot;VALID&quot;, kernel_size=2, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; output = maxpool_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[ 5.  6.  7.]</span>
<span class="sd">           [ 9. 10. 11.]]</span>
<span class="sd">          [[17. 18. 19.]</span>
<span class="sd">           [21. 22. 23.]]</span>
<span class="sd">          [[29. 30. 31.]</span>
<span class="sd">           [33. 34. 35.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxPool.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MaxPool</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">MaxPoolV1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Maxpooling operation.</span>

<span class="sd">    Applies a 2D maxpooling over an input Tensor which can be regarded as a composition of 2D planes.</span>

<span class="sd">    Typically, the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPoolV1</span>
<span class="sd">    outputs regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and stride :math:`s = (s_h, s_w)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_h \times h + m, s_w \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the max value,</span>
<span class="sd">            is an integer that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two integers that represent height and width respectively. Default: ``1`` .</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an integer that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two integers that</span>
<span class="sd">            represent height and width of movement, respectively. Default: ``1`` .</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;same&quot;`` or ``&quot;valid&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Pad the input around its edges so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally, If the amount is even, it is</span>
<span class="sd">              uniformly distributed around the input, if it is odd, the excess amount goes to the right/bottom side.</span>
<span class="sd">            - ``&quot;valid&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded.</span>

<span class="sd">        data_format (str) : The optional value for data format, is ``&#39;NCHW&#39;`` or ``&#39;NHWC&#39;`` .</span>
<span class="sd">            Default: ``&#39;NCHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NHWC&#39; nor &#39;NCHW&#39;.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If the length of shape of `input` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 3 * 3 * 4).reshape((1, 3, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maxpoolv1_op = ops.MaxPoolV1(pad_mode=&quot;VALID&quot;, kernel_size=2, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; output_ = maxpoolv1_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output_)</span>
<span class="sd">        [[[[ 5.  6.  7.]</span>
<span class="sd">           [ 9. 10. 11.]]</span>
<span class="sd">          [[17. 18. 19.]</span>
<span class="sd">           [21. 22. 23.]]</span>
<span class="sd">          [[29. 30. 31.]</span>
<span class="sd">           [33. 34. 35.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxPoolV1.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">kernel_size_adapted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s1">&#39;NCHW&#39;</span> <span class="k">else</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">strides_adapted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s1">&#39;NCHW&#39;</span> <span class="k">else</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size_adapted</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides_adapted</span><span class="p">)</span>


<div class="viewcode-block" id="MaxPool3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaxPool3D.html#mindspore.ops.MaxPool3D">[文档]</a><span class="k">class</span> <span class="nc">MaxPool3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D max pooling over an input Tensor which can be regarded as a composition of 3D planes.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1, s_2)`, the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \max_{l=0, \ldots, d_{ker}-1} \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents depth, height and width of the kernel, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively. Default: ``1`` .</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            not only the depth, height of movement but also the width of movement,, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: ``1`` .</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;SAME&quot;`` , ``&quot;VALID&quot;`` or ``&quot;PAD&quot;`` . Default: ``&quot;VALID&quot;`` .</span>

<span class="sd">            - ``&quot;SAME&quot;``: Pad the input around its depth/height/width dimension so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally.  If the amount is even,</span>
<span class="sd">              it isuniformly distributed around the input, if it is odd, the excess amount goes</span>
<span class="sd">              to the front/right/bottom side.</span>
<span class="sd">              If this mode is set, `pad_list` must be 0.</span>
<span class="sd">            - ``&quot;VALID&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible depth, height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded. If this mode is set, `pad_list` must be 0.</span>
<span class="sd">            - ``&quot;PAD&quot;``: Pad the input with a specified amount. In this mode, the amount of padding</span>
<span class="sd">              in the depth, height and width dimension is determined by the `pad_list` parameter.</span>
<span class="sd">              If this mode is set, `pad_list` must be greater than or equal to 0.</span>

<span class="sd">        pad_list (Union(int, tuple[int])): The pad value to be filled. Default: ``0`` . If `pad` is an integer, the</span>
<span class="sd">            paddings of head, tail, top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of six</span>
<span class="sd">            integers, the padding of head, tail, top, bottom, left and right equals to pad[0], pad[1], pad[2],</span>
<span class="sd">            pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        ceil_mode (Union[bool, None]): Whether to use ceil instead of floor to calculate output shape.</span>
<span class="sd">            Only effective in &quot;pad&quot; mode.</span>
<span class="sd">            When `pad_mode` is ``&quot;pad&quot;`` and &quot;ceil_mode&quot; is ``None`` , `ceil_mode` will be set as ``False``.</span>
<span class="sd">            Default: ``None`` .</span>
<span class="sd">        data_format (str) : The optional value for data format. Currently only support ``&quot;NCDHW&quot;`` .</span>
<span class="sd">            Default: ``&quot;NCDHW&quot;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})`. Has the data type of `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `pad_mode` or `data_format` is not a string.</span>
<span class="sd">        ValueError: If numbers in `kernel_size` or `strides` are not positive.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of ``&quot;SAME&quot;``, ``&quot;VALID&quot;`` or ``&quot;PAD&quot;``.</span>
<span class="sd">        ValueError: If `pad_mode` is ``&quot;SAME&quot;`` or ``&quot;VALID&quot;``, `ceil_mode` is not ``None``.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is a tuple whose length is not equal to 3.</span>
<span class="sd">        ValueError: If `data_format` is not ``&quot;NCDHW&quot;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 2 * 2 * 2 * 3).reshape((1, 2, 2, 2, 3)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; max_pool3d = ops.MaxPool3D(kernel_size=2, strides=1, pad_mode=&quot;VALID&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = max_pool3d(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[10. 11.]]]</span>
<span class="sd">          [[[22. 23.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;VALID&quot;</span><span class="p">,</span> <span class="n">pad_list</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxPool3D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="s1">&#39;PAD&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;PAD&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="s2">&quot;CALCULATED&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ceil_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s2">&quot;CALCULATED&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;When the &#39;pad_mode&#39; is &#39;same&#39; or &#39;valid&#39;, the &#39;ceil_mode&#39; only supports &#39;None&#39;.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ceil_mode&quot;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span><span class="p">))</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="n">pad_list</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad_list</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pad_list</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pad_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pad_list</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">pad_list</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">pad_list</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;pad_list&#39; must be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;three or six positive int numbers, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span><span class="si">}</span><span class="s2"> numbers.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;CALCULATED&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad_list&#39; must be zero or (0, 0, 0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad_list&#39; is </span><span class="si">{</span><span class="n">pad_list</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;CALCULATED&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad_list item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_list&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span></div>


<div class="viewcode-block" id="MaxUnpool2D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaxUnpool2D.html#mindspore.ops.MaxUnpool2D">[文档]</a><span class="k">class</span> <span class="nc">MaxUnpool2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the partial inverse of MaxPool2D operation.</span>

<span class="sd">    Since MaxPool2D loses non-maximal values, it is not fully invertible.</span>
<span class="sd">    Therefore, MaxUnpool2D takes the output of MaxPool2D, including the indices of</span>
<span class="sd">    the maximal values, and computes a partial inverse where all non-maximal values are set to zero.</span>
<span class="sd">    Typically the input is of shape :math:`(N, C, H_{in}, W_{in})` ,</span>
<span class="sd">    the output is of shape :math:`(N, C, H_{out}, W_{out})` , the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        H_{out} = (H{in} - 1) \times strides[0] - 2 \times pads[0] + ksize[0] \\</span>
<span class="sd">        W_{out} = (W{in} - 1) \times strides[1] - 2 \times pads[1] + ksize[1] \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two int numbers that represent height and width respectively.</span>
<span class="sd">        strides (Union[int, tuple[int]], optional): The strides of kernel moving.</span>
<span class="sd">            If `strides` is 0 or (0, 0), then `strides` equal to `ksize` . Default: ``0`` .</span>

<span class="sd">            - An int number that represents the height and width of movement are both `strides` .</span>
<span class="sd">            - A tuple of two int numbers that represent height and width of movement respectively.</span>

<span class="sd">        pads (Union[int, tuple[int]], optional): The pad value to be filled. Default: ``0`` .</span>

<span class="sd">            - If `pads` is an integer, the paddings of height and width are the same, equal to pads.</span>
<span class="sd">            - If `pads` is a tuple of two integers, the padding of height and width equal to pads[0]</span>
<span class="sd">              and pads[1] correspondingly.</span>

<span class="sd">        output_shape (tuple[int], optional): The target output size is an optional input. Default: ``()`` .</span>

<span class="sd">            - If :math:`output\_shape == ()` , then the shape of output computed by `kszie`, `strides` and `pads` .</span>
<span class="sd">            - If :math:`output\_shape != ()` , then `output_shape` must be :math:`(N, C, H, W)` or :math:`(N, H, W, C)`</span>
<span class="sd">              and `output_shape` must belong to :math:`[(N, C, H_{out} - strides[0], W_{out} - strides[1]),</span>
<span class="sd">              (N, C, H_{out} + strides[0], W_{out} + strides[1])]`.</span>

<span class="sd">        data_format (str, optional): The optional value for data format.</span>
<span class="sd">            Currently support ``&quot;NCHW&quot;`` and ``&quot;NHWC&quot;`` . Default: ``&quot;NCHW&quot;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor to invert.</span>
<span class="sd">          Tensor of shape :math:`(N, C, H_{in}, W_{in})` or :math:`(N, H_{in}, W_{in}, C)`.</span>
<span class="sd">        - **argmax** (Tensor) - Max values&#39; index represented by the `argmax`.</span>
<span class="sd">          Tensor of shape must be same with input `x`.</span>
<span class="sd">          Values of `argmax` must belong to :math:`[0, H_{in} \times W_{in} - 1]`.</span>
<span class="sd">          Data type must be in int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, H_{out}, W_{out})` or :math:`(N, H_{out}, W_{out}, C)`.</span>
<span class="sd">        Has the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` or `argmax` is not supported.</span>
<span class="sd">        TypeError: If `ksize`, `strides` or `pads` is neither int nor tuple.</span>
<span class="sd">        ValueError: If numbers in `strides` (also support 0 and (0, 0)) or `ksize` is not positive.</span>
<span class="sd">        ValueError: If numbers in `pads` is negative.</span>
<span class="sd">        ValueError: If `ksize`, `strides` or `pads` is a tuple whose length is not equal to 2.</span>
<span class="sd">        ValueError: If `data_format` is not a str or is neither `NCHW` nor `NHWC`.</span>
<span class="sd">        ValueError: If `output_shape` whose length is neither 0 or 4.</span>
<span class="sd">        ValueError: If `output_shape` is not close to output size</span>
<span class="sd">                    computed by attr `ksize`, `strides` and `pads`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[[0, 1], [8, 9]]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; argmax = Tensor(np.array([[[[0, 1], [2, 3]]]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; maxunpool2d = ops.MaxUnpool2D(ksize=1, strides=1, pads=0)</span>
<span class="sd">        &gt;&gt;&gt; output = maxunpool2d(x, argmax)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [[[[0. 1.]</span>
<span class="sd">            [8. 9.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pads</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxUnpool2D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">strides</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)):</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="n">ksize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">strict_positive</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">output_shape</span></div>


<div class="viewcode-block" id="MaxUnpool3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaxUnpool3D.html#mindspore.ops.MaxUnpool3D">[文档]</a><span class="k">class</span> <span class="nc">MaxUnpool3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the inverse of :class:`mindspore.ops.MaxPool3D`.</span>

<span class="sd">    MaxUnpool3D keeps the maximal value and set all position of non-maximal values to zero.</span>
<span class="sd">    Typically the input is of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`, the output is of</span>
<span class="sd">    shape :math:`(N, C, D_{out}, H_{out}, W_{out})`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">        D_{out} = (D{in} - 1) \times strides[0] - 2 \times pads[0] + ksize[0] \\</span>
<span class="sd">        H_{out} = (H{in} - 1) \times strides[1] - 2 \times pads[1] + ksize[1] \\</span>
<span class="sd">        W_{out} = (W{in} - 1) \times strides[2] - 2 \times pads[2] + ksize[2] \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of kernel used to take the maximum value,</span>
<span class="sd">            is an int number that represents depth, height and width of the kernel, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively.</span>
<span class="sd">        strides (Union[int, tuple[int]], optional): The distance of kernel moving. Default: ``0`` .</span>

<span class="sd">            - If it is an int number, the depth, height and width of movement are all equal to `strides`.</span>
<span class="sd">            - If it is a tuple of three int numbers, they represent depth, height and width of movement respectively.</span>
<span class="sd">            - If strides is 0 or (0, 0, 0), then `strides` equal to `ksize`.</span>

<span class="sd">        pads (Union[int, tuple[int]], optional): The pad value to be filled. Default: ``0`` .</span>

<span class="sd">            - If `pads` is an integer, the paddings of depth, height and width are the same, equal to pads.</span>
<span class="sd">            - If `pads` is a tuple of three integers, the padding of depth, height and width equal to pads[0],</span>
<span class="sd">              pads[1] and pads[2] correspondingly.</span>

<span class="sd">        output_shape (tuple[int], optional) : The target output size. Default: ``()`` .</span>
<span class="sd">            If :math:`output\_shape == ()`, then the shape of output computed by kszie, strides and pads shown above.</span>
<span class="sd">            If :math:`output\_shape != ()`, then output_shape format must be :math:`(N, C, D, H, W)` or</span>
<span class="sd">            :math:`(N, D, H, W, C)` and output_shape must be in range</span>
<span class="sd">            :math:`[(N, C, D_{out} - strides[0], H_{out} - strides[1], W_{out} - strides[2]),</span>
<span class="sd">            (N, C, D_{out} + strides[0], H_{out} + strides[1], W_{out} + strides[2])]`.</span>
<span class="sd">        data_format (str, optional) : The optional value for data format. Currently</span>
<span class="sd">            support ``&#39;NCDHW&#39;`` and ``&#39;NDHWC&#39;`` . Default: ``&#39;NCDHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor to invert.</span>
<span class="sd">          Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(N, D_{in}, H_{in}, W_{in}, C)`.</span>
<span class="sd">        - **argmax** (Tensor) - Max values&#39; index. Tensor that has the same shape as `x`.</span>
<span class="sd">          Values of `argmax` must be in range :math:`[0, D_{in} \times H_{in} \times W_{in} - 1]`.</span>
<span class="sd">          Data type must be int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(N, D_{out}, H_{out}, W_{out}, C)`.</span>
<span class="sd">        Has the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` or `argmax` is Number.</span>
<span class="sd">        TypeError: If `ksize`, `strides` or `pads` is neither int nor tuple.</span>
<span class="sd">        ValueError: If numbers in `strides` or `ksize` is negative.</span>
<span class="sd">        ValueError: If numbers in `pads` is negative.</span>
<span class="sd">        ValueError: If `ksize`, `strides` or `pads` is a tuple whose length is not equal to 3.</span>
<span class="sd">        ValueError: If `data_format` is not a str or is neither ``&#39;NCDHW&#39;`` nor ``&#39;NDHWC&#39;``.</span>
<span class="sd">        ValueError: If `output_shape` whose length is neither 0 or 5.</span>
<span class="sd">        ValueError: If `output_shape` is not close to output size range</span>
<span class="sd">                    computed by attr `ksize, strides, pads`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[[[0, 1], [8, 9]]]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; argmax = Tensor(np.array([[[[[0, 1], [2, 3]]]]]).astype(np.int64))</span>
<span class="sd">        &gt;&gt;&gt; maxunpool3d = ops.MaxUnpool3D(ksize=1, strides=1, pads=0)</span>
<span class="sd">        &gt;&gt;&gt; output = maxunpool3d(x, argmax)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [[[[[0. 1.]</span>
<span class="sd">            [8. 9.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">pads</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxUnpool3D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">strides</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)):</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="n">ksize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">greater_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NDHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NDHWC&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">output_shape</span></div>


<span class="k">class</span> <span class="nc">AvgPoolV1</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Average-pooling operation.</span>

<span class="sd">    Applies a 2D average pooling over an input Tensor which can be regarded as a composition of 2D planes.</span>
<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, AvgPoolV1 outputs</span>
<span class="sd">    regional average in the :math:`(H_{in}, W_{in})`-dimension. Given window size</span>
<span class="sd">    :math:`ks = (h_{ker}, w_{ker})` and strides :math:`s = (s_0, s_1)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \frac{1}{h_{ker} * w_{ker}} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - Only single input and single output are supported.</span>
<span class="sd">        - Global average pooling is supported.</span>
<span class="sd">        - The height of &quot;kernel_size&quot; and the weight of &quot;kernel_size&quot; are positive integers within the range [1, 255].</span>
<span class="sd">          ksize_h * ksize_w &lt; 256.</span>
<span class="sd">        - Due to instruction restrictions, the values of &quot;strides_h&quot; and &quot;strides_w&quot; are</span>
<span class="sd">          positive integers within the range [1, 64).</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the kernel used to take the average value,</span>
<span class="sd">            is an integer that represents height and width of the kernel, or a tuple</span>
<span class="sd">            of two integers that represent height and width respectively. Default: ``1`` .</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an integer that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two integers that</span>
<span class="sd">            represent height and width of movement, respectively. Default: ``1`` .</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;same&quot;`` or ``&quot;valid&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Pad the input around its edges so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally, If the amount is even, it is</span>
<span class="sd">              uniformly distributed around the input, if it is odd, the excess amount goes to the right/bottom side.</span>
<span class="sd">            - ``&quot;valid&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded.</span>

<span class="sd">        data_format (str): The format of input and output data. Should be ``&#39;NHWC&#39;`` or ``&#39;NCHW&#39;`` .</span>
<span class="sd">            Default: ``&#39;NCHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size` or `strides` is neither int nor tuple.</span>
<span class="sd">        ValueError: If `pad_mode` is neither &#39;valid&#39; nor &#39;same&#39; with not case sensitive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCHW&#39; nor &#39;NHWC&#39;.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 2 * 4 * 4).reshape((1, 2, 4, 4)), mindspore.float64)</span>
<span class="sd">        &gt;&gt;&gt; avgpoolv1_op = ops.AvgPoolV1(pad_mode=&quot;VALID&quot;, kernel_size=3, strides=1)</span>
<span class="sd">        &gt;&gt;&gt; _output = avgpoolv1_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(_output)</span>
<span class="sd">        [[[[ 5.  6.]</span>
<span class="sd">           [ 9. 10.]]</span>
<span class="sd">          [[21. 22.]</span>
<span class="sd">           [25. 26.]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AvgPoolV1.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># adapt data_format</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size_adapted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size_adapted</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides_adapted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="k">else</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides_adapted</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Conv2DBackpropInput</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Conv2DBackpropInput interface is deprecated, please refer to :class:`mindspore.ops.Conv2DTranspose` if you</span>
<span class="sd">    want to do unsampling.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;input_sizes&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2DBackpropInput&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="s1">&#39;input_sizes&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;GPU&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;NHWC&#39; format is only supported in GPU target, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got the &#39;data_format&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="si">}</span><span class="s2"> and &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;the platform is </span><span class="si">{</span><span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s1">&#39;device_target&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_update_attr_by_format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_update_attr_by_format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero or (0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="n">pad_mode</span> <span class="o">=</span> <span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">pad_list</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pad_list</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">x</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                    <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;element of pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad_list</span>


<div class="viewcode-block" id="MaxPool3DWithArgmax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaxPool3DWithArgmax.html#mindspore.ops.MaxPool3DWithArgmax">[文档]</a><span class="k">class</span> <span class="nc">MaxPool3DWithArgmax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs a 3D max pooling on the input Tensor and returns both max values and indices.</span>

<span class="sd">    Typically the input is a Tensor with shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})`, outputs</span>
<span class="sd">    regional maximum in the :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given `ksize`</span>
<span class="sd">    :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and `strides` :math:`s = (s_0, s_1, s_2)`, the operation is as follows.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \max_{l=0, \ldots, d_{ker}-1} \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    The output is a Tensor with shape :math:`(N_{out}, C_{out}, D_{out}, H_{out}, W_{out})` and its depth, height and</span>
<span class="sd">    width are:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            D_{out} = \frac{D_{in} + 2 \times \text{pads}[0] - \text{dilation}[0] \times (\text{ksize}[0] - 1) - 1}</span>
<span class="sd">                {\text{stride}[0]} + 1 \\</span>
<span class="sd">            H_{out} = \frac{H_{in} + 2 \times \text{pads}[1] - \text{dilation}[1] \times (\text{ksize}[1] - 1) - 1}</span>
<span class="sd">                {\text{stride}[1]} + 1 \\</span>
<span class="sd">            W_{out} = \frac{W_{in} + 2 \times \text{pads}[2] - \text{dilation}[2] \times (\text{ksize}[2] - 1) - 1}</span>
<span class="sd">                {\text{stride}[2]} + 1 \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): The size of kernel used to take the maximum value and arg</span>
<span class="sd">            value, is an int number that represents depth, height and width of the kernel, or a tuple of</span>
<span class="sd">            three int numbers that represent depth, height and width respectively.</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents the depth,</span>
<span class="sd">            height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively.</span>
<span class="sd">        pads (Union[int, tuple[int]]): An int number that represents the depth, height and width of movement are both</span>
<span class="sd">            strides, or a tuple of three int numbers that represent depth, height and width of movement respectively.</span>
<span class="sd">        dilation (Union[int, tuple[int]]): Default: ``(1, 1, 1)`` .</span>
<span class="sd">        ceil_mode (bool): Whether to use ceil instead of floor to calculate output shape. Default: ``False`` .</span>
<span class="sd">        data_format (str) : The optional value for data format. Currently only support ``&#39;NCDHW&#39;`` .</span>
<span class="sd">            Default: ``&#39;NCDHW&#39;`` .</span>
<span class="sd">        argmax_type (mindspore.dtype) : The dtype for argmax. Default: ``mstype.int64`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N_{in}, C_{in}, D_{in}, H_{in}, W_{in})` with data type of int8,</span>
<span class="sd">          int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, representing the maxpool result and where the max values are generated.</span>

<span class="sd">        - **output** (Tensor) - Maxpooling result, with shape :math:`(N_{out}, C_{out}, D_{out}, H_{out}, W_{out})`.</span>
<span class="sd">          It has the same data type as `x`.</span>
<span class="sd">        - **argmax** (Tensor) - Index corresponding to the maximum value. Data type is int32 or int64.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 5.</span>
<span class="sd">        TypeError: If `ksize` , `strides` , `pads` or `dilation` is not int or tuple.</span>
<span class="sd">        ValueError: If `ksize` or `strides` is less than 1.</span>
<span class="sd">        ValueError: If `pads` is less than 0.</span>
<span class="sd">        ValueError: If `data_format` is not ``&#39;NCDHW&#39;``.</span>
<span class="sd">        ValueError: If `argmax_type` is not mindspore.int64 or mindspore.int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(2 * 1 * 2 * 2 * 2).reshape((2, 1, 2, 2, 2)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; max_pool3d_with_arg_op = ops.MaxPool3DWithArgmax(ksize=2, strides=1, pads=1)</span>
<span class="sd">        &gt;&gt;&gt; output_tensor, argmax = max_pool3d_with_arg_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output_tensor.shape)</span>
<span class="sd">        (2, 1, 3, 3, 3)</span>
<span class="sd">        &gt;&gt;&gt; print(argmax.shape)</span>
<span class="sd">        (2, 1, 3, 3, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">,</span> <span class="n">argmax_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxPool3DWithArgmax.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;argmax_type&quot;</span><span class="p">,</span> <span class="n">argmax_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">argmax_type_valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span>
            <span class="s2">&quot;argmax_type&quot;</span><span class="p">,</span> <span class="n">argmax_type</span><span class="p">,</span> <span class="n">argmax_type_valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">argmax_type</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;argmax_type&#39;</span><span class="p">,</span> <span class="s1">&#39;int32&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">argmax_type</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;argmax_type&#39;</span><span class="p">,</span> <span class="s1">&#39;int64&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;argmax_type&#39; must be mstype.int32 or mstype.int64, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">argmax_type</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;pads&quot;</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">greater_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2DTranspose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Conv2DTranspose.html#mindspore.ops.Conv2DTranspose">[文档]</a><span class="k">class</span> <span class="nc">Conv2DTranspose</span><span class="p">(</span><span class="n">Conv2DBackpropInput</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates a 2D transposed convolution, which can be regarded as Conv2d for the gradient of the input,</span>
<span class="sd">    also called deconvolution, although it is not an actual deconvolution. Because it cannot restore</span>
<span class="sd">    the original input data completely, but it can restore the shape of the original input.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimensionality of the output space.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of the convolution window.</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;same&quot;`` , ``&quot;valid&quot;`` or ``&quot;pad&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Pad the input around its edges so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally, If the amount is even, it is</span>
<span class="sd">              uniformly distributed around the input, if it is odd, the excess amount goes to the right/bottom side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;valid&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded. If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;pad&quot;``: Pad the input with a specified amount. In this mode, the amount of padding</span>
<span class="sd">              in the height and width directions is determined by the `pad` parameter.</span>
<span class="sd">              If this mode is set, `pad` must be greater than or equal to 0.</span>

<span class="sd">            Please refer to :class:`mindspore.nn.Conv2dTranspose` for more specifications about `pad_mode`.</span>
<span class="sd">        pad (Union[int, tuple[int]]): The pad value to be filled. Default: ``0`` . If `pad` is an integer, the paddings</span>
<span class="sd">                    of top, bottom, left and right are the same, equal to pad. If `pad` is a tuple of four integers,</span>
<span class="sd">                    the padding of top, bottom, left and right equal to pad[0], pad[1], pad[2], and pad[3]</span>
<span class="sd">                    correspondingly.</span>
<span class="sd">        pad_list (Union[str, None]): The pad list like (top, bottom, left, right). Default: ``None`` .</span>
<span class="sd">        mode (int): Modes for different convolutions. The value is currently not used. Default: ``1`` .</span>
<span class="sd">        stride (Union[int, tuple[int]]): The stride to be applied to the convolution filter. Default: ``1`` .</span>
<span class="sd">        dilation (Union[int, tuple[int]]): Specifies the dilation rate to be used for the dilated convolution.</span>
<span class="sd">            Default: ``1`` .</span>
<span class="sd">        group (int): Splits input into groups. Default: ``1`` .</span>
<span class="sd">        data_format (str): The format of input and output data. It should be ``&#39;NHWC&#39;`` or ``&#39;NCHW&#39;`` .</span>
<span class="sd">            Default is ``&#39;NCHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **dout** (Tensor) - the gradients with respect to the output of the convolution.</span>
<span class="sd">          The shape conforms to the default data_format :math:`(N, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(K_1, K_2)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}, K_1, K_2)`.</span>
<span class="sd">        - **input_size** (Tensor) - A tuple describes the shape of the input which conforms to the format</span>
<span class="sd">          :math:`(N, C_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients with respect to the input of convolution. It has the same shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        ValueError: If `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of ``&#39;same&#39;``, ``&#39;valid&#39;`` or ``&#39;pad&#39;``.</span>
<span class="sd">        ValueError: If `padding` is a tuple whose length is not equal to 4.</span>
<span class="sd">        ValueError: If `pad_mode` it not equal to ``&#39;pad&#39;`` and `pad` is not equal to (0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is neither ``&#39;NCHW&#39;`` nor ``&#39;NHWC&#39;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([10, 32, 30, 30]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 32, 32, 32]))</span>
<span class="sd">        &gt;&gt;&gt; conv2d_transpose_input = ops.Conv2DTranspose(out_channel=32, kernel_size=3)</span>
<span class="sd">        &gt;&gt;&gt; output = conv2d_transpose_input(dout, weight, ops.shape(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 32, 32, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">pad_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv2DTranspose.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2DTranspose</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span>
                                              <span class="n">pad_list</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="n">data_format</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SoftmaxCrossEntropyWithLogits.html#mindspore.ops.SoftmaxCrossEntropyWithLogits">[文档]</a><span class="k">class</span> <span class="nc">SoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Gets the softmax cross-entropy value between logits and labels with one-hot encoding.</span>

<span class="sd">    The updating formulas of SoftmaxCrossEntropyWithLogits algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{\exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)} \\</span>
<span class="sd">            loss_{ij} = -\sum_j{Y_{ij} * ln(p_{ij})}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`X` represents `logits`.</span>
<span class="sd">    :math:`Y` represents `label`.</span>
<span class="sd">    :math:`loss` represents `output`.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N, C)`, has the same data type with `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors(loss, dlogits), the `loss` shape is :math:`(N,)`,</span>
<span class="sd">        and the `dlogits` with the same shape as `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `logits` or `labels` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor([[2, 4, 1, 4, 5], [2, 1, 2, 4, 3]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([[0, 0, 0, 0, 1], [0, 0, 0, 1, 0]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; softmax_cross = ops.SoftmaxCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; loss, dlogits = softmax_cross(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        [0.5899297  0.52374405]</span>
<span class="sd">        &gt;&gt;&gt; print(dlogits)</span>
<span class="sd">        [[ 0.02760027  0.20393994  0.01015357  0.20393994 -0.44563377]</span>
<span class="sd">         [ 0.08015892  0.02948882  0.08015892 -0.4077012   0.21789455]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="SparseSoftmaxCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseSoftmaxCrossEntropyWithLogits.html#mindspore.ops.SparseSoftmaxCrossEntropyWithLogits">[文档]</a><span class="k">class</span> <span class="nc">SparseSoftmaxCrossEntropyWithLogits</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the softmax cross-entropy value between logits and sparse encoding labels.</span>

<span class="sd">    Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{\exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)} \\</span>
<span class="sd">            loss_{ij} = \begin{cases} -ln(p_{ij}), &amp;j = y_i \cr 0, &amp; j \neq y_i \end{cases} \\</span>
<span class="sd">            loss = \sum_{ij} loss_{ij}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Args:</span>
<span class="sd">        is_grad (bool): If ``True`` , this operation returns the computed gradient. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N)`.</span>
<span class="sd">          Data type must be int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, if `is_grad` is False, the output tensor is the value of loss which is a scalar tensor;</span>
<span class="sd">        if `is_grad` is ``True`` , the output tensor is the gradient of input with the same shape as `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `is_grad` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `labels` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If :math:`logits.shape[0] != labels.shape[0]`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor([[2, 3, 1, 4, 5], [2, 1, 2, 4, 3]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([0, 1], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sparse_softmax_cross = ops.SparseSoftmaxCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; loss = sparse_softmax_cross(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        3.4878292</span>
<span class="sd">        &gt;&gt;&gt; sparse_softmax_cross_grad = ops.SparseSoftmaxCrossEntropyWithLogits(is_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; loss_grad = sparse_softmax_cross_grad(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss_grad)</span>
<span class="sd">        [[-0.48415753  0.04306427  0.00582811  0.11706084  0.3182043 ]</span>
<span class="sd">         [ 0.04007946 -0.4852556   0.04007946  0.2961494   0.10894729]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseSoftmaxCrossEntropyWithLogits.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;is_grad&#39;</span><span class="p">,</span> <span class="n">is_grad</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;features&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_grad</span> <span class="o">=</span> <span class="n">is_grad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;sens&#39;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">SparseSoftmaxCrossEntropyWithLogitsV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the softmax cross-entropy value between logits and sparse encoding labels.</span>

<span class="sd">    Sets input logits as `X`, input label as `Y`, output as `loss`. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = softmax(X_{ij}) = \frac{\exp(x_i)}{\sum_{j = 0}^{N-1}\exp(x_j)} \\</span>
<span class="sd">            loss_{ij} = \begin{cases} -ln(p_{ij}), &amp;j = y_i \cr 0, &amp; j \neq y_i \end{cases}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits, with shape :math:`(N, C)`. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth labels, with shape :math:`(N)`.</span>
<span class="sd">          Data type must be int32 or int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **loss** (Tensor) - With the same shape as `labels`, the same type as `logits`.</span>
<span class="sd">        - **backprop** (Tensor) - With the same shape and same type as `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `labels` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If logits.shape is not [batch x classes] or labels.shape is not [batch].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor([[2, 3, 1, 4, 5], [2, 1, 2, 4, 3]], mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor([0, 1], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sparse_softmax_cross = ops.SparseSoftmaxCrossEntropyWithLogitsV2()</span>
<span class="sd">        &gt;&gt;&gt; loss, backprop = sparse_softmax_cross(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        [3.4519143 3.523744 ]</span>
<span class="sd">        &gt;&gt;&gt; print(backprop)</span>
<span class="sd">        [[-0.96831506  0.08612854  0.01165623  0.23412165  0.6364086 ]</span>
<span class="sd">         [ 0.08015893 -0.9705112   0.08015893  0.5922988   0.21789455]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseSoftmaxCrossEntropyWithLogitsV2.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;features&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">,</span> <span class="s1">&#39;backprop&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="ApplyMomentum"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyMomentum.html#mindspore.ops.ApplyMomentum">[文档]</a><span class="k">class</span> <span class="nc">ApplyMomentum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the Momentum algorithm.</span>

<span class="sd">    Refer to the paper `On the importance of initialization and momentum in deep</span>
<span class="sd">    learning &lt;https://dl.acm.org/doi/10.5555/3042817.3043064&gt;`_  for more details.</span>

<span class="sd">    Inputs of `variable`, `accumulation` and `gradient` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Refer to :class:`mindspore.nn.Momentum` for more details about the formula and usage.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumulation tensors</span>
<span class="sd">                            from being updated. Default: ``False`` .</span>
<span class="sd">        use_nesterov (bool): Enable Nesterov momentum. Default: ``False`` .</span>
<span class="sd">        gradient_scale (float): The scale of the gradient. Default: ``1.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **variable** (Parameter) - Weights to be updated. Data type must be float64, int64, float, float16,</span>
<span class="sd">          int16, int32, int8, uint16, uint32, uint64, uint8, complex64, complex128.</span>
<span class="sd">        - **accumulation** (Parameter) - Accumulated gradient value by moment weight,</span>
<span class="sd">          has the same data type with `variable`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - The learning rate value, must be a float64, int64, float,</span>
<span class="sd">          float16, int16, int32, int8, uint16, uint32, uint64, uint8, complex64, complex128 number or</span>
<span class="sd">          a scalar tensor with float64, int64, float, float16, int16, int32, int8, uint16, uint32, uint64, uint8,</span>
<span class="sd">          complex64, complex128 data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same data type as `variable`.</span>
<span class="sd">        - **momentum** (Union[Number, Tensor]) - Momentum, must be a float64, int64, float, float16, int16, int32,</span>
<span class="sd">          int8, uint16, uint32, uint64, uint8, complex64, complex128 number or</span>
<span class="sd">          a scalar tensor with float64, int64, float, float16, int16, int32, int8, uint16, uint32, uint64, uint8,</span>
<span class="sd">          complex64, complex128 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the `use_locking` or `use_nesterov` is not a bool or `gradient_scale` is not a float.</span>
<span class="sd">        TypeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...    def __init__(self):</span>
<span class="sd">        ...        super(Net, self).__init__()</span>
<span class="sd">        ...        self.apply_momentum = ops.ApplyMomentum()</span>
<span class="sd">        ...        self.variable = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                            [0.1, 0.5]]).astype(np.float32)), name=&quot;variable&quot;)</span>
<span class="sd">        ...        self.accumulate = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                            [0.2, 0.6]]).astype(np.float32)), name=&quot;accumulate&quot;)</span>
<span class="sd">        ...    def construct(self, lr, grad, moment):</span>
<span class="sd">        ...        out = self.apply_momentum(self.variable, self.accumulate, lr, grad, moment)</span>
<span class="sd">        ...        return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; moment = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, grad, moment)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.51600003 0.285     ]</span>
<span class="sd">        [0.072      0.366     ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accumulation&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">gradient_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyMomentum.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_nesterov</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">use_nesterov</span><span class="p">,</span> <span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">use_locking</span><span class="p">,</span> <span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;gradient_scale&#39;</span><span class="p">,</span> <span class="n">gradient_scale</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="s1">&#39;accumulation&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="SmoothL1Loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SmoothL1Loss.html#mindspore.ops.SmoothL1Loss">[文档]</a><span class="k">class</span> <span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the smooth L1 loss, and the L1 loss function has robustness.</span>

<span class="sd">    Refer to :func:`mindspore.ops.smooth_l1_loss` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        beta (float, optional): A parameter used to control the point where the function will change between</span>
<span class="sd">            L1 to L2 loss. The value should be greater than zero. Default: ``1.0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;none&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input Tensor of any dimension. Data type must be float16, float32 or float64.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth data, has the same shape and dtype as the `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, loss float tensor, same shape and dtype as the `logits`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.SmoothL1Loss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0.  0.  0.5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SmoothL1Loss.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span>
            <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="MultiMarginLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MultiMarginLoss.html#mindspore.ops.MultiMarginLoss">[文档]</a><span class="k">class</span> <span class="nc">MultiMarginLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a loss function that minimizes the hinge loss</span>
<span class="sd">    for multi-class classification tasks.</span>
<span class="sd">    The loss is calculated by comparing the input and output of the function.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.multi_margin_loss` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (int, optional): The norm degree for pairwise distance. Should be 1 or 2. Default: ``1`` .</span>
<span class="sd">        margin (int, optional): A parameter to change pairwise distance. Default: ``1.0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **inputs** (Tensor) - Input , with shape :math:`(N, C)`. Data type only support float32, float16</span>
<span class="sd">          or float64.</span>
<span class="sd">        - **target** (Tensor) - Ground truth labels, with shape :math:`(N,)`. Data type only support int64. The</span>
<span class="sd">          value of target should be non-negative, less than C.</span>
<span class="sd">        - **weight** (Tensor, optional) - The rescaling weight to each class with shape :math:`(C,)`. Data type only</span>
<span class="sd">          support float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, When `reduction` is ``&#39;none&#39;``, the shape is :math:`(N,)`.</span>
<span class="sd">        Otherwise, it is a scalar. Has the same data type with `inputs`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones(shape=[3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = Tensor(np.array([1, 2, 1]), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 1, 1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.MultiMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(x, target, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.6666667</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MultiMarginLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="p">{</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">},</span> <span class="n">validator</span><span class="o">.</span><span class="n">IN</span><span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="SoftMarginLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SoftMarginLoss.html#mindspore.ops.SoftMarginLoss">[文档]</a><span class="k">class</span> <span class="nc">SoftMarginLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SoftMarginLoss operation.</span>

<span class="sd">    Creates a criterion that optimizes a two-class classification</span>
<span class="sd">    logistic loss between input tensor :math:`x` and target tensor :math:`y`</span>
<span class="sd">    (containing 1 or -1).</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</span>

<span class="sd">    where :math:`x.nelement()` is the number of elements of x.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Predict data. Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Ground truth data, with the same type and shape as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is ``&quot;none&quot;``, its shape is the same as `logits`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits` or `labels` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&quot;none&quot;`` , ``&quot;mean&quot;`` or ``&quot;sum&quot;`` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.SoftMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[0.3, 0.7], [0.5, 0.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[-1, 1], [1, -1]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.6764238</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SoftMarginLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;predict&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="L2Loss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.L2Loss.html#mindspore.ops.L2Loss">[文档]</a><span class="k">class</span> <span class="nc">L2Loss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates half of the L2 norm, but do not square the result.</span>

<span class="sd">    Set input as x and output as loss.</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss = \frac{\sum x ^ 2}{2}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor for computing the L2 norm. Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has a Scalar Tensor with the same data type as `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `input_x` is not float16, float32 or float64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([1, 2, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; l2_loss = ops.L2Loss()</span>
<span class="sd">        &gt;&gt;&gt; output = l2_loss(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        7.0</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize L2Loss&quot;&quot;&quot;</span></div>


<div class="viewcode-block" id="DataFormatDimMap"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.DataFormatDimMap.html#mindspore.ops.DataFormatDimMap">[文档]</a><span class="k">class</span> <span class="nc">DataFormatDimMap</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the dimension index in the destination data format given in the source data format.</span>

<span class="sd">    Args:</span>
<span class="sd">        src_format (str): An optional value for source data format. The format can be ``&#39;NHWC&#39;`` and ``&#39;NCHW&#39;`` .</span>
<span class="sd">            Default: ``&#39;NHWC&#39;`` .</span>
<span class="sd">        dst_format (str): An optional value for destination data format. The format can be ``&#39;NHWC&#39;`` and ``&#39;NCHW&#39;`` .</span>
<span class="sd">            Default: ``&#39;NCHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - A Tensor, each element is used as a dimension index of the source data format.</span>
<span class="sd">          The suggested values are in the range [-4, 4). Only supports int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, Return the dimension index in the given target data format,</span>
<span class="sd">        has the same data type and shape as the `input_x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `src_format` or `dst_format` is not a str.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor whose dtype is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([0, 1, 2, 3], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; dfdm = ops.DataFormatDimMap()</span>
<span class="sd">        &gt;&gt;&gt; output = dfdm(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [0 3 1 2]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_format</span><span class="o">=</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="n">dst_format</span><span class="o">=</span><span class="s1">&#39;NCHW&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DataFormatDimMap.&quot;&quot;&quot;</span>
        <span class="n">valid_values</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;NHWC&#39;</span><span class="p">,</span> <span class="s1">&#39;NCHW&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">src_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">src_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;src_format&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dst_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">dst_format</span><span class="p">,</span> <span class="n">valid_values</span><span class="p">,</span> <span class="s2">&quot;dst_format&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="RNNTLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.RNNTLoss.html#mindspore.ops.RNNTLoss">[文档]</a><span class="k">class</span> <span class="nc">RNNTLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the RNNTLoss and its gradient with respect to the softmax outputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank_label (int): blank label. Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **acts** (Tensor) - Tensor of shape :math:`(B, T, U, V)`, where :math:`B` is batch,</span>
<span class="sd">          :math:`T` is sequence length, :math:`U` is label length and :math:`V` is output dim.</span>
<span class="sd">          Data type must be float16 or float32.</span>
<span class="sd">        - **labels** (Tensor) - Tensor of shape :math:`(B, U-1)`. Data type is int32.</span>
<span class="sd">        - **input_lengths** (Tensor) - Tensor of shape :math:`(B,)`. Data type is int32.</span>
<span class="sd">        - **label_lengths** (Tensor) - Tensor of shape :math:`(B,)`. Data type is int32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **costs** (Tensor) - Tensor of shape :math:`(B,)`. Data type is int32.</span>
<span class="sd">        - **grads** (Tensor) - Has the same shape and dtype as `acts`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `acts`, `labels`, `input_lengths` or `label_lengths` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `acts` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `labels`, `input_lengths` or `label_lengths` is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; B, T, U, V = 1, 2, 3, 5</span>
<span class="sd">        &gt;&gt;&gt; blank = 0</span>
<span class="sd">        &gt;&gt;&gt; acts = np.random.random((B, T, U, V)).astype(np.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = np.array([[1, 2]]).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_length = np.array([T] * B).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; label_length = np.array([len(l) for l in labels]).astype(np.int32)</span>
<span class="sd">        &gt;&gt;&gt; rnnt_loss = ops.RNNTLoss(blank_label=0)</span>
<span class="sd">        &gt;&gt;&gt; costs, grads = rnnt_loss(Tensor(acts), Tensor(labels), Tensor(input_length), Tensor(label_length))</span>
<span class="sd">        &gt;&gt;&gt; print(costs.shape)</span>
<span class="sd">        (1,)</span>
<span class="sd">        &gt;&gt;&gt; print(grads.shape)</span>
<span class="sd">        (1, 2, 3, 5)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank_label</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize RNNTLoss.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;blank_label&#39;</span><span class="p">,</span> <span class="n">blank_label</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acts&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="s1">&#39;input_length&#39;</span><span class="p">,</span> <span class="s1">&#39;label_length&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;costs&#39;</span><span class="p">,</span> <span class="s1">&#39;grads&#39;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">,</span> <span class="n">input_length_shape</span><span class="p">,</span> <span class="n">label_length_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">acts_shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;acts_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;labels_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_length_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;input_length_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label_length_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;label_length_rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;labels shape[0]&#39;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span> <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;labels shape[1]&#39;</span><span class="p">,</span> <span class="n">labels_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;acts shape[2]-1&#39;</span><span class="p">,</span>
                        <span class="n">acts_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;input_length size&#39;</span><span class="p">,</span> <span class="n">input_length_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span>
                        <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;label_length size&#39;</span><span class="p">,</span> <span class="n">label_length_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;acts shape[0]&#39;</span><span class="p">,</span>
                        <span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">costs_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">acts_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span>
        <span class="k">return</span> <span class="n">costs_shape</span><span class="p">,</span> <span class="n">acts_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">acts_type</span><span class="p">,</span> <span class="n">labels_type</span><span class="p">,</span> <span class="n">input_length_type</span><span class="p">,</span> <span class="n">label_length_type</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;acts_type&quot;</span><span class="p">,</span> <span class="n">acts_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span>
                          <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,),</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">,</span> <span class="s2">&quot;input_length&quot;</span><span class="p">,</span> <span class="s2">&quot;label_length&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">labels_type</span><span class="p">,</span> <span class="n">input_length_type</span><span class="p">,</span> <span class="n">label_length_type</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">acts_type</span><span class="p">,</span> <span class="n">acts_type</span></div>


<div class="viewcode-block" id="SGD"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SGD.html#mindspore.ops.SGD">[文档]</a><span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">PrimitiveWithCheck</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the stochastic gradient descent. Momentum is optional.</span>

<span class="sd">    Nesterov momentum is based on the formula from paper `On the importance of</span>
<span class="sd">    initialization and momentum in deep learning &lt;http://proceedings.mlr.press/v28/sutskever13.html&gt;`_.</span>

<span class="sd">    Note:</span>
<span class="sd">        If parameters are not grouped, the `weight_decay` in optimizer will be applied on the network parameters without</span>
<span class="sd">        &#39;beta&#39; or &#39;gamma&#39; in their names. Users can group parameters to change the strategy of decaying weight. When</span>
<span class="sd">        parameters are grouped, each group can set `weight_decay`. If not, the `weight_decay` in optimizer will be</span>
<span class="sd">        applied.</span>
<span class="sd">        For more details, please refer to :class:`mindspore.nn.SGD`.</span>

<span class="sd">    Args:</span>
<span class="sd">        dampening (float): The dampening for momentum. Default: ``0.0`` .</span>
<span class="sd">        weight_decay (float): Weight decay (L2 penalty). Default: ``0.0`` .</span>
<span class="sd">        nesterov (bool): Enable Nesterov momentum. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **parameters** (Tensor) - Parameters to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, with float16 or float32 data type.</span>
<span class="sd">        - **learning_rate** (Tensor) - Learning rate, a scalar tensor with float16 or float32 data type.</span>
<span class="sd">          e.g. Tensor(0.1, mindspore.float32)</span>
<span class="sd">        - **accum** (Tensor) - Accum(velocity) to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **momentum** (Tensor) - Momentum, a scalar tensor with float16 or float32 data type.</span>
<span class="sd">          e.g. Tensor(0.1, mindspore.float32).</span>
<span class="sd">        - **stat** (Tensor) - States to be updated with the same shape as gradient, with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `dampening` or `weight_decay` is not a float.</span>
<span class="sd">        TypeError: If `nesterov` is not a bool.</span>
<span class="sd">        TypeError: If `parameters`, `gradient`, `learning_rate`, `accum`, `momentum` or `stat` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `parameters`, `gradient`, `learning_rate`, `accum`, `momentum` or `stat` is neither</span>
<span class="sd">                   float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; sgd = ops.SGD()</span>
<span class="sd">        &gt;&gt;&gt; parameters = Tensor(np.array([2, -0.5, 1.7, 4]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([1, -1, 0.5, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; learning_rate = Tensor(0.01, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; accum = Tensor(np.array([0.1, 0.3, -0.2, -0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; momentum = Tensor(0.1, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; stat = Tensor(np.array([1.5, -0.3, 0.2, -0.7]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = sgd(parameters, gradient, learning_rate, accum, momentum, stat)</span>
<span class="sd">        &gt;&gt;&gt; print(output.asnumpy())</span>
<span class="sd">        [1.99 -0.4903 1.695 3.9801]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dampening</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SGD.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;nesterov&quot;</span><span class="p">,</span> <span class="n">nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">nesterov</span> <span class="ow">and</span> <span class="n">dampening</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;dampening&#39; must be 0 when &#39;nesterov&#39; is True, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got &#39;dampening&#39; is </span><span class="si">{</span><span class="n">dampening</span><span class="si">}</span><span class="s2"> and &#39;nesterov&#39; is </span><span class="si">{</span><span class="n">nesterov</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parameters&#39;</span><span class="p">,</span> <span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;stat&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_shape</span><span class="p">,</span> <span class="n">gradient_shape</span><span class="p">,</span> <span class="n">learning_rate_shape</span><span class="p">,</span>
                    <span class="n">accum_shape</span><span class="p">,</span> <span class="n">momentum_shape</span><span class="p">,</span> <span class="n">stat_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gradient_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;gradient rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">learning_rate_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;learning rate rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">momentum_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;momentum rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">stat_shape</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;stat rank&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">check_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span>
                    <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">stat_dtype</span><span class="p">):</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span>
                          <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;parameters&quot;</span><span class="p">,</span> <span class="s2">&quot;gradient&quot;</span><span class="p">,</span> <span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="s2">&quot;accum&quot;</span><span class="p">,</span> <span class="s2">&quot;momentum&quot;</span><span class="p">,</span> <span class="s2">&quot;stat&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">parameters_dtype</span><span class="p">,</span> <span class="n">gradient_dtype</span><span class="p">,</span> <span class="n">learning_rate_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">momentum_dtype</span><span class="p">,</span> <span class="n">stat_dtype</span><span class="p">)))</span></div>


<div class="viewcode-block" id="ApplyRMSProp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyRMSProp.html#mindspore.ops.ApplyRMSProp">[文档]</a><span class="k">class</span> <span class="nc">ApplyRMSProp</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the Root Mean Square prop(RMSProp) algorithm.</span>
<span class="sd">    Please refer to the usage in source code of :class:`mindspore.nn.RMSProp`.</span>

<span class="sd">    The updating formulas of ApplyRMSProp algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            s_{t+1} = \rho s_{t} + (1 - \rho)(\nabla Q_{i}(w))^2 \\</span>
<span class="sd">            m_{t+1} = \beta m_{t} + \frac{\eta} {\sqrt{s_{t+1} + \epsilon}} \nabla Q_{i}(w) \\</span>
<span class="sd">            w = w - m_{t+1}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`w` represents `var`, which will be updated.</span>
<span class="sd">    :math:`s_{t+1}` represents `mean_square`, :math:`s_{t}` is the last moment of :math:`s_{t+1}`,</span>
<span class="sd">    :math:`m_{t+1}` represents `moment`, :math:`m_{t}` is the last moment of :math:`m_{t+1}`.</span>
<span class="sd">    :math:`\rho` represents `decay`. :math:`\beta` is the momentum term, represents `momentum`.</span>
<span class="sd">    :math:`\epsilon` is a smoothing term to avoid division by zero, represents `epsilon`.</span>
<span class="sd">    :math:`\eta` represents `learning_rate`. :math:`\nabla Q_{i}(w)` represents `grad`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Note that in dense implementation of this algorithm, &quot;mean_square&quot; and &quot;moment&quot; will update even if &quot;grad&quot; is 0,</span>
<span class="sd">        but in this sparse implementation, &quot;mean_square&quot; and &quot;moment&quot; will not update</span>
<span class="sd">        in iterations during which &quot;grad&quot; is 0.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumulation tensors</span>
<span class="sd">                            from being updated. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated.</span>
<span class="sd">        - **mean_square** (Tensor) - Mean square gradients, must be the same type as `var`.</span>
<span class="sd">        - **moment** (Tensor) - Delta of `var`, must be the same type as `var`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. Must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - Gradient, must be the same type as `var`.</span>
<span class="sd">        - **decay** (float) - Decay rate. Only constant value is allowed.</span>
<span class="sd">        - **momentum** (float) - Momentum. Only constant value is allowed.</span>
<span class="sd">        - **epsilon** (float) - Ridge term. Only constant value is allowed.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `var`, `mean_square`, `moment` or `decay` is not a Tensor.</span>
<span class="sd">        TypeError: If `learning_rate` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `decay`, `momentum` or `epsilon` is not float.</span>
<span class="sd">        TypeError: If dtype of `learning_rate` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `decay`, `momentum` or `epsilon` is not a constant value.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_rms_prop = ops.ApplyRMSProp()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, mean_square, moment, grad, decay, momentum, epsilon, lr):</span>
<span class="sd">        ...         out = self.apply_rms_prop(self.var, mean_square, moment, lr, grad, decay, momentum, epsilon)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; mean_square = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; moment = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(mean_square, moment, grad, 0.0, 1e-10, 0.001, 0.01)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.990005  0.990005]</span>
<span class="sd">         [0.990005  0.990005]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyRMSProp.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;mean_square&#39;</span><span class="p">,</span> <span class="s1">&#39;moment&#39;</span><span class="p">,</span> <span class="s1">&#39;learning_rate&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="s1">&#39;epsilon&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyCenteredRMSProp"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyCenteredRMSProp.html#mindspore.ops.ApplyCenteredRMSProp">[文档]</a><span class="k">class</span> <span class="nc">ApplyCenteredRMSProp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimizer that implements the centered RMSProp algorithm.</span>
<span class="sd">    Please refer to the usage in source code of :class:`mindspore.nn.RMSProp`.</span>

<span class="sd">    The updating formulas of ApplyCenteredRMSProp algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            g_{t+1} = \rho g_{t} + (1 - \rho)\nabla Q_{i}(w) \\</span>
<span class="sd">            s_{t+1} = \rho s_{t} + (1 - \rho)(\nabla Q_{i}(w))^2 \\</span>
<span class="sd">            m_{t+1} = \beta m_{t} + \frac{\eta} {\sqrt{s_{t+1} - g_{t+1}^2 + \epsilon}} \nabla Q_{i}(w) \\</span>
<span class="sd">            w = w - m_{t+1}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`w` represents `var`, which will be updated.</span>
<span class="sd">    :math:`g_{t+1}` represents `mean_gradient`, :math:`g_{t}` is the last moment of :math:`g_{t+1}`.</span>
<span class="sd">    :math:`s_{t+1}` represents `mean_square`, :math:`s_{t}` is the last moment of :math:`s_{t+1}`,</span>
<span class="sd">    :math:`m_{t+1}` represents `moment`, :math:`m_{t}` is the last moment of :math:`m_{t+1}`.</span>
<span class="sd">    :math:`\rho` represents `decay`. :math:`\beta` is the momentum term, represents `momentum`.</span>
<span class="sd">    :math:`\epsilon` is a smoothing term to avoid division by zero, represents `epsilon`.</span>
<span class="sd">    :math:`\eta` represents `learning_rate`. :math:`\nabla Q_{i}(w)` represents `grad`.</span>

<span class="sd">    Note:</span>
<span class="sd">        The difference between `ApplyCenteredRMSProp` and `ApplyRMSProp` is that the former</span>
<span class="sd">        uses the centered RMSProp algorithm, and the centered RRMSProp algorithm uses an estimate of the centered second</span>
<span class="sd">        moment(i.e., the variance) for normalization, as opposed to regular RMSProp, which uses the (uncertained)</span>
<span class="sd">        second moment. This often helps with training, but is slightly more expensive in terms of computation and</span>
<span class="sd">        memory.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        In dense implementation of this algorithm, `mean_gradient`, `mean_square`, and `moment` will update</span>
<span class="sd">        even if the `grad` is zero. But in this sparse implementation, `mean_gradient`, `mean_square`, and `moment`</span>
<span class="sd">        will not update in iterations during which the `grad` is zero.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect the variable and accumulation tensors</span>
<span class="sd">                            from being updated. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated.</span>
<span class="sd">        - **mean_gradient** (Tensor) - Mean gradients, must be the same type as `var`.</span>
<span class="sd">        - **mean_square** (Tensor) - Mean square gradients, must be the same type as `var`.</span>
<span class="sd">        - **moment** (Tensor) - Delta of `var`, must be the same type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient, must be the same type as `var`.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. Must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **decay** (float) - Decay rate.</span>
<span class="sd">        - **momentum** (float) - Momentum.</span>
<span class="sd">        - **epsilon** (float) - Ridge term.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, parameters to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `var`, `mean_gradient`, `mean_square`, `moment` or `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If `learing_rate` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `learing_rate` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `decay`, `momentum` or `epsilon` is not a float.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_centerd_rms_prop = ops.ApplyCenteredRMSProp()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, mean_grad, mean_square, moment, grad, decay, momentum, epsilon, lr):</span>
<span class="sd">        ...         out = self.apply_centerd_rms_prop(self.var, mean_grad, mean_square, moment, grad,</span>
<span class="sd">        ...                                           lr, decay, momentum, epsilon)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; mean_grad = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; mean_square = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; moment = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(mean_grad, mean_square, moment, grad, 0.0, 1e-10, 0.001, 0.01)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.68377227  0.68377227]</span>
<span class="sd">         [0.68377227  0.68377227]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyCenteredRMSProp.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="L2Normalize"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.L2Normalize.html#mindspore.ops.L2Normalize">[文档]</a><span class="k">class</span> <span class="nc">L2Normalize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    L2 Normalization Operator.</span>

<span class="sd">    This operator will normalize the input using the given axis. The function is shown as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \displaylines{{\text{output} = \frac{x}{\sqrt{\text{max}( \sum_{i}^{}\left | x_i  \right | ^2, \epsilon)}}}}</span>

<span class="sd">    where :math:`\epsilon` is epsilon and :math:`\sum_{i}^{}\left | x_i  \right | ^2` calculate the sum of squares of</span>
<span class="sd">    the input `x` along the dimension `axis`.</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend, input data type of float64 is currently not supported.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (Union[list(int), tuple(int), int], optional): Specify the axis for calculating the L2 norm.</span>
<span class="sd">            Default: ``0`` .</span>
<span class="sd">        epsilon (float, optional): A small value added for numerical stability. Default: ``1e-4`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input to compute the normalization. Tensor of shape :math:`(N, *)`,</span>
<span class="sd">          where :math:`*` means any number of additional dimensions.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as the `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `axis` is not one of the following: list, tuple or int.</span>
<span class="sd">        TypeError: If `epsilon` is not a float.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is not in [float16, float32, float64].</span>
<span class="sd">        ValueError: If dimension of `x` is not greater than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; l2_normalize = ops.L2Normalize()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.randint(-256, 256, (2, 3, 4)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = l2_normalize(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 3, 4)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize L2Normalize.&quot;&quot;&quot;</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="p">[</span><span class="n">axis</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="k">else</span> <span class="n">axis</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_attrs</span><span class="p">[</span><span class="s1">&#39;axis&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">axis</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the length of &#39;axis&#39; must be 1, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span><span class="si">}</span><span class="s2">, &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;later will support multiple axis!&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="n">axis</span></div>


<div class="viewcode-block" id="UpsampleTrilinear3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.UpsampleTrilinear3D.html#mindspore.ops.UpsampleTrilinear3D">[文档]</a><span class="k">class</span> <span class="nc">UpsampleTrilinear3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs upsampling with trilinear interpolation across 3dims for 5dim input Tensor.</span>

<span class="sd">    This operator scale up the volumetric input with specified `output_size` or `scales` factors,</span>
<span class="sd">    using trilinear upscaling algorithm.</span>

<span class="sd">    Note:</span>
<span class="sd">        One of `scales` and `output_size` must be specified. And it is an error if both are specified.</span>

<span class="sd">    Args:</span>
<span class="sd">        align_corners (bool, optional): An optional bool. Default: ``False``.</span>
<span class="sd">            If ``True``, the input and output tensors are aligned by the center points of their corner pixels,</span>
<span class="sd">            preserving the values at the corner pixels.</span>
<span class="sd">            If ``False`` , the input and output tensors are aligned by the corner points of their corner pixels,</span>
<span class="sd">            and the interpolation use edge value padding for out of boundary values.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - 5D tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`. Supporting types:</span>
<span class="sd">          [float16, float32, float64].</span>
<span class="sd">        - **output_size** (Union[tuple[int], list[int]]):  A tuple or list of 3 int elements</span>
<span class="sd">          :math:`(output\_depth, output\_height, output\_width)`. Default: ``None``.</span>
<span class="sd">        - **scales** (Union[tuple[float], list[float]]): A tuple or list of 3 float</span>
<span class="sd">          elements :math:`(scale\_depth, scale\_height, scale\_width)`. Default: ``None``.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Upsampled output with the same data type as `x`, whose shape is</span>
<span class="sd">          :math:`(N, C, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: When `output_size` is not ``None`` and `output_size` is not list[int] or tuple[int].</span>
<span class="sd">        TypeError: When `scales` is not ``None`` and `scales` is not list[float] or tuple[float].</span>
<span class="sd">        TypeError: If dtype of `x` is not in [float16, float32, float64].</span>
<span class="sd">        TypeError: If type of `align_corners` is not bool.</span>
<span class="sd">        ValueError: If any value of `output_size` is negative or zero when `output_size` is not ``None``.</span>
<span class="sd">        ValueError: If any value of `scales` is negative or zero when `scales` is not ``None``.</span>
<span class="sd">        ValueError: If shape of `x` is not 5D.</span>
<span class="sd">        ValueError: If none of `scales` and `output_size` is specified or both specified.</span>
<span class="sd">        ValueError: If size of `scales` is not equal 3 when `scales` is specified.</span>
<span class="sd">        ValueError: If size of `output_size` is not equal 3 when `output_size` is specified.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; net = ops.UpsampleTrilinear3D()</span>
<span class="sd">        &gt;&gt;&gt; in_x = Tensor(input_data=np.random.randn(2, 3, 4, 512, 256))</span>
<span class="sd">        &gt;&gt;&gt; output_size=[4, 64, 48]</span>
<span class="sd">        &gt;&gt;&gt; out = net(in_x, output_size, None)</span>
<span class="sd">        &gt;&gt;&gt; print(out.shape)</span>
<span class="sd">        (2, 3, 4, 64, 48)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = ops.UpsampleTrilinear3D()</span>
<span class="sd">        &gt;&gt;&gt; in_x = Tensor(np.arange(1, 5, dtype=np.float32).reshape((1, 1, 1, 2, 2)))</span>
<span class="sd">        &gt;&gt;&gt; output_size=[2, 4, 4]</span>
<span class="sd">        &gt;&gt;&gt; out = net(in_x, output_size, None)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [[[[[1.   1.25 1.75 2.  ]</span>
<span class="sd">            [1.5  1.75 2.25 2.5 ]</span>
<span class="sd">            [2.5  2.75 3.25 3.5 ]</span>
<span class="sd">            [3.   3.25 3.75 4.  ]]</span>
<span class="sd">           [[1.   1.25 1.75 2.  ]</span>
<span class="sd">            [1.5  1.75 2.25 2.5 ]</span>
<span class="sd">            [2.5  2.75 3.25 3.5 ]</span>
<span class="sd">            [3.   3.25 3.75 4.  ]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize UpsampleTrilinear3D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="s1">&#39;scales&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span> <span class="o">=</span> <span class="n">align_corners</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">,</span> <span class="s2">&quot;align_corners&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;align_corners&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">align_corners</span><span class="p">)</span></div>


<div class="viewcode-block" id="GetNext"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GetNext.html#mindspore.ops.GetNext">[文档]</a><span class="k">class</span> <span class="nc">GetNext</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the next element in the dataset queue.</span>

<span class="sd">    Note:</span>
<span class="sd">        The GetNext operation needs to be associated with network and it also depends</span>
<span class="sd">        on the &#39;dataset&#39; interface, For example, please refer to :class:`mindspore.dataset.MnistDataset` .</span>
<span class="sd">        it can&#39;t be used directly as a single operation.</span>
<span class="sd">        For details, please refer to :class:`mindspore.connect_network_with_dataset` source code.</span>

<span class="sd">    Args:</span>
<span class="sd">        types (list[:class:`mindspore.dtype`]): The type of the outputs.</span>
<span class="sd">        shapes (list[tuple[int]]): The dimensionality of the outputs.</span>
<span class="sd">        output_num (int): The output number, length of `types` and `shapes`.</span>
<span class="sd">        shared_name (str): Queue name to fetch the data.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        No inputs.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        tuple[Tensor], the output of dataset. The shape is described in `shapes`</span>
<span class="sd">        and the type is described in `types`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dataset as ds</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; data_path = &quot;/path/to/MNIST_Data/train/&quot;</span>
<span class="sd">        &gt;&gt;&gt; train_dataset = ds.MnistDataset(data_path, num_samples=10)</span>
<span class="sd">        &gt;&gt;&gt; dataset_helper = mindspore.DatasetHelper(train_dataset, dataset_sink_mode=True)</span>
<span class="sd">        &gt;&gt;&gt; dataset = dataset_helper.iter.dataset</span>
<span class="sd">        &gt;&gt;&gt; dataset_types, dataset_shapes = dataset_helper.types_shapes()</span>
<span class="sd">        &gt;&gt;&gt; queue_name = dataset.__transfer_dataset__.queue_name</span>
<span class="sd">        &gt;&gt;&gt; get_next = ops.GetNext(dataset_types, dataset_shapes, len(dataset_types), queue_name)</span>
<span class="sd">        &gt;&gt;&gt; data, label = get_next()</span>
<span class="sd">        &gt;&gt;&gt; relu = ops.ReLU()</span>
<span class="sd">        &gt;&gt;&gt; result = relu(data.astype(mstype.float32))</span>
<span class="sd">        &gt;&gt;&gt; print(result.shape)</span>
<span class="sd">        (28, 28, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="n">shared_name</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GetNext.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;types&quot;</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;shapes&quot;</span><span class="p">,</span> <span class="n">shapes</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;types length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">types</span><span class="p">),</span> <span class="s2">&quot;shapes length&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">),</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_num&quot;</span><span class="p">,</span> <span class="n">output_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="LSTM"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LSTM.html#mindspore.ops.LSTM">[文档]</a><span class="k">class</span> <span class="nc">LSTM</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs the Long Short-Term Memory (LSTM) on the input.</span>

<span class="sd">    For more information, please refer to :class:`mindspore.nn.LSTM`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Number of features of input.</span>
<span class="sd">        hidden_size (int):  Number of features of hidden layer.</span>
<span class="sd">        num_layers (int): Number of layers of stacked LSTM.</span>
<span class="sd">        has_bias (bool): Whether the cell has bias `b_ih` and `b_hh`.</span>
<span class="sd">        bidirectional (bool): Specifies whether it is a bidirectional LSTM.</span>
<span class="sd">        dropout (float): If not 0, append `Dropout` layer on the outputs of each</span>
<span class="sd">            LSTM layer except the last layer. The range of dropout is [0.0, 1.0].</span>
<span class="sd">        proj_size (int): If `proj_size` &gt; 0, a projection of the corresponding size will be used,</span>
<span class="sd">            which is only supported on CPU now. Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Tensor of shape :math:`(seq\_len, batch\_size, input\_size)` or</span>
<span class="sd">          :math:`(batch\_size, seq\_len, input\_size)`.</span>
<span class="sd">        - **h** (Tensor) - Tensor of shape :math:`(num\_directions * num\_layers, batch\_size, real\_hidden\_size)`.</span>
<span class="sd">        - **c** (Tensor) - Tensor of shape :math:`(num\_directions * num\_layers, batch\_size, hidden\_size)`.</span>
<span class="sd">        - **w** (Tensor) - A weight Tensor.</span>

<span class="sd">        If :math:`proj\_size &gt; 0` , :math:`real\_hidden\_size = proj\_size` , otherwise</span>
<span class="sd">        :math:`real\_hidden\_size = hidden\_size` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple, a tuple contains `(output, h_n, c_n, reserve, state)`.</span>

<span class="sd">        - **output** (Tensor) - Tensor of shape :math:`(seq\_len, batch\_size, num\_directions * real\_hidden\_size)`.</span>
<span class="sd">        - **h_n** (Tensor) - Tensor of shape :math:`(num\_directions * num\_layers, batch\_size, real\_hidden\_size)`.</span>
<span class="sd">        - **c_n** (Tensor) - Tensor of shape :math:`(num\_directions * num\_layers, batch\_size, hidden\_size)`.</span>
<span class="sd">        - **reserve** (Tensor) - Tensor of shape :math:`(r, 1)`.</span>
<span class="sd">        - **state** (Tensor) - Random number generator state and its shape is :math:`(s, 1)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_size`, `hidden_size` or `num_layers` is not an int.</span>
<span class="sd">        TypeError: If `has_bias` or `bidirectional` is not a bool.</span>
<span class="sd">        TypeError: If `dropout` is not a float.</span>
<span class="sd">        ValueError: If `dropout` is not in range [0.0, 1.0].</span>
<span class="sd">        ValueError: If `proj_size` is not in range [0, `hidden_size`).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_size = 10</span>
<span class="sd">        &gt;&gt;&gt; hidden_size = 2</span>
<span class="sd">        &gt;&gt;&gt; num_layers = 1</span>
<span class="sd">        &gt;&gt;&gt; seq_len = 5</span>
<span class="sd">        &gt;&gt;&gt; batch_size = 2</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; net = ops.LSTM(input_size, hidden_size, num_layers, True, False, 0.0)</span>
<span class="sd">        &gt;&gt;&gt; input_tensor = Tensor(np.ones([seq_len, batch_size, input_size]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; h0 = Tensor(np.ones([num_layers, batch_size, hidden_size]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; c0 = Tensor(np.ones([num_layers, batch_size, hidden_size]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; w = Tensor(np.ones([112, 1, 1]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output, hn, cn, _, _ = net(input_tensor, h0, c0, w)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[0.9640267  0.9640267 ]</span>
<span class="sd">          [0.9640267  0.9640267 ]]</span>
<span class="sd">         [[0.9950539  0.9950539 ]</span>
<span class="sd">          [0.9950539  0.9950539 ]]</span>
<span class="sd">         [[0.99932843 0.99932843]</span>
<span class="sd">          [0.99932843 0.99932843]]</span>
<span class="sd">         [[0.9999084  0.9999084 ]</span>
<span class="sd">          [0.9999084  0.9999084 ]]</span>
<span class="sd">         [[0.9999869  0.9999869 ]</span>
<span class="sd">          [0.9999869  0.9999869 ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">proj_size</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LSTM.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="s2">&quot;input_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_size</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">proj_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                                   <span class="s1">&#39;proj_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">num_layers</span><span class="p">,</span> <span class="s2">&quot;num_layers&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;has_bias&quot;</span><span class="p">,</span> <span class="n">has_bias</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;bidirectional&quot;</span><span class="p">,</span> <span class="n">bidirectional</span><span class="p">,</span> <span class="p">(</span><span class="nb">bool</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">dropout</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s1">&#39;dropout&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">bidirectional</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_directions</span> <span class="o">=</span> <span class="mi">1</span></div>


<div class="viewcode-block" id="SigmoidCrossEntropyWithLogits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SigmoidCrossEntropyWithLogits.html#mindspore.ops.SigmoidCrossEntropyWithLogits">[文档]</a><span class="k">class</span> <span class="nc">SigmoidCrossEntropyWithLogits</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses the given logits to compute sigmoid cross entropy between the logits and the label.</span>

<span class="sd">    Measures the distribution error in discrete classification tasks where each class is independent</span>
<span class="sd">    and not mutually exclusive using cross entropy loss.</span>

<span class="sd">    Sets input logits as :math:`X`, input label as :math:`Y`, output as :math:`loss`. Then,</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}} \\</span>
<span class="sd">            loss_{ij} = -[Y_{ij} * ln(p_{ij}) + (1 - Y_{ij})ln(1 - p_{ij})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits. Tensor of shape :math:`(N, *)` where :math:`*` means any number</span>
<span class="sd">          of additional dimensions.</span>
<span class="sd">        - **label** (Tensor) - Ground truth label. With the same shape and type as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and type as input `logits`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `logits` or `label` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[-0.8, 1.2, 0.7], [-0.1, -0.4, 0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([[0.3, 0.8, 1.2], [-0.6, 0.1, 2.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; sigmoid = ops.SigmoidCrossEntropyWithLogits()</span>
<span class="sd">        &gt;&gt;&gt; output = sigmoid(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.6111007   0.5032824   0.26318604]</span>
<span class="sd">         [ 0.58439666  0.5530153  -0.4368139 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SigmoidCrossEntropyWithLogits&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;predict&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="BCEWithLogitsLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BCEWithLogitsLoss.html#mindspore.ops.BCEWithLogitsLoss">[文档]</a><span class="k">class</span> <span class="nc">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adds sigmoid activation function to input `logits`, and uses the given logits to compute binary cross entropy</span>
<span class="sd">    between the logits and the label.</span>

<span class="sd">    Sets input logits as :math:`X`, input label as :math:`Y`, input weight as :math:`W`, output as :math:`L`. Then,</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij} = sigmoid(X_{ij}) = \frac{1}{1 + e^{-X_{ij}}} \\</span>
<span class="sd">            L_{ij} = -[Y_{ij}log(p_{ij}) + (1 - Y_{ij})log(1 - p_{ij})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`i` indicates the :math:`i^{th}` sample, :math:`j` indicates the category. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`\ell` indicates the method of calculating the loss. There are three methods:</span>
<span class="sd">    the first method is to provide the loss value directly,</span>
<span class="sd">    the second method is to calculate the average value of all losses,</span>
<span class="sd">    and the third method is to calculate the sum of all losses.</span>

<span class="sd">    This operator will multiply the output by the corresponding weight.</span>
<span class="sd">    The tensor `weight` assigns different weights to each piece of data in the batch,</span>
<span class="sd">    and the tensor `pos_weight` adds corresponding weights to the positive examples of each category.</span>

<span class="sd">    In addition, it can trade off recall and precision by adding weights to positive examples.</span>
<span class="sd">    In the case of multi-label classification the loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            p_{ij,c} = sigmoid(X_{ij,c}) = \frac{1}{1 + e^{-X_{ij,c}}} \\</span>
<span class="sd">            L_{ij,c} = -[P_{c}Y_{ij,c} * log(p_{ij,c}) + (1 - Y_{ij,c})log(1 - p_{ij,c})]</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where c is the class number (c&gt;1 for multi-label binary classification, c=1 for single-label binary classification),</span>
<span class="sd">    n is the number of the sample in the batch and :math:`P_c` is the weight of the positive answer for the class c.</span>
<span class="sd">    :math:`P_c&gt;1` increases the recall, :math:`P_c&lt;1` increases the precision.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - Input logits. Data type must be float16 or float32.</span>
<span class="sd">          Tensor of shape :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **label** (Tensor) - Ground truth label, has the same shape as `logits`.</span>
<span class="sd">          Data type must be float16 or float32.</span>
<span class="sd">        - **weight** (Tensor) - A rescaling weight applied to the loss of each batch element. It can be</span>
<span class="sd">          broadcast to a tensor with shape of `logits`. Data type must be float16 or float32.</span>
<span class="sd">        - **pos_weight** (Tensor) - A weight of positive examples. Must be a vector with length equal to the</span>
<span class="sd">          number of classes. It can be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">          Data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is ``&#39;none&#39;``, it&#39;s a tensor with the same shape and type as input `logits`.</span>
<span class="sd">        Otherwise, the output is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If any input is not Tensor.</span>
<span class="sd">        TypeError: If data type of any input is neither float16 nor float32.</span>
<span class="sd">        TypeError: If data type of `reduction` is not string.</span>
<span class="sd">        ValueError: If `weight` or `pos_weight` can not be broadcast to a tensor with shape of `logits`.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;`` or ``&#39;sum&#39;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([[-0.8, 1.2, 0.7], [-0.1, -0.4, 0.7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; label = Tensor(np.array([[0.3, 0.8, 1.2], [-0.6, 0.1, 2.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pos_weight = Tensor(np.array([1.0, 1.0, 1.0]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.BCEWithLogitsLoss()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(logits, label, weight, pos_weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.3463612</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BCEWithLogitsLoss&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;BCEWithLogitsLoss&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Pad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Pad.html#mindspore.ops.Pad">[文档]</a><span class="k">class</span> <span class="nc">Pad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings.</span>

<span class="sd">    Refer to :func:`mindspore.ops.pad` for more details. Use :func:`mindspore.ops.pad` instead if `paddings` has</span>
<span class="sd">    negative values.</span>

<span class="sd">    Args:</span>
<span class="sd">        paddings (tuple): The shape of parameter `paddings` is (N, 2). N is the rank of input data. All elements of</span>
<span class="sd">            paddings are int type. For the input in `D` th dimension, paddings[D, 0] indicates how many sizes to be</span>
<span class="sd">            extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1] indicates how many sizes to</span>
<span class="sd">            be extended behind the input tensor in the `D` th dimension.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor to be padded. It has shape :math:`(N, *)`, where :math:`*` means</span>
<span class="sd">          any number of additional dimensions.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `paddings` is not a tuple.</span>
<span class="sd">        TypeError: If `input_x` is not a Tensor.</span>
<span class="sd">        ValueError: If shape of `paddings` is not :math:`(N, 2)`.</span>
<span class="sd">        ValueError: If paddings.size is not equal to 2 * len(input_x).</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[-0.1, 0.3, 3.6], [0.4, 0.5, -3.2]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; pad_op = ops.Pad(((1, 2), (2, 1)))</span>
<span class="sd">        &gt;&gt;&gt; output = pad_op(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.   0.   0.   0.   0.   0. ]</span>
<span class="sd">         [ 0.   0.  -0.1  0.3  3.6  0. ]</span>
<span class="sd">         [ 0.   0.   0.4  0.5 -3.2  0. ]</span>
<span class="sd">         [ 0.   0.   0.   0.   0.   0. ]</span>
<span class="sd">         [ 0.   0.   0.   0.   0.   0. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">paddings</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;paddings&quot;</span><span class="p">,</span> <span class="n">paddings</span><span class="p">,</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paddings</span> <span class="o">=</span> <span class="n">paddings</span></div>


<span class="k">class</span> <span class="nc">PadV3</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input Tensor according to the `paddings`, `mode` and `paddings_contiguous`.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (str, optional): An optional string indicates padding mode,</span>
<span class="sd">            support ``&quot;constant&quot;`` , ``&quot;reflect&quot;`` , ``&quot;edge&quot;`` , ``&quot;circular&quot;`` . Default: ``&quot;constant&quot;`` .</span>
<span class="sd">            The effects of various padding modes are as follows:</span>

<span class="sd">            - ``&quot;constant&quot;``: Pads the input Tensor with value specified by `constant_value`.</span>
<span class="sd">            - ``&quot;reflect&quot;``: Pads the input Tensor by reflecting the values of the pixels at the</span>
<span class="sd">              boundary of the Tensor.</span>
<span class="sd">            - ``&quot;edge&quot;``: Pads the input Tensor with the values of the pixels on the border of the Tensor.</span>
<span class="sd">            - ``&quot;circular&quot;``: Circular padding mode. In this mode, the pixels from one edge of the image</span>
<span class="sd">              are wrapped around to the opposite edge, such that the pixel on the right edge of the</span>
<span class="sd">              image is replaced with the pixel on the left edge, and the pixel on the bottom edge</span>
<span class="sd">              is replaced with the pixel on the top edge.</span>

<span class="sd">        paddings_contiguous (bool, optional): An optional bool value indicates if the padding is paddings_contiguous.</span>
<span class="sd">            If ``True`` , paddings is arranged as [begin0, end0, begin1, end1, ...]</span>
<span class="sd">            If ``False`` , paddings is arranged as [begin0, begin1, ..., end1, end2, ...]</span>
<span class="sd">            Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor to be padded. It has shape :math:`(N, *)`, where :math:`*` means</span>
<span class="sd">          any number of additional dimensions.</span>
<span class="sd">        - **paddings** (Tensor) -  Specifies the number of zeros to be padded before and after each</span>
<span class="sd">          dimension of the input Tensor `x`. It&#39;s a 1D Tensor of type int32 or int64.</span>
<span class="sd">        - **constant_value** (Tensor, optional) - Padding value to use in &#39;constant&#39; mode,</span>
<span class="sd">          if not specified, 0 is used instead. It has the same type as `x`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `paddings` is not a Tensor.</span>
<span class="sd">        TypeError: If `padding_contiguous` is not a bool.</span>
<span class="sd">        ValueError: If `mode` is not a str or not in support modes.</span>
<span class="sd">        ValueError: If `mode` is &quot;constant&quot;, the element&#39;s number of `paddings` not be even.</span>
<span class="sd">        ValueError: If `mode` is &quot;constant&quot;, the element&#39;s number of `paddings` large than input dim * 2.</span>
<span class="sd">        ValueError: If `mode` is &quot;edge&quot; &quot;reflect&quot; or &quot;circular&quot;, the element&#39;s number of `paddings` is not 2, 4 or 6.</span>
<span class="sd">        ValueError: If `mode` is &quot;edge&quot; &quot;reflect&quot; or &quot;circular&quot;, `x` dims equals 3,</span>
<span class="sd">            the element&#39;s number of `paddings` is not 2.</span>
<span class="sd">        ValueError: If `mode` is &quot;edge&quot; &quot;reflect&quot; or &quot;circular&quot;, `x` dims equals 4,</span>
<span class="sd">            the element&#39;s number of `paddings` is not 4.</span>
<span class="sd">        ValueError: If `mode` is &quot;circular&quot;, `x` dims equals 5, the element&#39;s number of `paddings` is not 6.</span>
<span class="sd">        ValueError: If `mode` is &quot;edge&quot;, &quot;reflect&quot; or &quot;circular&quot;, `x` dims smaller than 3.</span>
<span class="sd">        ValueError: If `mode` is &quot;edge&quot; or &quot;circular&quot;, x dims bigger than 5.</span>
<span class="sd">        ValueError: If `mode` is &quot;reflect&quot;, x dims bigger than 4.</span>
<span class="sd">        ValueError: If `mode` is &quot;reflect&quot;, padding size bigger than the corresponding `x` dimension.</span>
<span class="sd">        ValueError: After padding, output&#39;s shape number is not greater than 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # case1: mode=&quot;reflect&quot;, paddings_contiguous=True</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...    def __init__(self, mode, paddings_contiguous):</span>
<span class="sd">        ...        super(Net, self).__init__()</span>
<span class="sd">        ...        self.pad = ops.PadV3(mode=mode, paddings_contiguous=paddings_contiguous)</span>
<span class="sd">        ...        self.paddings = Tensor([1, 1])</span>
<span class="sd">        ...    def construct(self, x):</span>
<span class="sd">        ...        return self.pad(x, self.paddings)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[[0., 1.]]])</span>
<span class="sd">        &gt;&gt;&gt; pad = Net(mode=&quot;reflect&quot;, paddings_contiguous=True)</span>
<span class="sd">        &gt;&gt;&gt; output = pad(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[1. 0. 1. 0.]]]</span>
<span class="sd">        &gt;&gt;&gt; # case2: mode=&quot;constant&quot;, padding_contigous=False</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...    def __init__(self, mode, paddings_contiguous):</span>
<span class="sd">        ...        super(Net, self).__init__()</span>
<span class="sd">        ...        self.pad = ops.PadV3(mode=mode, paddings_contiguous=paddings_contiguous)</span>
<span class="sd">        ...        self.paddings = Tensor([1, 0, 1, 0])</span>
<span class="sd">        ...        self.value = Tensor(1.5)</span>
<span class="sd">        ...    def construct(self, x):</span>
<span class="sd">        ...        return self.pad(x, self.paddings, self.value)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor([[0., 1., 2.]])</span>
<span class="sd">        &gt;&gt;&gt; pad = Net(mode=&quot;constant&quot;, paddings_contiguous=False)</span>
<span class="sd">        &gt;&gt;&gt; output = pad(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[1.5 0. 1. 2. 1.5]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">paddings_contiguous</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize PadV3&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;paddings&#39;</span><span class="p">,</span> <span class="s1">&#39;constant_value&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="s1">&#39;reflect&#39;</span><span class="p">,</span> <span class="s1">&#39;edge&#39;</span><span class="p">,</span> <span class="s1">&#39;circular&#39;</span><span class="p">],</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">paddings_contiguous</span><span class="p">,</span> <span class="s2">&quot;paddings_contiguous&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">paddings_contiguous</span> <span class="o">=</span> <span class="n">paddings_contiguous</span>


<div class="viewcode-block" id="MirrorPad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MirrorPad.html#mindspore.ops.MirrorPad">[文档]</a><span class="k">class</span> <span class="nc">MirrorPad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pads the input tensor according to the paddings and mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (str, optional): An optional string specifying the pad method.</span>
<span class="sd">            The optional values are ``&#39;REFLECT&#39;`` and ``&#39;SYMMETRIC&#39;`` .</span>
<span class="sd">            Default: ``&#39;REFLECT&#39;`` .</span>

<span class="sd">            - ``&#39;REFLECT&#39;``: Reflect the value on the edge while omitting the last one.</span>
<span class="sd">              For example, pad [1, 2, 3, 4] with 2 elements on both sides will result in [3, 2, 1, 2, 3, 4, 3, 2].</span>
<span class="sd">            - ``&#39;SYMMETRIC&#39;``: Reflect the value on the edge while repeating the last one.</span>
<span class="sd">              For example, pad [1, 2, 3, 4] with 2 elements on both sides will result in [2, 1, 1, 2, 3, 4, 4, 3].</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, *)`, where :math:`*` means, any number of</span>
<span class="sd">          additional dimensions.</span>
<span class="sd">        - **paddings** (Tensor) - Paddings requires constant tensor. The value of `paddings` is a</span>
<span class="sd">          matrix(list), and its shape is :math:`(N, 2)`. N is the rank of input data. All elements of paddings</span>
<span class="sd">          are int type. For the input in the `D` th dimension, paddings[D, 0] indicates how many sizes</span>
<span class="sd">          to be extended ahead of the input tensor in the `D` th dimension, and paddings[D, 1]</span>
<span class="sd">          indicates how many sizes to be extended behind the input tensor in the `D` th dimension. Both</span>
<span class="sd">          paddings[D, 0] and paddings[D, 1] must be no greater than input_x.dim_size(D)</span>
<span class="sd">          (or input_x.dim_size(D) - 1) if mode is SYMMETRIC (if REFLECT, respectively).</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the tensor after padding.</span>

<span class="sd">        - If `mode` is ``&#39;REFLECT&#39;``, it uses a way of symmetrical copying through the axis of symmetry to fill in.</span>
<span class="sd">          If the `input_x` is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the</span>
<span class="sd">          `Outputs` is [[6,5,4,5,6,5,4], [3,2,1,2,3,2,1], [6,5,4,5,6,5,4], [9,8,7,8,9,8,7], [6,5,4,5,6,5,4]].</span>
<span class="sd">          For a more intuitive understanding, please see the example below.</span>
<span class="sd">        - If `mode` is ``&#39;SYMMETRIC&#39;``, the filling method is similar to the ``&#39;REFLECT&#39;``. It is also copied</span>
<span class="sd">          according to the symmetry axis, except that it includes the symmetry axis. If the `input_x`</span>
<span class="sd">          is [[1,2,3], [4,5,6], [7,8,9]] and `paddings` is [[1,1], [2,2]], then the `Outputs` is</span>
<span class="sd">          [[2,1,1,2,3,3,2], [2,1,1,2,3,3,2], [5,4,4,5,6,6,5], [8,7,7,8,9,9,8], [8,7,7,8,9,9,8]].</span>
<span class="sd">          For a more intuitive understanding, please see the example below.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` or `paddings` is not a Tensor.</span>
<span class="sd">        TypeError: If `mode` is not a str.</span>
<span class="sd">        ValueError: If paddings.size is not equal to 2 * rank of input_x.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops</span>
<span class="sd">        &gt;&gt;&gt; # case1: mode=&quot;REFLECT&quot;</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...    def __init__(self, mode):</span>
<span class="sd">        ...        super(Net, self).__init__()</span>
<span class="sd">        ...        self.pad = ops.MirrorPad(mode=mode)</span>
<span class="sd">        ...        self.paddings = Tensor([[1, 1], [2, 2]])</span>
<span class="sd">        ...    def construct(self, input_x):</span>
<span class="sd">        ...        return self.pad(input_x, self.paddings)</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor([[1,2,3], [4,5,6], [7,8,9]])</span>
<span class="sd">        &gt;&gt;&gt; pad = Net(&quot;REFLECT&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = pad(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[6 5 4 5 6 5 4]</span>
<span class="sd">         [3 2 1 2 3 2 1]</span>
<span class="sd">         [6 5 4 5 6 5 4]</span>
<span class="sd">         [9 8 7 8 9 8 7]</span>
<span class="sd">         [6 5 4 5 6 5 4]]</span>
<span class="sd">        &gt;&gt;&gt; # case2: mode=&quot;SYMMETRIC&quot;</span>
<span class="sd">        &gt;&gt;&gt; pad = Net(&quot;SYMMETRIC&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = pad(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2 1 1 2 3 3 2]</span>
<span class="sd">         [2 1 1 2 3 3 2]</span>
<span class="sd">         [5 4 4 5 6 6 5]</span>
<span class="sd">         [8 7 7 8 9 9 8]</span>
<span class="sd">         [8 7 7 8 9 9 8]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;REFLECT&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;paddings&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;REFLECT&#39;</span><span class="p">,</span> <span class="s1">&#39;SYMMETRIC&#39;</span><span class="p">],</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span></div>


<div class="viewcode-block" id="ComputeAccidentalHits"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ComputeAccidentalHits.html#mindspore.ops.ComputeAccidentalHits">[文档]</a><span class="k">class</span> <span class="nc">ComputeAccidentalHits</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute accidental hits of sampled classes which match target classes.</span>

<span class="sd">    When a target class matches the sample class, we call it &quot;accidental hit&quot;.</span>
<span class="sd">    The result of calculating accidental hits contain three parts (index, id, weight),</span>
<span class="sd">    where index represents the row number in true_classes, and id represents the position in sampled_candidates,</span>
<span class="sd">    the weight is FLOAT_MAX. FLOAT_MAX indicates the max value in the type of Float</span>

<span class="sd">    Args:</span>
<span class="sd">        num_true (int): The number of target classes per training example. Default: ``1`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **true_classes** (Tensor) - The target classes. With data type of int64</span>
<span class="sd">          and shape :math:`(batch\_size, num\_true)`.</span>
<span class="sd">        - **sampled_candidates** (Tensor) - The Candidate sampling results of operators, types of training samples,</span>
<span class="sd">          with data type of int64 and shape :math:`(num\_sampled, )`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors.</span>

<span class="sd">        - **indices** (Tensor) - A Tensor with shape :math:`(num\_accidental\_hits, )`,</span>
<span class="sd">          with data type of int32.</span>
<span class="sd">        - **ids** (Tensor) - A Tensor with shape :math:`(num\_accidental\_hits, )`,</span>
<span class="sd">          with data type of int64.</span>
<span class="sd">        - **weights** (Tensor) - A Tensor with shape :math:`(num\_accidental\_hits, )`, with the type float32.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `num_true` is not int.</span>
<span class="sd">        TypeError: If `true_classes` or `sampled_candidates` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `true_classes` or `sampled_candidates` is neither int32 nor int64.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; true_classes = np.array([[1, 2], [0, 4], [3, 3]])</span>
<span class="sd">        &gt;&gt;&gt; sampled_candidates = np.array([0, 1, 2, 3, 4])</span>
<span class="sd">        &gt;&gt;&gt; sampler = ops.ComputeAccidentalHits(2)</span>
<span class="sd">        &gt;&gt;&gt; indices, ids, weights = sampler(Tensor(true_classes), Tensor(sampled_candidates))</span>
<span class="sd">        &gt;&gt;&gt; print(indices, ids, weights)</span>
<span class="sd">        [0 0 1 1 2 2]</span>
<span class="sd">        [1 2 0 4 3 3]</span>
<span class="sd">        [-3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38 -3.4028235e+38]</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_true</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ComputeAccidentalHits&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;true_classes&#39;</span><span class="p">,</span> <span class="s1">&#39;sampled_candidates&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;ids&#39;</span><span class="p">,</span> <span class="s1">&#39;weights&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;num_true&quot;</span><span class="p">,</span> <span class="n">num_true</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;num_true&quot;</span><span class="p">,</span> <span class="n">num_true</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_true</span> <span class="o">=</span> <span class="n">num_true</span></div>


<div class="viewcode-block" id="ROIAlign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ROIAlign.html#mindspore.ops.ROIAlign">[文档]</a><span class="k">class</span> <span class="nc">ROIAlign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Region of Interest (RoI) Align operator.</span>

<span class="sd">    The operator computes the value of each sampling point by bilinear interpolation from the nearby grid points on the</span>
<span class="sd">    feature map. No quantization is performed on any coordinates involved in the RoI, its bins, or the sampling</span>
<span class="sd">    points. The details of (RoI) Align operator are described in `Mask R-CNN &lt;https://arxiv.org/abs/1703.06870&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        pooled_height (int): The output features height.</span>
<span class="sd">        pooled_width (int): The output features width.</span>
<span class="sd">        spatial_scale (float): A scaling factor that maps the raw image coordinates to the input</span>
<span class="sd">            feature map coordinates. Suppose the height of a RoI is `ori_h` in the raw image and `fea_h` in the</span>
<span class="sd">            input feature map, the `spatial_scale` must be `fea_h / ori_h`.</span>
<span class="sd">        sample_num (int): Number of sampling points. Default: ``2`` .</span>
<span class="sd">        roi_end_mode (int): Number must be 0 or 1. If roi_end_mode=0, use the legacy implementation.</span>
<span class="sd">            If roi_end_mode=1, end pixel of the roi_box will be shifted by +1*spatial_scale. Default: ``1`` .</span>


<span class="sd">    Inputs:</span>
<span class="sd">        - **features** (Tensor) - The input features, whose shape must be :math:`(N, C, H, W)`.</span>
<span class="sd">        - **rois** (Tensor) - The shape is :math:`(rois\_n, 5)`. With data type of float16 or float32.</span>
<span class="sd">          `rois_n` represents the number of RoI. The size of the second dimension must be `5` and the `5` colunms</span>
<span class="sd">          are :math:`(image\_index, top\_left\_x, top\_left\_y, bottom\_right\_x, bottom\_right\_y)`.</span>
<span class="sd">          `image_index` represents the index of image. `top_left_x` and `top_left_y` represent the `x, y`</span>
<span class="sd">          coordinates of the top left corner of corresponding RoI, respectively. `bottom_right_x` and `bottom_right_y`</span>
<span class="sd">          represent the `x, y` coordinates of the bottom right corner of corresponding RoI, respectively.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the shape is :math:`(rois\_n, C, pooled\_height, pooled\_width)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `pooled_height`, `pooled_width`, `sample_num` or `roi_end_mode` is not an int.</span>
<span class="sd">        TypeError: If `spatial_scale` is not a float.</span>
<span class="sd">        TypeError: If `features` or `rois` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; features = Tensor(np.array([[[[1., 2.], [3., 4.]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rois = Tensor(np.array([[0, 0.2, 0.3, 0.2, 0.3]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; roi_align = ops.ROIAlign(2, 2, 0.5, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = roi_align(features, rois)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[1.775 2.025]</span>
<span class="sd">           [2.275 2.525]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="n">sample_num</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">roi_end_mode</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ROIAlign&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooled_height&quot;</span><span class="p">,</span> <span class="n">pooled_height</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooled_width&quot;</span><span class="p">,</span> <span class="n">pooled_width</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;spatial_scale&quot;</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;sample_num&quot;</span><span class="p">,</span> <span class="n">sample_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;roi_end_mode&quot;</span><span class="p">,</span> <span class="n">roi_end_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">roi_end_mode</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;roi_end_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_height</span> <span class="o">=</span> <span class="n">pooled_height</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooled_width</span> <span class="o">=</span> <span class="n">pooled_width</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_scale</span> <span class="o">=</span> <span class="n">spatial_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_num</span> <span class="o">=</span> <span class="n">sample_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">roi_end_mode</span> <span class="o">=</span> <span class="n">roi_end_mode</span></div>


<div class="viewcode-block" id="Adam"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Adam.html#mindspore.ops.Adam">[文档]</a><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates gradients by the Adaptive Moment Estimation (Adam) algorithm.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    For more details, please refer to :class:`mindspore.nn.Adam`.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t(\beta_1^{t})` and :math:`beta_2^t(\beta_2^{t})`</span>
<span class="sd">    represent `beta1_power` and `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`,</span>
<span class="sd">    :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    Inputs of `var`, `m`, `v` and `gradient`</span>
<span class="sd">    comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If ``True`` , updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If ``False`` , the result is unpredictable. Default: ``False`` .</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If ``True`` , update the gradients using NAG.</span>
<span class="sd">            If ``False`` , update the gradients without using NAG. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated. The shape is :math:`(N, *)` where :math:`*` means,</span>
<span class="sd">          any number of additional dimensions. The data type can be float16 or float32.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula,</span>
<span class="sd">          the shape should be the same as `var`.</span>
<span class="sd">        - **v** (Parameter) - the 2nd moment vector in the updating formula,</span>
<span class="sd">          the shape should be the same as `var`.</span>
<span class="sd">        - **beta1_power** (float) - :math:`beta_1^t(\beta_1^{t})` in the updating formula.</span>
<span class="sd">        - **beta2_power** (float) - :math:`beta_2^t(\beta_2^{t})` in the updating formula.</span>
<span class="sd">        - **lr** (float) - :math:`l` in the updating formula. The paper suggested value is :math:`10^{-8}`.</span>
<span class="sd">        - **beta1** (float) - The exponential decay rate for the 1st moment estimations.</span>
<span class="sd">          The paper suggested value is :math:`0.9`.</span>
<span class="sd">        - **beta2** (float) - The exponential decay rate for the 2nd moment estimations.</span>
<span class="sd">          The paper suggested value is :math:`0.999`.</span>
<span class="sd">        - **epsilon** (float) - Term added to the denominator to improve numerical stability.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same shape and data type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as Inputs `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as Inputs `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as Inputs `v`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_nesterov` is a bool.</span>
<span class="sd">        TypeError: If `var`, `m` or `v` is not a Parameter.</span>
<span class="sd">        TypeError: If `beta1_power`, `beta2_power1`, `lr`, `beta1`, `beta2`, `epsilon` or `gradient` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adam = ops.Adam()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        ...         out = self.apply_adam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2,</span>
<span class="sd">        ...                               epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.ones([2, 2]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(0.9, 0.999, 0.001, 0.9, 0.999, 1e-8, gradient)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.9996838 0.9996838]</span>
<span class="sd">         [0.9996838 0.9996838]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T5</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T6</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T7</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T8</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Adam.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">AdamNoUpdateParam</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates gradients by the Adaptive Moment Estimation (Adam) algorithm. This operator do not update the parameter, but</span>
<span class="sd">    calculate the value that should be added to the parameter instead.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            \Delta{w} = - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`beta_1^t(\beta_1^{t})` and :math:`beta_2^t(\beta_2^{t})`</span>
<span class="sd">    represent `beta1_power` and `beta2_power`, :math:`\alpha` represents `learning_rate`,</span>
<span class="sd">    :math:`w` represents the parameter to be updated, :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If ``True`` , updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If ``False`` , the result is unpredictable. Default: ``False`` .</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If ``True`` , update the gradients using NAG.</span>
<span class="sd">            If ``False`` , update the gradients without using NAG. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **m** (Tensor) - The 1st moment vector in the updating formula. The shape is :math:`(N, *)`</span>
<span class="sd">          where :math:`*` means, any number of additional dimensions. The data type must be float32.</span>
<span class="sd">        - **v** (Tensor) - the 2nd moment vector in the updating formula. The shape must be the same as `m`.</span>
<span class="sd">          The data type must be float32.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t(\beta_1^{t})` in the updating formula.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t(\beta_2^{t})` in the updating formula.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability.</span>
<span class="sd">          The shape is :math:`(1, )` and the data type must be float32.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, the shape must be the same as `m`, the data type must be float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, whose shape and data type are the same with Inputs `gradient`, is a value that should be added to the</span>
<span class="sd">        parameter to be updated.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_nesterov` is a bool.</span>
<span class="sd">        TypeError: If `m`,  `v`, `beta1_power`, `beta2_power1`, `lr`, `beta1`, `beta2`, `epsilon` or `gradient`</span>
<span class="sd">                   is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.adam = ops.AdamNoUpdateParam()</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2]]).astype(np.float32)),</span>
<span class="sd">        ...                            name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.array([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2]]).astype(np.float32)),</span>
<span class="sd">        ...                            name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        ...         out = self.adam(self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; result = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient)</span>
<span class="sd">        &gt;&gt;&gt; print(result)</span>
<span class="sd">        [[-0.00010004 -0.00010004 -0.00010004]</span>
<span class="sd">        [-0.00013441 -0.00013441 -0.00013441]]</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AdamNoUpdateParam.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FusedSparseAdam</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates parameters by the Adaptive Moment Estimation (Adam)</span>
<span class="sd">    algorithm. This operator is used when the gradient is sparse.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`\beta_1^t` and :math:`\beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If ``True`` , updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If ``False`` , the result is unpredictable. Default: ``False`` .</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If ``True`` , update the gradients using NAG.</span>
<span class="sd">            If ``False`` , update the gradients without using NAG. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Parameters to be updated with float32 data type. The shape is :math:`(N, *)`</span>
<span class="sd">          where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same shape and data type as `var`.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula, has the same shape and data type as `var`.</span>
<span class="sd">          Mean square gradients, has the same type as `var` with float32 data type.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t` in the updating formula with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t` in the updating formula with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula. With float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient, has the same data type as `var` and</span>
<span class="sd">          gradient.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - Gradient indices with int32 data type and indices.shape[0] = gradient.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape :math:`(N, *)`.</span>
<span class="sd">        - **m** (Tensor) - A Tensor with shape :math:`(1, )`.</span>
<span class="sd">        - **v** (Tensor) - A Tensor with shape :math:`(1, )`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_neserov` is a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `beta1_power`, `beta2_power`, `lr`, `beta1`, `beta2`, `epsilon`,</span>
<span class="sd">                   `gradient` or `indices` is not float32.</span>
<span class="sd">        RuntimeError: If the data type of all inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_adam = ops.FusedSparseAdam()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_adam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1, beta2,</span>
<span class="sd">        ...                                      epsilon, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([[[0.1, 0.1]], [[0.1, 0.1]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[[0.9997121  0.9997121 ]]</span>
<span class="sd">         [[0.9997121  0.9997121 ]]</span>
<span class="sd">         [[0.99971527 0.99971527]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FusedSparseAdam.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FusedSparseLazyAdam</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates parameters by the Adaptive Moment Estimation (Adam)</span>
<span class="sd">    algorithm. This operator is used when the gradient is sparse. The behavior is not equivalent to the</span>
<span class="sd">    original Adam algorithm, as only the current indices parameters will be updated.</span>

<span class="sd">    The Adam algorithm is proposed in `Adam: A Method for Stochastic Optimization &lt;https://arxiv.org/abs/1412.6980&gt;`_.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m = \beta_1 * m + (1 - \beta_1) * g \\</span>
<span class="sd">            v = \beta_2 * v + (1 - \beta_2) * g * g \\</span>
<span class="sd">            l = \alpha * \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \\</span>
<span class="sd">            w = w - l * \frac{m}{\sqrt{v} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`m` represents the 1st moment vector, :math:`v` represents the 2nd moment vector, :math:`g` represents</span>
<span class="sd">    `gradient`, :math:`l` represents scaling factor `lr`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`t` represents updating step while :math:`\beta_1^t` and :math:`\beta_2^t` represent `beta1_power` and</span>
<span class="sd">    `beta2_power`, :math:`\alpha` represents `learning_rate`, :math:`w` represents `var`, :math:`\epsilon` represents</span>
<span class="sd">    `epsilon`.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Whether to enable a lock to protect variable tensors from being updated.</span>
<span class="sd">            If ``True`` , updates of the var, m, and v tensors will be protected by a lock.</span>
<span class="sd">            If ``False`` , the result is unpredictable. Default: ``False`` .</span>
<span class="sd">        use_nesterov (bool): Whether to use Nesterov Accelerated Gradient (NAG) algorithm to update the gradients.</span>
<span class="sd">            If ``True`` , update the gradients using NAG.</span>
<span class="sd">            If ``False`` , update the gradients without using NAG. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Parameters to be updated with float32 data type. The shape is :math:`(N, *)`</span>
<span class="sd">          where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same shape and data type as `var`.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula, has the same shape and data type as `var`.</span>
<span class="sd">          Mean square gradients, has the same type as `var` with float32 data type.</span>
<span class="sd">        - **beta1_power** (Tensor) - :math:`beta_1^t` in the updating formula with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta2_power** (Tensor) - :math:`beta_2^t` in the updating formula with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **lr** (Tensor) - :math:`l` in the updating formula with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta1** (Tensor) - The exponential decay rate for the 1st moment estimations with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **beta2** (Tensor) - The exponential decay rate for the 2nd moment estimations with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **epsilon** (Tensor) - Term added to the denominator to improve numerical stability with float32 data type.</span>
<span class="sd">          The shape is :math:`(1, )`.</span>
<span class="sd">        - **gradient** (Tensor) - Gradient value with float32 data type and</span>
<span class="sd">          gradient.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - Gradient indices with int32 data type and indices.shape[0] = gradient.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape :math:`(N, *)`.</span>
<span class="sd">        - **m** (Tensor) - A Tensor with shape :math:`(1, )`.</span>
<span class="sd">        - **v** (Tensor) - A Tensor with shape :math:`(1, )`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `use_locking` nor `use_nestrov` is a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `beta1_power`, `beta2_power`, `lr`, `beta1`, `beta2`, `epsilon` or</span>
<span class="sd">                   gradient is not float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        RuntimeError: If the data type of all inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_lazyadam = ops.FusedSparseLazyAdam()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_lazyadam(self.var, self.m, self.v, beta1_power, beta2_power, lr, beta1,</span>
<span class="sd">        ...                                          beta2, epsilon, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2_power = Tensor(0.999, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.999, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-8, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([[[0.1, 0.1]], [[0.1, 0.1]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor([0, 1], mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(beta1_power, beta2_power, lr, beta1, beta2, epsilon, gradient, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[[0.9997121  0.9997121 ]]</span>
<span class="sd">         [[0.9997121  0.9997121 ]]</span>
<span class="sd">         [[1.         1.        ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FusedSparseLazyAdam.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="s1">&#39;beta2&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="s1">&#39;v&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FusedSparseFtrl</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates relevant entries according to the FTRL-proximal scheme.</span>

<span class="sd">    All inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if True . Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float32. The shape is :math:`(N, *)`</span>
<span class="sd">          where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - the linear coefficient to be updated, must be same type and shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var` and</span>
<span class="sd">          grad.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The type must be int32 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape :math:`(N, *)`.</span>
<span class="sd">        - **accum** (Tensor) - A Tensor with shape :math:`(1, )`.</span>
<span class="sd">        - **linear** (Tensor) - A Tensor with shape :math:`(1, )`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `lr_power` is not a float.</span>
<span class="sd">        ValueError: If shape of `lr_power` less than or equal to zero.</span>
<span class="sd">        TypeError: If dtype of `var` is not float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        TypeError: If shape of `accum`, `linear` or `grad` is not same as `var`.</span>
<span class="sd">        TypeError: If shape of `indices` is not same as shape of first dimension of `grad`.</span>
<span class="sd">        RuntimeError: If the data type of all of inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(SparseApplyFtrlNet, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_ftrl = ops.FusedSparseFtrl(lr=0.01, l1=0.0, l2=0.0, lr_power=-0.5)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_ftrl(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[[0.1, 0.1]], [[0.1, 0.1]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[[-0.00598256 -0.00598256]]</span>
<span class="sd">         [[-0.00598256 -0.00598256]]</span>
<span class="sd">         [[ 1.          1.        ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FusedSparseFtrl.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">FusedSparseProximalAdagrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Merges the duplicate value of the gradient and then updates relevant entries according to the proximal adagrad</span>
<span class="sd">    algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}} \\</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>
<span class="sd">        \end{array}</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If ``True`` , the variable and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Variable tensor to be updated, has the same shape and data type as `var`.</span>
<span class="sd">        - **lr** (Tensor) - The learning rate value. The data type must be float32. The shape is :math:`(1, )`.</span>
<span class="sd">        - **l1** (Tensor) - l1 regularization strength. The data type must be float32. The shape is :math:`(1, )`.</span>
<span class="sd">        - **l2** (Tensor) - l2 regularization strength. The data type must be float32. The shape is :math:`(1, )`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same data type as `var` and</span>
<span class="sd">          grad.shape[1:] = var.shape[1:] if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The type must be int32 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, this operator will update the input parameters directly, the outputs are useless.</span>

<span class="sd">        - **var** (Tensor) - A Tensor with shape :math:`(N, *)`.</span>
<span class="sd">        - **accum** (Tensor) - A Tensor with shape :math:`(1, )`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr`, `l1`, `l2` or `grad` is not float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        RuntimeError: If the data type of all inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_proximal_adagrad = ops.FusedSparseProximalAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.ones([3, 1, 2]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.lr = Tensor(0.01, mindspore.float32)</span>
<span class="sd">        ...         self.l1 = Tensor(0.0, mindspore.float32)</span>
<span class="sd">        ...         self.l2 = Tensor(0.0, mindspore.float32)</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1,</span>
<span class="sd">        ...                                                  self.l2, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[[0.1, 0.1]], [[0.1, 0.1]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[[0.99900496 0.99900496]]</span>
<span class="sd">         [[0.99900496 0.99900496]]</span>
<span class="sd">         [[1.         1.        ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FusedSparseProximalAdagrad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="KLDivLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.KLDivLoss.html#mindspore.ops.KLDivLoss">[文档]</a><span class="k">class</span> <span class="nc">KLDivLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Kullback-Leibler divergence between the logits and the labels.</span>

<span class="sd">    For tensors of the same shape :math:`x` and :math:`target`,</span>
<span class="sd">    the updating formulas of KLDivLoss algorithm are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(x, target) = target \cdot (\log target - x)</span>

<span class="sd">    Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, target) = \begin{cases}</span>
<span class="sd">        L(x, target), &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L(x, target)), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L(x, target)) / x.\operatorname{shape}[0], &amp; \text{if reduction} = \text{&#39;batchmean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L(x, target)),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`x` represents `logits`,</span>
<span class="sd">    :math:`target` represents `labels`, and</span>
<span class="sd">    :math:`\ell(x, target)` represents `output`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - On Ascend, float64 dtype is not currently supported.</span>
<span class="sd">        - The output aligns with the mathematical definition of Kullback-Leibler divergence</span>
<span class="sd">          only when `reduction` is set to ``&#39;batchmean&#39;``.</span>
<span class="sd">        - On Ascend, the value of `reduction` must be one of ``&#39;batchmean&#39;``, ``&#39;none&#39;`` or ``&#39;sum&#39;``.</span>
<span class="sd">        - On GPU, the value of `reduction` must be one of ``&#39;mean&#39;``, ``&#39;none&#39;`` or ``&#39;sum&#39;``.</span>
<span class="sd">        - On CPU, the value of `reduction` must be one of ``&#39;mean&#39;``, ``&#39;batchmean&#39;``, ``&#39;none&#39;``</span>
<span class="sd">          or ``&#39;sum&#39;``.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str): Specifies the reduction to be applied to the output.</span>
<span class="sd">            Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>
<span class="sd">            - ``&#39;batchmean&#39;``: average loss is taken over the batch, similar to the mean mode.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The input Tensor. The data type must be float16, float32 or float64.</span>
<span class="sd">        - **labels** (Tensor) - The label Tensor which has the same shape and data type as `logits`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar, if `reduction` is ``&#39;none&#39;``, then output is a tensor and has the same shape as `logits`.</span>
<span class="sd">        Otherwise it is a scalar.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `reduction` is not a str.</span>
<span class="sd">        TypeError: If neither `logits` nor `labels` is a Tensor.</span>
<span class="sd">        TypeError: If dtype of `logits` or `labels` is not currently supported.</span>
<span class="sd">        ValueError: If shape of `logits` is not the same as `labels`.</span>
<span class="sd">        RuntimeError: If `logits` or `labels` is a scalar when `reduction` is &#39;batchmean&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.kldiv_loss = ops.KLDivLoss(reduction=&#39;sum&#39;)</span>
<span class="sd">        ...     def construct(self, logits, labels):</span>
<span class="sd">        ...         result = self.kldiv_loss(logits, labels)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(logits, labels)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        -0.7</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize KLDivLoss.&quot;&quot;&quot;</span>
        <span class="n">device_target</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device_target</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
            <span class="n">support_mode</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;batchmean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">device_target</span> <span class="o">==</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span>
            <span class="n">support_mode</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">device_target</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">:</span>
            <span class="n">support_mode</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;batchmean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39; unknown device target: &#39;</span><span class="si">{</span><span class="n">device_target</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="n">support_mode</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="BinaryCrossEntropy"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.BinaryCrossEntropy.html#mindspore.ops.BinaryCrossEntropy">[文档]</a><span class="k">class</span> <span class="nc">BinaryCrossEntropy</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the binary cross entropy between the logits and the labels.</span>

<span class="sd">    Sets logits as :math:`x`, labels as :math:`y`, output as :math:`\ell(x, y)`.</span>
<span class="sd">    Let,</span>

<span class="sd">    .. math::</span>
<span class="sd">        L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]</span>

<span class="sd">    In which, :math:`L` indicates the loss of all batch_sizes, :math:`l` indicates the loss of one batch_size,</span>
<span class="sd">    and n indicates one batch_size in the 1-N range, :math:`w_n` indicates the</span>
<span class="sd">    weight of :math:`n`-th batch of binary cross entropy. Then,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">        L, &amp; \text{if reduction} = \text{&#39;none&#39;;}\\</span>
<span class="sd">        \operatorname{mean}(L), &amp; \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">        \operatorname{sum}(L),  &amp; \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        - The value of :math:`x` must range from 0 to 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the weighted mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **logits** (Tensor) - The predictive value whose data type must be float16 or float32,</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **labels** (Tensor) - The target value which has the same shape and data type as `logits`.</span>
<span class="sd">        - **weight** (Tensor, optional) - A rescaling weight applied to the loss of each batch element.</span>
<span class="sd">          And it must have the same shape and data type as `logits`. Default: ``None`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor or Scalar. Returns Tensor that has the same dtype and shape as `logits` if `reduction` is &#39;none&#39;.</span>
<span class="sd">        Otherwise, returns a scalar Tensor.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `logits`, `labels` or `weight` (if given) is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;`` or ``&#39;sum&#39;``.</span>
<span class="sd">        ValueError: If shape of `labels` is not the same as `logits` or `weight` (if given).</span>
<span class="sd">        TypeError: If `logits`, `labels` or `weight` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.binary_cross_entropy = ops.BinaryCrossEntropy()</span>
<span class="sd">        ...     def construct(self, logits, labels, weight):</span>
<span class="sd">        ...         result = self.binary_cross_entropy(logits, labels, weight)</span>
<span class="sd">        ...         return result</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; logits = Tensor(np.array([0.2, 0.7, 0.1]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; labels = Tensor(np.array([0., 1., 0.]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([1, 2, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(logits, labels, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.38240486</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BinaryCrossEntropy.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAdaMax"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdaMax.html#mindspore.ops.ApplyAdaMax">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdaMax</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adamax scheme.</span>

<span class="sd">    The updating formulas are as follows,</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t+1} = \beta_1 * m_{t} + (1 - \beta_1) * g \\</span>
<span class="sd">            v_{t+1} = \max(\beta_2 * v_{t}, \left| g \right|) \\</span>
<span class="sd">            var = var - \frac{l}{1 - \beta_1^{t+1}} * \frac{m_{t+1}}{v_{t+1} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t}`</span>
<span class="sd">    is the last moment of :math:`m_{t+1}`, :math:`v` represents the 2nd moment vector, :math:`v_{t}`</span>
<span class="sd">    is the last moment of :math:`v_{t+1}`, :math:`l` represents scaling factor `lr`,</span>
<span class="sd">    :math:`g` represents `grad`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`\beta_1^{t+1}` represents `beta1_power`, :math:`var` represents the variable to be updated,</span>
<span class="sd">    :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Inputs of `var`, `m`, `v` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float32 or float16 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula, has the same shape as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula. Mean square gradients</span>
<span class="sd">          with the same shape as `var`. With float32 or float16 data type.</span>
<span class="sd">        - **beta1_power** (Union[Number, Tensor]) - :math:`beta_1^t` in the updating formula, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Learning rate, :math:`l` in the updating formula, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **beta1** (Union[Number, Tensor]) - The exponential decay rate for the 1st moment estimations,</span>
<span class="sd">          must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **beta2** (Union[Number, Tensor]) - The exponential decay rate for the 2nd moment estimations,</span>
<span class="sd">          must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **epsilon** (Union[Number, Tensor]) - A small value added for numerical stability, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient, has the same shape as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `beta_power`, `lr`, `beta1`, `beta2`, `epsilon` or `grad` is neither</span>
<span class="sd">                   float16 nor float32.</span>
<span class="sd">        TypeError: If `beta_power`, `lr`, `beta1`, `beta2` or `epsilon` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type of `var`, `m`, `v` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_ada_max = ops.ApplyAdaMax()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                             [0.2, 0.6]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.array([[0.9, 0.1],</span>
<span class="sd">        ...                                             [0.7, 0.8]]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, lr, beta1, beta2, epsilon, grad):</span>
<span class="sd">        ...         out = self.apply_ada_max(self.var, self.m, self.v, beta1_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; beta1_power =Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta1 = Tensor(0.9, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; beta2 = Tensor(0.99, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-10, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(beta1_power, lr, beta1, beta2, epsilon, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.93602717e-01,  3.92571449e-01],</span>
<span class="sd">         [ 9.72582996e-02,  4.92249995e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.69999993e-01,  5.19999981e-01],</span>
<span class="sd">         [ 1.89999998e-01,  6.20000005e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 8.90999973e-01,  6.99999988e-01],</span>
<span class="sd">         [ 6.93000019e-01,  8.00000012e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T5</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdaMax&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAdadelta"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdadelta.html#mindspore.ops.ApplyAdadelta">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdadelta</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adadelta scheme.</span>

<span class="sd">    The Adadelta algorithm is proposed in</span>
<span class="sd">    `ADADELTA: AN ADAPTIVE LEARNING RATE METHOD &lt;https://arxiv.org/abs/1212.5701&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            \text{accum} = \rho * \text{accum} + (1 - \rho) * \text{grad}^2 \\</span>
<span class="sd">            \text{update} = \sqrt{\text{accum_update} +</span>
<span class="sd">              \epsilon} * \frac{\text{grad}}{\sqrt{\text{accum} + \epsilon}} \\</span>
<span class="sd">            \text{accum_update} = \rho * \text{accum_update} + (1 - \rho) * \text{update}^2 \\</span>
<span class="sd">            \text{var} = \text{var} - \text{lr} * \text{update}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`\rho` represents `rho`, :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Inputs of `var`, `accum`, `accum_update` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated. With float32 or float16 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated, has the same shape and data type as `var`.</span>
<span class="sd">        - **accum_update** (Parameter) - Accum_update to be updated, has the same shape and data type as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Learning rate, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **rho** (Union[Number, Tensor]) - Decay rate, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **epsilon** (Union[Number, Tensor]) - A small value added for numerical stability, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - Gradients, has the same shape and data type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>
<span class="sd">        - **accum_update** (Tensor) - The same shape and data type as `accum_update`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `accum_update`, `lr`, `rho`, `epsilon` or `grad` is neither float16 nor</span>
<span class="sd">                   float32.</span>
<span class="sd">        TypeError: If `accum_update`, `lr`, `rho` or `epsilon` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If the data type of `var`, `accum`, `accum_update` and `grad` conversion of Parameter</span>
<span class="sd">                      is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import nn, Tensor, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adadelta = ops.ApplyAdadelta()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.accum_update = Parameter(Tensor(np.array([[0.9, 0.1],</span>
<span class="sd">        ...                                                        [0.7, 0.8]]).astype(np.float32)),</span>
<span class="sd">        ...                                                             name=&quot;accum_update&quot;)</span>
<span class="sd">        ...     def construct(self, lr, rho, epsilon, grad):</span>
<span class="sd">        ...         out = self.apply_adadelta(self.var, self.accum, self.accum_update, lr, rho, epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rho = Tensor(0.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(1e-6, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, rho, epsilon, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99051356e-01,  3.99683774e-01],</span>
<span class="sd">         [ 9.91633832e-02,  4.99105573e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 9.00000036e-02,  4.89999980e-01],</span>
<span class="sd">         [ 1.00000007e-02,  6.40000045e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 8.99990857e-01,  1.00000791e-01],</span>
<span class="sd">         [ 6.99930906e-01,  7.99999774e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum_update&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdadelta&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAdagrad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdagrad.html#mindspore.ops.ApplyAdagrad">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdagrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagrad scheme.</span>
<span class="sd">    The Adagrad algorithm was proposed in</span>
<span class="sd">    `Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</span>
<span class="sd">    &lt;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&gt;`_.</span>
<span class="sd">    This module can adaptively assign different learning rates for each parameter in view of the uneven number</span>
<span class="sd">    of samples for different parameters.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum}}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad`  comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        update_slots (bool): If ``True`` , `accum` will be updated. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float or complex data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape must be the same as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a scalar. With float or complex data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. The shape must be the same as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr` or `grad` is neither float nor complex.</span>
<span class="sd">        TypeError: If `lr` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adagrad = ops.ApplyAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...     def construct(self, lr, grad):</span>
<span class="sd">        ...         out = self.apply_adagrad(self.var, self.accum, lr, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99638879e-01,  3.99296492e-01],</span>
<span class="sd">         [ 9.97817814e-02,  4.99281585e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.90000057e-01,  9.90000010e-01],</span>
<span class="sd">         [ 2.10000008e-01,  1.24000001e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdagrad.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAdagradV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdagradV2.html#mindspore.ops.ApplyAdagradV2">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdagradV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagradv2 scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Note:</span>
<span class="sd">        The difference is that `ApplyAdagradV2` has one more small constant value :math:`\epsilon` than `ApplyAdagrad`.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): A small value added for numerical stability.</span>
<span class="sd">        update_slots (bool): If ``True`` , `accum` will be updated. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float16 or float32 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape must be the same as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. The shape must be the same as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_adagrad_v2 = ops.ApplyAdagradV2(epsilon=1e-6)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...     def construct(self, lr, grad):</span>
<span class="sd">        ...         out = self.apply_adagrad_v2(self.var, self.accum, lr, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99638879e-01,  3.99296492e-01],</span>
<span class="sd">         [ 9.97817814e-02,  4.99281585e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.90000057e-01,  9.90000010e-01],</span>
<span class="sd">         [ 2.10000008e-01,  1.24000001e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdagradV2.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">SparseApplyAdagrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;1.9&quot;</span><span class="p">,</span> <span class="s2">&quot;SparseApplyAdagrad&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyAdagrad.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_is_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<div class="viewcode-block" id="SparseApplyAdagradV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseApplyAdagradV2.html#mindspore.ops.SparseApplyAdagradV2">[文档]</a><span class="k">class</span> <span class="nc">SparseApplyAdagradV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adagrad scheme, one more epsilon attribute than SparseApplyAdagrad.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            var -= lr * grad * \frac{1}{\sqrt{accum} + \epsilon}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): Learning rate.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability.</span>
<span class="sd">        use_locking (bool): If ``True`` , the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        update_slots (bool): If ``True`` , the computation logic will be different to `False`. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. The shape must be the same as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradients has the same shape as `var` and</span>
<span class="sd">          :math:`grad.shape[1:] = var.shape[1:]` if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A vector of indices into the first dimension of `var` and `accum`.</span>
<span class="sd">          The type must be int32 and :math:`indices.shape[0] = grad.shape[0]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `lr` nor `epsilon` is a float.</span>
<span class="sd">        TypeError: If neither `update_slots` nor `use_locking` is a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_adagrad_v2 = ops.SparseApplyAdagradV2(lr=1e-8, epsilon=1e-6)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.1]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_adagrad_v2(self.var, self.accum, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[ 1.99999988e-01]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[ 5.89999974e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">update_slots</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyAdagradV2.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;update_slots&quot;</span><span class="p">,</span> <span class="n">update_slots</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_slots</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyProximalAdagrad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyProximalAdagrad.html#mindspore.ops.ApplyProximalAdagrad">[文档]</a><span class="k">class</span> <span class="nc">ApplyProximalAdagrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the proximal adagrad algorithm.</span>
<span class="sd">    The proximal adagrad algorithm was proposed in `Efficient Learning using Forward-Backward Splitting</span>
<span class="sd">    &lt;http://papers.nips.cc//paper/3793-efficient-learning-using-forward-backward-splitting.pdf&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}} \\</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If ``True`` , the var and accumulation tensors will be protected from being updated.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated, must have the same shape and dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be a scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be a scalar. The data type must be</span>
<span class="sd">          float16 or float32.</span>
<span class="sd">        - **grad** (Tensor) - Gradient with the same shape and dtype as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_blocking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `lr`, `l1` or `l2` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr`, `l1` or `l2` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_proximal_adagrad = ops.ApplyProximalAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.lr = 0.01</span>
<span class="sd">        ...         self.l1 = 0.0</span>
<span class="sd">        ...         self.l2 = 0.0</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1, self.l2, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.96388459e-01,  3.92964751e-01],</span>
<span class="sd">         [ 9.78178233e-02,  4.92815793e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 6.90000057e-01,  9.90000010e-01],</span>
<span class="sd">         [ 2.10000008e-01,  1.24000001e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyProximalAdagrad.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseApplyProximalAdagrad"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseApplyProximalAdagrad.html#mindspore.ops.SparseApplyProximalAdagrad">[文档]</a><span class="k">class</span> <span class="nc">SparseApplyProximalAdagrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the proximal adagrad algorithm.</span>
<span class="sd">    Compared with :class:`mindspore.ops.ApplyProximalAdagrad`,</span>
<span class="sd">    an additional index tensor is input.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum += grad * grad \\</span>
<span class="sd">            \text{prox_v} = var - lr * grad * \frac{1}{\sqrt{accum}} \\</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + lr * l2} * \max(\left| \text{prox_v} \right| - lr * l1, 0)</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If ``True`` , the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Variable tensor to be updated, has the same shape as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type. It must be positive.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type. It must be non-negative.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be a float number or</span>
<span class="sd">          a scalar tensor with float16 or float32 data type. It must be non-negative.</span>
<span class="sd">        - **grad** (Tensor) - A tensor must meet with</span>
<span class="sd">          :math:`grad.shape[1:] = var.shape[1:]` if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64 and :math:`indices.shape[0] = grad.shape[0]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr`, `l1`, `l2` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `lr` &lt;= 0 or `l1` &lt; 0 or `l2` &lt; 0.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_proximal_adagrad = ops.SparseApplyProximalAdagrad()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[4.1, 7.2], [1.1, 3.0]], np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0, 0], [0, 0]], np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.lr = 1.0</span>
<span class="sd">        ...         self.l1 = 1.0</span>
<span class="sd">        ...         self.l2 = 0.0</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_proximal_adagrad(self.var, self.accum, self.lr, self.l1,</span>
<span class="sd">        ...                                                  self.l2, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[1, 1], [1, 1]], np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1], np.int32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 2.09999990e+00,  5.19999981e+00],</span>
<span class="sd">         [ 0.00000000e+00,  1.00000000e+00]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.00000000e+00,  1.00000000e+00],</span>
<span class="sd">         [ 1.00000000e+00,  1.00000000e+00]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyProximalAdagrad.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAddSign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAddSign.html#mindspore.ops.ApplyAddSign">[文档]</a><span class="k">class</span> <span class="nc">ApplyAddSign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the AddSign algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t+1} = \beta * m_{t} + (1 - \beta) * g \\</span>
<span class="sd">            \text{update} = (\alpha + \text{sign_decay} * sign(g) * sign(m)) * g \\</span>
<span class="sd">            var = var - lr_{t+1} * \text{update}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t}`</span>
<span class="sd">    is the last moment of :math:`m_{t+1}`, :math:`lr` represents scaling factor `lr`, :math:`g` represents `grad`,</span>
<span class="sd">    :math:`\alpha` represents `alpha`, :math:`\beta` represents `beta`.</span>

<span class="sd">    The data type of all inputs must be float16 or float32 on Ascend and float16, float32 or float64 on CPU and GPU.</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` , `sign_decay` and `beta` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **m** (Parameter) - Variable tensor to be updated, has the same data type as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be a scalar.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Must be a scalar.</span>
<span class="sd">        - **sign_decay** (Union[Number, Tensor]) - Must be a scalar.</span>
<span class="sd">        - **beta** (Union[Number, Tensor]) - The exponential decay rate, must be a scalar.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same shape as `var`, for the gradient.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `lr` and `alpha` is not float16, float32 or float64.</span>
<span class="sd">        TypeError: If dtype of `sign_decay` and `beta` are both not float16, float32 or float64.</span>
<span class="sd">        TypeError: If `lr`, `alpha` or `sign_decay` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type of `var`, `accum` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_add_sign = ops.ApplyAddSign()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                             [0.2, 0.6]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.lr = 0.001</span>
<span class="sd">        ...         self.alpha = 1.0</span>
<span class="sd">        ...         self.sign_decay = 0.99</span>
<span class="sd">        ...         self.beta = 0.9</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_add_sign(self.var, self.m, self.lr, self.alpha, self.sign_decay, self.beta, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.99403024e-01,  3.98607016e-01],</span>
<span class="sd">         [ 9.98010039e-02,  4.98407990e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.70000052e-01,  5.19999981e-01],</span>
<span class="sd">         [ 1.89999998e-01,  6.20000064e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sign_decay&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAddSign.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyPowerSign"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyPowerSign.html#mindspore.ops.ApplyPowerSign">[文档]</a><span class="k">class</span> <span class="nc">ApplyPowerSign</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the AddSign algorithm.</span>

<span class="sd">    The AddSign algorithm was proposed in `Neural Optimizer Search with Reinforcement Learning</span>
<span class="sd">    &lt;https://arxiv.org/abs/1709.07417&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            m_{t+1} = \beta * m_{t} + (1 - \beta) * g \\</span>
<span class="sd">            \text{update} = \exp(\text{logbase} * \text{sign_decay} * sign(g) * sign(m)) * g \\</span>
<span class="sd">            var = var - lr_{t+1} * \text{update}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector, :math:`m_{t}`</span>
<span class="sd">    is the last moment of :math:`m_{t+1}`, :math:`lr` represents scaling factor `lr`, :math:`g` represents `grad`,</span>
<span class="sd">    :math:`\beta` represents `beta`.</span>

<span class="sd">    All of inputs comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If `lr`, `logbase`, `sign_decay` or `beta` is a number, the number is automatically converted to Tensor,</span>
<span class="sd">    and the data type is consistent with the Tensor data type involved in the operation.</span>
<span class="sd">    If inputs are tensors and have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend, input data type of float64 is currently not supported.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float64, float32 or float16 data type.</span>
<span class="sd">          If data type of `var` is float16, all inputs must have the same data type as `var`.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **m** (Parameter) - Variable tensor to be updated, has the same shape as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, should be a scalar or Tensor</span>
<span class="sd">          with float64, float32 or float16 data type.</span>
<span class="sd">        - **logbase** (Union[Number, Tensor]) - Should be a scalar or Tensor with float64, float32 or float16 data type.</span>
<span class="sd">        - **sign_decay** (Union[Number, Tensor]) - Should be a scalar or Tensor with float64, float32 or</span>
<span class="sd">          float16 data type.</span>
<span class="sd">        - **beta** (Union[Number, Tensor]) - The exponential decay rate, should be a scalar or Tensor</span>
<span class="sd">          with float64, float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same shape as `var`, for the gradient.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `lr`, `logbase`, `sign_decay`, `beta` or `grad` is not one of float16,</span>
<span class="sd">        float32 or float64.</span>
<span class="sd">        TypeError: If `lr`, `logbase`, `sign_decay` or `beta` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type of `lr`, `logbase`, `sign_decay` and `grad` conversion of Parameter</span>
<span class="sd">                      is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_power_sign = ops.ApplyPowerSign()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                             [0.2, 0.6]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.lr = 0.001</span>
<span class="sd">        ...         self.logbase = np.e</span>
<span class="sd">        ...         self.sign_decay = 0.99</span>
<span class="sd">        ...         self.beta = 0.9</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_power_sign(self.var, self.m, self.lr, self.logbase,</span>
<span class="sd">        ...                                        self.sign_decay, self.beta, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.95575690e-01,  3.89676481e-01],</span>
<span class="sd">         [ 9.85252112e-02,  4.88201708e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.70000052e-01,  5.19999981e-01],</span>
<span class="sd">         [ 1.89999998e-01,  6.20000064e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;logbase&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;sign_decay&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyPowerSign.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyGradientDescent"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyGradientDescent.html#mindspore.ops.ApplyGradientDescent">[文档]</a><span class="k">class</span> <span class="nc">ApplyGradientDescent</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates `var` by subtracting `alpha` * `delta` from it.</span>

<span class="sd">    .. math::</span>
<span class="sd">        var = var - \alpha * \delta</span>

<span class="sd">    where :math:`\alpha` represents `alpha`, :math:`\delta` represents `delta`.</span>

<span class="sd">    Inputs of `var` and `delta` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Scaling factor, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **delta** (Tensor) - A tensor for the change, has the same shape as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the updated `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var` or `alpha` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `delta` is not a Tensor.</span>
<span class="sd">        TypeError: If `alpha` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If the data type of `var` and `delta` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_gradient_descent = ops.ApplyGradientDescent()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.alpha = 0.001</span>
<span class="sd">        ...     def construct(self, delta):</span>
<span class="sd">        ...         out = self.apply_gradient_descent(self.var, self.alpha, delta)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(np.array([[0.1, 0.1], [0.1, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(delta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.9999 0.9999]</span>
<span class="sd">         [0.9999 0.9999]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;delta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyGradientDescent.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyProximalGradientDescent"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyProximalGradientDescent.html#mindspore.ops.ApplyProximalGradientDescent">[文档]</a><span class="k">class</span> <span class="nc">ApplyProximalGradientDescent</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FOBOS(Forward Backward Splitting) algorithm.</span>
<span class="sd">    Refer to the paper `Efficient Learning using Forward-Backward Splitting</span>
<span class="sd">    &lt;http://papers.nips.cc//paper/3793-efficient-learning-using-forward-backward-splitting.pdf&gt;`_ for more details.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            \text{prox_v} = var - \alpha * \delta \\</span>
<span class="sd">            var = \frac{sign(\text{prox_v})}{1 + \alpha * l2} * \max(\left| \text{prox_v} \right| - \alpha * l1, 0)</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`\alpha` represents `alpha`, :math:`\delta` represents `delta`.</span>

<span class="sd">    Inputs of `var` and `delta` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. With float32 or float16 data type.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Scaling factor, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be a scalar.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **delta** (Tensor) - A tensor for the change.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the updated `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If dtype of `var`, `alpha`, `l1` or `l2` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `alpha`, `l1` or `l2` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `delta` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type of `var`, and `delta` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.apply_proximal_gradient_descent = ops.ApplyProximalGradientDescent()</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.ones([2, 2]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.alpha = 0.001</span>
<span class="sd">        ...         self.l1 = 0.1</span>
<span class="sd">        ...         self.l2 = 0.1</span>
<span class="sd">        ...     def construct(self, delta):</span>
<span class="sd">        ...         out = self.apply_proximal_gradient_descent(self.var, self.alpha, self.l1, self.l2, delta)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; delta = Tensor(np.array([[0.1, 0.1], [0.1, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(delta)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.99969995 0.99969995]</span>
<span class="sd">         [0.99969995 0.99969995]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;delta&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyGradientDescent.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="LARSUpdate"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LARSUpdate.html#mindspore.ops.LARSUpdate">[文档]</a><span class="k">class</span> <span class="nc">LARSUpdate</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Conducts LARS (layer-wise adaptive rate scaling) update on the sum of squares of gradient.</span>

<span class="sd">    For more details, please refer to :class:`mindspore.nn.LARS`.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float, optional): Term added to the denominator to improve numerical stability.</span>
<span class="sd">            Default: ``1e-05`` .</span>
<span class="sd">        hyperpara (float, optional): Trust coefficient for calculating the local learning rate.</span>
<span class="sd">            Default: ``0.001`` .</span>
<span class="sd">        use_clip (bool, optional): Whether to use clip operation for calculating the local learning rate.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **weight** (Tensor) - A tensor, representing the weight.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **gradient** (Tensor) - The gradient of weight, which has the same shape and dtype with weight.</span>
<span class="sd">        - **norm_weight** (Tensor) - A scalar tensor, representing the sum of squares of weight.</span>
<span class="sd">        - **norm_gradient** (Tensor) - A scalar tensor, representing the sum of squares of gradient.</span>
<span class="sd">        - **weight_decay** (Union[Number, Tensor]) - Weight decay. It must be a scalar tensor or number.</span>
<span class="sd">        - **learning_rate** (Union[Number, Tensor]) - Learning rate. It must be a scalar tensor or number.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, represents the new gradient.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If neither `epsilon` nor `hyperpara` is a float.</span>
<span class="sd">        TypeError: If `use_clip` is not a bool.</span>
<span class="sd">        TypeError: If `weight`, `gradient`, `norm_weight` or `norm_gradient` is not a Tensor.</span>
<span class="sd">        TypeError: If `weight_decay` or `learning_rate` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If shape of `gradient` is not the same as `weight`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.lars = ops.LARSUpdate()</span>
<span class="sd">        ...         self.reduce = ops.ReduceSum()</span>
<span class="sd">        ...         self.square = ops.Square()</span>
<span class="sd">        ...     def construct(self, weight, gradient):</span>
<span class="sd">        ...         w_square_sum = self.reduce(self.square(weight))</span>
<span class="sd">        ...         grad_square_sum = self.reduce(self.square(gradient))</span>
<span class="sd">        ...         grad_t = self.lars(weight, gradient, w_square_sum, grad_square_sum, 0.0, 1.0)</span>
<span class="sd">        ...         return grad_t</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.array([[0.5, 0.8, 0.2], [0.6, 0.4, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; gradient = Tensor(np.array([[0.4, 0.4, 0.5], [0.2, 0.4, 0.3]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; net = Net()</span>
<span class="sd">        &gt;&gt;&gt; output = net(Tensor(weight), Tensor(gradient))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.0005265  0.0005265 0.00065813]</span>
<span class="sd">         [0.00026325 0.0005265 0.00039488]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">hyperpara</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">use_clip</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LARSUpdate.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;hyperpara&quot;</span><span class="p">,</span> <span class="n">hyperpara</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_clip&quot;</span><span class="p">,</span> <span class="n">use_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyFtrl"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyFtrl.html#mindspore.ops.ApplyFtrl">[文档]</a><span class="k">class</span> <span class="nc">ApplyFtrl</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL scheme.</span>

<span class="sd">    For more details, please refer to :class:`mindspore.nn.FTRL`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - Currently, only positive numbers are supported on the Ascend platform,</span>
<span class="sd">          and the calculation results for other scenarios are not defined.</span>
<span class="sd">        - Inputs of `var`, `accum`, `linear` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">          to make the data types consistent.</span>
<span class="sd">          If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">          the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): Use locks for updating operation if ``True`` . Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - The linear coefficient to be updated, must be same shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - Gradient. The data type must be float16 or float32.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value, must be positive. Default: ``0.001`` .</span>
<span class="sd">          It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">          Default: ``0.0`` . It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">          Default: ``0.0`` . It must be a float number or a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **lr_power** (Union[Number, Tensor]) - Learning rate power controls how the learning rate decreases</span>
<span class="sd">          during training, must be less than or equal to zero. Use fixed learning rate if lr_power is zero.</span>
<span class="sd">          Default: ``-0.5`` . It must be a float number or a scalar tensor with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Represents the updated `var`. As the input parameters has been updated in-place, this</span>
<span class="sd">          value is always zero when the platform is GPU.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `grad`, `lr`, `l1`, `l2` or `lr_power` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `lr_power` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If the parameter types of `var`, `accum` and `linear` are inconsistent.</span>
<span class="sd">        TypeError: If the parameter types of `grad`, `lr`, `l1`, `l2`, `lr_power` are inconsistent with `var`</span>
<span class="sd">            and the precision is greater than `var`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class ApplyFtrlNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(ApplyFtrlNet, self).__init__()</span>
<span class="sd">        ...         self.apply_ftrl = ops.ApplyFtrl()</span>
<span class="sd">        ...         self.lr = 0.001</span>
<span class="sd">        ...         self.l1 = 0.0</span>
<span class="sd">        ...         self.l2 = 0.0</span>
<span class="sd">        ...         self.lr_power = -0.5</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4],</span>
<span class="sd">        ...                                               [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.6, 0.5],</span>
<span class="sd">        ...                                                 [0.2, 0.6]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.array([[0.9, 0.1],</span>
<span class="sd">        ...                                                  [0.7, 0.8]]).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_ftrl(self.var, self.accum, self.linear, grad, self.lr, self.l1, self.l2,</span>
<span class="sd">        ...                               self.lr_power)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[ 0.0390525  0.11492836]</span>
<span class="sd">         [ 0.00066425 0.15075898]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyFtrl.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;lr_power&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseApplyFtrl"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseApplyFtrl.html#mindspore.ops.SparseApplyFtrl">[文档]</a><span class="k">class</span> <span class="nc">SparseApplyFtrl</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the FTRL-proximal scheme</span>
<span class="sd">    For more details, please refer to :class:`mindspore.nn.FTRL`.</span>

<span class="sd">    All of inputs except `indices` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        lr (float): The learning rate value, must be positive.</span>
<span class="sd">        l1 (float): l1 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        l2 (float): l2 regularization strength, must be greater than or equal to zero.</span>
<span class="sd">        lr_power (float): Learning rate power controls how the learning rate decreases during training,</span>
<span class="sd">            must be less than or equal to zero. Use fixed learning rate if `lr_power` is zero.</span>
<span class="sd">        use_locking (bool, optional): Use locks for updating operation if ``True`` . Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - The variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - The accumulation to be updated, must be same shape as `var`.</span>
<span class="sd">        - **linear** (Parameter) - The linear coefficient to be updated, must be the same shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor must meet with :math:`grad.shape[1:] = var.shape[1:]`</span>
<span class="sd">          if var.shape &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined.</span>
<span class="sd">          The type must be int32 or int64 and :math:`indices.shape[0] = grad.shape[0]`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - Tensor, has the same shape and data type as `accum`.</span>
<span class="sd">        - **linear** (Tensor) - Tensor, has the same shape and data type as `linear`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `lr_power` is not a float.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `linear` or `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        RuntimeError: If the data type of all of inputs except `indices` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyFtrlNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self):</span>
<span class="sd">        ...         super(SparseApplyFtrlNet, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_ftrl = ops.SparseApplyFtrl(lr=0.01, l1=0.0, l2=0.0, lr_power=-0.5)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.1]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.linear = Parameter(Tensor(np.array([[0.6]]).astype(np.float32)), name=&quot;linear&quot;)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_ftrl(self.var, self.accum, self.linear, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyFtrlNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.7]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.ones([1]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[2.00000003e-01]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[1.00000001e-01]]), Tensor(shape=[1, 1], dtype=Float32, value=</span>
<span class="sd">        [[6.00000024e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyFtrl.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="SparseApplyFtrlV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SparseApplyFtrlV2.html#mindspore.ops.SparseApplyFtrlV2">[文档]</a><span class="k">class</span> <span class="nc">SparseApplyFtrlV2</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The SparseApplyFtrlV2 interface is deprecated, please use the :class:`mindspore.ops.SparseApplyFtrl` instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@deprecated</span><span class="p">(</span><span class="s2">&quot;2.1&quot;</span><span class="p">,</span> <span class="s2">&quot;ops.SparseApplyFtrl&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="n">l2_shrinkage</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyFtrlV2.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">l2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="n">l2</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_power</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lr_power&quot;</span><span class="p">,</span> <span class="n">lr_power</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2_shrinkage</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;l2_shrinkage&quot;</span><span class="p">,</span> <span class="n">l2_shrinkage</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_locking</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;accum shape&#39;</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var shape&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">,</span> <span class="s1">&#39;linear shape&#39;</span><span class="p">,</span> <span class="n">linear_shape</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">var_shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;var_shape[1:]&#39;</span><span class="p">,</span> <span class="n">var_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s1">&#39;grad_shape[1:]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;indices rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s1">&#39;grad_shape[0]&#39;</span><span class="p">,</span> <span class="n">grad_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s1">&#39;indices_shape[0]&#39;</span><span class="p">,</span> <span class="n">indices_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_shape</span><span class="p">,</span> <span class="n">accum_shape</span><span class="p">,</span> <span class="n">linear_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="n">grad_dtype</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">):</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;var_dtype&quot;</span><span class="p">:</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="s2">&quot;accum_dtype&quot;</span><span class="p">:</span> <span class="n">accum_dtype</span><span class="p">,</span>
                <span class="s2">&quot;linear_dtype&quot;</span><span class="p">:</span> <span class="n">linear_dtype</span><span class="p">,</span> <span class="s2">&quot;grad_dtype&quot;</span><span class="p">:</span> <span class="n">grad_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">(</span><span class="s2">&quot;indicese&quot;</span><span class="p">,</span> <span class="n">indices_dtype</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">var_dtype</span><span class="p">,</span> <span class="n">accum_dtype</span><span class="p">,</span> <span class="n">linear_dtype</span></div>


<div class="viewcode-block" id="Dropout2D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Dropout2D.html#mindspore.ops.Dropout2D">[文档]</a><span class="k">class</span> <span class="nc">Dropout2D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor with probability :math:`1-keep\_prob`</span>
<span class="sd">    from a Bernoulli distribution(For a 4-dimensional tensor with a shape of :math:`(N, C, H, W)`,</span>
<span class="sd">    the channel feature map refers</span>
<span class="sd">    to a 2-dimensional feature map with the shape of :math:`(H, W)`).</span>

<span class="sd">    Dropout2D can improve the independence between channel feature maps.</span>

<span class="sd">    Note:</span>
<span class="sd">        The keep probability :math:`keep\_prob` is equal to :math:`1 - p` in :func:`mindspore.ops.dropout2d`.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_prob (float, optional): The keep probability of a channel, between 0 and 1, e.g. `keep_prob` = 0.8,</span>
<span class="sd">            means dropping out 20% of channels. Default: ``0.5`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 4-D tensor with shape :math:`(N, C, H, W)`, where N is the batch size, C is the number</span>
<span class="sd">          of channels, H is the feature height, and W is the feature width.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - With the same shape and data type as `x`.</span>
<span class="sd">        - **mask** (Tensor) - With the same shape as `x` and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If the data type of `keep_prob` is not float.</span>
<span class="sd">        ValueError: If `keep_prob` is out of the range `[0.0, 1.0]`.</span>
<span class="sd">        ValueError: If `x` shape is not `4D`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; dropout = ops.Dropout2D(keep_prob=0.5)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([2, 1, 2, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1, 2, 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Dropout2D.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;Dropout2D&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Dropout3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Dropout3D.html#mindspore.ops.Dropout3D">[文档]</a><span class="k">class</span> <span class="nc">Dropout3D</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some channels of the input tensor</span>
<span class="sd">    with probability :math:`1-keep\_prob` from a Bernoulli distribution(For a 5-dimensional</span>
<span class="sd">    tensor with a shape of NCDHW,</span>
<span class="sd">    the channel feature map refers to a 3-dimensional feature map with a shape of DHW).</span>

<span class="sd">    Note:</span>
<span class="sd">        The keep probability :math:`keep\_prob` is equal to :math:`1 - p` in :func:`mindspore.ops.dropout3d`.</span>

<span class="sd">    Dropout3D can improve the independence between channel feature maps.</span>

<span class="sd">    Args:</span>
<span class="sd">        keep_prob (float): The keep probability of a channel, between 0 and 1, e.g. `keep_prob` = 0.8,</span>
<span class="sd">            means dropping out 20% of channels. Default: ``0.5`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 5-D tensor with shape :math:`(N, C, D, H, W)`, where N is the batch size, C is the number</span>
<span class="sd">          of channels, D is the feature depth, H is the feature height, and W is the feature width.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **output** (Tensor) - With the same shape and data type as `x`.</span>
<span class="sd">        - **mask** (Tensor) - With the same shape as `x` and the data type is bool.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the data type of `keep_prob` is not float.</span>
<span class="sd">        ValueError: If `keep_prob` is out of the range [0.0, 1.0];</span>
<span class="sd">                    or if the dim of input is not 5-D.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; dropout = ops.Dropout3D(keep_prob=0.5)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([2, 1, 2, 1, 2]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output, mask = dropout(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (2, 1, 2, 1, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Dropout3D.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;Dropout3D&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="CTCLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.CTCLoss.html#mindspore.ops.CTCLoss">[文档]</a><span class="k">class</span> <span class="nc">CTCLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the CTC (Connectionist Temporal Classification) loss and the gradient.</span>

<span class="sd">    The bottom layer of this interface calls the implementation of the third-party baidu-research::warp-ctc.</span>
<span class="sd">    The CTC algorithm is proposed in `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_.</span>

<span class="sd">    CTCLoss calculates loss between a continuous time series and a target sequence.</span>
<span class="sd">    CTCLoss sums over the probability of input to target, producing a loss value which is differentiable with</span>
<span class="sd">    respect to each input node. The alignment of input to target is assumed to be “many-to-one”,</span>
<span class="sd">    such that the length of target series must be less than or equal to the length of input.</span>

<span class="sd">    Args:</span>
<span class="sd">        preprocess_collapse_repeated (bool): If ``True`` , repeated labels will be collapsed prior to the CTC</span>
<span class="sd">                                             calculation. Default: ``False`` .</span>
<span class="sd">        ctc_merge_repeated (bool): If ``False`` , during CTC calculation, repeated non-blank labels will not be merged</span>
<span class="sd">                                   and these labels will be interpreted as individual ones. This is a simplified</span>
<span class="sd">                                   version of CTC. Default: ``True`` .</span>
<span class="sd">        ignore_longer_outputs_than_inputs (bool): If ``True`` , sequences with longer outputs than inputs will be</span>
<span class="sd">                                                  ignored. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input Tensor must be a `3-D` tensor whose shape is</span>
<span class="sd">          :math:`(max\_time, batch\_size, num\_classes)`. `num_classes` must be `num_labels + 1` classes, `num_labels`</span>
<span class="sd">          indicates the number of actual labels. Blank labels are reserved. Default blank label is `num_classes - 1`.</span>
<span class="sd">          Data type must be float16, float32 or float64.</span>
<span class="sd">        - **labels_indices** (Tensor) - The indices of labels. `labels_indices[i, :] = [b, t]` means</span>
<span class="sd">          `labels_values[i]` stores the id for `(batch b, time t)`. The type must be int64 and rank must be 2.</span>
<span class="sd">        - **labels_values** (Tensor) - A `1-D` input tensor. The values are associated with the given batch and time.</span>
<span class="sd">          The type must be int32. `labels_values[i]` must be in the range of `[0, num_classes)`.</span>
<span class="sd">        - **sequence_length** (Tensor) - A tensor containing sequence lengths with the shape of :math:`(batch\_size, )`.</span>
<span class="sd">          The type must be int32. Each value in the tensor must not be greater than `max_time`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **loss** (Tensor) - A tensor containing log-probabilities, the shape is :math:`(batch\_size, )`.</span>
<span class="sd">          The tensor has the same data type as `x`.</span>
<span class="sd">        - **gradient** (Tensor) - The gradient of `loss`, has the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `preprocess_collapse_repeated`, `ctc_merge_repeated` or `ignore_longer_outputs_than_inputs`</span>
<span class="sd">                   is not a bool.</span>
<span class="sd">        TypeError: If `x`, `labels_indices`, `labels_values` or `sequence_length` is not a Tensor.</span>
<span class="sd">        ValueError: If rank of `labels_indices` is not equal to 2.</span>
<span class="sd">        TypeError: If dtype of `x` is not one of the following: float16, float32 nor float64.</span>
<span class="sd">        TypeError: If dtype of `labels_indices` is not int64.</span>
<span class="sd">        TypeError: If dtype of `labels_values` or `sequence_length` is not int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[0.3, 0.6, 0.6],</span>
<span class="sd">        ...                       [0.4, 0.3, 0.9]],</span>
<span class="sd">        ...</span>
<span class="sd">        ...                      [[0.9, 0.4, 0.2],</span>
<span class="sd">        ...                       [0.9, 0.9, 0.1]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; labels_indices = Tensor(np.array([[0, 0], [1, 0]]), mindspore.int64)</span>
<span class="sd">        &gt;&gt;&gt; labels_values = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; sequence_length = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = ops.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss, gradient = ctc_loss(x, labels_indices, labels_values, sequence_length)</span>
<span class="sd">        &gt;&gt;&gt; print(loss)</span>
<span class="sd">        [ 0.79628  0.5995158 ]</span>
<span class="sd">        &gt;&gt;&gt; print(gradient)</span>
<span class="sd">        [[[ 0.27029088  0.36485454  -0.6351454  ]</span>
<span class="sd">          [ 0.28140804  0.25462854  -0.5360366 ]]</span>
<span class="sd">         [[ 0.47548494  0.2883962    0.04510255 ]</span>
<span class="sd">          [ 0.4082751   0.4082751    0.02843709 ]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preprocess_collapse_repeated</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ctc_merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">ignore_longer_outputs_than_inputs</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CTCLoss.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;inputs&quot;</span><span class="p">,</span> <span class="s2">&quot;labels_indices&quot;</span><span class="p">,</span> <span class="s2">&quot;labels_values&quot;</span><span class="p">,</span> <span class="s2">&quot;sequence_length&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="s2">&quot;gradient&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;preprocess_collapse_repeated&quot;</span><span class="p">,</span> <span class="n">preprocess_collapse_repeated</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_collapse_repeated_</span> <span class="o">=</span> <span class="n">preprocess_collapse_repeated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ctc_merge_repeated_</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ctc_merge_repeated&quot;</span><span class="p">,</span> <span class="n">ctc_merge_repeated</span><span class="p">,</span>
                                                              <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ignore_longer_outputs_than_inputs&quot;</span><span class="p">,</span>
                                   <span class="n">ignore_longer_outputs_than_inputs</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_longer_outputs_than_inputs_</span> <span class="o">=</span> <span class="n">ignore_longer_outputs_than_inputs</span></div>


<div class="viewcode-block" id="CTCGreedyDecoder"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.CTCGreedyDecoder.html#mindspore.ops.CTCGreedyDecoder">[文档]</a><span class="k">class</span> <span class="nc">CTCGreedyDecoder</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs greedy decoding on the logits given in inputs.</span>

<span class="sd">    Refer to :func:`mindspore.ops.ctc_greedy_decoder` for more details.</span>

<span class="sd">    Note:</span>
<span class="sd">        On Ascend, &#39;merge_repeated&#39; can not be set to false.</span>

<span class="sd">    Args:</span>
<span class="sd">        merge_repeated (bool, optional): If ``True`` , merge repeated classes in output. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **inputs** (Tensor) - The input Tensor must be a 3-D tensor whose shape is</span>
<span class="sd">          :math:`(max\_time, batch\_size, num\_classes)`. `num_classes` must be `num_labels + 1` classes,</span>
<span class="sd">          `num_labels` indicates the number of actual labels. Blank labels are reserved.</span>
<span class="sd">          Default blank label is `num_classes - 1`. Data type must be float32 or float64.</span>
<span class="sd">        - **sequence_length** (Tensor) - A tensor containing sequence lengths with the shape of :math:`(batch\_size, )`.</span>
<span class="sd">          The type must be int32. Each value in the tensor must be equal to or less than `max_time`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **decoded_indices** (Tensor) - A tensor with shape of :math:`(total\_decoded\_outputs, 2)`.</span>
<span class="sd">          Data type is int64.</span>
<span class="sd">        - **decoded_values** (Tensor) - A tensor with shape of :math:`(total\_decoded\_outputs, )`,</span>
<span class="sd">          it stores the decoded classes. Data type is int64.</span>
<span class="sd">        - **decoded_shape** (Tensor) - A tensor with shape of :math:`(batch\_size, max\_decoded\_length)`.</span>
<span class="sd">          Data type is int64.</span>
<span class="sd">        - **log_probability** (Tensor) - A tensor with shape of :math:`(batch\_size, 1)`,</span>
<span class="sd">          containing sequence log-probability, has the same type as `inputs`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; inputs = Tensor(np.array([[[0.6, 0.4, 0.2], [0.8, 0.6, 0.3]],</span>
<span class="sd">        ...                           [[0.0, 0.6, 0.0], [0.5, 0.4, 0.5]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; sequence_length = Tensor(np.array([2, 2]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; decoded_indices, decoded_values, decoded_shape, log_probability = ops.CTCGreedyDecoder()(inputs,</span>
<span class="sd">        ...                                                                                          sequence_length)</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_indices)</span>
<span class="sd">        [[0 0]</span>
<span class="sd">         [0 1]</span>
<span class="sd">         [1 0]]</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_values)</span>
<span class="sd">        [0 1 0]</span>
<span class="sd">        &gt;&gt;&gt; print(decoded_shape)</span>
<span class="sd">        [2 2]</span>
<span class="sd">        &gt;&gt;&gt; print(log_probability)</span>
<span class="sd">        [[-1.2]</span>
<span class="sd">         [-1.3]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CTCGreedyDecoder.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">merge_repeated</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;merge_repeated&quot;</span><span class="p">,</span> <span class="n">merge_repeated</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">BasicLSTMCell</span><span class="p">(</span><span class="n">PrimitiveWithInfer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    It&#39;s similar to operator :class:`mindspore.ops.DynamicRNN`. BasicLSTMCell will be deprecated in the future.</span>
<span class="sd">    Please use DynamicRNN instead.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        Deprecated</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize BasicLSTMCell.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_is_tuple</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;state_is_tuple&quot;</span><span class="p">,</span> <span class="n">state_is_tuple</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">infer_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;x rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">h_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;h rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">c_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;c rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">w_shape</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;w rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">b_shape</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s2">&quot;b rank&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;x_shape[0]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape[0]&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;h_shape[0]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;c_shape[1]&quot;</span><span class="p">,</span> <span class="n">c_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;h_shape[1]&quot;</span><span class="p">,</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[1]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;4*h_shape[1]&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;w_shape[0]&quot;</span><span class="p">,</span> <span class="n">w_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;x_shape[1]+h_shape[1]&quot;</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="s2">&quot;b_shape[0]&quot;</span><span class="p">,</span> <span class="n">b_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;4*h_shape[1]&quot;</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">h_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">ct_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ht_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">it_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">jt_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ft_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">ot_shape</span> <span class="o">=</span> <span class="n">c_shape</span>
        <span class="n">tanhct_shape</span> <span class="o">=</span> <span class="n">c_shape</span>

        <span class="k">return</span> <span class="n">ct_shape</span><span class="p">,</span> <span class="n">ht_shape</span><span class="p">,</span> <span class="n">it_shape</span><span class="p">,</span> <span class="n">jt_shape</span><span class="p">,</span> <span class="n">ft_shape</span><span class="p">,</span> <span class="n">ot_shape</span><span class="p">,</span> <span class="n">tanhct_shape</span>

    <span class="k">def</span> <span class="nf">infer_dtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">,</span> <span class="n">b_dtype</span><span class="p">):</span>
        <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">validator</span><span class="o">.</span><span class="n">check_tensor_dtype_valid</span><span class="p">,</span>
                          <span class="n">valid_dtypes</span><span class="o">=</span><span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">prim_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">),</span>
                  <span class="p">(</span><span class="s2">&quot;x_dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;h_dtype&quot;</span><span class="p">,</span> <span class="s2">&quot;w_dtype&quot;</span><span class="p">),</span>
                  <span class="p">(</span><span class="n">x_dtype</span><span class="p">,</span> <span class="n">h_dtype</span><span class="p">,</span> <span class="n">w_dtype</span><span class="p">)))</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;c_dtype&quot;</span><span class="p">:</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="s2">&quot;b_dtype&quot;</span><span class="p">:</span> <span class="n">b_dtype</span><span class="p">}</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_tensors_dtypes_same_and_valid</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span><span class="p">,</span> <span class="n">c_dtype</span>


<div class="viewcode-block" id="DynamicRNN"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.DynamicRNN.html#mindspore.ops.DynamicRNN">[文档]</a><span class="k">class</span> <span class="nc">DynamicRNN</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a recurrent neural network to the input.</span>
<span class="sd">    Only long short-term memory (LSTM) is supported currently.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            i_{t+1} = \sigma(W_{ix} x_{t+1} + b_{ix} + W_{ih} h_{(t)} + b_{ih}) \\</span>
<span class="sd">            f_{t+1} = \sigma(W_{fx} x_{t+1} + b_{fx} + W_{fh} h_{(t)} + b_{fh}) \\</span>
<span class="sd">            \tilde{c}_{t+1} = \tanh(W_{cx} x_{t+1} + b_{cx} + W_{ch} h_{(t)} + b_{ch}) \\</span>
<span class="sd">            o_{t+1} = \sigma(W_{ox} x_{t+1} + b_{ox} + W_{oh} h_{(t)} + b_{oh}) \\</span>
<span class="sd">            c_{t+1} = f_{t+1} * c_{(t)} + i_t * \tilde{c}_{t+1} \\</span>
<span class="sd">            h_{t+1} = o_{t+1} * \tanh(c_{t+1}) \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`h_{t+1}` is the hidden state at time `t+1`. :math:`x_{t+1}` is the input</span>
<span class="sd">    at time `t+1`. :math:`h_{t}` is the hidden state of the layer</span>
<span class="sd">    at time `t` or the initial hidden state at time `0`.</span>
<span class="sd">    :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product. :math:`W, b`</span>
<span class="sd">    are learnable weights between the output and the input in the formula. For instance,</span>
<span class="sd">    :math:`W_{ix}, b_{ix}` are the weight and bias used to transform from input :math:`x` to :math:`i`.</span>

<span class="sd">    Args:</span>
<span class="sd">        cell_type (str, optional): A string identifying the cell type in the operator. Default: ``&#39;LSTM&#39;`` .</span>
<span class="sd">            Only &#39;LSTM&#39; is currently supported.</span>
<span class="sd">        direction (str, optional): A string identifying the direction in the operator. Default: ``&#39;UNIDIRECTIONAL&#39;`` .</span>
<span class="sd">            Only &#39;UNIDIRECTIONAL&#39; is currently supported.</span>
<span class="sd">        cell_depth (int, optional): An integer identifying the cell depth in the operator. Default: ``1`` .</span>
<span class="sd">        use_peephole (bool, optional): A bool identifying if use peephole in the operator. Default: ``False`` .</span>
<span class="sd">        keep_prob (float, optional): A float identifying the keep prob in the operator. Default: ``1.0`` .</span>
<span class="sd">        cell_clip (float, optional): A float identifying the cell clip in the operator. Default: ``-1.0`` .</span>
<span class="sd">        num_proj (int, optional): An integer identifying the number projection in the operator. Default: ``0`` .</span>
<span class="sd">        time_major (bool, optional): A bool specify the data format of `x`. If it is set to ``True`` , the format is</span>
<span class="sd">            :math:`(num\_step, batch\_size, input\_size)`, if it is set to False, the format is</span>
<span class="sd">            :math:`(batch\_size, num\_step, input\_size)`.</span>
<span class="sd">            Default: ``True`` . Only supports ``True`` at present.</span>
<span class="sd">        activation (str, optional): A string identifying the type of activation function in the operator.</span>
<span class="sd">            Default: ``&#39;tanh&#39;`` . Only &#39;tanh&#39; is currently supported.</span>
<span class="sd">        forget_bias (float, optional): A float identifying the forget bias in the operator. Default: ``0.0`` .</span>
<span class="sd">        is_training (bool, optional): A bool identifying is training in the operator. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Current words. Tensor of shape :math:`(num\_step, batch\_size, input\_size)`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **w** (Tensor) - Weight. Tensor of shape :math:`(input\_size + hidden\_size, 4 * hidden\_size)`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **b** (Tensor) - Bias. Tensor of shape :math:`(4 * hidden\_size)`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **seq_length** (Tensor) - The length of each batch. Tensor of shape :math:`(batch\_size, )`.</span>
<span class="sd">          Only `None` is currently supported.</span>
<span class="sd">        - **init_h** (Tensor) - Hidden state of initial time. Tensor of shape :math:`(1, batch\_size, hidden\_size)`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **init_c** (Tensor) - Cell state of initial time. Tensor of shape :math:`(1, batch\_size, hidden\_size)`.</span>
<span class="sd">          The data type must be float16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **output_h** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          With data type of float16.</span>
<span class="sd">        - **output_c** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **i** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **j** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **f** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **o** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>
<span class="sd">        - **tanhct** (Tensor) - A Tensor of shape :math:`(num\_step, batch\_size, hidden\_size)`.</span>
<span class="sd">          Has the same type with input `b`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `cell_type`, `direction` or `activation` is not a str.</span>
<span class="sd">        TypeError: If `cell_depth` or `num_proj` is not an int.</span>
<span class="sd">        TypeError: If `keep_prob`, `cell_clip` or `forget_bias` is not a float.</span>
<span class="sd">        TypeError: If `use_peehpole`, `time_major` or `is_training` is not a bool.</span>
<span class="sd">        TypeError: If `x`, `w`, `b`, `seq_length`, `init_h` or `init_c` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x`, `w`, `init_h` or `init_c` is not float16.</span>
<span class="sd">        TypeError: If dtype of `b` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(2, 16, 64).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; w = Tensor(np.random.rand(96, 128).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; b = Tensor(np.random.rand(128).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_h = Tensor(np.random.rand(1, 16, 32).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_c = Tensor(np.random.rand(1, 16, 32).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; dynamic_rnn = ops.DynamicRNN()</span>
<span class="sd">        &gt;&gt;&gt; output = dynamic_rnn(x, w, b, None, init_h, init_c)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].shape)</span>
<span class="sd">        (2, 16, 32)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">cell_type</span><span class="o">=</span><span class="s1">&#39;LSTM&#39;</span><span class="p">,</span>
                 <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">,</span>
                 <span class="n">cell_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">use_peephole</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">time_major</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DynamicRNN.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_depth</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_depth&quot;</span><span class="p">,</span> <span class="n">cell_depth</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number_range</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_clip</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_clip&quot;</span><span class="p">,</span> <span class="n">cell_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">num_proj</span><span class="p">,</span> <span class="s2">&quot;num_proj&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forget_bias</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;forget_bias&quot;</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_peephole</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_peephole&quot;</span><span class="p">,</span> <span class="n">use_peephole</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_major</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;time_major&quot;</span><span class="p">,</span> <span class="n">time_major</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_type&quot;</span><span class="p">,</span> <span class="n">cell_type</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_type</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">cell_type</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;LSTM&#39;</span><span class="p">],</span> <span class="s2">&quot;cell_type&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;direction&quot;</span><span class="p">,</span> <span class="n">direction</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">direction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">direction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">],</span> <span class="s2">&quot;direction&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="DynamicGRUV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.DynamicGRUV2.html#mindspore.ops.DynamicGRUV2">[文档]</a><span class="k">class</span> <span class="nc">DynamicGRUV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a single-layer gated recurrent unit (GRU) to an input sequence.</span>

<span class="sd">    .. math::</span>

<span class="sd">        \begin{array}{ll}</span>
<span class="sd">            r_{t+1} = \sigma(W_{ir} x_{t+1} + b_{ir} + W_{hr} h_{(t)} + b_{hr}) \\</span>
<span class="sd">            z_{t+1} = \sigma(W_{iz} x_{t+1} + b_{iz} + W_{hz} h_{(t)} + b_{hz}) \\</span>
<span class="sd">            n_{t+1} = \tanh(W_{in} x_{t+1} + b_{in} + r_{t+1} * (W_{hn} h_{(t)}+ b_{hn})) \\</span>
<span class="sd">            h_{t+1} = (1 - z_{t+1}) * n_{t+1} + z_{t+1} * h_{(t)}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    where :math:`h_{t+1}` is the hidden state at time `t+1`, :math:`x_{t+1}` is the input</span>
<span class="sd">    at time `t+1`, :math:`h_{t}` is the hidden state of the layer</span>
<span class="sd">    at time `t` or the initial hidden state at time `0`. :math:`r_{t+1}`,</span>
<span class="sd">    :math:`z_{t+1}`, :math:`n_{t+1}` are the reset, update, and new gates, respectively.</span>
<span class="sd">    :math:`W`, :math:`b` are the weight parameter and the deviation parameter respectively.</span>
<span class="sd">    :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.</span>

<span class="sd">    Args:</span>
<span class="sd">        direction (str, optional): A string identifying the direction in the operator. Default: ``&#39;UNIDIRECTIONAL&#39;`` .</span>
<span class="sd">            Only ``&#39;UNIDIRECTIONAL&#39;`` is currently supported.</span>
<span class="sd">        cell_depth (int, optional): An integer identifying the cell depth in the operator. Default: ``1`` .</span>
<span class="sd">        keep_prob (float, optional): A float identifying the keep prob in the operator. Default: ``1.0`` .</span>
<span class="sd">        cell_clip (float, optional): A float identifying the cell clip in the operator. Default: ``-1.0`` .</span>
<span class="sd">        num_proj (int, optional): An integer identifying the number projection in the operator. Default: ``0`` .</span>
<span class="sd">        time_major (bool, optional): A bool identifying the time major in the operator. Default: ``True`` .</span>
<span class="sd">        activation (str, optional) : A string identifying the type of activation function in the operator.</span>
<span class="sd">            Default: ``&#39;tanh&#39;`` . Only ``&#39;tanh&#39;`` is currently supported.</span>
<span class="sd">        gate_order (str, optional): A string identifying the gate order in weight and bias. Default: ``&#39;rzh&#39;`` .</span>
<span class="sd">            ``&#39;zrh&#39;`` is another option. Here, ``&#39;rzh&#39;`` means the gate order is: reset gate, update gate, hidden gate.</span>
<span class="sd">            ``&#39;zrh&#39;`` means the gate order is: update gate, reset gate, hidden gate.</span>
<span class="sd">        reset_after (bool, optional): A bool identifying whether to apply reset gate after matrix multiplication.</span>
<span class="sd">            Default: ``True`` .</span>
<span class="sd">        is_training (bool, optional): A bool identifying is training in the operator. Default: ``True`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Current words.</span>
<span class="sd">          Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{input_size})`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **weight_input** (Tensor) - Input-hidden weight :math:`W_{\{ir,iz,in\}}`.</span>
<span class="sd">          Tensor of shape :math:`(\text{input_size}, 3 \times \text{hidden_size})`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **weight_hidden** (Tensor) - Hidden-hidden weight :math:`W_{\{hr,hz,hn\}}`.</span>
<span class="sd">          Tensor of shape :math:`(\text{hidden_size}, 3 \times \text{hidden_size})`.</span>
<span class="sd">          The data type must be float16.</span>
<span class="sd">        - **bias_input** (Tensor) - Input-hidden bias :math:`b_{\{ir,iz,in\}}`.</span>
<span class="sd">          Tensor of shape :math:`(3 \times \text{hidden_size})`, or None.</span>
<span class="sd">          Has the same data type with input `init_h`.</span>
<span class="sd">        - **bias_hidden** (Tensor) - Hidden-hidden bias :math:`b_{\{hr,hz,hn\}}`.</span>
<span class="sd">          Tensor of shape :math:`(3 \times \text{hidden_size})`,</span>
<span class="sd">          or None. Has the same data type with input `init_h`.</span>
<span class="sd">        - **seq_length** (Tensor) - The length of each batch. Tensor of shape :math:`(\text{batch_size})`.</span>
<span class="sd">          Only `None` is currently supported.</span>
<span class="sd">        - **init_h** (Tensor) - Hidden state of initial time.</span>
<span class="sd">          Tensor of shape :math:`(\text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          The data type must be float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A Tensor of shape:</span>

<span class="sd">          - y_shape = :math:`(num\_step, batch\_size, min(hidden\_size, num\_proj))`: `If num_proj &gt; 0`,</span>
<span class="sd">          - y_shape = :math:`(num\_step, batch\_size, hidden\_size)`: `If num_proj = 0`.</span>

<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **output_h** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **update** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **reset** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **new** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>
<span class="sd">        - **hidden_new** (Tensor) - A Tensor of shape :math:`(\text{num_step}, \text{batch_size}, \text{hidden_size})`.</span>
<span class="sd">          Has the same data type with input `bias_type`.</span>

<span class="sd">        A note about the bias_type:</span>

<span class="sd">        - If `bias_input` and `bias_hidden` both are `None`, `bias_type` is the data type of `init_h`.</span>
<span class="sd">        - If `bias_input` is not `None`, `bias_type` is the data type of `bias_input`.</span>
<span class="sd">        - If `bias_input` is `None` and `bias_hidden` is not `None`, `bias_type` is the data type of `bias_hidden`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `direction`, `activation` or `gate_order` is not a str.</span>
<span class="sd">        TypeError: If `cell_depth` or `num_proj` is not an int.</span>
<span class="sd">        TypeError: If `keep_prob` or `cell_clip` is not a float.</span>
<span class="sd">        TypeError: If `time_major`, `reset_after` or `is_training` is not a bool.</span>
<span class="sd">        TypeError: If `x`, `weight_input`, `weight_hidden`, `bias_input`, `bias_hidden`, `seq_length` or `ini_h` is not</span>
<span class="sd">                   a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x`, `weight_input` or `weight_hidden` is not float16.</span>
<span class="sd">        TypeError: If dtype of `init_h` is neither float16 nor float32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.rand(2, 8, 64).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; weight_i = Tensor(np.random.rand(64, 48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; weight_h = Tensor(np.random.rand(16, 48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; bias_i = Tensor(np.random.rand(48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; bias_h = Tensor(np.random.rand(48).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; init_h = Tensor(np.random.rand(8, 16).astype(np.float16))</span>
<span class="sd">        &gt;&gt;&gt; dynamic_gru_v2 = ops.DynamicGRUV2()</span>
<span class="sd">        &gt;&gt;&gt; output = dynamic_gru_v2(x, weight_i, weight_h, bias_i, bias_h, None, init_h)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].shape)</span>
<span class="sd">        (2, 8, 16)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">direction</span><span class="o">=</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">,</span>
                 <span class="n">cell_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">time_major</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
                 <span class="n">gate_order</span><span class="o">=</span><span class="s2">&quot;rzh&quot;</span><span class="p">,</span>
                 <span class="n">reset_after</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">is_training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DynamicGRUV2.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_depth</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_depth&quot;</span><span class="p">,</span> <span class="n">cell_depth</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">keep_prob</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_clip</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;cell_clip&quot;</span><span class="p">,</span> <span class="n">cell_clip</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_proj</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">num_proj</span><span class="p">,</span> <span class="s2">&quot;num_proj&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_major</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;time_major&quot;</span><span class="p">,</span> <span class="n">time_major</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_training</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;is_training&quot;</span><span class="p">,</span> <span class="n">is_training</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">direction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">direction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;UNIDIRECTIONAL&#39;</span><span class="p">],</span> <span class="s2">&quot;direction&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;tanh&#39;</span><span class="p">],</span> <span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_order</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">gate_order</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;zrh&#39;</span><span class="p">,</span> <span class="s1">&#39;rzh&#39;</span><span class="p">],</span> <span class="s2">&quot;gate_order&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_after</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reset_after&quot;</span><span class="p">,</span> <span class="n">reset_after</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
                <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;weight_input&quot;</span><span class="p">,</span> <span class="s2">&quot;weight_hidden&quot;</span><span class="p">,</span> <span class="s2">&quot;bias_input&quot;</span><span class="p">,</span>
                <span class="s2">&quot;bias_hidden&quot;</span><span class="p">,</span> <span class="s2">&quot;seq_length&quot;</span><span class="p">,</span> <span class="s2">&quot;init_h&quot;</span>
            <span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;output_h&quot;</span><span class="p">,</span> <span class="s2">&quot;update&quot;</span><span class="p">,</span> <span class="s2">&quot;reset&quot;</span><span class="p">,</span> <span class="s2">&quot;new&quot;</span><span class="p">,</span> <span class="s2">&quot;hidden_new&quot;</span><span class="p">])</span></div>


<div class="viewcode-block" id="InTopK"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.InTopK.html#mindspore.ops.InTopK">[文档]</a><span class="k">class</span> <span class="nc">InTopK</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determines whether the targets are in the top `k` predictions.</span>

<span class="sd">    Refer to :func:`mindspore.ops.intopk` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        k (int): Specifies the number of top elements to be used for computing precision along the last dimension.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x1** (Tensor) - A 2D Tensor defines the predictions of a batch of samples with float16 or float32</span>
<span class="sd">          data type.</span>
<span class="sd">        - **x2** (Tensor) - A 1D Tensor defines the labels of a batch of samples with int32 data type. The size of `x2`</span>
<span class="sd">          must be equal to the first dimension of `x1`. The values of `x2` can not be negative and</span>
<span class="sd">          must be equal to or less than index of x1&#39;s second dimension.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor has 1 dimension of type bool and the same shape with `x2`. For labeling sample `i` in `x2`,</span>
<span class="sd">        if the label in the first `k` predictions for sample `i` is in `x1`, then the value is ``True`` ,</span>
<span class="sd">        otherwise ``False`` .</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x1 = Tensor(np.array([[1, 8, 5, 2, 7], [4, 9, 1, 3, 5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; x2 = Tensor(np.array([1, 3]), mindspore.int32)</span>
<span class="sd">        &gt;&gt;&gt; in_top_k = ops.InTopK(3)</span>
<span class="sd">        &gt;&gt;&gt; output = in_top_k(x1, x2)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [ True  False]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize InTopK&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="LRN"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.LRN.html#mindspore.ops.LRN">[文档]</a><span class="k">class</span> <span class="nc">LRN</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Local Response Normalization.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        LRN is deprecated on Ascend due to potential accuracy problem. It&#39;s recommended to use other</span>
<span class="sd">        normalization methods, e.g. :class:`mindspore.ops.BatchNorm`.</span>

<span class="sd">    .. math::</span>

<span class="sd">        b_{c} = a_{c}\left(k + \frac{\alpha}{n}</span>
<span class="sd">        \sum_{c&#39;=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c&#39;}^2\right)^{-\beta}</span>

<span class="sd">    where the :math:`a_{c}` indicates the specific value of the pixel corresponding to :math:`c` in feature map;</span>
<span class="sd">    where the :math:`n/2` indicates the `depth_radius`; where the :math:`k` indicates the `bias`;</span>
<span class="sd">    where the :math:`\alpha` indicates the `alpha`; where the :math:`\beta` indicates the `beta`.</span>

<span class="sd">    Args:</span>
<span class="sd">        depth_radius (int): Half-width of the 1-D normalization window with the shape of 0-D. Default: ``5`` .</span>
<span class="sd">        bias (float): An offset (usually positive to avoid dividing by 0). Default: ``1.0`` .</span>
<span class="sd">        alpha (float): A scale factor, usually positive. Default: ``1.0`` .</span>
<span class="sd">        beta (float): An exponent. Default: ``0.5`` .</span>
<span class="sd">        norm_region (str): Specifies normalization region. Options: ``&quot;ACROSS_CHANNELS&quot;`` .</span>
<span class="sd">            Default: ``&quot;ACROSS_CHANNELS&quot;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A 4-D Tensor with float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same shape and data type as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `depth_radius` is not an int.</span>
<span class="sd">        TypeError: If `bias`, `alpha` or `beta` is not a float.</span>
<span class="sd">        TypeError: If `norm_region` is not a str.</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[[[0.1], [0.2]],</span>
<span class="sd">        ...                       [[0.3], [0.4]]]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; lrn = ops.LRN()</span>
<span class="sd">        &gt;&gt;&gt; output = lrn(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[0.09534626]</span>
<span class="sd">           [0.1825742 ]]</span>
<span class="sd">          [[0.2860388 ]</span>
<span class="sd">           [0.3651484 ]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">depth_radius</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">norm_region</span><span class="o">=</span><span class="s2">&quot;ACROSS_CHANNELS&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize LRN&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;LRN&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;depth_radius&quot;</span><span class="p">,</span> <span class="n">depth_radius</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;norm_region&quot;</span><span class="p">,</span> <span class="n">norm_region</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">norm_region</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;ACROSS_CHANNELS&#39;</span><span class="p">],</span> <span class="s1">&#39;norm_region&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">depth_radius</span><span class="p">,</span> <span class="s2">&quot;depth_radius&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="AvgPool3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.AvgPool3D.html#mindspore.ops.AvgPool3D">[文档]</a><span class="k">class</span> <span class="nc">AvgPool3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    3D Average pooling operation.</span>

<span class="sd">    Typically the input is of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`, AvgPool3D outputs</span>
<span class="sd">    regional average in the :math:`(D_{in}, H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`ks = (d_{ker}, h_{ker}, w_{ker})` and stride :math:`s = (s_0, s_1, s_2)`, the operation is as follows.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        &quot;kernel_size&quot; is in the range [1, 255]. &quot;strides&quot; is in the range [1, 63].</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, d, h, w) =</span>
<span class="sd">        \frac{1}{d_{ker} * h_{ker} * w_{ker}} \sum_{l=0}^{d_{ker}-1} \sum_{m=0}^{h_{ker}-1} \sum_{n=0}^{w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times d + l, s_1 \times h + m, s_2 \times w + n)</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the average value,</span>
<span class="sd">            is an int number that represents depth, height and width are both kernel_size, or a tuple</span>
<span class="sd">            of three int numbers that represent depth, height and width respectively. Default: ``1`` .</span>
<span class="sd">        strides (Union[int, tuple[int]]): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: ``1`` .</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;same&quot;`` , ``&quot;valid&quot;`` or ``&quot;pad&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Pad the input around its depth/height/width dimension so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally.  If the amount is even,</span>
<span class="sd">              it isuniformly distributed around the input, if it is odd, the excess amount goes</span>
<span class="sd">              to the front/right/bottom side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;valid&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible depth, height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded. If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;pad&quot;``: Pad the input with a specified amount. In this mode, the amount of padding</span>
<span class="sd">              in the depth, height and width dimension is determined by the `pad` parameter.</span>
<span class="sd">              If this mode is set, `pad` must be greater than or equal to 0.</span>

<span class="sd">        pad (Union(int, tuple[int], list[int])): The pad value to be filled. Default: ``0`` . If `pad` is an integer,</span>
<span class="sd">            the paddings of head, tail, top, bottom, left and right are the same, equal to pad.</span>
<span class="sd">            If `pad` is a tuple of six integers, the padding of head, tail, top, bottom, left and right equal to</span>
<span class="sd">            pad[0], pad[1], pad[2], pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        ceil_mode (bool): If ``True`` , ceil instead of floor to compute the output shape. Default: ``False`` .</span>
<span class="sd">        count_include_pad (bool): If ``True`` , averaging calculation will include the zero-padding.</span>
<span class="sd">            Default: ``True`` .</span>
<span class="sd">        divisor_override (int): If specified, it will be used as divisor in the averaging calculation,</span>
<span class="sd">            otherwise kernel_size will be used. Default: ``0`` .</span>
<span class="sd">        data_format (str) : The optional value for data format. Currently only support ``&#39;NCDHW&#39;`` .</span>
<span class="sd">            Default: ``&#39;NCDHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Currently support float16, float32 and float64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with shape :math:`(N, C, D_{out}, H_{out}, W_{out})`. Has the same data type with `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `kernel_size`, `strides` or `pad` is neither an int not a tuple.</span>
<span class="sd">        TypeError: If `ceil_mode` or `count_include_pad` is not a bool.</span>
<span class="sd">        TypeError: If `pad_mode` or `data_format` is not a string.</span>
<span class="sd">        TypeError: If `divisor_override` is not an int.</span>
<span class="sd">        ValueError: If numbers in `kernel_size` or `strides` are not positive.</span>
<span class="sd">        ValueError: If `kernel_size` or `strides` is a tuple whose length is not equal to 3.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If element of `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to 0 or (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 2 * 2 * 2 * 3).reshape((1, 2, 2, 2, 3)), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; avg_pool3d = ops.AvgPool3D(kernel_size=2, strides=1, pad_mode=&quot;valid&quot;)</span>
<span class="sd">        &gt;&gt;&gt; output = avg_pool3d(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[ 5.  6.]]]</span>
<span class="sd">          [[[17. 18.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">count_include_pad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">divisor_override</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize AvgPool3D&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;pad&#39; must be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;six positive int numbers, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="s1">&#39;PAD&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;PAD&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero or (0, 0, 0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">PAD</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;PAD&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad or item of pad&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ceil_mode&#39;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count_include_pad</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;count_include_pad&#39;</span><span class="p">,</span> <span class="n">count_include_pad</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">divisor_override</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">divisor_override</span><span class="p">,</span> <span class="s1">&#39;divisor_override&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Conv3D.html#mindspore.ops.Conv3D">[文档]</a><span class="k">class</span> <span class="nc">Conv3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    3D convolution layer.</span>

<span class="sd">    Applies a 3D convolution over an input tensor which is typically of shape</span>
<span class="sd">    :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size, :math:`C` is channel number,</span>
<span class="sd">    :math:`D, H, W`</span>
<span class="sd">    are the depth, height and width of the feature map, respectively.</span>

<span class="sd">    The output is calculated based on formula:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +</span>
<span class="sd">        \sum_{k = 0}^{C_{in} - 1} \text{ccor}({\text{weight}(C_{\text{out}_j}, k), \text{X}(N_i, k)})</span>

<span class="sd">    where :math:`bias` is the output channel bias, :math:`ccor` is</span>
<span class="sd">    the `cross-correlation &lt;https://en.wikipedia.org/wiki/Cross-correlation&gt;`_,</span>
<span class="sd">    :math:`weight` is the convolution kernel value and :math:`X` represents the input feature map.</span>

<span class="sd">    Here are the indices&#39; meanings:</span>

<span class="sd">    - :math:`i` corresponds to the batch number, the range is :math:`[0, N-1]`,</span>
<span class="sd">      where :math:`N` is the batch size of the input.</span>

<span class="sd">    - :math:`j` corresponds to the output channel, the range is :math:`[0, C_{out}-1]`,</span>
<span class="sd">      where :math:`C_{out}` is the number of</span>
<span class="sd">      output channels, which is also equal to the number of kernels.</span>

<span class="sd">    - :math:`k` corresponds to the input channel, the range is :math:`[0, C_{in}-1]`,</span>
<span class="sd">      where :math:`C_{in}` is the number of</span>
<span class="sd">      input channels, which is also equal to the number of channels in the convolutional kernels.</span>

<span class="sd">    Therefore, in the above formula, :math:`{bias}(C_{\text{out}_j})` represents the bias of the :math:`j`-th</span>
<span class="sd">    output channel, :math:`{weight}(C_{\text{out}_j}, k)`represents the slice of the :math:`j`-th convolutional</span>
<span class="sd">    kernel in the :math:`k`-th channel, and :math:`{X}(N_i, k)` represents the slice of the :math:`k`-th input</span>
<span class="sd">    channel in the :math:`i`-th batch of the input feature map.</span>

<span class="sd">    The shape of the convolutional kernel is given by</span>
<span class="sd">    :math:`(\text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})`</span>
<span class="sd">    where :math:`\text{kernel_size[0]}` ,</span>
<span class="sd">    :math:`\text{kernel_size[1]}` and :math:`\text{kernel_size[2]}` are the depth,</span>
<span class="sd">    height and width of the kernel, respectively.</span>
<span class="sd">    If we consider the input and output channels as well as the `group` parameter, the complete kernel shape</span>
<span class="sd">    will be :math:`(C_{out}, C_{in} / \text{group}, \text{kernel_size[0]},</span>
<span class="sd">    \text{kernel_size[1]}, \text{kernel_size[2]})`,</span>
<span class="sd">    where `group` is the number of groups dividing `x`&#39;s input channel when applying group convolution.</span>

<span class="sd">    For more details about convolution layer, please refer to `Gradient Based Learning Applied to Document Recognition</span>
<span class="sd">    &lt;http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf&gt;`_.</span>

<span class="sd">    Note:</span>
<span class="sd">        1. On Ascend platform, :math:`groups=1` must be satisfied.</span>
<span class="sd">        2. On Ascend :math:`dilation` on depth only supports the case of 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): Specifies output channel :math:`C_{out}`.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): Specifies the depth, height and width of the 3D convolution kernel.</span>
<span class="sd">            It can be a single int or a tuple of 3 integers. A single int means the value is for depth, height</span>
<span class="sd">            and the width. A tuple of 3 ints means the first value is for depth and</span>
<span class="sd">            the rest is for the height and width.</span>
<span class="sd">        mode (int, optional): Modes for different convolutions. It is currently not used. Default: ``1`` .</span>
<span class="sd">        stride (Union[int, tuple[int]], optional): The distance of kernel moving, it can be an int number</span>
<span class="sd">            that represents the depth, height and width of movement or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width movement respectively. Default: ``1`` .</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;same&quot;`` , ``&quot;valid&quot;`` or ``&quot;pad&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Pad the input around its depth/height/width dimension so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally.  If the amount is even,</span>
<span class="sd">              it isuniformly distributed around the input, if it is odd, the excess amount goes</span>
<span class="sd">              to the front/right/bottom side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;valid&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible depth, height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded. If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;pad&quot;``: Pad the input with a specified amount. In this mode, the amount of padding</span>
<span class="sd">              in the depth, height and width dimension is determined by the `pad` parameter.</span>
<span class="sd">              If this mode is set, `pad` must be greater than or equal to 0.</span>

<span class="sd">        pad (Union(int, tuple[int]), optional): Specifies the amount of padding to apply on input</span>
<span class="sd">            when `pad_mode` is set to ``&quot;pad&quot;``. It can be a single int or a tuple of 6 ints.</span>
<span class="sd">            If `pad` is one integer, the paddings of head, tail, top, bottom,</span>
<span class="sd">            left and right are the same, equal to `pad`. If `pad` is a tuple with 6 integers, the</span>
<span class="sd">            paddings of head, tail, top, bottom, left and right is equal to pad[0],</span>
<span class="sd">            pad[1], pad[2], pad[3], pad[4] and pad[5] accordingly. Default: ``0`` .</span>
<span class="sd">        dilation (Union[int, tuple[int]], optional): Specifies the dilation rate to use for dilated convolution.</span>
<span class="sd">            It can be a single int or a tuple of 3 integers. A single int means the dilation size is the same</span>
<span class="sd">            in the depth, height and width directions. A tuple of 3 ints represents the dilation size in</span>
<span class="sd">            the depth, height and width directions, respectively.</span>
<span class="sd">            Assuming :math:`dilation=(d0, d1, d2)`, the convolutional kernel samples the input with a</span>
<span class="sd">            spacing of :math:`d0-1` elements in the depth direction,</span>
<span class="sd">            :math:`d1-1` elements in the height direction, :math:`d2-1` elements in the</span>
<span class="sd">            width direction respectively. The values in the depth, height and width dimensions are in the</span>
<span class="sd">            ranges [1, D], [1, H] and [1, W], respectively.</span>
<span class="sd">            Default: ``1`` .</span>
<span class="sd">        group (int, optional): The number of groups into which the filter is divided. `in_channels`</span>
<span class="sd">            and `out_channels` must be divisible by `group`. Default: ``1`` .</span>
<span class="sd">        data_format (str, optional): The optional value for data format. Currently only support ``&quot;NCDHW&quot;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Currently input data type only support float16 and float32.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(k_d, K_h, K_w)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}/groups, k_d, K_h, K_w)`.</span>
<span class="sd">          Currently weight data type only support float16 and float32.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`(C_{out})`. When bias is None, zeros will be used.</span>
<span class="sd">          Default: ``None`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied 3D convolution. The shape is :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">        `pad_mode` is ``&quot;same&quot;``:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                D_{out} = \left \lceil{\frac{D_{in}}{\text{stride[0]}}} \right \rceil \\</span>
<span class="sd">                H_{out} = \left \lceil{\frac{H_{in}}{\text{stride[1]}}} \right \rceil \\</span>
<span class="sd">                W_{out} = \left \lceil{\frac{W_{in}}{\text{stride[2]}}} \right \rceil \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">        `pad_mode` is ``&quot;valid&quot;``:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                D_{out} = \left \lfloor{\frac{D_{in} - \text{dilation[0]} \times (\text{kernel_size[0]} - 1) }</span>
<span class="sd">                {\text{stride[0]}} + 1} \right \rfloor \\</span>
<span class="sd">                H_{out} = \left \lfloor{\frac{H_{in} - \text{dilation[1]} \times (\text{kernel_size[1]} - 1) }</span>
<span class="sd">                {\text{stride[1]}} + 1} \right \rfloor \\</span>
<span class="sd">                W_{out} = \left \lfloor{\frac{W_{in} - \text{dilation[2]} \times (\text{kernel_size[2]} - 1) }</span>
<span class="sd">                {\text{stride[2]}} + 1} \right \rfloor \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">        `pad_mode` is ``&quot;pad&quot;``:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                D_{out} = \left \lfloor{\frac{D_{in} + pad[0] + pad[1] - (\text{dilation[0]} - 1) \times</span>
<span class="sd">                \text{kernel_size[0]} - 1 }{\text{stride[0]}} + 1} \right \rfloor \\</span>
<span class="sd">                H_{out} = \left \lfloor{\frac{H_{in} + pad[2] + pad[3] - (\text{dilation[1]} - 1) \times</span>
<span class="sd">                \text{kernel_size[1]} - 1 }{\text{stride[1]}} + 1} \right \rfloor \\</span>
<span class="sd">                W_{out} = \left \lfloor{\frac{W_{in} + pad[4] + pad[5] - (\text{dilation[2]} - 1) \times</span>
<span class="sd">                \text{kernel_size[2]} - 1 }{\text{stride[2]}} + 1} \right \rfloor \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int nor a tuple.</span>
<span class="sd">        ValueError: If `out_channel`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; or &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; # case 1: specify kernel_size with tuple, all parameters use default values.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([16, 3, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 3, 4, 3, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; conv3d = ops.Conv3D(out_channel=32, kernel_size=(4, 3, 3))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 7, 30, 30)</span>
<span class="sd">        &gt;&gt;&gt; # case 2: specify kernel_size with int, all parameters use default values.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 20, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([40, 20, 3, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv3d = ops.Conv3D(out_channel=40, kernel_size=3)</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 40, 30, 30, 30)</span>
<span class="sd">         &gt;&gt;&gt; # case 3: stride=(1, 2, 3), other parameters being default.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 20, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([40, 20, 3, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv3d = ops.Conv3D(out_channel=40, kernel_size=3, stride=(1, 2, 3))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 40, 30, 15, 10)</span>
<span class="sd">         &gt;&gt;&gt; # case 4: pad_mode=&quot;pad&quot;, other parameters being default.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 20, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([40, 20, 3, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv3d = ops.Conv3D(out_channel=40, kernel_size=3, pad_mode=&quot;pad&quot;, pad=2)</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 40, 34, 34, 34)</span>
<span class="sd">         &gt;&gt;&gt; # case 5: dilation=(1, 1, 1), other parameters being default.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 20, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([40, 20, 3, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv3d = ops.Conv3D(out_channel=40, kernel_size=3, dilation=(1, 1, 1))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 40, 30, 30, 30)</span>
<span class="sd">        &gt;&gt;&gt; # case 6: group=1, other parameters being default.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 20, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([40, 20, 3, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv3d = ops.Conv3D(out_channel=40, kernel_size=3, group=1)</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 40, 30, 30, 30)</span>
<span class="sd">        &gt;&gt;&gt; # case 7: All parameters are specified.</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 20, 32, 32, 32]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([40, 20, 3, 3, 3]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; conv3d = ops.Conv3D(out_channel=40, kernel_size=3, stride=(1, 2, 3), pad_mode=&quot;pad&quot;,</span>
<span class="sd">        ...                     pad=2, dilation=(1), group=1)</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d(x, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 40, 34, 17, 12)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3D&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;ascend&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                   <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">third_one</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                   <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">third_one</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;pad&#39; must be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;six positive int numbers, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero or (0, 0, 0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;group&quot;</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;group&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">device_target</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">%</span> <span class="n">group</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The argument &#39;group&#39; should be divisible by &#39;out_channel&#39;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device_target</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span> <span class="ow">and</span> <span class="n">group</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;On Ascend platform, group = 1 must be satisfied.&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;offset_x&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Conv3DBackpropInput</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the gradients of convolution 3D with respect to the input.</span>

<span class="sd">    Args:</span>
<span class="sd">        out_channel (int): The dimension of the output.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The kernel size of the 3D convolution.</span>
<span class="sd">        mode (int): Modes for different convolutions. Not currently used.</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;same&quot;`` , ``&quot;valid&quot;`` or ``&quot;pad&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Pad the input around its depth/height/width dimension so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally.  If the amount is even,</span>
<span class="sd">              it isuniformly distributed around the input, if it is odd, the excess amount goes</span>
<span class="sd">              to the front/right/bottom side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;valid&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible depth, height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded. If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;pad&quot;``: Pad the input with a specified amount. In this mode, the amount of padding</span>
<span class="sd">              in the depth, height and width dimension is determined by the `pad` parameter.</span>
<span class="sd">              If this mode is set, `pad` must be greater than or equal to 0.</span>

<span class="sd">        pad (Union(int, tuple[int])): The pad value to be filled. Default: ``0`` . If `pad` is an integer, the</span>
<span class="sd">                    paddings of head, tail, top, bottom, left and right are the same, equal to pad. If `pad` is a</span>
<span class="sd">                    tuple of four integers, the padding of head, tail, top, bottom, left and right equal to pad[0],</span>
<span class="sd">                    pad[1], pad[2], pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int])): The stride to be applied to the convolution filter. Default: ``1`` .</span>
<span class="sd">        dilation (Union(int, tuple[int])): Specifies the space to use between kernel elements. Default: ``1`` .</span>
<span class="sd">        group (int): Splits input into groups. Default: ``1`` .</span>
<span class="sd">        data_format (str): The optional value for data format. Currently only support ``&#39;NCDHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(D_{in}, K_h, K_w)`, then the shape is</span>
<span class="sd">          :math:`(C_{out}, C_{in}, D_{in}, K_h, K_w)`. Currently weight data type only support float16 and float32.</span>
<span class="sd">        - **dout** (Tensor) - the gradients with respect to the output of the convolution.</span>
<span class="sd">          The shape conforms to the default.</span>
<span class="sd">          data_format :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`. Currently dout data type only support float16</span>
<span class="sd">          and float32.</span>
<span class="sd">        - **input_size** (tuple(int)) - A tuple describes the shape of the input which conforms to the format</span>
<span class="sd">          :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients with respect to the input of convolution 3D. It has the same shape as the input.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `out_channel` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` or `dilation` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If `out_channel`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39;, &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([16, 32, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([32, 32, 4, 6, 2]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([16, 32, 13, 37, 33]))</span>
<span class="sd">        &gt;&gt;&gt; conv3d_backprop_input = ops.Conv3DBackpropInput(out_channel=4, kernel_size=(4, 6, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d_backprop_input(dout, weight, ops.shape(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (16, 32, 13, 37, 33)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3DBackpropInput&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;filter&#39;</span><span class="p">,</span> <span class="s1">&#39;out_backprop&#39;</span><span class="p">,</span> <span class="s1">&#39;input_size&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;pad size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pad&quot;</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be (0, 0, 0, 0, 0, 0) &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;when &#39;pad_mode&#39; is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pad</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="s1">&#39;group&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_deconv_output_length</span><span class="p">(</span><span class="n">input_length</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride_size</span><span class="p">,</span> <span class="n">dilation_size</span><span class="p">):</span>
    <span class="n">filter_size</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">+</span> <span class="p">(</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dilation_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">filter_size</span> <span class="o">-</span> <span class="n">stride_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">input_length</span> <span class="o">*</span> <span class="n">stride_size</span> <span class="o">+</span> <span class="n">filter_size</span> <span class="o">-</span> <span class="n">stride_size</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">input_length</span> <span class="o">*</span> <span class="n">stride_size</span>
    <span class="k">return</span> <span class="n">length</span>


<span class="k">class</span> <span class="nc">SparseApplyAdadelta</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates relevant entries according to the adadelta scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">            \begin{array}{ll} \\</span>
<span class="sd">                accum = \rho * accum + (1 - \rho) * grad^2 \\</span>
<span class="sd">                \text{update} = \sqrt{\text{accum_update} + \epsilon} * \frac{grad}{\sqrt{accum + \epsilon}} \\</span>
<span class="sd">                var = var -  update * lr \\</span>
<span class="sd">                \text{accum_update} = \rho * \text{accum_update} + (1 - \rho) * update^2 \\</span>
<span class="sd">            \end{array}</span>

<span class="sd">    Inputs of &#39;var&#39;, &#39;accum&#39;, &#39;accum_update&#39; and &#39;grad&#39; comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent. Besides, inputs of &#39;lr&#39; and &#39;rho&#39; also support implicit type conversion.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Note:</span>
<span class="sd">        If there are negative values or values greater than or equal to var.shape[0] in `indices`,</span>
<span class="sd">        the behavior is undefined. Besides, this operator doesn&#39;t support duplicates in `indices`.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. Its value must be greater or equal to 0.</span>
<span class="sd">        use_locking (bool): If ``True`` , the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Weights to be updated. With float32 or float16 data type.</span>
<span class="sd">        - **accum** (Parameter) - Accumulation to be updated. Mush have the same shape and dtype as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **accum_update** (Parameter) - Accum_update to be updated. Must have the same shape and dtype as `var`.</span>
<span class="sd">          With float32 or float16 data type.</span>
<span class="sd">        - **lr** (Union[float, Tensor]) - Learning rate, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **rho** (Union[float, Tensor]) - Decay rate, must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          Must be one of the following types: int32, int64 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensor, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>
<span class="sd">        - **accum_update** (Tensor) - The same shape and data type as `accum_update`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `epsilon` is not a float.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `var`, &#39;accum&#39;, &#39;accum_update&#39; is not a Parameter.</span>
<span class="sd">        TypeError: If dtype of `accum`, `accum_updata`, `grad` is not same as `var`.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `accum_update`, `lr`, `rho` or `grad` is neither float16 nor</span>
<span class="sd">                   float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If `epsilon` is less than 0.</span>
<span class="sd">        ValueError: If the shape of `accum`, `accum_updata`, `grad` is not same as `var`.</span>
<span class="sd">        ValueError: If the rank of `indices` is not equal to 1.</span>
<span class="sd">        ValueError: If shape of `indices` is not same as shape of first dimension of `grad`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class Net(nn.Cell):</span>
<span class="sd">        ...     def __init__(self,epsilon,use_locking = False):</span>
<span class="sd">        ...         super(Net, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_adadelta = P.SparseApplyAdadelta(epsilon,use_locking)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[1.0,2.0],[2.0,3.0]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[1.5,2.5],[3.5,4.5]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...         self.accum_update = Parameter(Tensor(np.array([[1.2,2.4],[1.8,0.6]]).astype(np.float32)),</span>
<span class="sd">        ...                name=&quot;accum_update&quot;)</span>
<span class="sd">        ...     def construct(self, lr, rho, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_adadelta(self.var, self.accum, self.accum_update, lr, rho, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; epsilon = 1e-6</span>
<span class="sd">        &gt;&gt;&gt; net = Net(epsilon)</span>
<span class="sd">        &gt;&gt;&gt; lr = 0.01</span>
<span class="sd">        &gt;&gt;&gt; rho = 0.2</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, rho, grad, Tensor(np.array([0,1],dtype=np.int32)))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 9.94611859e-01,  1.98851788e+00],</span>
<span class="sd">         [ 1.99840558e+00,  2.99478507e+00]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 3.72000009e-01,  8.91999960e-01],</span>
<span class="sd">         [ 7.08000004e-01,  1.41200006e+00]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 4.72257614e-01,  1.53470778e+00],</span>
<span class="sd">         [ 3.80338937e-01,  3.37563992e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum_updata&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyAdadelta&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="CTCLossV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.CTCLossV2.html#mindspore.ops.CTCLossV2">[文档]</a><span class="k">class</span> <span class="nc">CTCLossV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the CTC (Connectionist Temporal Classification) loss and the gradient.</span>

<span class="sd">    The CTC algorithm is proposed in `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank (int, optional): The blank label. Default: ``0`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output. Currently only support ``&#39;none&#39;``.</span>
<span class="sd">            Default: ``&#39;none&#39;`` .</span>

<span class="sd">        zero_infinity (bool, optional): If loss is infinite, this parameter determines whether to set that loss</span>
<span class="sd">            and its correlated gradient to zero. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **log_probs** (Tensor) - A tensor of shape :math:`(T, N, C)`, where :math:`T` is input length, :math:`N` is</span>
<span class="sd">          batch size and :math:`C` is number of classes (including blank). Supported dtypes: float32, float64.</span>
<span class="sd">        - **targets** (Tensor) - A tensor of shape :math:`(N, S)`, where :math:`S` is max target length,</span>
<span class="sd">          means the target sequences. Supported dtypes: int32, int64.</span>
<span class="sd">        - **input_lengths** (Union(Tuple, Tensor)) - A tuple or Tensor of shape :math:`(N)`.</span>
<span class="sd">          It means the lengths of the input. Supported dtypes: int32, int64.</span>
<span class="sd">        - **target_lengths** (Union(Tuple, Tensor)) - A tuple or Tensor of shape :math:`(N)`.</span>
<span class="sd">          It means the lengths of the target. Supported dtypes: int32, int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **neg_log_likelihood** (Tensor) - A loss value which is differentiable with respect to each input node.</span>
<span class="sd">        - **log_alpha** (Tensor) - The probability of possible trace of input to target.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `zero_infinity` is not a bool.</span>
<span class="sd">        TypeError: If `reduction` is not string.</span>
<span class="sd">        TypeError: If the dtype of `log_probs` is not float or double.</span>
<span class="sd">        TypeError: If the dtype of `targets`, `input_lengths` or `target_lengths` is not int32 or int64.</span>
<span class="sd">        ValueError: If the rank of `log_probs` is not 3.</span>
<span class="sd">        ValueError: If the rank of `targets` is not 2.</span>
<span class="sd">        ValueError: If the shape of `input_lengths` does not match batch_size :math:`N`.</span>
<span class="sd">        ValueError: If the shape of `target_lengths` does not match batch_size :math:`N`.</span>
<span class="sd">        TypeError: If the types of `targets`, `input_lengths` or `target_lengths` are different.</span>
<span class="sd">        ValueError: If the value of `blank` is not in range [0, C).</span>
<span class="sd">        RuntimeError: If any value of `input_lengths` is larger than (num_labels|C).</span>
<span class="sd">        RuntimeError: If any `target_lengths[i]` is not in range [0, `input_length[i]`].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; log_probs = Tensor(np.array([[[0.3, 0.6, 0.6]],</span>
<span class="sd">        ...                              [[0.9, 0.4, 0.2]]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; targets = Tensor(np.array([[0, 1]]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = Tensor(np.array([2]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = Tensor(np.array([1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; CTCLossV2 = ops.CTCLossV2(blank=0, reduction=&#39;none&#39;, zero_infinity=False)</span>
<span class="sd">        &gt;&gt;&gt; neg_log_hood, log_alpha = CTCLossV2(</span>
<span class="sd">        ...     log_probs, targets, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; print(neg_log_hood)</span>
<span class="sd">        [-2.2986124]</span>
<span class="sd">        &gt;&gt;&gt; print(log_alpha)</span>
<span class="sd">        [[[0.3       0.3            -inf      -inf      -inf]</span>
<span class="sd">          [1.2       1.8931472 1.2            -inf      -inf]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CTCLossV2&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;log_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;targets&quot;</span><span class="p">,</span> <span class="s2">&quot;input_lengths&quot;</span><span class="p">,</span> <span class="s2">&quot;target_lengths&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;neg_log_likelihood&quot;</span><span class="p">,</span> <span class="s2">&quot;log_alpha&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="n">blank</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;zero_infinity&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;zero_infinity&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">CTCLossV2Grad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the gradient of CTC (Connectionist Temporal Classification) loss.</span>

<span class="sd">    The CTC algorithm is proposed in `Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with</span>
<span class="sd">    Recurrent Neural Networks &lt;http://www.cs.toronto.edu/~graves/icml_2006.pdf&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank (int): The blank label. Default: ``0`` .</span>
<span class="sd">        reduction (string): Apply specific reduction method to the output. Currently only support &#39;none&#39;.</span>
<span class="sd">            Default: ``&quot;none&quot;`` .</span>
<span class="sd">        zero_infinity (bool): Whether to set infinite loss and correlation gradient to zero. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **grad_out** (Tenosr) - Gradient renewal codfficient, A tensor for shape (N), where N is batch size.</span>
<span class="sd">        - **log_probs** (Tensor) - A tensor of shape (T, N, C), where T is input length, N is batch size and C is number</span>
<span class="sd">          of classes (including blank).</span>
<span class="sd">        - **targets** (Tensor) - A tensor of shape (N, S), where S is max target length, means the target sequences.</span>
<span class="sd">        - **input_lengths** (Union(tuple, Tensor)) - A tuple or Tensor of shape(N). It means the lengths of the input.</span>
<span class="sd">        - **target_lengths** (Union(tuple, Tensor)) - A tuple or Tensor of shape(N). It means the lengths of the target.</span>
<span class="sd">        - **log_alpha** (Tensor) - The probability of possible trace of input to target.</span>
<span class="sd">        - **neg_log_likelihood** (Tensor) - A loss value which is differentiable with respect to each input node.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **grad** (Tensor) - The grad of Connectionist Temporal Classification Loss.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `zero_infinity` is not a bool, reduction is not string.</span>
<span class="sd">        TypeError: If the dtype of `log_probs` or `grad_out` is not float or double.</span>
<span class="sd">        TypeError: If the dtype of `targets`, `input_lengths` or `target_lengths` is not int32 or int64.</span>
<span class="sd">        RuntimeError: If the rank of `log_probs` is not 3.</span>
<span class="sd">        RuntimeError: If the rank of `targets` is not 2.</span>
<span class="sd">        RuntimeError: If the shape of `input_lengths` does not match {batch_size|N}.</span>
<span class="sd">        RuntimeError: If the shape of `target_lengths` does not match {batch_size|N}.</span>
<span class="sd">        RuntimeError: If the types of `targets`, `input_lengths`, `grad_out` or `target_lengths` are different.</span>
<span class="sd">        RuntimeError: If the value of `blank` is not in range [0, num_labels|C).</span>
<span class="sd">        RuntimeError: If any value of `input_lengths` is larger than (num_labels|C).</span>
<span class="sd">        RuntimeError: If any target_lengths[i] is not in range [0, input_length[i]].</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize CTCLossV2Grad&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;grad_out&quot;</span><span class="p">,</span> <span class="s2">&quot;log_probs&quot;</span><span class="p">,</span> <span class="s2">&quot;targets&quot;</span><span class="p">,</span> <span class="s2">&quot;input_lengths&quot;</span><span class="p">,</span> <span class="s2">&quot;target_lengths&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;neg_log_likelihood&quot;</span><span class="p">,</span> <span class="s2">&quot;log_alpha&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;grad&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="n">blank</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;zero_infinity&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;zero_infinity&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">)</span>


<div class="viewcode-block" id="Conv3DTranspose"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Conv3DTranspose.html#mindspore.ops.Conv3DTranspose">[文档]</a><span class="k">class</span> <span class="nc">Conv3DTranspose</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes a 3D transposed convolution, which is also known as a deconvolution</span>
<span class="sd">    (although it is not an actual deconvolution).</span>

<span class="sd">    Input is typically of shape :math:`(N, C, D, H, W)`, where :math:`N` is batch size, :math:`C` is channel number,</span>
<span class="sd">    :math:`D` is depth, :math:`H` is height, :math:`W` is width.</span>

<span class="sd">    If the &#39;pad_mode&#39; is set to be &quot;pad&quot;, the depth, height and width of output are defined as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{pad}[0] + \text{dilation}[0]</span>
<span class="sd">        \times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1</span>

<span class="sd">        H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{pad}[1] + \text{dilation}[1]</span>
<span class="sd">        \times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1</span>

<span class="sd">        W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{pad}[2] + \text{dilation}[2]</span>
<span class="sd">        \times (\text{kernel_size}[2] - 1) + \text{output_padding}[2] + 1</span>

<span class="sd">    Note:</span>
<span class="sd">        In Ascend, only support :math:`group=1`.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channel (int): The channel of the input x.</span>
<span class="sd">        out_channel (int): The channel of the weight x.</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The data type is int or a tuple of 3 integers.</span>
<span class="sd">            Specifies the depth, height and width of the 3D convolution window.</span>
<span class="sd">            Single int means the value is for the depth, height and width of the kernel.</span>
<span class="sd">            A tuple of 3 ints means the first value is for the depth, the second value is for the height and the</span>
<span class="sd">            other is for the width of the kernel.</span>
<span class="sd">        mode (int, optional): Modes for different convolutions. Default is ``1`` . It is currently not used.</span>
<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;same&quot;`` , ``&quot;valid&quot;`` or ``&quot;pad&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Pad the input around its depth/height/width dimension so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally.  If the amount is even,</span>
<span class="sd">              it isuniformly distributed around the input, if it is odd, the excess amount goes</span>
<span class="sd">              to the front/right/bottom side.</span>
<span class="sd">              If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;valid&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible depth, height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded. If this mode is set, `pad` must be 0.</span>
<span class="sd">            - ``&quot;pad&quot;``: Pad the input with a specified amount. In this mode, the amount of padding</span>
<span class="sd">              in the depth, height and width dimension is determined by the `pad` parameter.</span>
<span class="sd">              If this mode is set, `pad` must be greater than or equal to 0.</span>

<span class="sd">        pad (Union(int, tuple[int]), optional): The pad value to be filled. Default: ``0`` . If `pad` is an integer,</span>
<span class="sd">            the paddings of head, tail, top, bottom, left and right are the same, equal to pad.</span>
<span class="sd">            If `pad` is a tuple of six integers, the padding of head, tail, top, bottom, left and right equal</span>
<span class="sd">            to pad[0], pad[1], pad[2], pad[3], pad[4] and pad[5] correspondingly.</span>
<span class="sd">        stride (Union(int, tuple[int]), optional): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the depth, height and width of movement are both strides, or a tuple of three int numbers that</span>
<span class="sd">            represent depth, height and width of movement respectively. Default: ``1`` .</span>
<span class="sd">        dilation (Union(int, tuple[int]), optional): Specifies the space to use between kernel elements.</span>
<span class="sd">            Default: ``1`` .</span>
<span class="sd">        group (int, optional): The number of groups into which the filter is divided. `in_channels`</span>
<span class="sd">            and `out_channels` must be divisible by `group`. Default: ``1`` .</span>
<span class="sd">        output_padding (Union(int, tuple[int]), optional): Add extra size to each dimension of the output.</span>
<span class="sd">            Default: ``0`` .</span>
<span class="sd">        data_format (str, optional): The optional value for data format. Currently only ``&#39;NCDHW&#39;`` is supported.</span>
<span class="sd">            Default: ``&#39;NCDHW&#39;``.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **dout** (Tensor) - The gradients with respect to the output of the convolution.</span>
<span class="sd">          The shape conforms to the default.</span>
<span class="sd">          data_format :math:`(N, C_{in}, D_{out}, H_{out}, W_{out})`. Currently dout data type only supports float16</span>
<span class="sd">          and float32.</span>
<span class="sd">        - **weight** (Tensor) - Set size of kernel is :math:`(K_d, K_h, K_w)`, then the shape is</span>
<span class="sd">          :math:`(C_{in}, C_{out}//group, K_d, K_h, K_w)`. Where :math:`group` is the Args parameter,</span>
<span class="sd">          :math:`//` is the symbol for integer division.</span>
<span class="sd">          Currently weight data type only supports float16 and float32.</span>
<span class="sd">        - **bias** (Tensor) - Tensor of shape :math:`C_{out}`. Currently, only support none. Default: ``None`` .</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the gradients with respect to the input of convolution 3D.</span>
<span class="sd">        Tensor of shape :math:`(N, C_{out}//group, D_{out}, H_{out}, W_{out})`,</span>
<span class="sd">        where :math:`group` is the Args parameter.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `in_channel`, `out_channel` or `group` is not an int.</span>
<span class="sd">        TypeError: If `kernel_size`, `stride`, `pad` , `dilation` or `output_padding` is neither an int not a tuple.</span>
<span class="sd">        ValueError: If `in_channel`, `out_channel`, `kernel_size`, `stride` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad` is less than 0.</span>
<span class="sd">        ValueError: If `pad_mode` is not one of &#39;same&#39;, &#39;valid&#39; nor &#39;pad&#39;.</span>
<span class="sd">        ValueError: If `pad` is a tuple whose length is not equal to 6.</span>
<span class="sd">        ValueError: If `pad_mode` is not equal to &#39;pad&#39; and `pad` is not equal to (0, 0, 0, 0, 0, 0).</span>
<span class="sd">        ValueError: If `data_format` is not &#39;NCDHW&#39;.</span>
<span class="sd">        TypeError: If data type of dout and weight is neither float16 nor float32.</span>
<span class="sd">        ValueError: If bias is not none. The rank of dout and weight is not 5.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; dout = Tensor(np.ones([32, 16, 10, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.ones([16, 3, 4, 6, 2]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; conv3d_transpose = ops.Conv3DTranspose(in_channel=16, out_channel=3, kernel_size=(4, 6, 2))</span>
<span class="sd">        &gt;&gt;&gt; output = conv3d_transpose(dout, weight)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (32, 3, 13, 37, 33)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">in_channel</span><span class="p">,</span>
                 <span class="n">out_channel</span><span class="p">,</span>
                 <span class="n">kernel_size</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span>
                 <span class="n">pad</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">group</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Conv3DTranspose&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;filter&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">in_channel</span><span class="p">,</span> <span class="s1">&#39;in_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;in_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channel</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">out_channel</span><span class="p">,</span> <span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;out_channel&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                             <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                               <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">third_one</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">pad</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">6</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">6</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;pad&#39; must be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;six positive int numbers, but got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span> <span class="o">=</span> <span class="n">pad</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="s1">&#39;pad&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">pad</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;pad&#39; must be zero or (0, 0, 0, 0, 0, 0) when &#39;pad_mode&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;pad&#39; is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pad</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">==</span> <span class="s1">&#39;pad&#39;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">:</span>
                <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;pad item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_list&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_equal_int</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;group&quot;</span><span class="p">,</span> <span class="n">group</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,),</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">out_channel</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;group&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channel</span> <span class="o">%</span> <span class="n">group</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The argument &#39;group&#39; should be divisible by &#39;out_channel&#39;&quot;</span><span class="p">)</span>
        <span class="n">device_target</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="s2">&quot;device_target&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device_target</span> <span class="o">==</span> <span class="s2">&quot;Ascend&quot;</span> <span class="ow">and</span> <span class="n">group</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;On Ascend platform, group = 1 must be satisfied.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group</span> <span class="o">=</span> <span class="n">group</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">],</span> <span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s1">&#39;output_padding&#39;</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                                                     <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">greater_zero</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">output_padding_</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">!=</span> <span class="s1">&#39;pad&#39;</span> <span class="ow">and</span> <span class="n">output_padding_</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;output_padding&#39; must be zero or (0, 0, 0) &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;when &#39;pad_mode&#39; is not </span><span class="se">\&quot;</span><span class="s2">pad</span><span class="se">\&quot;</span><span class="s2">, but got &#39;output_padding&#39; is &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">output_padding</span><span class="si">}</span><span class="s2"> and &#39;pad_mode&#39; is </span><span class="si">{</span><span class="n">pad_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_padding&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                                  <span class="mi">1</span><span class="p">,</span> <span class="mi">343</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span>
                                  <span class="s1">&#39;The product of height, width and depth of kernel_size belonging [1, 343]&#39;</span><span class="p">,</span>
                                  <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">343</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span>
                                  <span class="s1">&#39;The product of height, width and depth of stride belonging [1, 343]&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span>
                                  <span class="s1">&#39;The product of height, width and depth of stride belonging [1, 256]&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;output_padding_d belonging [0, max(stride_d, dilation_d))&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;output_padding_h belonging [0, max(stride_h,dilation_h))&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_int_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_LEFT</span><span class="p">,</span>
                                  <span class="s1">&#39;output_padding_w belonging [0, max(stride_w,dilation_w))&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">Dilation2D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the grayscale dilation of 4-D input and 3-D filters tensors.</span>

<span class="sd">    Applies a 2D dilation over an input tensor which is typically of shape :math:`(N, C_{in}, H_{in}, W_{in})`,</span>
<span class="sd">    where :math:`N` is batch size, :math:`H` is height, :math:`W` is width, :math:`C` is channel number.</span>
<span class="sd">    Given kernel size :math:`ks = (h_{ker}, w_{ker})`, stride :math:`s = (s_0, s_1)` and</span>
<span class="sd">    dilation :math:`d = (d_0, d_1)`, the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + d_0 \times m, s_1 \times w + d_1 \times n) + \text{filter}(C_j, m, n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subjected to change or deletion.</span>

<span class="sd">    Note:</span>
<span class="sd">        If the input data type is float32, this operator is still executed in float16 mode.</span>

<span class="sd">    Args:</span>
<span class="sd">        stride (Union(int, tuple[int])): The distance of kernel moving, an int number that represents</span>
<span class="sd">            the height and width of movement are both strides, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively, or a tuple of four int numbers when</span>
<span class="sd">            data_format is &#39;NCHW&#39; represents [1, 1, stride_height, stride_width].</span>

<span class="sd">        dilation (Union(int, tuple[int])): The data type is int or a tuple of 2 integers or a tuple of 4 integers.</span>
<span class="sd">                                      Specifies the dilation rate to use for dilated convolution.</span>
<span class="sd">                                      If set to be :math:`k &gt; 1`, there will be :math:`k - 1` pixels skipped for</span>
<span class="sd">                                      each sampling location. Its value must be greater or equal to 1 and bounded by</span>
<span class="sd">                                      the height and width of the input `x`.</span>

<span class="sd">        pad_mode (str, optional): Specifies the padding mode with a padding value of 0. It can be set to:</span>
<span class="sd">            ``&quot;same&quot;`` or ``&quot;valid&quot;`` . Default: ``&quot;valid&quot;`` .</span>

<span class="sd">            - ``&quot;same&quot;``: Pad the input around its edges so that the shape of input and output</span>
<span class="sd">              are the same when `stride` is set to ``1``.</span>
<span class="sd">              The amount of padding to is calculated by the operator internally, If the amount is even, it is</span>
<span class="sd">              uniformly distributed around the input, if it is odd, the excess amount goes to the right/bottom side.</span>
<span class="sd">            - ``&quot;valid&quot;``: No padding is applied to the input, and the output returns the maximum</span>
<span class="sd">              possible height and width. Extra pixels that could not complete a full stride will</span>
<span class="sd">              be discarded.</span>

<span class="sd">        data_format (str, optional): The value for data format, only ``&#39;NCHW&#39;`` is supported at present.</span>
<span class="sd">            Default: ``&quot;NCHW&quot;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input data. A 4-D Tensor, its shape must be</span>
<span class="sd">          :math:`(N, C_{in}, H_{in}, W_{in})`.</span>
<span class="sd">        - **filter** (Tensor) - A three dimension tensor with the same type as input. The shape must be</span>
<span class="sd">          :math:`(C_{in}, H_{filter}, W_{filter})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, the value that applied 2D dilation. The shape is :math:`(N, C_{out}, H_{out}, W_{out})` which</span>
<span class="sd">        is not necessarily the same as the input x, the type is the same as the input x.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If type of `x` or `filter` is not the type in [uint8, uint16, uint32, uint64, int8, int16,</span>
<span class="sd">                                  int32, int64, float16, float32, float64].</span>
<span class="sd">        TypeError: If `stride` or `dilation` is not an int number or a tuple of two or four int numbers.</span>
<span class="sd">        ValueError: If the length of `stride` or `dilation` is neither two nor four when they are tuple.</span>
<span class="sd">        ValueError: If `stride` or `dilation` shape is not (1, 1, height, width) when it is a tuple of four int numbers.</span>
<span class="sd">        ValueError: If `stride` is not in the range of [1, 255].</span>
<span class="sd">        ValueError: If `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pad_mode` is not a str of &#39;same&#39;, &#39;valid&#39;, &#39;SAME&#39; or &#39;VALID&#39;.</span>
<span class="sd">        ValueError: If `data_format` is not the str of &#39;NCHW&#39;.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.ones([10, 5, 32, 32]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; filter = Tensor(np.ones([5, 3, 3]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; dilation2d = ops.Dilation2D(stride=1, dilation=1, pad_mode=&#39;VALID&#39;)</span>
<span class="sd">        &gt;&gt;&gt; output = dilation2d(x, filter)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (10, 5, 30, 30)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Dilation2D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;filter&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

        <span class="k">def</span> <span class="nf">_check_format_stride_or_dilation</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">prim_name</span><span class="p">,</span> <span class="n">data_format</span><span class="p">):</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="n">arg_name</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">),</span> <span class="n">prim_name</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg_value</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="n">ret_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">ret_value</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span> <span class="k">else</span> \
                    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg_value</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; should be [1, </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_height, </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_weigth, 1]&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;when data_format is &#39;NHWC&#39;, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s2">&quot;NCHW&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="n">arg_value</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">arg_value</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; should be [1, 1, </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_height, </span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">_weigth]&quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;when data_format is &#39;NCHW&#39;, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">ret_value</span> <span class="o">=</span> <span class="n">arg_value</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; should be an positive int number or a tuple of two &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;or four positive int numbers, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">ret_value</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span> <span class="ow">and</span> <span class="n">item</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="n">prim_name</span><span class="si">}</span><span class="s2">&#39; attr &#39;</span><span class="si">{</span><span class="n">arg_name</span><span class="si">}</span><span class="s2">&#39; should be an positive int number or a tuple of two &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;or four positive int numbers, but got </span><span class="si">{</span><span class="n">arg_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">ret_value</span>

        <span class="k">if</span> <span class="n">data_format</span> <span class="o">==</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, NHWC format is not supported at present.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">pad_mode</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;VALID&#39;</span><span class="p">,</span> <span class="s1">&#39;SAME&#39;</span><span class="p">,</span> <span class="s1">&#39;valid&#39;</span><span class="p">,</span> <span class="s1">&#39;same&#39;</span><span class="p">],</span> <span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pad_mode&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">_check_format_stride_or_dilation</span><span class="p">(</span><span class="s2">&quot;stride&quot;</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">is_in_range</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="k">return</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">255</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_in_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">is_in_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;For Dilation2D, size of stride is not supported, &#39;</span>
                             <span class="sa">f</span><span class="s1">&#39;stride should be in the range of [1, 255], &#39;</span>
                             <span class="sa">f</span><span class="s1">&#39;but got stride_h: `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="si">}</span><span class="s1">`, stride_w: `</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="si">}</span><span class="s1">`.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_format_stride_or_dilation</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>


<div class="viewcode-block" id="SoftShrink"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.SoftShrink.html#mindspore.ops.SoftShrink">[文档]</a><span class="k">class</span> <span class="nc">SoftShrink</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies the SoftShrink function element-wise.</span>

<span class="sd">    Refer to :func:`mindspore.ops.softshrink` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        lambd(float, optional): The :math:`\lambda` must be no less than zero. Default: ``0.5`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - The input of soft shrink with data type of float16 or float32.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same shape and data type as `input_x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[ 0.5297,  0.7871,  1.1754], [ 0.7836,  0.6218, -1.1542]]), mindspore.float16)</span>
<span class="sd">        &gt;&gt;&gt; softshrink = ops.SoftShrink()</span>
<span class="sd">        &gt;&gt;&gt; output = softshrink(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[ 0.02979  0.287    0.676  ]</span>
<span class="sd">         [ 0.2837   0.1216  -0.6543 ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SoftShrink&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;lambd&quot;</span><span class="p">,</span> <span class="n">lambd</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;lambd&quot;</span><span class="p">,</span> <span class="n">lambd</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<div class="viewcode-block" id="ApplyAdagradDA"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdagradDA.html#mindspore.ops.ApplyAdagradDA">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdagradDA</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update `var` according to the proximal adagrad scheme.</span>
<span class="sd">    The Adagrad algorithm was proposed in</span>
<span class="sd">    `Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</span>
<span class="sd">    &lt;http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf&gt;`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            grad\_accum += grad \\</span>
<span class="sd">            grad\_squared\_accum += grad * grad \\</span>
<span class="sd">            tmp\_val=</span>
<span class="sd">                \begin{cases}</span>
<span class="sd">                     sign(grad\_accum) * max\left \{|grad\_accum|-l1*global\_step, 0\right \} &amp; \text{ if } l1&gt;0 \\</span>
<span class="sd">                     grad\_accum &amp; \text{ otherwise } \\</span>
<span class="sd">                 \end{cases} \\</span>
<span class="sd">            x\_value = -1 * lr * tmp\_val \\</span>
<span class="sd">            y\_value = l2 * global\_step * lr + \sqrt{grad\_squared\_accum} \\</span>
<span class="sd">            var = \frac{ x\_value }{ y\_value }</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `gradient_accumulator`, `gradient_squared_accumulator` and `grad`</span>
<span class="sd">    comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If ``True`` , updating of the `var` and `accum` tensors will be protected by a lock.</span>
<span class="sd">                            Otherwise the behavior is undefined, but may exhibit less contention. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **gradient_accumulator** (Parameter) - The dict of mutable tensor :math:`grad\_accum`. Must have the same</span>
<span class="sd">          shape as `var`.</span>
<span class="sd">        - **gradient_squared_accumulator** (Parameter) - The dict of mutable tensor :math:`grad\_squared\_accum`.</span>
<span class="sd">          Must have the same shape as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. Must have the same shape as `var`.</span>
<span class="sd">        - **lr** ([Number, Tensor]) - Scaling factor. Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **l1** ([Number, Tensor]) -  L1 regularization. Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **l2** ([Number, Tensor]) -  L2 regularization. Must be a scalar. With float32 or float16 data type.</span>
<span class="sd">        - **global_step** ([Number, Tensor]) - Training step number. Must be a scalar. With int32 or int64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 1 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `gradient_accumulator` or `gradient_squared_accumulator` is not a Parameter.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `global_step` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If use_locking is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `gradient_accumulator`, `gradient_squared_accumulator`, `grad`,</span>
<span class="sd">                   `lr`, `l1` or `l2` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `gradient_accumulator`, `gradient_squared_accumulator` or `grad` is not same as `var`.</span>
<span class="sd">        TypeError: If dtype of `global_step` is not int32 nor int64.</span>
<span class="sd">        ValueError: If the shape size of `lr`, `l1`, `l2` and `global_step` is not 0.</span>
<span class="sd">        TypeError: If the data type of `var`, `gradient_accumulator`, `gradient_squared_accumulator` and `grad`</span>
<span class="sd">                      conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, nn, ops, Parameter</span>
<span class="sd">        &gt;&gt;&gt; class ApplyAdagradDANet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, use_locking=False):</span>
<span class="sd">        ...         super(ApplyAdagradDANet, self).__init__()</span>
<span class="sd">        ...         self.apply_adagrad_d_a = ops.ApplyAdagradDA(use_locking)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.4], [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.gradient_accumulator = Parameter(Tensor(np.array([[0.1, 0.3],</span>
<span class="sd">        ...                                                                [0.1, 0.5]]).astype(np.float32)),</span>
<span class="sd">        ...                                               name=&quot;gradient_accumulator&quot;)</span>
<span class="sd">        ...         self.gradient_squared_accumulator = Parameter(Tensor(np.array([[0.2, 0.1],</span>
<span class="sd">        ...                                                                        [0.1, 0.2]]).astype(np.float32)),</span>
<span class="sd">        ...                                                       name=&quot;gradient_squared_accumulator&quot;)</span>
<span class="sd">        ...         self.gradient_accumulator = Parameter(Tensor(np.array([[0.1, 0.3],</span>
<span class="sd">        ...                                                                [0.1, 0.5]]).astype(np.float32)),</span>
<span class="sd">        ...                                               name=&quot;gradient_accumulator&quot;)</span>
<span class="sd">        ...     def construct(self, grad, lr, l1, l2, global_step):</span>
<span class="sd">        ...         out = self.apply_adagrad_d_a(self.var, self.gradient_accumulator,</span>
<span class="sd">        ...                                      self.gradient_squared_accumulator, grad, lr, l1, l2, global_step)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyAdagradDANet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.4], [0.1, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l1 = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l2 = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; global_step = Tensor(2, mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad, lr, l1, l2, global_step)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[-7.39064650e-04, -1.36888528e-03],</span>
<span class="sd">         [-5.96988888e-04, -1.42478070e-03]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient_accumulator&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;gradient_squared_accumulator&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;global_step&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T4</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdagradDA&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;side_effect_mem&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">SparseApplyRMSProp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update relevant entries according to the rmsprop algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            ms = rho * ms_{t-1} + (1 - rho) * grad * grad \\</span>
<span class="sd">            mom = momentum * mom_{t-1} + lr * grad / sqrt(ms + epsilon) \\</span>
<span class="sd">            var = var - mom</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `ms`, `mom` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        rho (float): Decay rate. The value should be between 0 and 1, otherwise the behavior is undefined.</span>
<span class="sd">        momentum (float): Momentum. The value should be greater or equal to 0, otherwise the behavior is undefined.</span>
<span class="sd">        epsilon (float): A small value added for numerical stability. The value should be greater than 0,</span>
<span class="sd">                         otherwise the behavior is undefined.</span>
<span class="sd">        use_locking (bool): If ``True`` , updating of the var, ms, and mom tensors are protected by a lock;</span>
<span class="sd">                            otherwise the behavior is undefined, but may exhibit less contention. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type must be float16 or float32.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **ms** (Parameter) - The dict of mutable tensor ms. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **mom** (Parameter) - The dict of mutable tensor mom. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **lr** ([Number, Tensor]) - Learning rate. Must be a scalar. With float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var`, `ms` and `mom`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64 and indices.shape[0] = var.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 3 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) -  The same shape and data type as `var`.</span>
<span class="sd">        - **ms** (Tensor) - The same shape and data type as `ms`.</span>
<span class="sd">        - **mom** (Tensor) - The same shape and data type as `mom`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `ms` or `mom` is not a Parameter.</span>
<span class="sd">        TypeError: If `grad` or `indices` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `var`, `ms`, `mom`, `lr`, `grad` is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        TypeError: If `lr` is neither a Number or a Tensor.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `epsilon`, `rho`, `momentum` is not a float.</span>
<span class="sd">        ValueError: If shape of `ms`, `mom`, `grad` is not same as `var`.</span>
<span class="sd">        ValueError: If the shape size of `lr` is not 0.</span>
<span class="sd">        ValueError: If shape of `indices` is not same as shape of first dimension of `var`.</span>
<span class="sd">        ValueError: If `epsilon` is less than or equal to 0.</span>
<span class="sd">        ValueError: If `momentum` is less than 0.</span>
<span class="sd">        ValueError: If `rho` is less than 0 or greater than 1.</span>
<span class="sd">        ValueError: If dimension of `var` is less than 1.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `ms`, `mom` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``  ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class SparseApplyRMSPropNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, rho, momentum, epsilon, use_locking=False):</span>
<span class="sd">        ...         super(SparseApplyRMSPropNet, self).__init__()</span>
<span class="sd">        ...         self.sparse_apply_r_m_s_prop = P.SparseApplyRMSProp(rho, momentum, epsilon, use_locking)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.6, 0.3], [0.1, 0.5]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.ms = Parameter(Tensor(np.array([[0.2, 0.4], [0.1, 0.3]]).astype(np.float32)), name=&quot;ms&quot;)</span>
<span class="sd">        ...         self.mom = Parameter(Tensor(np.array([[0.3, 0.1], [0.3, 0.6]]).astype(np.float32)), name=&quot;mom&quot;)</span>
<span class="sd">        ...     def construct(self, lr, grad, indices):</span>
<span class="sd">        ...         out = self.sparse_apply_r_m_s_prop(self.var, self.ms, self.mom, lr, grad, indices)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; rho = 0.2</span>
<span class="sd">        &gt;&gt;&gt; momentum = 0.01</span>
<span class="sd">        &gt;&gt;&gt; epsilon = 1e-6</span>
<span class="sd">        &gt;&gt;&gt; net = SparseApplyRMSPropNet(rho, momentum, epsilon)</span>
<span class="sd">        &gt;&gt;&gt; lr = 0.01</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.7], [0.1, 0.8]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1], dtype=np.int32))</span>
<span class="sd">        &gt;&gt;&gt; out = net(lr, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 5.88035822e-01,  2.88811117e-01],</span>
<span class="sd">         [ 9.10239667e-02,  4.83422279e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.12000003e-01,  4.72000003e-01],</span>
<span class="sd">         [ 2.80000009e-02,  5.72000027e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.19641740e-02,  1.11888833e-02],</span>
<span class="sd">         [ 8.97603668e-03,  1.65777095e-02]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;ms&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mom&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;&quot;Initialize SparseApplyRMSProp&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;rho&quot;</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;momentum&quot;</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_number</span><span class="p">(</span><span class="s2">&quot;momentum&quot;</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rho</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="n">rho</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_BOTH</span><span class="p">,</span> <span class="s2">&quot;rho&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SparseApplyCenteredRMSProp</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update `var` according to the centered RMSProp algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{l}</span>
<span class="sd">            \text { mean_square }=\text { decay } * \text { mean_square }+(1-\text { decay }) *</span>
<span class="sd">            \text { gradient }^{2} \\</span>
<span class="sd">            \text { mean_grad }=\text { decay } * \text { mean_grad }+(1-\text { decay }) *</span>
<span class="sd">            \text { gradient } \\</span>
<span class="sd">            \text { Delta }=l r * \frac{\text { gradient }}{\sqrt{\text { mean_square }+</span>
<span class="sd">            \text { epsilon-mean_grad }^{2}}} \\</span>
<span class="sd">            \text { ms }&lt;-\text { rho } * \text { ms }_{t-1}+(1-\text { rho }) * \text { grad } * \text { grad } \\</span>
<span class="sd">            \text { mom }&lt;-\text { momentum } * \text { mom }_{t-1}+\operatorname{lr} *</span>
<span class="sd">            \frac{\text { grad }}{\sqrt{\text { ms+epsilon }}} \\</span>
<span class="sd">            \text { var }&lt;-\text { var }-\text { mom }</span>
<span class="sd">        \end{array}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        In dense implementation of this algorithm, `mean_gradient`, `mean_square`, and `moment` will update</span>
<span class="sd">        even if the `grad` is zero. But in this sparse implementation, `mean_gradient`, `mean_square`, and `moment`</span>
<span class="sd">        will not update in iterations during which the `grad` is zero.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If ``True`` , updating of the `var`, `mg`, `ms`, and `mom` tensors will be protected by a</span>
<span class="sd">                            lock. Otherwise the behavior is undefined, but may exhibit less contention.</span>
<span class="sd">                            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be int8, int16, int32, int64,</span>
<span class="sd">          uint8, uint16, uint32, uint64, float16, float32 or float64.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **mg** (Parameter) - Mean gradients. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **ms** (Parameter) - Mean square gradients. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **mom** (Parameter) - Delta of `var`. Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Learning rate. Must be a float number or a scalar tensor.</span>
<span class="sd">          Must have the same type as `var`.</span>
<span class="sd">        - **rho** (Union[Number, Tensor]) - Decay rate. Must be a float number or a scalar tensor.</span>
<span class="sd">          Must have the same type as `var`.</span>
<span class="sd">        - **momentum** (Union[Number, Tensor]) - Momentum. Must be a float number or a scalar tensor.</span>
<span class="sd">          Must have the same type as `var`.</span>
<span class="sd">        - **epsilon** (Union[Number, Tensor]) - Ridge term. Must be a float number or a scalar tensor.</span>
<span class="sd">          Must have the same type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var` and grad.shape[1:] = var.shape[1:] if rank(var) &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - Gradient indices. Must be one of the following types: int32, int64.</span>
<span class="sd">          and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and data type as `var`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If `var`, `mg`, `ms`, `mom`, `grad`, `indices` is not a Tensor.</span>
<span class="sd">        TypeError: If `lr`, `rho`, `momentum` or `epsilon` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `var`, `mg`, `ms`, `mom`, `lr`, `rho`, `momentum`, `epsilon` or `grad`</span>
<span class="sd">                   is neither float16 nor float32.</span>
<span class="sd">        TypeError: If dtype of `mg`, `ms`, `mom`, `grad` is not same as `var`.</span>
<span class="sd">        TypeError: If dtype of `indices` is not int32 or int64.</span>
<span class="sd">        ValueError: If shape of `mg`, `ms` or `mom` is not same as `var`.</span>
<span class="sd">        ValueError: If the rank of `indices` is not equal to 1.</span>
<span class="sd">        ValueError: If dimension of `grad` is not equal or greater than 1.</span>
<span class="sd">        ValueError: If shape of `indices` is not same as shape of first dimension of `grad`.</span>
<span class="sd">        ValueError: If shape of `grad` is not same as shape of `var` except first dimension.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; var = Tensor(np.array([[0.6, 0.4], [0.1, 0.5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; mg = Tensor(np.array([[0.1, 0.3], [0.1, 0.5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; ms = Tensor(np.array([[0.2, 0.1], [0.1, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; mom = Tensor(np.array([[0.2, 0.1], [0.1, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; rho = Tensor(1e-10, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; momentum = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; epsilon = Tensor(0.01, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.4], [0.1, 0.2]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; sparse_apply_centered_rms_prop = nn_ops.SparseApplyCenteredRMSProp()</span>
<span class="sd">        &gt;&gt;&gt; output = sparse_apply_centered_rms_prop(var, mg, ms, mom, lr, rho, momentum, epsilon, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[0.5968 0.3959]</span>
<span class="sd">         [0.0989 0.4978]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mg&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;ms&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;mom&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyCenteredRMSProp.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;mg&#39;</span><span class="p">,</span> <span class="s1">&#39;ms&#39;</span><span class="p">,</span> <span class="s1">&#39;mom&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;rho&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ApplyKerasMomentum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update `var` according to the momentum scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum = accum * momentum - grad * lr \\</span>
<span class="sd">            var =</span>
<span class="sd">            \begin{cases}</span>
<span class="sd">                var + accum * momentum - grad * lr, &amp;\text{if use_nesterov} \\</span>
<span class="sd">                var + accum, &amp;\text{else}</span>
<span class="sd">            \end{cases}</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Refer to the paper `On the importance of initialization and momentum in deep</span>
<span class="sd">    learning &lt;https://dl.acm.org/doi/10.5555/3042817.3043064&gt;`_  for more details.</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    relatively highest priority data type.</span>
<span class="sd">    RuntimeError exception will be thrown when the data type conversion of Parameter is required.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If ``True`` , updating of the `var` and `accum` tensors will be protected by a lock;</span>
<span class="sd">                            Otherwise the behavior is undefined, but may exhibit less contention. Default: ``False`` .</span>
<span class="sd">        use_nesterov (bool): If ``True`` , the tensor passed to compute grad will be var + momentum * accum,</span>
<span class="sd">                            so in the end, the var you get is actually var + momentum * accum. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. With float16 or float32 data type.</span>
<span class="sd">        - **accum** (Parameter) - Must have the same shape and type as `var`. With float16 or float32 data type.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Scaling factor. Must be a scalar. With float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - The gradient. Must have the same shape and type as `var`.</span>
<span class="sd">          With float16 or float32 data type.</span>
<span class="sd">        - **momentum** (Union[Number, Tensor]) - Momentum. Must be a scalar. With float16 or float32 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **accum** (Tensor) - The same shape and data type as `accum`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If the use_locking or use_nesterov is not a bool.</span>
<span class="sd">        TypeError: If `var` or `accum` is not a Parameter.</span>
<span class="sd">        TypeError: If `lr` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If `momentum` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr`, `grad`, `momentum` is neither float16 nor float32.</span>
<span class="sd">        ValueError: If `accum` or `grad` doesn&#39;t have the same shape as `var`.</span>
<span class="sd">        ValueError: If the shape size of `lr`, `momentum` is not 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class ApplyKerasMomentumNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, use_locking=False, use_nesterov=False):</span>
<span class="sd">        ...         super(ApplyKerasMomentumNet, self).__init__()</span>
<span class="sd">        ...         self.apply_keras_momentum = P.ApplyKerasMomentum(use_locking, use_nesterov)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2, 0.3], [0.1, 0.4]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.accum = Parameter(Tensor(np.array([[0.2, 0.3], [0.1, 0.4]]).astype(np.float32)), name=&quot;accum&quot;)</span>
<span class="sd">        ...     def construct(self, lr, grad, momentum):</span>
<span class="sd">        ...         out = self.apply_keras_momentum(self.var, self.accum, lr, grad, momentum)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        ...</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyKerasMomentumNet()</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.001, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.2], [0.4, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; momentum = Tensor(0.99, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = net(lr, grad, momentum)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 3.97700012e-01,  5.96800029e-01],</span>
<span class="sd">        [ 1.98599994e-01,  7.95899987e-01]]), Tensor(shape=[2, 2], dtype=Float32, value=</span>
<span class="sd">        [[ 1.97699994e-01,  2.96800017e-01],</span>
<span class="sd">        [ 9.86000001e-02,  3.95900011e-01]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyKerasMomentum&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="MultilabelMarginLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MultilabelMarginLoss.html#mindspore.ops.MultilabelMarginLoss">[文档]</a><span class="k">class</span> <span class="nc">MultilabelMarginLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a loss criterion that minimizes the hinge loss for multi-class</span>
<span class="sd">    classification tasks.</span>
<span class="sd">    It takes a 2D mini-batch Tensor :math:`x` as input and a 2D</span>
<span class="sd">    Tensor :math:`y` containing target class indices as output.</span>

<span class="sd">    Refer to :func:`mindspore.ops.multilabel_margin_loss` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Predict data. Tensor of shape :math:`(C)` or :math:`(N, C)`, where :math:`N`</span>
<span class="sd">          is the batch size and :math:`C` is the number of classes. Data type must be float16 or float32.</span>
<span class="sd">        - **target** (Tensor) - Ground truth data, with the same shape as `input`, data type must be int32 and</span>
<span class="sd">          label targets padded by -1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Union[Tensor, Scalar]) - The loss of MultilabelMarginLoss. If `reduction` is ``&quot;none&quot;``, its shape</span>
<span class="sd">          is :math:`(N)`. Otherwise, a scalar value will be returned.</span>
<span class="sd">        - **is_target** (Tensor) - Output tensor for backward input, with the same shape as `target`,</span>
<span class="sd">          data type must be int32.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">       &gt;&gt;&gt; import mindspore</span>
<span class="sd">       &gt;&gt;&gt; import numpy as np</span>
<span class="sd">       &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">       &gt;&gt;&gt; loss = ops.MultilabelMarginLoss()</span>
<span class="sd">       &gt;&gt;&gt; x = Tensor(np.array([[0.1, 0.2, 0.4, 0.8], [0.2, 0.3, 0.5, 0.7]]), mindspore.float32)</span>
<span class="sd">       &gt;&gt;&gt; target = Tensor(np.array([[1, 2, 0, 3], [2, 3, -1, 1]]), mindspore.int32)</span>
<span class="sd">       &gt;&gt;&gt; output = loss(x, target)</span>
<span class="sd">       &gt;&gt;&gt; print(output)</span>
<span class="sd">       (Tensor(shape=[], dtype=Float32, value= 0.325), Tensor(shape=[2, 4], dtype=Int32, value=</span>
<span class="sd">       [[1, 1, 1, 1], [0, 0, 1, 1]]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MultilabelMarginLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;is_target&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">ApplyAdamWithAmsgrad</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update var according to the Adam algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{l1} \\</span>
<span class="sd">            lr_t:=learning\_rate*\sqrt{1-\beta_2^t}/(1-\beta_1^t) \\</span>
<span class="sd">            m_t:=\beta_1*m_{t-1}+(1-\beta_1)*g \\</span>
<span class="sd">            v_t:=\beta_2*v_{t-1}+(1-\beta_2)*g*g \\</span>
<span class="sd">            \hat v_t:=max(\hat v_{t-1}, v_t) \\</span>
<span class="sd">            var:=var-lr_t*m_t/(\sqrt{\hat v_t}+\epsilon) \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `m`, `v`, `vhat` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Inputs of `beta1_power`, `beta1`, `beta2` and `epsilon` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    However, note that there is no implicit type conversion rule between `var` and `beta1_power`;</span>
<span class="sd">    the two sets of rules are independent of each other.</span>

<span class="sd">    Args:</span>
<span class="sd">        beta1 (float): A Tensor. Must have the same type as beta1_power. Momentum factor. Must be a scalar.</span>
<span class="sd">        beta2 (float): A Tensor. Must have the same type as beta1_power. Momentum factor. Must be a scalar.</span>
<span class="sd">        epsilon (float): A Tensor. Must have the same type as beta1_power. Ridge term. Must be a scalar.</span>
<span class="sd">        use_locking (bool): use_locking: If ``True`` , updating of the `var`, `m`, and `v` tensors will</span>
<span class="sd">          be protected by a lock; Otherwise the behavior is undefined, but may exhibit less contention.</span>
<span class="sd">          Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type can be float16 or float32.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`.</span>
<span class="sd">        - **v** (Parameter) - the 2nd moment vector in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`.</span>
<span class="sd">        - **vhat** (Parameter) - :math:`\hat v_t` in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`.</span>
<span class="sd">        - **beta1_power** (Union[float, Tensor]) - :math:`beta_1^t(\beta_1^{t})` in the updating formula,</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **beta2_power** (Union[float, Tensor]) - :math:`beta_2^t(\beta_2^{t})` in the updating formula,</span>
<span class="sd">          a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **lr** (Union[float, Tensor]) - Scaling factor, a scalar tensor with float16 or float32 data type.</span>
<span class="sd">        - **grad** (Tensor) - The gradient, has the same shape and data type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 4 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>
<span class="sd">        - **vhat** (Tensor) - The same shape and data type as `vhat`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `m`, `v`, `vhat` is not a Parameter.</span>
<span class="sd">        TypeError: If `beta1_power`, `beta2_power`, `lr` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `vhat`, `beta1_power`, `beta2_power`,</span>
<span class="sd">          `lr`, `grad`, `momentum` is not float32 or float16.</span>
<span class="sd">        ValueError: If `m` or `v` or `vhat` or `grad` doesn&#39;t have the same shape of `var`.</span>
<span class="sd">        ValueError: If the shape of `beta1_power`, `beta2_power`, `lr` is not 0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; class ApplyAdamWithAmsgradNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, beta1=0.9, beta2=0.999, epsilon=1e-8, use_locking=False):</span>
<span class="sd">        ...         super(ApplyAdamWithAmsgradNet, self).__init__()</span>
<span class="sd">        ...         self.apply_adam_with_amsgrad = P.ApplyAdamWithAmsgrad(beta1, beta2, epsilon, use_locking)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2, 0.2], [0.2, 0.2]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.1, 0.2], [0.4, 0.3]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.array([[0.2, 0.1], [0.3, 0.4]]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...         self.vhat = Parameter(Tensor(np.array([[0.1, 0.2], [0.6, 0.2]]).astype(np.float32)), name=&quot;vhat&quot;)</span>
<span class="sd">        ...     def construct(self, beta1_power, beta2_power, lr, grad):</span>
<span class="sd">        ...         out = self.apply_adam_with_amsgrad(self.var, self.m, self.v, self.vhat,</span>
<span class="sd">        ...                                            beta1_power, beta2_power, lr, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyAdamWithAmsgradNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.4, 0.2], [0.2, 0.3]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(Tensor(0.9, mstype.float32), Tensor(0.999, mstype.float32), Tensor(0.01, mstype.float32), grad)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.19908068 0.1985858 ]</span>
<span class="sd">        [0.19844866 0.19849943]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;vhat&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T3</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdamWithAmsgrad&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;beta1&quot;</span><span class="p">,</span> <span class="n">beta1</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;beta2&quot;</span><span class="p">,</span> <span class="n">beta2</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>


<div class="viewcode-block" id="ApplyAdamWithAmsgradV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ApplyAdamWithAmsgradV2.html#mindspore.ops.ApplyAdamWithAmsgradV2">[文档]</a><span class="k">class</span> <span class="nc">ApplyAdamWithAmsgradV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update var according to the Adam algorithm.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{l1} \\</span>
<span class="sd">            lr_t:=learning\_rate*\sqrt{1-\beta_2^t}/(1-\beta_1^t) \\</span>
<span class="sd">            m_t:=\beta_1*m_{t-1}+(1-\beta_1)*g \\</span>
<span class="sd">            v_t:=\beta_2*v_{t-1}+(1-\beta_2)*g*g \\</span>
<span class="sd">            \hat v_t:=\max(\hat v_{t-1}, v_t) \\</span>
<span class="sd">            var:=var-lr_t*m_t/(\sqrt{\hat v_t}+\epsilon) \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    :math:`t` represents updating step while :math:`m` represents the 1st moment vector,</span>
<span class="sd">    :math:`v` represents the 2nd moment vector,  :math:`\hat v_t` represents `vhat`,</span>
<span class="sd">    :math:`lr` represents learning rate,</span>
<span class="sd">    :math:`g` represents `grad`, :math:`\beta_1, \beta_2` represent `beta1` and `beta2`,</span>
<span class="sd">    :math:`\beta_1^{t}` represents `beta1_power`, :math:`\beta_2^{t}` represents `beta2_power`,</span>
<span class="sd">    :math:`var` represents the variable to be updated,</span>
<span class="sd">    :math:`\epsilon` represents `epsilon`.</span>

<span class="sd">    All of the inputs are consistent with implicit type conversion rules,</span>
<span class="sd">    which ensure that the data types are the same. If they have different data types, the lower precision data type</span>
<span class="sd">    will be converted to the data type with relatively higher precision.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If ``True`` , updating of the `var`, `m`, and `v` tensors will</span>
<span class="sd">            be protected by a lock; Otherwise the behavior is undefined, but may exhibit less contention.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated. The data type can be float16, float32 or float64.</span>
<span class="sd">        - **m** (Parameter) - The 1st moment vector in the updating formula,</span>
<span class="sd">          the shape should be the same as `var`.</span>
<span class="sd">        - **v** (Parameter) - The 2nd moment vector in the updating formula,</span>
<span class="sd">          the shape should be the same as `var`.</span>
<span class="sd">        - **vhat** (Parameter) - :math:`\hat v_t` in the updating formula,</span>
<span class="sd">          the shape and data type value should be the same as `var`.</span>
<span class="sd">        - **beta1_power** (Union[float, Tensor]) - :math:`beta_1^t(\beta_1^{t})` in the updating formula,</span>
<span class="sd">          with float16, float32 or float64 data type.</span>
<span class="sd">        - **beta2_power** (Union[float, Tensor]) - :math:`beta_2^t(\beta_2^{t})` in the updating formula,</span>
<span class="sd">          with float16, float32 or float64 data type.</span>
<span class="sd">        - **lr** (Union[float, Tensor]) - Learning rate, with float16, float32 or float64 data type.</span>
<span class="sd">        - **beta1** (Union[float, Tensor]) - Exponential decay rate of the first moment.</span>
<span class="sd">          The data type can be float16, float32 or float64.</span>
<span class="sd">        - **beta2** (Union[float, Tensor]) - Exponential decay rate of the second moment.</span>
<span class="sd">          The data type can be float16, float32 or float64.</span>
<span class="sd">        - **epsilon** (Union[float, Tensor]) - A value added to the denominator to ensure numerical stability.</span>
<span class="sd">          The data type can be float16, float32 or float64.</span>
<span class="sd">        - **grad** (Tensor) - The gradient, has the same shape as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 4 Tensors, the updated parameters.</span>

<span class="sd">        - **var** (Tensor) - The same shape and data type as `var`.</span>
<span class="sd">        - **m** (Tensor) - The same shape and data type as `m`.</span>
<span class="sd">        - **v** (Tensor) - The same shape and data type as `v`.</span>
<span class="sd">        - **vhat** (Tensor) - The same shape and data type as `vhat`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `m`, `v`, `vhat` is not a Parameter.</span>
<span class="sd">        TypeError: If dtype of `var`, `m`, `v`, `vhat`, `beta1_power`, `beta2_power`,</span>
<span class="sd">            `lr`, `beta1` , `beta2` , `epsilon` or `grad` is not float64, float32 or float16.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `m`, `v` , `vhat` and `grad` conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops as ops</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.nn as nn</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, Parameter</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; class ApplyAdamWithAmsgradNet(nn.Cell):</span>
<span class="sd">        ...     def __init__(self, use_locking=False):</span>
<span class="sd">        ...         super(ApplyAdamWithAmsgradNet, self).__init__()</span>
<span class="sd">        ...         self.apply_adam_with_amsgrad = ops.ApplyAdamWithAmsgradV2(use_locking)</span>
<span class="sd">        ...         self.var = Parameter(Tensor(np.array([[0.2, 0.2], [0.2, 0.2]]).astype(np.float32)), name=&quot;var&quot;)</span>
<span class="sd">        ...         self.m = Parameter(Tensor(np.array([[0.1, 0.2], [0.4, 0.3]]).astype(np.float32)), name=&quot;m&quot;)</span>
<span class="sd">        ...         self.v = Parameter(Tensor(np.array([[0.2, 0.1], [0.3, 0.4]]).astype(np.float32)), name=&quot;v&quot;)</span>
<span class="sd">        ...         self.vhat = Parameter(Tensor(np.array([[0.1, 0.2], [0.6, 0.2]]).astype(np.float32)), name=&quot;vhat&quot;)</span>
<span class="sd">        ...         self.beta1 = 0.8</span>
<span class="sd">        ...         self.beta2 = 0.999</span>
<span class="sd">        ...         self.epsilon = 1e-8</span>
<span class="sd">        ...         self.beta1_power = 0.9</span>
<span class="sd">        ...         self.beta2_power = 0.999</span>
<span class="sd">        ...         self.lr = 0.01</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def construct(self, grad):</span>
<span class="sd">        ...         out = self.apply_adam_with_amsgrad(self.var, self.m, self.v, self.vhat,</span>
<span class="sd">        ...                                            self.beta1_power, self.beta2_power, self.lr,</span>
<span class="sd">        ...                                            self.beta1, self.beta2, self.epsilon, grad)</span>
<span class="sd">        ...         return out</span>
<span class="sd">        &gt;&gt;&gt; net = ApplyAdamWithAmsgradNet()</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.4, 0.2], [0.2, 0.3]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; output = net(grad)</span>
<span class="sd">        &gt;&gt;&gt; print(net.var.asnumpy())</span>
<span class="sd">        [[0.19886853 0.1985858 ]</span>
<span class="sd">        [0.19853032 0.19849943]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;m&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;vhat&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2_power&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;beta2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ApplyAdamWithAmsgradv2&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;side_effect_mem&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">FractionalMaxPool</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs fractional max pooling on the input.</span>

<span class="sd">    Fractional max pooling is similar to regular max pooling, but with the added flexibility of</span>
<span class="sd">    allowing the overall reduction ratio `N` to be a non-integer value. In regular max pooling,</span>
<span class="sd">    an input set is reduced in size by taking the maximum value of  `N x N` (usually 2x2)</span>
<span class="sd">    subsections of the set, with the goal of reducing the set by a factor of `N`, where `N` is an integer.</span>

<span class="sd">    In contrast, fractional max pooling uses randomly generated pool sizes that are fairly uniform in size.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        &quot;pooling_ratio&quot;, currently only supports row and col dimension and should be &gt;= 1.0, the first</span>
<span class="sd">        and last elements must be 1.0 because pooling on batch and channels dimensions is not allowed.</span>

<span class="sd">    Args:</span>
<span class="sd">        pooling_ratio (list(float)): Decide the shape of output, is a list of float numbers has length &gt;= 4.</span>
<span class="sd">            Pooling ratio for each dimension of value should not be less than 0, currently only support</span>
<span class="sd">            for row and col dimension.</span>
<span class="sd">        pseudo_random(bool, optional): Generate the pooling sequence either randomly or pseudo-randomly.</span>
<span class="sd">            If the pseudo_random parameter is set to ``True`` , the sequence will be generated in a</span>
<span class="sd">            pseudo-random fashion, otherwise it will be generated randomly.</span>
<span class="sd">            Refer to `Fractional Max-Pooling  &lt;https://arxiv.org/pdf/1412.6071&gt;`_</span>
<span class="sd">            by Benjamin Graham to understand the distinction between the two.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        overlapping(bool, optional): When set to ``True`` , the values at the boundary of adjacent pooling cells</span>
<span class="sd">            will be shared by both cells during pooling process. When set to ``False`` , the values are not reused.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        deterministic(bool, optional): If deterministic is set to ``True`` , a fixed pooling region will be used</span>
<span class="sd">            in the computation graph, ensuring that the FractionalMaxPool is deterministic.</span>
<span class="sd">            This is often used in unit tests. When set to ``False`` , fixed pool regions will not be used.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        seed(int, optional): If either seed or seed2 are set to a non-zero value, the random number</span>
<span class="sd">            generator will be seeded using the specified seed. If neither seed nor seed2 are set,</span>
<span class="sd">            the generator will be seeded by a random seed.</span>
<span class="sd">            Default: ``0`` .</span>
<span class="sd">        seed2(int, optional): The second seed to avoid seed collision.</span>
<span class="sd">            Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) -The data type must be one of the following types: float32, float64, int32, int64.</span>
<span class="sd">          Tensor of shape :math:`(N, H_{in}, W_{in}, C_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - the output of FractionalMaxPool, has the same data type with `x`.</span>
<span class="sd">          Tensor of shape :math:`(N, H_{out}, W_{out}, C_{out})`.</span>

<span class="sd">        - **row_pooling_sequence** (Tensor) - A tensor of type int64, the result list of pool boundary rows.</span>

<span class="sd">        - **col_pooling_sequence** (Tensor) - A tensor of type int64, the result list of pool boundary cols.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` is not float32, float64, int32, int64.</span>
<span class="sd">        TypeError: If `x` is not a 4D tensor.</span>
<span class="sd">        ValueError: If element of `x` equals 0 or is less than 0.</span>
<span class="sd">        ValueError: If `pooling_ratio` is a list whose length is not equal to 4.</span>
<span class="sd">        ValueError: If the first and last element of `pooling_ratio` is not equal to 1.0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]).reshape([1,4,4,1]).astype(np.int64)</span>
<span class="sd">        &gt;&gt;&gt; pooling_ratio=[1.0,1.5,1.5,1.0]</span>
<span class="sd">        &gt;&gt;&gt; fractionalmaxpool_op = ops.FractionalMaxPool(pooling_ratio=pooling_ratio)</span>
<span class="sd">        &gt;&gt;&gt; output = fractionalmaxpool_op(Tensor(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2, 2, 1], dtype=Int64, value=</span>
<span class="sd">        [[[[ 6],</span>
<span class="sd">           [ 8]],</span>
<span class="sd">          [[14],</span>
<span class="sd">           [16]]]]), Tensor(shape=[3], dtype=Int64, value= [0, 2, 4]), Tensor(shape=[3], dtype=Int64, value= [0, 2, 4]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooling_ratio</span><span class="p">,</span> <span class="n">pseudo_random</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">overlapping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed2</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FractionalMaxPool.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;row_pooling_sequence&quot;</span><span class="p">,</span> <span class="s2">&quot;col_pooling_sequence&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pooling_ratio&#39;</span><span class="p">,</span> <span class="n">pooling_ratio</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pooling_ratio</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooling_ratio_item&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pseudo_random&quot;</span><span class="p">,</span> <span class="n">pseudo_random</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;overlapping&quot;</span><span class="p">,</span> <span class="n">overlapping</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;deterministic&quot;</span><span class="p">,</span> <span class="n">deterministic</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;seed2&quot;</span><span class="p">,</span> <span class="n">seed2</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="FractionalMaxPool3DWithFixedKsize"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.FractionalMaxPool3DWithFixedKsize.html#mindspore.ops.FractionalMaxPool3DWithFixedKsize">[文档]</a><span class="k">class</span> <span class="nc">FractionalMaxPool3DWithFixedKsize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D fractional max pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    The max-pooling operation is applied in :math:`(kD, kH, kW)` regions by a stochastic step size determined by</span>
<span class="sd">    the target output size `output_shape`.</span>

<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Refer to the paper `Fractional MaxPooling by Ben Graham &lt;https://arxiv.org/abs/1412.6071&gt;`_  for more details.</span>

<span class="sd">    The input and output data format can be &quot;NCDHW&quot; and &quot;NDHWC&quot;. N is the batch size, C is the number of channels,</span>
<span class="sd">    D the feature depth, H is the feature height, and W is the feature width.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[float, tuple]): Size of the pooling window. `ksize` can be a tuple of three values specify a</span>
<span class="sd">            shape :math:`(k_D, k_H, k_W)`, or a single int `K` for :math:`(K, K, K)`.</span>
<span class="sd">        output_shape (Union[int, tuple]): The target output shape. `output_shape` can be a tuple of three values</span>
<span class="sd">            specify a shape :math:`(D_{out}, H_{out}, W_{out})`, or a single float `S` for :math:`(S, S, S)`.</span>
<span class="sd">        data_format (str, optional): The optional value for data format.</span>
<span class="sd">            Currently support ``&#39;NCDHW&#39;`` and ``&#39;NHDWC&#39;`` . Default: ``&#39;NCDHW&#39;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The input of FractionalMaxPool3DWithFixedKsize, which is a 4D or 5D tensor.</span>
<span class="sd">          Tensor of data type : float16, float32, double, int32, int64.</span>
<span class="sd">          Supported shape :math:`(N, C, D_{in}, H_{in}, W_{in})` or :math:`(N, D_{in}, H_{in}, W_{in}, C)`.</span>
<span class="sd">        - **random_samples** (Tensor) - The random step of FractionalMaxPool3DWithFixedKsize, which is a 3D tensor.</span>
<span class="sd">          Tensor of data type : float16, float32, double, and value is between (0, 1).</span>
<span class="sd">          Supported shape :math:`(N, C, 3)`</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A tensor, the output of FractionalMaxPool3DWithFixedKsize.</span>
<span class="sd">          Has the same data type with `x`.</span>
<span class="sd">          Tensor of shape :math:`(N, C, D_{out}, H_{out}, W_{out})` or :math:`(N, D_{out}, H_{out}, W_{out}, C)`.</span>
<span class="sd">        - **argmax** (Tensor) - A tensor, the indices along with the outputs.</span>
<span class="sd">          Has the same shape as the `y` and int32 or int64 data type.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `input_x` is not a 4D or 5D tensor.</span>
<span class="sd">        TypeError: If `random_samples` is not a 3D tensor.</span>
<span class="sd">        TypeError: If data type of `x` is not float16, float32, double, int32, int64.</span>
<span class="sd">        TypeError: If dtype of `random_samples` is not float16, float32, double.</span>
<span class="sd">        TypeError: If dtype of `argmax` is not int32, int64.</span>
<span class="sd">        ValueError: If `output_shape` is a tuple and if `output_shape` length is not 3.</span>
<span class="sd">        ValueError: If `ksize` is a tuple and if `ksize` length is not 3.</span>
<span class="sd">        ValueError: If numbers in `output_shape` or `ksize` is not positive.</span>
<span class="sd">        ValueError: If `data_format` is neither &#39;NCDHW&#39; nor &#39;NDHWC&#39;.</span>
<span class="sd">        ValueError: If the first dimension size of `input_x` and `random_samples` is not equal.</span>
<span class="sd">        ValueError: If the second dimension size of `input_x` and `random_samples` is not equal.</span>
<span class="sd">        ValueError: If the third dimension size of `random_samples` is not 3.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])</span>
<span class="sd">        ...       .reshape([1, 1, 2, 2, 4]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; random_samples = Tensor(np.array([0.7, 0.7, 0.7]).reshape([1, 1, 3]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; ksize = (1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; output_shape = (1, 1, 2)</span>
<span class="sd">        &gt;&gt;&gt; net = ops.FractionalMaxPool3DWithFixedKsize(ksize = ksize, output_shape = output_shape)</span>
<span class="sd">        &gt;&gt;&gt; output, argmax = net(x, random_samples)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[13. 16.]]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[[12 15]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCDHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FractionalMaxPool3DWithFixedKsize.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;random_samples&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;argmax&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">ksize</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="p">(</span><span class="n">ksize</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">ksize</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;ksize&#39; must be an positive int number or a tuple of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;three positive int numbers, but got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span><span class="si">}</span><span class="s2"> numbers.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;ksize item&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCDHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NDHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">_check_3d_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span>
                                                   <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_five</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_five</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">FractionalAvgPool</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs fractional avg pooling on the input.</span>

<span class="sd">    Fractional avg pooling is similar to regular avg pooling, but with the added flexibility of</span>
<span class="sd">    allowing the overall reduction ratio `N` to be a non-integer value. In regular avg pooling,</span>
<span class="sd">    an input set is reduced in size by taking the average value of  `N x N` (usually 2x2)</span>
<span class="sd">    subsections of the set, with the goal of reducing the set by a factor of `N`, where `N` is an integer.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        &quot;pooling_ratio&quot;, currently only supports row and col dimension and should be &gt;= 1.0, the first</span>
<span class="sd">        and last elements must be 1.0 because we don&#39;t allow pooling on batch and channels dimensions.</span>

<span class="sd">    Args:</span>
<span class="sd">        pooling_ratio (list(float)): Decide the shape of output, is a list of floats that has length &gt;= 4.</span>
<span class="sd">            Pooling ratio for each dimension of value should be &gt;=0, currently only support for row and col</span>
<span class="sd">            dimension. The first and last elements must be 1.0 because we don&#39;t allow pooling on batch and</span>
<span class="sd">            channels dimensions.</span>
<span class="sd">        pseudo_random(bool, optional): Generate the pooling sequence either randomly or pseudo-randomly.</span>
<span class="sd">            If the pseudo_random parameter is set to ``True`` , the sequence will be generated in a</span>
<span class="sd">            pseudo-random fashion, otherwise it will be generated randomly.</span>
<span class="sd">            Refer to `Fractional Max-Pooling  &lt;https://arxiv.org/pdf/1412.6071&gt;`_</span>
<span class="sd">            by Benjamin Graham to understand the distinction between the two.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        overlapping(bool, optional): When set to ``True`` , the values at the boundary of adjacent pooling cells</span>
<span class="sd">            will be shared by both cells during pooling process. When set to ``False`` , the values are not reused.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        deterministic(bool, optional): If deterministic is set to ``True`` , a fixed pooling region will be used</span>
<span class="sd">            in the computation graph, ensuring that the FractionalAvgPool is deterministic.</span>
<span class="sd">            This is often used in unit tests. When set to ``False`` , fixed pool regions will not be used.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        seed(int, optional): If either seed or seed2 are set to a non-zero value, the random number</span>
<span class="sd">            generator will be seeded using the specified seed. If neither seed nor seed2 are set,</span>
<span class="sd">            the generator will be seeded by a random seed.</span>
<span class="sd">            Default: ``0`` .</span>
<span class="sd">        seed2(int, optional): The second seed to avoid seed collision.</span>
<span class="sd">            Default: ``0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) -The data type must be one of the following types: float32, float64, int32, int64.</span>
<span class="sd">          Tensor of shape :math:`(N, H_{in}, W_{in}, C_{in})`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - A tensor, the output of FractionalAvgPool, has the same data type with `x`.</span>
<span class="sd">          Tensor of shape :math:`(N, H_{out}, W_{out}, C_{out})`.</span>

<span class="sd">        - **row_pooling_sequence** (Tensor) - A tensor of type int64, the result list of pool boundary rows.</span>

<span class="sd">        - **col_pooling_sequence** (Tensor) - A tensor of type int64, the result list of pool boundary cols.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `x` is not float32, float64, int32, int64.</span>
<span class="sd">        TypeError: If `x` is not a 4D tensor.</span>
<span class="sd">        ValueError: If element of `x` equals 0 or is less than 0.</span>
<span class="sd">        ValueError: If `pooling_ratio` is a list whose length is not equal to 4.</span>
<span class="sd">        ValueError: If the first and last element of `pooling_ratio` is not equal to 1.0.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; x = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]).reshape([1,4,4,1]).astype(np.int64)</span>
<span class="sd">        &gt;&gt;&gt; pooling_ratio=[1.0,1.5,1.5,1.0]</span>
<span class="sd">        &gt;&gt;&gt; fractionalavgpool_op = ops.FractionalAvgPool(pooling_ratio=pooling_ratio)</span>
<span class="sd">        &gt;&gt;&gt; output = fractionalavgpool_op(Tensor(x))</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        (Tensor(shape=[1, 2, 2, 1], dtype=Int64, value=</span>
<span class="sd">        [[[[ 3],</span>
<span class="sd">           [ 5]],</span>
<span class="sd">          [[11],</span>
<span class="sd">           [13]]]]), Tensor(shape=[3], dtype=Int64, value= [0, 2, 4]), Tensor(shape=[3], dtype=Int64, value= [0, 2, 4]))</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pooling_ratio</span><span class="p">,</span> <span class="n">pseudo_random</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">overlapping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed2</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FractionalAvgPool.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;row_pooling_sequence&quot;</span><span class="p">,</span> <span class="s2">&quot;col_pooling_sequence&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pooling_ratio&#39;</span><span class="p">,</span> <span class="n">pooling_ratio</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pooling_ratio</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pooling_ratio_item&quot;</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;pseudo_random&quot;</span><span class="p">,</span> <span class="n">pseudo_random</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;overlapping&quot;</span><span class="p">,</span> <span class="n">overlapping</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;deterministic&quot;</span><span class="p">,</span> <span class="n">deterministic</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;seed&quot;</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;seed2&quot;</span><span class="p">,</span> <span class="n">seed2</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NthElement</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the n-th smallest values for the last dimension of the input Tensor.</span>

<span class="sd">    - When `input` is a 1-D Tensor (i.e. Vector), it finds the nth-smallest value in the vector</span>
<span class="sd">      and outputs its value as a scalar Tensor.</span>
<span class="sd">    - When `input` is matrices or has higher rank, it finds the nth-smallest value</span>
<span class="sd">      in each row (or vector along the last dimension) and outputs</span>
<span class="sd">      these values in a Tensor with shape of `values.shape = input.shape[:-1]`.</span>

<span class="sd">    Args:</span>
<span class="sd">        reverse (bool, optional): An optional bool. If set to ``True`` , it find the :math:`n`-th largest value</span>
<span class="sd">          in the vector instead of the nth-smallest. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input** (Tensor) - Input Tensor with 1-D or higher dimension.</span>
<span class="sd">        - **n** (Union[int, Tensor]) -  If the `n` is a Tensor, it should be a 0-D Tensor, dtype is int32.</span>
<span class="sd">          Valid range of `n` is :math:`[0, input.shape[-1])` where :math:`input.shape[-1]` is</span>
<span class="sd">          last dimension size of `input`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **values** (Tensor) - Its shape satisfies:  `values`.shape = `input`.shape[:-1].</span>
<span class="sd">          The dtype is the same as `input`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError**: If the type  of `input` is out of the valid list.</span>
<span class="sd">        TypeError**: If `n` is not int32 or not a Tensor.</span>
<span class="sd">        ValueError**: If n is out of :math:`[0, input.shape[-1])`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input = Tensor(np.array([[1,2,3],[4,5,6]]) , mstype.int8)</span>
<span class="sd">        &gt;&gt;&gt; n = 1</span>
<span class="sd">        &gt;&gt;&gt; net = ops.NthElement()</span>
<span class="sd">        &gt;&gt;&gt; out = net(input, n)</span>
<span class="sd">        &gt;&gt;&gt; print(out)</span>
<span class="sd">        [2 5]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NthElement.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reverse</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;reverse&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">PSROIPooling</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies Position Sensitive ROI-Pooling on input Tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        spatial_scale (float): a scaling factor that maps the box coordinates to the input coordinates.</span>
<span class="sd">                               For example, if your boxes are defined on the scale of a 224x224 image and</span>
<span class="sd">                               your input is a 112x112 feature map (resulting from a 0.5x scaling of the original</span>
<span class="sd">                               image), you&#39;ll want to set this to 0.5.</span>
<span class="sd">        group_size (int): the size of the output (in pixels) after the pooling is performed, as (height, width).</span>
<span class="sd">        output_dim (int): the dim of the output after the pooling is performed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **features** (Tensor) - The input features, whose shape must be :math:`(N, C, H, W)`. With data type is</span>
<span class="sd">          float16 or float32. This formula should hold: :math:`(C == output\_dim * group\_size * group\_size)`.</span>
<span class="sd">        - **rois** (Tensor) - The shape is `(batch, 5, rois_n)`. With data type of float16 or float32.</span>
<span class="sd">          The size of first dimension `batch` is batch_size. The size of the second dimension must be `5`.</span>
<span class="sd">          The size of third dimension `rois_n` is the number of rois. The value of `rois` like:</span>
<span class="sd">          (index, x1, y1, x2, y2). The first element of `rois_n` is the index of the `rois`. And the box coordinates</span>
<span class="sd">          in (x1, y1, x2, y2) format where the regions will be taken from. The coordinate must satisfy</span>
<span class="sd">          0 &lt;= x1 &lt; x2 and 0 &lt;= y1 &lt; y2.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **out** (Tensor) - The result after pooling. Its shape</span>
<span class="sd">          is :math:`(rois.shape[0] * rois.shape[2], output\_dim, group\_size, group\_size)`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `spatial_scale` is not a float.</span>
<span class="sd">        TypeError: If `group_size` or `output_dim` is not an int.</span>
<span class="sd">        TypeError: If `features` or `rois` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `rois` is not float16 or float32.</span>
<span class="sd">        ValueError: If shape of `features` does not satisfy :math:`(C == output\_dim * group\_size * group\_size)`.</span>
<span class="sd">        ValueError: If `spatial_scale` is negative.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; features = np.random.randn(4, 3 * 7 * 7, 80, 48)</span>
<span class="sd">        &gt;&gt;&gt; features = Tensor.from_numpy(features).astype(mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; rois = Tensor.from_numpy(</span>
<span class="sd">        ...     np.array([[[0.0000],</span>
<span class="sd">        ...                [150.3563],</span>
<span class="sd">        ...                [200.1320],</span>
<span class="sd">        ...                [579.3563],</span>
<span class="sd">        ...                [602.3452]],</span>
<span class="sd">        ...               [[1.0000],</span>
<span class="sd">        ...                [657.1263],</span>
<span class="sd">        ...                [302.8564],</span>
<span class="sd">        ...                [762.4214],</span>
<span class="sd">        ...                [567.9854]],</span>
<span class="sd">        ...               [[2.0000],</span>
<span class="sd">        ...                [321.3122],</span>
<span class="sd">        ...                [232.2410],</span>
<span class="sd">        ...                [679.0281],</span>
<span class="sd">        ...                [587.6346]],</span>
<span class="sd">        ...               [[3.0000],</span>
<span class="sd">        ...                [664.1630],</span>
<span class="sd">        ...                [387.4919],</span>
<span class="sd">        ...                [778.7322],</span>
<span class="sd">        ...                [562.7321]]])).astype(mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; psROIPooling = ops.PSROIPooling(spatial_scale=1.0/16, output_dim=3,</span>
<span class="sd">        ...                                       group_size=7)</span>
<span class="sd">        &gt;&gt;&gt; out = psROIPooling(features, rois)</span>
<span class="sd">        &gt;&gt;&gt; print(out.shape)</span>
<span class="sd">        (4, 3, 7, 7)</span>
<span class="sd">        &gt;&gt;&gt; print(out.dtype)</span>
<span class="sd">        Float32</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spatial_scale</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize PSROIPooling&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_float</span><span class="p">(</span><span class="n">spatial_scale</span><span class="p">,</span> <span class="s2">&quot;spatial_scale&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">group_size</span><span class="p">,</span> <span class="s2">&quot;group_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="s2">&quot;output_dim&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spatial_scale</span> <span class="o">=</span> <span class="n">spatial_scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span> <span class="o">=</span> <span class="n">group_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;spatial_scale&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">spatial_scale</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;group_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">group_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;output_dim&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">)</span>


<div class="viewcode-block" id="TripletMarginLoss"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.TripletMarginLoss.html#mindspore.ops.TripletMarginLoss">[文档]</a><span class="k">class</span> <span class="nc">TripletMarginLoss</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TripletMarginLoss operation.</span>

<span class="sd">    Creates a criterion that measures the triplet loss given an input</span>
<span class="sd">    tensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.</span>
<span class="sd">    This is used for measuring a relative similarity between samples. A triplet</span>
<span class="sd">    is composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative</span>
<span class="sd">    examples` respectively). The shapes of all input tensors should be</span>
<span class="sd">    :math:`(N, D)`.</span>

<span class="sd">    The distance swap is described in detail in the paper</span>
<span class="sd">    `Learning local feature descriptors with triplets and shallow convolutional neural</span>
<span class="sd">    networks &lt;http://158.109.8.37/files/BRP2016.pdf&gt;`_</span>
<span class="sd">    by V. Balntas, E. Riba et al.</span>

<span class="sd">    The loss function for each sample in the mini-batch is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>

<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</span>

<span class="sd">    Args:</span>
<span class="sd">        p (int, optional): The norm degree for pairwise distance. Default: ``2`` .</span>
<span class="sd">        eps (float, optional): Default: ``1e-6`` .</span>
<span class="sd">        swap (bool, optional): The distance swap. Default: ``False`` .</span>
<span class="sd">        reduction (str, optional): Apply specific reduction method to the output: ``&#39;none&#39;`` , ``&#39;mean&#39;`` ,</span>
<span class="sd">            ``&#39;sum&#39;`` . Default: ``&#39;mean&#39;`` .</span>

<span class="sd">            - ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            - ``&#39;mean&#39;``: compute and return the mean of elements in the output.</span>
<span class="sd">            - ``&#39;sum&#39;``: the output elements will be summed.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - A sample randomly selected from the training set. Data type must be BasicType.</span>
<span class="sd">        - **positive** (Tensor) - A sample belonging to the same category as x,</span>
<span class="sd">          with the same type and shape as `x`.</span>
<span class="sd">        - **negative** (Tensor) - A sample belonging to the different class from x,</span>
<span class="sd">          with the same type and shape as `x`.</span>
<span class="sd">        - **margin** (Tensor) - Make a margin between the positive pair and the negative pair.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Union[Tensor, Scalar], if `reduction` is ``&quot;none&quot;``, its shape is :math:`(N)`.</span>
<span class="sd">        Otherwise, a scalar value will be returned.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` or `positive` or `negative` or `margin` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` or `positive` or `negative` is not BasicType.</span>
<span class="sd">        TypeError: If dtype of `x`, `positive` and `negative` is not the same.</span>
<span class="sd">        TypeError: If `margin` is not float32.</span>
<span class="sd">        TypeError: If `p` is not an int.</span>
<span class="sd">        TypeError: If `eps` is not a float.</span>
<span class="sd">        TypeError: If `swap` is not a bool.</span>
<span class="sd">        ValueError: If dimensions of input `x`, `positive` and `negative` are</span>
<span class="sd">          less than or equal to 1 at the same time.</span>
<span class="sd">        ValueError: If the dimension of input `x` or `positive` or `negative`</span>
<span class="sd">          is bigger than or equal to 8.</span>
<span class="sd">        ValueError: If length of shape of `margin` is not 0.</span>
<span class="sd">        ValueError: If shape of `x`, `positive` and `negative` cannot broadcast.</span>
<span class="sd">        ValueError: If `reduction` is not one of ``&#39;none&#39;``, ``&#39;mean&#39;``, ``&#39;sum&#39;``.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; loss = ops.TripletMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[0.3, 0.7], [0.5, 0.5]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; positive = Tensor(np.array([[0.4, 0.6], [0.4, 0.6]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; negative = Tensor(np.array([[0.2, 0.9], [0.3, 0.7]]), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; margin = Tensor(1.0, mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(x, positive, negative, margin)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        0.8881968</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize TripletMarginLoss&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;positive&#39;</span><span class="p">,</span> <span class="s1">&#39;negative&#39;</span><span class="p">,</span> <span class="s1">&#39;margin&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;swap&quot;</span><span class="p">,</span> <span class="n">swap</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">reduction</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">],</span> <span class="s1">&#39;reduction&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">DeformableOffsets</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the deformed convolution output with the expected input.</span>

<span class="sd">    Refer to :func:`mindspore.ops.deformable_conv2d` for more details.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">strides</span><span class="p">,</span>
                 <span class="n">pads</span><span class="p">,</span>
                 <span class="n">ksize</span><span class="p">,</span>
                 <span class="n">dilations</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                 <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
                 <span class="n">deformable_groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">modulated</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize DeformableOffsets&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;offsets&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">,</span> <span class="s1">&#39;NHWC&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">pos_c</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span> <span class="o">==</span> <span class="s2">&quot;NHWC&quot;</span><span class="p">:</span>
            <span class="n">pos_c</span> <span class="o">=</span> <span class="mi">3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">format</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_size_and_element_type_of_tuple</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">strides</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">strides</span><span class="p">[</span><span class="n">pos_c</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, The N and C dimensions of &#39;strides&#39; must be set to 1.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;strides&#39;</span><span class="p">,</span> <span class="n">strides</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_size_and_element_type_of_tuple</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;pads&#39;</span><span class="p">,</span> <span class="n">pads</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_size_and_element_type_of_tuple</span><span class="p">(</span><span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_size_and_element_type_of_tuple</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="n">dilations</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dilations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">dilations</span><span class="p">[</span><span class="n">pos_c</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, The N and C dimensions of &#39;dilations&#39; must be set to 1.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dilations&#39;</span><span class="p">,</span> <span class="n">dilations</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">deformable_groups</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">deformable_groups</span><span class="p">,</span> <span class="s1">&#39;deformable_groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;deformable_groups&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">deformable_groups</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">modulated</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_bool</span><span class="p">(</span><span class="n">modulated</span><span class="p">,</span> <span class="s1">&#39;modulated&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">modulated</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, The modulated must be set to True.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;modulated&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">modulated</span><span class="p">)</span>


<div class="viewcode-block" id="Pdist"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Pdist.html#mindspore.ops.Pdist">[文档]</a><span class="k">class</span> <span class="nc">Pdist</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the p-norm distance between each pair of row vectors in the input.</span>

<span class="sd">    Refer to :func:`mindspore.ops.pdist` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (float, optional): The order of norm distance, :math:`p∈[0, ∞)`. Default: ``2.0`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input tensor. Supported dtypes: float16, float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same dtype as `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; op = ops.Pdist(p=2.0)</span>
<span class="sd">        &gt;&gt;&gt; y = op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [1.4142135 2.828427  1.4142135]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Pdist&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Pdist p must be a non-negative value, but got `</span><span class="si">{}</span><span class="s1">`.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="UpsampleNearest3D"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.UpsampleNearest3D.html#mindspore.ops.UpsampleNearest3D">[文档]</a><span class="k">class</span> <span class="nc">UpsampleNearest3D</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs nearest neighbor upsampling operation.</span>

<span class="sd">    This operator scale up the volumetric input with specified `output_size` or `scales` factors, using nearest</span>
<span class="sd">    neighbor algorithm.</span>

<span class="sd">    One of `output_size` or `scales` must be given, and can not specified both at the same time.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - 5D tensor of shape :math:`(N, C, D_{in}, H_{in}, W_{in})`.</span>
<span class="sd">          Supporting types: [float16, float32, float64].</span>
<span class="sd">        - **output_size** (Union[tuple[int], list[int]]): A tuple or list of int specifying the output volumetric size.</span>
<span class="sd">          Default: ``None``.</span>
<span class="sd">        - **scales** (Union[tuple[float], list[float]]): A tuple or list of float specifying the upsampling factors.</span>
<span class="sd">          Default: ``None``.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Upsampled output with the same type as `x` , whose shape is</span>
<span class="sd">          :math:`(N, C, D_{out}, H_{out}, W_{out})`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: When `output_size` is not ``None`` and `output_size` is not list[int] or tuple[int].</span>
<span class="sd">        TypeError: When `scales` is not ``None`` and `scales` is not list[float] or tuple[float].</span>
<span class="sd">        TypeError: If dtype of `x` is not int [uint8, float16, float32, float64].</span>
<span class="sd">        ValueError: If any value of `output_size` is negative or zero when `output_size` is not ``None``.</span>
<span class="sd">        ValueError: If any value of `scales` is negative or zero when `scales` is not ``None``.</span>
<span class="sd">        ValueError: If shape of `x` is not 5D.</span>
<span class="sd">        ValueError: If none of `scales` and `output_size` is specified or both specified.</span>
<span class="sd">        ValueError: If size of `scales` is not equal 3 when `scales` is specified.</span>
<span class="sd">        ValueError: If size of `output_size` is not equal 3 when `output_size` is specified.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])</span>
<span class="sd">        ...       .reshape([1, 1, 2, 2, 4]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; output_size = [3, 4, 5]</span>
<span class="sd">        &gt;&gt;&gt; net = ops.UpsampleNearest3D()</span>
<span class="sd">        &gt;&gt;&gt; output = net(x, output_size, None)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[[[ 1.  1.  2.  3.  4.]</span>
<span class="sd">            [ 1.  1.  2.  3.  4.]</span>
<span class="sd">            [ 5.  5.  6.  7.  8.]</span>
<span class="sd">            [ 5.  5.  6.  7.  8.]]</span>
<span class="sd">           [[ 1.  1.  2.  3.  4.]</span>
<span class="sd">            [ 1.  1.  2.  3.  4.]</span>
<span class="sd">            [ 5.  5.  6.  7.  8.]</span>
<span class="sd">            [ 5.  5.  6.  7.  8.]]</span>
<span class="sd">           [[ 9.  9. 10. 11. 12.]</span>
<span class="sd">            [ 9.  9. 10. 11. 12.]</span>
<span class="sd">            [13. 13. 14. 15. 16.]</span>
<span class="sd">            [13. 13. 14. 15. 16.]]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize UpsampleNearest3D.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="s1">&#39;scales&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<span class="k">class</span> <span class="nc">SparseApplyAdagradDA</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update `var` according to the proximal adagrad scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            grad_accum += grad \\</span>
<span class="sd">            grad_squared_accum += grad * grad \\</span>
<span class="sd">            tmp_val=sign(grad_accum) * max\left \{|grad_accum|-l1*global_step, 0\right \}</span>
<span class="sd">                    if l1&gt;0 else grad_accum \\</span>
<span class="sd">            x_value = -1 * lr * tmp_val \\</span>
<span class="sd">            y_value = l2 * global_step * lr + \sqrt{grad_squared_accum} \\</span>
<span class="sd">            var = x_value / y_value</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `grad_accum`, `grad_square_accum` and `grad`</span>
<span class="sd">    comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to the</span>
<span class="sd">    relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If ``True`` , updating of the `var` and `accum` tensors will be protected by a lock.</span>
<span class="sd">                            Otherwise the behavior is undefined, but may exhibit less contention. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable to be updated.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **grad_accum** (Parameter) - The dict of mutable tensor grad_accum. Must have the same</span>
<span class="sd">          shape and dtype as `var`.</span>
<span class="sd">        - **grad_square_accum** (Parameter) - The dict of mutable tensor grad_square_accum.</span>
<span class="sd">          Must have the same shape and dtype as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor of the same type as `var` and grad.shape[1:] = var.shape[1:] if rank(var) &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64 and indices.shape[0] = grad.shape[0].</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - Scaling factor. Must be a scalar. Must have the same type as `var`.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) -  L1 regularization. Must be a scalar. Must have the same type as `var`.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) -  L2 regularization. Must be a scalar. Must have the same type as `var`.</span>
<span class="sd">        - **global_step** (Union[Number, Tensor]) - Training step number. Must be a scalar.</span>
<span class="sd">          Must be one of the following types: int32, int64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, with the same type and shape as &#39;var&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `grad_accum`, `grad_square_accum` is not a Parameter.</span>
<span class="sd">        TypeError: If `grad` is not a Tensor.</span>
<span class="sd">        TypeError: If `lr`, `l1`, `l2` or `global_step` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If use_locking is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `grad_accum`, `grad_square_accum`, `grad_accum` is not the same.</span>
<span class="sd">        TypeError: If dtype of `grad_accum`, `grad_square_accum`, `grad_accum`</span>
<span class="sd">                     is not same as `var`.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        TypeError: If shape of `indices` is not same as shape of first dimension of `grad`.</span>
<span class="sd">        TypeError: If dtype of `global_step` is not int64.</span>
<span class="sd">        ValueError: If the shape size of `lr`, `l1`, `l2` and `global_step` is not 0.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `grad_accum`, `grad_square_accum` and `grad`</span>
<span class="sd">                      conversion of Parameter is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; var = Parameter(Tensor(np.array([[1,2], [1,2]]).astype(np.float32)))</span>
<span class="sd">        &gt;&gt;&gt; grad_accum = Parameter(Tensor(np.array([[2,1], [3,1]]).astype(np.float32)))</span>
<span class="sd">        &gt;&gt;&gt; grad_square_accum = Parameter(Tensor(np.array([[4,1], [5,1]]).astype(np.float32)))</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[5,1], [6,1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1], dtype=np.int32))</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(2, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l1 = Tensor(-1, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l2 = Tensor(1, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; global_step=Tensor(1, mstype.int64)</span>
<span class="sd">        &gt;&gt;&gt; sparse_apply_adagrad_da = nn_ops.SparseApplyAdagradDA()</span>
<span class="sd">        &gt;&gt;&gt; output = sparse_apply_adagrad_da(var, grad_accum, grad_square_accum,</span>
<span class="sd">        ...                                  grad, indices, lr, l1, l2, global_step)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[-1.8956923 -1.1715728]</span>
<span class="sd">         [-2.1420605 -1.1715728]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad_accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad_square_accum&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;global_step&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T2</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyAdagradDA&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;grad_accum&#39;</span><span class="p">,</span> <span class="s1">&#39;grad_square_accum&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;global_step&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SparseApplyMomentum</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update relevant entries in &#39;*var&#39; and &#39;*accum&#39; according to the momentum scheme.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            accum = accum * momentum + grad \\</span>
<span class="sd">            var -= lr * accum</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var`, `accum` and `grad` comply with the implicit type conversion rules</span>
<span class="sd">    to make the data types consistent.</span>
<span class="sd">    If they have different data types, lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If ``True`` , the `var` and `accum` tensors will be protected from being updated.</span>
<span class="sd">            Default: ``False`` .</span>
<span class="sd">        use_nesterov (bool): If `True`, the tensor passed to compute grad will be var + momentum * accum,</span>
<span class="sd">            so in the end, the var you get is actually var + momentum * accum. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be int8, int16, int32, int64,</span>
<span class="sd">          uint8, uint16, uint32, uint64, float16, float32 or float64.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **accum** (Parameter) - Variable tensor to be updated, has the same shape and type as `var`.</span>
<span class="sd">        - **lr** (Union[Number, Tensor]) - The learning rate value. Must be a scalar with same type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient, has the same type as `var`,</span>
<span class="sd">          and grad.shape[1:] = var.shape[1:] if rank(var) &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64 and indices.shape[0] = grad.shape[0].</span>
<span class="sd">        - **momentum** (Union[Number, Tensor]) - Momentum. Must be a scalar with same type as `var`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and type as &#39;var&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `accum`, `grad` or `indices` is not a Parameter.</span>
<span class="sd">        TypeError: If `lr`, `momentum` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `use_locking` or `use_nesterov` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `accum`, `lr`, `grad`, or `momentum` is not one of int8, int16,</span>
<span class="sd">                   int32, int64, uint8, uint16, uint32, uint64, float16, float32, float64.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If the shape of `var`, `accum` or `grad` is rank 0.</span>
<span class="sd">        ValueError: If shape of `accum` or `grad` is not same as `var`.</span>
<span class="sd">        ValueError: If shape of `indices` is not same as the shape of first dimension of `grad`.</span>
<span class="sd">        ValueError: If the shape of `lr` or `momentum` is not rank 0.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `accum`, `lr`, `grad` and &#39;momentum&#39; conversion of Parameter</span>
<span class="sd">                      is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops.operations.nn_ops as nn_ops</span>
<span class="sd">        &gt;&gt;&gt; var = Tensor(np.array([[4.1, 7.2], [1.1, 3.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; accum = Tensor(np.array([[2.2, 3.0], [3.1, 0.5]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; lr = Tensor(0.01, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[0.3, 0.2], [0.4, 0.1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]), mstype.int32)</span>
<span class="sd">        &gt;&gt;&gt; momentum = Tensor(0.99, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; sparse_apply_momentum = nn_ops.SparseApplyMomentum()</span>
<span class="sd">        &gt;&gt;&gt; output = sparse_apply_momentum(var, accum, lr, grad, indices, momentum)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[4.07522   7.1682997]</span>
<span class="sd">         [1.06531   2.99405  ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;momentum&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyMomentum&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;accum&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="s1">&#39;momentum&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_nesterov&quot;</span><span class="p">,</span> <span class="n">use_nesterov</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SparseApplyProximalGradientDescent</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Sparse update &#39;*var&#39; as FOBOS algorithm with fixed learning rate.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            \text{prox_v} = var - alpha \\</span>
<span class="sd">            var = sign(\text{prox_v})/(1 + alpha * l2) * \max(\left| \text{prox_v} \right| - alpha * l1,0)</span>
<span class="sd">        \end{array}</span>

<span class="sd">    Inputs of `var` and `delta` comply with the implicit type conversion rules to make the data types consistent.</span>
<span class="sd">    If they have different data types, the lower priority data type will be converted to</span>
<span class="sd">    the relatively highest priority data type.</span>

<span class="sd">    Args:</span>
<span class="sd">        use_locking (bool): If ``True`` , the `var` tensors will be protected from being updated.</span>
<span class="sd">            Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **var** (Parameter) - Variable tensor to be updated. The data type must be int8, int16, int32, int64,</span>
<span class="sd">          uint8, uint16, uint32, uint64, float16, float32 or float64.</span>
<span class="sd">          The shape is :math:`(N, *)` where :math:`*` means, any number of additional dimensions.</span>
<span class="sd">        - **alpha** (Union[Number, Tensor]) - Scaling factor. Must be a scalar with same type as `var`.</span>
<span class="sd">        - **l1** (Union[Number, Tensor]) - L1 regularization. Must be a scalar with same type as `var`.</span>
<span class="sd">        - **l2** (Union[Number, Tensor]) - l2 regularization. Must be a scalar with same type as `var`.</span>
<span class="sd">        - **grad** (Tensor) - A tensor for gradient, has the same type as `var`,</span>
<span class="sd">          and grad.shape[1:] = var.shape[1:] if rank(var) &gt; 1.</span>
<span class="sd">        - **indices** (Tensor) - A tensor of indices in the first dimension of `var` and `accum`.</span>
<span class="sd">          If there are duplicates in `indices`, the behavior is undefined. Must be one of the</span>
<span class="sd">          following types: int32, int64 and indices.shape[0] = grad.shape[0].</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **var** (Tensor) - Tensor, has the same shape and type as &#39;var&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `var`, `grad` or `indices` is not a Parameter..</span>
<span class="sd">        TypeError: If `alpha`, `l1`, `l2` is neither a Number nor a Tensor.</span>
<span class="sd">        TypeError: If `use_locking` is not a bool.</span>
<span class="sd">        TypeError: If dtype of `var`, `alpha`, `l1`, `l2` or `grad` is not one of int8, int16,</span>
<span class="sd">                   int32, int64, uint8, uint16, uint32, uint64, float16, float32, float64.</span>
<span class="sd">        TypeError: If dtype of `indices` is neither int32 nor int64.</span>
<span class="sd">        ValueError: If the shape of `var` or `grad` is rank 0.</span>
<span class="sd">        ValueError: If shape of `grad` is not same as `var`.</span>
<span class="sd">        ValueError: If the shape of `alpha`, `l1` or `l2` is not rank 0.</span>
<span class="sd">        ValueError: If shape of `indices` is not same as the shape of first dimension of `grad`.</span>
<span class="sd">        RuntimeError: If the data type of `var`, `alpha`, `l1`, `l2`, `grad` conversion of Parameter</span>
<span class="sd">                      is not supported.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops.operations.nn_ops as nn_ops</span>
<span class="sd">        &gt;&gt;&gt; var = Tensor(np.array([[4.1, 7.2], [1.1, 3.0]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; alpha = Tensor(1.0, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l1 = Tensor(1.0, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; l2 = Tensor(0.0, mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; grad = Tensor(np.array([[1, 1], [1, 1]]).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; indices = Tensor(np.array([0, 1]).astype(np.int32))</span>
<span class="sd">        &gt;&gt;&gt; sparse_apply_proximal_gradient_descent = nn_ops.SparseApplyProximalGradientDescent()</span>
<span class="sd">        &gt;&gt;&gt; output = sparse_apply_proximal_gradient_descent(var, alpha, l1, l2, grad, indices)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[2.1 5.2]</span>
<span class="sd">         [0.  1. ]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__mindspore_signature__</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="n">sig</span><span class="o">.</span><span class="n">sig_rw</span><span class="o">.</span><span class="n">RW_WRITE</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T</span><span class="p">),</span>
        <span class="n">sig</span><span class="o">.</span><span class="n">make_sig</span><span class="p">(</span><span class="s1">&#39;indices&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">sig</span><span class="o">.</span><span class="n">sig_dtype</span><span class="o">.</span><span class="n">T1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_locking</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize SparseApplyProximalGradientDescent.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">,</span> <span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="s1">&#39;indices&#39;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;var&#39;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;use_locking&quot;</span><span class="p">,</span> <span class="n">use_locking</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NuclearNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the matrix nuclear norm of a given Tensor.</span>

<span class="sd">    Attr `dim` specifies which two dimensions of the input `x` to calculate the nuclear norm across. If `dim` is None,</span>
<span class="sd">    the nuclear norm will be calculated across all dimensions of input. Because the nuclear norm is the sum of the</span>
<span class="sd">    singular values of the matrix, the input at this time should be 2-dimensional. That is, if the input is</span>
<span class="sd">    2-dimensional, we compute the nuclear norm of the input matrix. At this point, `dim` should be None. If you set</span>
<span class="sd">    `dim`, it also needs to be in the proper range, otherwise it wonn&#39;t work. If the input is 3-dimensional and above,</span>
<span class="sd">    the attribute `dim` is required. It specifies which two dimensions of input to calculate the nuclear norm across.</span>

<span class="sd">    According to the `dim` list, the input Tensor is reordered by `dim`. The two dimensions pointed to by the attribute</span>
<span class="sd">    `dim` are placed at the end, and the order of the other dimensions is relatively unchanged. Perform the SVD of each</span>
<span class="sd">    slice of the adjusted Tensor to obtain the singular value. Sum all of the singular value of each slice/matrix to</span>
<span class="sd">    obtain the nuclear norm.</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (Union[list(int), tuple(int)], optional): Specifies which two</span>
<span class="sd">            dimensions of `x` to calculate the matrix nuclear norm</span>
<span class="sd">            across. If `dim` is None, the nuclear norm will be calculated across all dimensions of `x`. The length of</span>
<span class="sd">            `dim` should be 2. The value in `dim` should be in this range:[-x_rank, x_rank). x_rank is the dimension of</span>
<span class="sd">            Tensor `x`. The value of `dim[0]` or `dim[1]` can not point to the same dimension. Default: ``None`` .</span>
<span class="sd">        keepdim (bool, optional): Whether the output Tensor have `dim` retained or not. Default: ``False`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input to compute the matrix nuclear norm. The dimension of `x` should be greater than or</span>
<span class="sd">          equal to 2. Data type must be float32 or float64.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, output Tensor with dimensions in `dim` reduced to 1 will be returned if `keepdim` is `True`;</span>
<span class="sd">        otherwise a Tensor with dimensions in `dim` removed is returned. The data type is same as `x`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        TypeError: If dtype of `x` is neither float32 nor float64.</span>
<span class="sd">        TypeError: If dtype of `dim` is neither list(int) nor tuple(int).</span>
<span class="sd">        TypeError: If dtype of `keepdim` is not bool.</span>
<span class="sd">        ValueError: If dimension of Tensor `x` is less than 2.</span>
<span class="sd">        ValueError: If the length of `dim` is not 2 when `dim` is set.</span>
<span class="sd">        ValueError: If the dimension of Tensor `x` is not 2 when `dim` is not set.</span>
<span class="sd">        ValueError: If `dim[0]` or `dim[1]` point to the same dimension.</span>
<span class="sd">        ValueError: If `dim[0]` or `dim[1]` is not in this range:[-x_rank, x_rank).</span>
<span class="sd">                    x_rank is the dimension of Tensor `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],</span>
<span class="sd">        ...                           [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]]), ms.float32)</span>
<span class="sd">        &gt;&gt;&gt; dim = [0, 2]</span>
<span class="sd">        &gt;&gt;&gt; keepdim = True</span>
<span class="sd">        &gt;&gt;&gt; nuclearnorm = nn_ops.NuclearNorm(dim = dim,keepdim = keepdim)</span>
<span class="sd">        &gt;&gt;&gt; output = nuclearnorm(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[15.407588]</span>
<span class="sd">        [21.711605]]]</span>
<span class="sd">        &gt;&gt;&gt; keepdim = False</span>
<span class="sd">        &gt;&gt;&gt; nuclearnorm = nn_ops.NuclearNorm(dim = dim,keepdim = keepdim)</span>
<span class="sd">        &gt;&gt;&gt; output = nuclearnorm(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [15.407588 21.711605]</span>
<span class="sd">        &gt;&gt;&gt; dim = [0, 1]</span>
<span class="sd">        &gt;&gt;&gt; keepdim = True</span>
<span class="sd">        &gt;&gt;&gt; nuclearnorm = nn_ops.NuclearNorm(dim = dim,keepdim = keepdim)</span>
<span class="sd">        &gt;&gt;&gt; output = nuclearnorm(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [[[14.212674 15.81139  17.492853]]]</span>
<span class="sd">        &gt;&gt;&gt; keepdim = False</span>
<span class="sd">        &gt;&gt;&gt; nuclearnorm = nn_ops.NuclearNorm(dim = dim,keepdim = keepdim)</span>
<span class="sd">        &gt;&gt;&gt; output = nuclearnorm(input_x)</span>
<span class="sd">        &gt;&gt;&gt; print(output)</span>
<span class="sd">        [14.212674 15.81139  17.492853]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize NuclearNorm.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;dim&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">)],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dim</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">EQ</span><span class="p">,</span> <span class="s1">&#39;length of dim_size&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;dim[0]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="n">validator</span><span class="o">.</span><span class="n">check_is_int</span><span class="p">(</span><span class="n">dim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;dim[1]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">,</span> <span class="p">[</span><span class="mi">1000</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;keepdim&quot;</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">,</span> <span class="p">[</span><span class="nb">bool</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<div class="viewcode-block" id="GLU"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.GLU.html#mindspore.ops.GLU">[文档]</a><span class="k">class</span> <span class="nc">GLU</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes GLU (Gated Linear Unit activation function) of input tensors.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.glu` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int, optional): Axis on which to split the input.</span>
<span class="sd">            The value of `axis` must be an int within range [-rank(`x`), rank(`x`)).</span>
<span class="sd">            Default: ``-1`` , specifying the last dimension.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Input tensor. `x.shape[axis]` must be even.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor, has the same data type with `x`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import ops, Tensor</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import dtype as mstype</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; axis = 0</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.array([0.3220, 0.9545, 0.7879, 0.0975, 0.3698,</span>
<span class="sd">        ...                            0.5135, 0.5740, 0.3435, 0.1895, 0.8764,</span>
<span class="sd">        ...                            0.4980, 0.9673, 0.9879, 0.6988, 0.9022,</span>
<span class="sd">        ...                            0.9304, 0.1558, 0.0153, 0.1559, 0.9852]).reshape([2, 2, 5]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; glu = ops.GLU(axis=axis)</span>
<span class="sd">        &gt;&gt;&gt; y = glu(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[0.20028052 0.6916126  0.57412136 0.06512236 0.26307625]</span>
<span class="sd">          [0.3682598  0.3093122  0.17306386 0.10212085 0.63814086]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize GLU&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;axis&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">FractionalMaxPoolWithFixedKsize</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D fractional max pooling to an input signal composed of multiple input planes.</span>
<span class="sd">    The max-pooling operation is applied in :math:`(kH, kW)` regions by a stochastic step size determined by</span>
<span class="sd">    the target output size `output_shape`.</span>

<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Fractional MaxPooling is described in the paper `Fractional Max-Pooling &lt;https://arxiv.org/pdf/1412.6071&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        ksize (Union[int, tuple[int]]): Size of the pooling window. `ksize` can be a tuple of two values</span>
<span class="sd">          specify a shape :math:`(k_H, k_W)`, or a single int `K` for :math:`(K, K)`.</span>
<span class="sd">        output_shape (Union[int, tuple[int]]): The target output shape. `output_shape` can be a</span>
<span class="sd">          tuple of two values specify a shape :math:`(H_{out}, W_{out})`, or a single float `S` for :math:`(S, S)`.</span>
<span class="sd">        data_format (str, optional): The optional value for data format, is ``&#39;NCHW&#39;`` .</span>
<span class="sd">            Default: ``&quot;NCHW&quot;`` .</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Tensor of shape :math:`(N, C, H_{in}, W_{in})`,</span>
<span class="sd">          with float16, float32, float64, int32, int64 data type.</span>
<span class="sd">        - **random_samples** (Tensor) - Tensor of shape :math:`(N, C, 2)`.</span>
<span class="sd">          with float16, float32, float64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Has the same type as the `input_x`.</span>
<span class="sd">          Has the shape :math:`(N, C, H_{out}, W_{out})`.</span>
<span class="sd">        - **argmax** (Tensor) -A tensor whose data type must be int64. Has the same shape as the `y`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `input_x` is not one of the following: float16, float32, float64, int32, int64.</span>
<span class="sd">        TypeError: If data type of `random_samples` is not one of the following: float16, float32, float64.</span>
<span class="sd">        ValueError: If `ksize` is not a number and `ksize` is not a tuple of length 2.</span>
<span class="sd">        ValueError: If `output_shape` is not a number and `output_shape` is not a tuple of length 2.</span>
<span class="sd">        ValueError: If the sum of `ksize` , `output_shape` and</span>
<span class="sd">          -1 is larger than the corresponding dimension of `input_x`.</span>
<span class="sd">        ValueError: If the dimension of `random_samples` is not 3.</span>
<span class="sd">        ValueError: If the first dimension size of `input_x` and `random_samples` is not equal.</span>
<span class="sd">        ValueError: If the second dimension size of `input_x` and `random_samples` is not equal.</span>
<span class="sd">        ValueError: If the third dimension size of `random_samples` is not 2.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # the ksize is an int number and the output_shape is a tuple.</span>
<span class="sd">        &gt;&gt;&gt; ksize = 2</span>
<span class="sd">        &gt;&gt;&gt; output_shape = (2,2)</span>
<span class="sd">        &gt;&gt;&gt; data_format = &quot;NCHW&quot;</span>
<span class="sd">        &gt;&gt;&gt; input_x = Tensor(np.array([0.3220, 0.9545, 0.7879, 0.0975, 0.3698,</span>
<span class="sd">        ...                            0.5135, 0.5740, 0.3435, 0.1895, 0.8764,</span>
<span class="sd">        ...                            0.9581, 0.4760, 0.9014, 0.8522, 0.3664,</span>
<span class="sd">        ...                            0.4980, 0.9673, 0.9879, 0.6988, 0.9022,</span>
<span class="sd">        ...                            0.9304, 0.1558, 0.0153, 0.1559, 0.9852]).reshape([1, 1, 5, 5]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; random_samples = Tensor(np.array([[[0.8, 0.8]]]), mstype.float32)</span>
<span class="sd">        &gt;&gt;&gt; net = ops.FractionalMaxPoolWithFixedKsize(ksize, output_shape, data_format)</span>
<span class="sd">        &gt;&gt;&gt; y, argmax = net(input_x, random_samples)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[[0.9545 0.8764]</span>
<span class="sd">           [0.9673 0.9852]]]]</span>
<span class="sd">        &gt;&gt;&gt; print(argmax)</span>
<span class="sd">        [[[[ 1  9]</span>
<span class="sd">           [16 24]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FractionalMaxPoolWithFixedKsize.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;ksize&#39;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="n">ksize</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ksize&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ksize</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;output_shape&#39;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span>
            <span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="n">output_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">allow_four</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;output_shape&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_format</span> <span class="o">=</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">data_format</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;NCHW&#39;</span><span class="p">],</span> <span class="s1">&#39;data_format&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;input_x&#39;</span><span class="p">,</span> <span class="s1">&#39;random_samples&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="s1">&#39;argmax&#39;</span><span class="p">])</span>


<div class="viewcode-block" id="ChannelShuffle"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.ChannelShuffle.html#mindspore.ops.ChannelShuffle">[文档]</a><span class="k">class</span> <span class="nc">ChannelShuffle</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Divide the channels in a tensor of shape :math:`(*, C, H, W)` into :math:`g` group and</span>
<span class="sd">    rearrange them as :math:`(*, \frac C g, g, H*W)`, while keeping the original tensor shapes.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Refer to :func:`mindspore.ops.channel_shuffle` for more detail.</span>

<span class="sd">    Args:</span>
<span class="sd">        group (int): Number of group to divide channels in.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor to be divided, it has shape :math:`(*, C, H, W)`,</span>
<span class="sd">          with float16, float32, int8, int16, int32, int64, uint8, uint16, uint32, uint64 data type.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        A Tensor, has the same type as the `x`, and has the shape :math:`(*, C, H, W)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; group = 2</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(1 * 4 * 2 * 2).reshape(1, 4, 2, 2).astype(np.int16))</span>
<span class="sd">        &gt;&gt;&gt; channel_shuffle_func = ops.ChannelShuffle(group)</span>
<span class="sd">        &gt;&gt;&gt; y = channel_shuffle_func(x)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        [[[[ 0  1]</span>
<span class="sd">           [ 2  3]]</span>
<span class="sd">           [[ 8  9]</span>
<span class="sd">           [10 11]]</span>
<span class="sd">           [[ 4  5]</span>
<span class="sd">           [ 6  7]]</span>
<span class="sd">           [[12 13]</span>
<span class="sd">           [14 15]]]]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">group</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize ChannelShuffle&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">group</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, attr &#39;group&#39; must be an positive int number&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span></div>


<div class="viewcode-block" id="MaxPoolWithArgmaxV2"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.MaxPoolWithArgmaxV2.html#mindspore.ops.MaxPoolWithArgmaxV2">[文档]</a><span class="k">class</span> <span class="nc">MaxPoolWithArgmaxV2</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Performs max pooling on the input Tensor and returns both max values and indices.</span>

<span class="sd">    Typically the input is of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})`, MaxPool outputs</span>
<span class="sd">    regional maximum in the :math:`(H_{in}, W_{in})`-dimension. Given kernel size</span>
<span class="sd">    :math:`(h_{ker}, w_{ker})` and stride :math:`(s_0, s_1)`, the operation is as follows:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{output}(N_i, C_j, h, w) = \max_{m=0, \ldots, h_{ker}-1} \max_{n=0, \ldots, w_{ker}-1}</span>
<span class="sd">        \text{input}(N_i, C_j, s_0 \times h + m, s_1 \times w + n)</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size (Union[int, tuple[int]]): The size of kernel used to take the maximum value and argmax</span>
<span class="sd">            value, is an int number that represents height and width of the kernel, or a tuple of</span>
<span class="sd">            two int numbers that represent height and width respectively.</span>
<span class="sd">        strides (Union[int, tuple[int]], optional): The distance of kernel moving, an int number that represents</span>
<span class="sd">            not only the height of movement but also the width of movement, or a tuple of two int numbers that</span>
<span class="sd">            represent height and width of movement respectively. Default: ``None`` , meaning that</span>
<span class="sd">            `strides = kernel_size`.</span>
<span class="sd">        pads (Union[int, tuple[int]], optional): An int number that represents the depth,</span>
<span class="sd">            height and width of movement are both strides, or a tuple of two int numbers that represent</span>
<span class="sd">            depth, height and width of movement respectively.</span>
<span class="sd">            Default: 0.</span>
<span class="sd">        dilation (Union[int, tuple[int]], optional): Control the stride of elements in the kernel. Default: ``(1, 1)`` .</span>
<span class="sd">        ceil_mode (bool, optional): Whether to use ceil instead of floor to calculate output shape. Default: ``False`` .</span>
<span class="sd">        argmax_type (mindspore.dtype, optional) : The dtype for argmax.</span>
<span class="sd">            Default: ``mstype.int64`` . [Disabled in Ascend.]</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - Tensor of shape :math:`(N_{in}, C_{in}, H_{in}, W_{in})` with data type of int8,</span>
<span class="sd">          int16, int32, int64, uint8, uint16, uint32, uint64, float16, float32 or float64 in CPU and GPU,</span>
<span class="sd">          with that of float16 in Ascend.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tuple of 2 Tensors, representing the maxpool result and where the max values are generated.</span>

<span class="sd">        - **output** (Tensor) - Maxpooling result, with shape :math:`(N_{out}, C_{out}, H_{out}, W_{out})`.</span>
<span class="sd">          It has the same data type as `x`.</span>

<span class="sd">          .. math::</span>
<span class="sd">              H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{pads[0]} - \text{dilation[0]}</span>
<span class="sd">               \times (\text{kernel_size[0]} - 1) - 1}{\text{strides[0]}} + 1\right\rfloor</span>

<span class="sd">          .. math::</span>
<span class="sd">              W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{pads[1]} - \text{dilation[1]}</span>
<span class="sd">               \times (\text{kernel_size[1]} - 1) - 1}{\text{strides[1]}} + 1\right\rfloor</span>

<span class="sd">        - **argmax** (Tensor) - Index corresponding to the maximum value.</span>
<span class="sd">          Data type is int32 or int64 in GPU and CPU, is uint16 in Ascend.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If `x` is not a Tensor.</span>
<span class="sd">        ValueError: If length of shape of `x` is not equal to 4.</span>
<span class="sd">        TypeError: If `kernel_size` , `strides` , `pads` or `dilation` is not int or tuple.</span>
<span class="sd">        ValueError: If `kernel_size`, `strides` or `dilation` is less than 1.</span>
<span class="sd">        ValueError: If `pads` is less than 0.</span>
<span class="sd">        ValueError: If `pads` is more than half of `kernel_size`.</span>
<span class="sd">        ValueError: If `argmax_type` is not mindspore.int64 or mindspore.int32.</span>
<span class="sd">        TypeError: If `ceil_mode` is not bool.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.arange(20 * 16 * 50 * 32).reshape((20, 16, 50, 32)), mindspore.float32)</span>
<span class="sd">        &gt;&gt;&gt; maxpool_arg_v2_op = ops.MaxPoolWithArgmaxV2(kernel_size=(3, 2), strides=(2, 1))</span>
<span class="sd">        &gt;&gt;&gt; output_tensor, argmax = maxpool_arg_v2_op(x)</span>
<span class="sd">        &gt;&gt;&gt; print(output_tensor.shape)</span>
<span class="sd">        (20, 16, 24, 31)</span>
<span class="sd">        &gt;&gt;&gt; print(argmax.shape)</span>
<span class="sd">        (20, 16, 24, 31)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pads</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">argmax_type</span><span class="o">=</span><span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize MaxPoolWithArgmaxV2.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;argmax&quot;</span><span class="p">])</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;ceil_mode&quot;</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span> <span class="o">=</span> <span class="n">ceil_mode</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;argmax_type&quot;</span><span class="p">,</span> <span class="n">argmax_type</span><span class="p">,</span> <span class="p">[</span><span class="n">mstype</span><span class="o">.</span><span class="n">Type</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">argmax_type_valid_values</span> <span class="o">=</span> <span class="p">(</span><span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_type_name</span><span class="p">(</span><span class="s2">&quot;argmax_type&quot;</span><span class="p">,</span> <span class="n">argmax_type</span><span class="p">,</span> <span class="n">argmax_type_valid_values</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">argmax_type</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;argmax_type&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">argmax_type</span> <span class="o">==</span> <span class="n">mstype</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;argmax_type&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;For &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&#39;, the &#39;argmax_type&#39; must be mstype.int32 or mstype.int64, but got </span><span class="si">{</span><span class="n">argmax_type</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">strides</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strides</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pads</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;pads&quot;</span><span class="p">,</span> <span class="n">pads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">strict_positive</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">_check_positive_int_or_tuple</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">ret_four</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;kernel_size&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;strides&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">strides</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;pads&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;dilation&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;ceil_mode&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ceil_mode</span><span class="p">)</span></div>


<div class="viewcode-block" id="Dense"><a class="viewcode-back" href="../../../../api_python/ops/mindspore.ops.Dense.html#mindspore.ops.Dense">[文档]</a><span class="k">class</span> <span class="nc">Dense</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The dense connected fusion operator.</span>

<span class="sd">    Applies dense connected operator for the input. The implement of the operation is as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        output = x @ w ^ T + b,</span>

<span class="sd">    where :math:`x` is the input tensor, :math:`w` is a weight matrix with the same data type as the :math:`x` ,</span>
<span class="sd">    and :math:`b` is a bias vector with the same data type as the :math:`x` (only if `b` is not ``None``).</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **x** (Tensor) - The shape must meet the following requirement: :math:`len(x.shape)&gt;0`.</span>
<span class="sd">        - **w** (Tensor) - The shape must meet the following requirements:</span>
<span class="sd">          If :math:`len(x.shape)&gt;1`, :math:`len(w.shape)=2`. If :math:`len(x.shape)=1`, :math:`len(w.shape)=1`.</span>
<span class="sd">          :math:`w.shape[-1]=x.shape[-1]`.</span>
<span class="sd">        - **b** (Union[Tensor, None]) - If `b` is not ``None``, the shape must meet the following requirements:</span>
<span class="sd">          If :math:`len(x.shape)&gt;1`, :math:`len(b.shape)=0` or :math:`len(b.shape)=1` .</span>
<span class="sd">          If :math:`len(b.shape)=1`, :math:`b.shape[0]=w.shape[0]`.</span>
<span class="sd">          If :math:`len(x.shape)=1`, :math:`len(b.shape)=0`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        If :math:`len(x.shape)&gt;1`, Tensor of shape :math:`(*x.shape[:-1], w.shape[0])`.</span>
<span class="sd">        If :math:`len(x.shape)=1`, Tensor of shape :math:`()`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend`` ``GPU`` ``CPU``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor, ops</span>
<span class="sd">        &gt;&gt;&gt; x = Tensor(np.random.random((4, 5, 6, 7)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; weight = Tensor(np.random.random((6, 7)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; bias = Tensor(np.random.random((6,)).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; dense = ops.Dense()</span>
<span class="sd">        &gt;&gt;&gt; output = dense(x, weight, bias)</span>
<span class="sd">        &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">        (4, 5, 6, 6)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Dense.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_prim_attr</span><span class="p">(</span><span class="s2">&quot;has_bias&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">WKV</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The WKV computation is similar to AFT(Zhai et al., 2021), but W is now a channel-wise vector multiplied</span>
<span class="sd">    by relative position rather than a pairwise matrix in AFT. We also introduce a vector U for separately</span>
<span class="sd">    attending to the current token in order to compensate for potential degeneration of W.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **w** (Tensor) - The time_first tensor with data type of float32.</span>
<span class="sd">          Input tensor of shape :math:`(hidden\_size,)`.</span>
<span class="sd">        - **u** (Tensor]) - The time_decay tensor with data type of float32.</span>
<span class="sd">          Input tensor of shape :math:`(hidden\_size,)`.</span>
<span class="sd">        - **k** (Tensor) - The key tensor with data type of float32.</span>
<span class="sd">          Input tensor of shape :math:`(batch\_size, seq\_length, hidden\_size)`.</span>
<span class="sd">        - **v** (Tensor) - The value tensor with data type of float32.</span>
<span class="sd">          Input tensor of shape :math:`(batch\_size, seq\_length, hidden\_size)`.</span>
<span class="sd">        - **sp** (Tensor) - The states_p tensor with data type of float32.</span>
<span class="sd">          Input tensor of shape :math:`(batch\_size, seq\_length, hidden\_size)`.</span>
<span class="sd">        - **sq** (Tensor) - The states_q tensor with data type of float32.</span>
<span class="sd">          Input tensor of shape :math:`(batch\_size, hidden\_size)`.</span>
<span class="sd">        - **sm** (Tensor) - The states_m tensor with data type of float32.</span>
<span class="sd">          Input tensor of shape :math:`(batch\_size, hidden\_size)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        Tensor of shape :math:`(batch\_size, seq\_length, hidden\_size)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore.ops.operations import nn_ops</span>
<span class="sd">        &gt;&gt;&gt; b = 32</span>
<span class="sd">        &gt;&gt;&gt; t = 2</span>
<span class="sd">        &gt;&gt;&gt; c = 128</span>
<span class="sd">        &gt;&gt;&gt; w = Tensor(np.random.randn(c).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; u = Tensor(np.random.randn(c).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; k = Tensor(np.random.randn(b, t, c).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; v = Tensor(np.random.randn(b, t, c).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; sp = Tensor(np.random.randn(b, c).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; sq = Tensor(np.random.randn(b, c).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; sm = Tensor(np.random.randn(b, c).astype(np.float32))</span>
<span class="sd">        &gt;&gt;&gt; dense = nn_ops.WKV()</span>
<span class="sd">        &gt;&gt;&gt; output = dense(w, u, k, v, sp, sq, sm)</span>
<span class="sd">        &gt;&gt;&gt; print(output[0].shape)</span>
<span class="sd">        (32, 2, 128)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize WKV.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;time_first&quot;</span><span class="p">,</span> <span class="s2">&quot;time_decay&quot;</span><span class="p">,</span> <span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="s2">&quot;sp&quot;</span><span class="p">,</span> <span class="s2">&quot;sq&quot;</span><span class="p">,</span> <span class="s2">&quot;sm&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="s2">&quot;out_sp&quot;</span><span class="p">,</span> <span class="s2">&quot;out_sq&quot;</span><span class="p">,</span> <span class="s2">&quot;out_sm&quot;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">PromptFlashAttention</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The interface for fully inference.</span>
<span class="sd">    B -- Batch size</span>
<span class="sd">    S -- Sequence length</span>
<span class="sd">    H -- Hidden size</span>

<span class="sd">    Note:</span>
<span class="sd">    experiment ops</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_heads (int): The number of heads.</span>
<span class="sd">        scale_value (float): The scale value indicating the scale coefficient, which is used as the scalar of</span>
<span class="sd">          Muls in the calculation. Default: 1.0.</span>
<span class="sd">        pre_tokens (int): Previous tokens. Default: 2147483547.</span>
<span class="sd">        next_tokens (int): next tokens.  Default: 0.</span>
<span class="sd">          indicate the upper triangle, Indicate the number of data blocks involved in the calculation. The value 0</span>
<span class="sd">          indicates that the data blocks in the upper triangle are not involved in the calculation</span>
<span class="sd">        input_layout (str): the data layout of the input qkv, support `(BSH)` and `(BNSD)`, Default `BSH`.</span>
<span class="sd">        num_key_value_heads (int): head numbers of key/value which are used in GQA algorithm.</span>
<span class="sd">          The value o indicates if the key and value have the same head nums, use numHeads.  Default: 0.</span>
<span class="sd">        sparse_mode (int): Default: 0</span>
<span class="sd">        inner_precise (int): 0, float16 high precision. 1, high performance. default 1</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **query** (Tensor) - The query tensor with data type of float16 or float32.</span>
<span class="sd">          Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.</span>
<span class="sd">        - **key** (Tensor) - The key tensor with data type of float16 or float32.</span>
<span class="sd">          Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.</span>
<span class="sd">        - **value** (Tensor) - The value tensor with data type of float16 or float32.</span>
<span class="sd">          Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.</span>
<span class="sd">        - **attn_mask** (Tensor) - The attention mask tensor with data type of float16 or float32.</span>
<span class="sd">          For each element, 0 indicates retention and 1 indicates discard. Input tensor of shape :math:`(B, 1, S, S)`.</span>
<span class="sd">        - **actual_seq_lengths** (Tensor): Describe actual sequence length of each input with data type of int64.</span>
<span class="sd">        - **actual_seq_lengths_kv** (Tensor): Describe actual sequence length of each input with data type of int64.</span>
<span class="sd">        - **pse_shift** (Tensor) - The position encoding tensor with data type of float16 or float32.</span>
<span class="sd">        - **dep_scale1** (Tensor)</span>
<span class="sd">        - **quant_scale1** (Tensor)</span>
<span class="sd">        - **deq_scale2** (Tensor)</span>
<span class="sd">        - **quant_scale2** (Tensor)</span>
<span class="sd">        - **quant_offset2** (Tensor)</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **attention_out** (Tensor) - Input tensor of shape :math:`(B, S, H)` / `(B, N, S, D)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import mindspore.ops.operations.nn_ops as P</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import Tensor</span>
<span class="sd">        &gt;&gt;&gt; import numpy as np</span>
<span class="sd">        &gt;&gt;&gt; B = 1</span>
<span class="sd">        &gt;&gt;&gt; N = 16</span>
<span class="sd">        &gt;&gt;&gt; S = 256</span>
<span class="sd">        &gt;&gt;&gt; D = 16</span>
<span class="sd">        &gt;&gt;&gt; query = Tensor(np.ones((B, N, S, D), dtype=np.float16))</span>
<span class="sd">        &gt;&gt;&gt; key = Tensor(np.ones((B, N, S, D), dtype=np.float16))</span>
<span class="sd">        &gt;&gt;&gt; value = Tensor(np.ones((B, N, S, D), dtype=np.float16))</span>
<span class="sd">        &gt;&gt;&gt; attn_mask = Tensor(np.ones((B, 1, S, S), dtype=np.float16))</span>
<span class="sd">        &gt;&gt;&gt; pfa = P.PromptFlashAttention(N, input_layout=&#39;BNSD&#39;)</span>
<span class="sd">        &gt;&gt;&gt; out = pfa(query, key, value, attn_mask, None, None, None, None, None, None, None, None)</span>
<span class="sd">        &gt;&gt;&gt; print(out.shape)</span>
<span class="sd">        (1, 16, 256, 16)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">214748647</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s1">&#39;BSH&#39;</span><span class="p">,</span>
                 <span class="n">num_key_value_heads</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sparse_mode</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inner_precise</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize PromptFlashAttention.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;scale_value&#39;</span><span class="p">,</span> <span class="n">scale_value</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pre_tokens&#39;</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;next_tokens&#39;</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;input_layout&#39;</span><span class="p">,</span> <span class="n">input_layout</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;num_key_value_heads&#39;</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;sparse_mode&#39;</span><span class="p">,</span> <span class="n">sparse_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;inner_precise&#39;</span><span class="p">,</span> <span class="n">inner_precise</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="s2">&quot;attn_mask&quot;</span><span class="p">,</span> <span class="s2">&quot;actual_seq_lengths&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;actual_seq_lengths_kv&quot;</span><span class="p">,</span> <span class="s2">&quot;pse_shift&quot;</span><span class="p">,</span> <span class="s2">&quot;deq_scale1&quot;</span><span class="p">,</span> <span class="s2">&quot;quant_scale1&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;deq_scale2&quot;</span><span class="p">,</span> <span class="s2">&quot;quant_scale2&quot;</span><span class="p">,</span> <span class="s2">&quot;quant_offset2&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;attention_out&quot;</span><span class="p">])</span>

<span class="k">class</span> <span class="nc">IncreFlashAttention</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The interface for fully inference.</span>

<span class="sd">    B -- Batch size</span>

<span class="sd">    S -- Sequence length</span>

<span class="sd">    H -- Hidden size</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **query** (Tensor) - The query tensor with data type of float16 or bfloat16.</span>
<span class="sd">          Input tensor of shape :math:`(B, 1, H)` / :math:`(B, N, 1, D)`.</span>
<span class="sd">        - **key** (TensorList) - The key tensor with data type of float16 or bfloat16.</span>
<span class="sd">          Input tensor of shape :math:`(B, S, H)` / :math:`(B, N, S, D)`.</span>
<span class="sd">        - **value** (TensorList) - The value tensor with data type of float16 or bfloat16.</span>
<span class="sd">          Input tensor of shape :math:`(B, S, H)` / :math:`(B, N, S, D)`.</span>
<span class="sd">        - **attn_mask** (Tensor) - The attention mask tensor with data type of float16 or bool.</span>
<span class="sd">          Input tensor of shape :math:`(B, S)` / :math:`(B, 1, S)` / :math:`(B, 1, 1, S)`.</span>
<span class="sd">        - **actual_seq_lengths** (Tensor) - Describe actual sequence length of each input with data type of int.</span>
<span class="sd">        - **padding_mask** (Tensor) - The padding mask tensor with data type of float16.</span>
<span class="sd">        - **dequant_scale1** (Tensor) - Quantitative parametor, the tensor with data type of uint64.</span>
<span class="sd">        - **quant_scale1** (Tensor) - Quantitative parametor, the tensor with data type of float.</span>
<span class="sd">        - **dequant_scale2** (Tensor) - Quantitative parametor, the tensor with data type of uint64.</span>
<span class="sd">        - **quant_scale2** (Tensor) - Quantitative parametor, the tensor with data type of float.</span>
<span class="sd">        - **quant_offset2** (Tensor) - Quantitative parametor, the tensor with data type of float.</span>
<span class="sd">        - **antiquant_scale** (Tensor) - Quantitative parametor, the tensor with data type of float.</span>
<span class="sd">        - **antiquant_offset** (Tensor) - Quantitative parametor, the tensor with data type of float.</span>
<span class="sd">        - **block_table** (Tensor) - The tensor with data type of float.</span>
<span class="sd">        - **num_heads**  (int) - The number of heads.</span>
<span class="sd">        - **input_layout** (str) - the data layout of the input qkv, support `(BSH)` and `(BNSD)`. Default `BSH`.</span>
<span class="sd">        - **scale_value** (double) - The scale value indicating the scale coefficient, which is used as the scalar of</span>
<span class="sd">          Muls in the calculation. Default: 1.0.</span>
<span class="sd">        - **num_key_value_heads** (int) - head numbers of key/value which are used in GQA algorithm.</span>
<span class="sd">          The value o indicates if the key and value have the same head nums, use numHeads.  Default: 0.</span>
<span class="sd">        - **block_size** (int) - Default: 0.</span>
<span class="sd">        - **inner_precise** (int) - Default: 1.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **attention_out** (Tensor) - Input tensor of shape :math:`(B, 1, H)` / :math:`(B, N, 1, D)`.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">inner_precise</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize IncreFlashAttention.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;num_heads&#39;</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;input_layout&#39;</span><span class="p">,</span> <span class="n">input_layout</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;scale_value&#39;</span><span class="p">,</span> <span class="n">scale_value</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;num_key_value_heads&#39;</span><span class="p">,</span> <span class="n">num_key_value_heads</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;block_size&#39;</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;inner_precise&#39;</span><span class="p">,</span> <span class="n">inner_precise</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="s2">&quot;key&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="s2">&quot;attn_mask&quot;</span><span class="p">,</span> <span class="s2">&quot;actual_seq_lengths&quot;</span><span class="p">,</span> <span class="s2">&quot;padding_mask&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;dequant_scale1&quot;</span><span class="p">,</span> <span class="s2">&quot;quant_scale1&quot;</span><span class="p">,</span> <span class="s2">&quot;dequant_scale2&quot;</span><span class="p">,</span> <span class="s2">&quot;quant_scale2&quot;</span><span class="p">,</span>
                                        <span class="s2">&quot;quant_offset2&quot;</span><span class="p">,</span> <span class="s2">&quot;antiquant_scale&quot;</span><span class="p">,</span> <span class="s2">&quot;antiquant_offset&quot;</span><span class="p">,</span> <span class="s2">&quot;block_table&quot;</span><span class="p">],</span>
                                <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;attention_out&quot;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">FlashAttentionScore</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    FlashAttentionScore.</span>
<span class="sd">    .. math::</span>
<span class="sd">        \begin{array}{ll} \\</span>
<span class="sd">            y = Dropout(Softmax(Mask(scale_value \mul (real_shift + query * key), attn_mask), -1), keep_prob) \\</span>
<span class="sd">            \mul value \\</span>
<span class="sd">        \end{array}</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>
<span class="sd">    B -- Batch size</span>
<span class="sd">    S1 -- Sequence length of query. The value ranges from 1 to 32768 and is a multiple of 16.</span>
<span class="sd">    S2 -- Sequence length of key and value. The value ranges from 1 to 32768 and is a multiple of 16.</span>
<span class="sd">    N1 -- Num heads of query</span>
<span class="sd">    N2 -- Num heads of key and value, and N2 must be a factor of N1</span>
<span class="sd">    D -- Head size. Support value: 64, 80, 96, 120, 128 and 256.</span>
<span class="sd">    H1 -- Hidden size of query, which equals to N1 * D</span>
<span class="sd">    H2 -- Hidden size of key and value, which equals to N2 * D</span>
<span class="sd">    Args:</span>
<span class="sd">        head_num (int): The head num of query.</span>
<span class="sd">        keep_prob (float): The keep probability of dropout. Default: 1.0.</span>
<span class="sd">        scale_value (float): The scale factor of score. Default: 1.0.</span>
<span class="sd">        pre_tokens (int): Parameter for sparse computation, represents how many tokens are counted forward.</span>
<span class="sd">        When sparse_mode is set to 1, 2, 3, or 5, this parameter does not take effect. Default: 2147483647.</span>
<span class="sd">        next_tokens (int): Parameter for sparse computation, represents how many tokens are counted backward.</span>
<span class="sd">        When sparse_mode is set to 1, 2, 3, or 5, this parameter does not take effect. Default: 2147483647.</span>
<span class="sd">        inner_precise (int): The parameter is reserved and not implemented yet. Default: 0.</span>
<span class="sd">        input_layout (str): Specifies the layout of input `query`, key and value. The value can be &quot;BSH&quot; or &quot;BNSD&quot;.</span>
<span class="sd">        Default: &quot;BSH&quot;.</span>
<span class="sd">        sparse_mode (int): Indicates sparse mode. Default 0.</span>

<span class="sd">            - 0: Indicates the defaultMask mode. If attn_mask is not passed, the mask operation is not performed,</span>
<span class="sd">              and preTokens and nextTokens(internally assigned as INT_MAX) are ignored. If passed in, the full attn_mask</span>
<span class="sd">              matrix (S1 * S2) needs to be passed in, indicating that the part between preTokens and nextTokens needs to</span>
<span class="sd">              be calculated.</span>
<span class="sd">            - 1: Represents allMask, that is, passing in the complete attn_mask matrix.</span>
<span class="sd">            - 2: Representing the leftUpCausal mode corresponds to the lower triangle scenario divided by the left</span>
<span class="sd">              vertex, and the optimized attn_mask matrix (2048*2048) is required.</span>
<span class="sd">            - 3: Representing the rightDownCausal model corresponds to the lower triangle scene divided by the lower</span>
<span class="sd">              right vertex, and the optimized attn_mask matrix (2048*2048) is required.</span>
<span class="sd">            - 4: Represents the band scenario, that is, the part between counting preTokens and nextTokens, and the</span>
<span class="sd">              optimized attn_mask matrix (2048*2048) is required..</span>
<span class="sd">            - 5: Represents the prefix scenario, that is, on the basis of rightDownCasual, a matrix with length S1 and</span>
<span class="sd">              width N is added to the left side. The value of N is obtained by the new input prefix, and the N value of</span>
<span class="sd">              each Batch axis is different. Not implemented yet.</span>
<span class="sd">            - 6: Represents the global scenario, not implemented yet.</span>
<span class="sd">            - 7: Represents the dilated scenario, not implemented yet.</span>
<span class="sd">            - 8: Represents the block_local scenario, not implemented yet.</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **query** (Tensor[float16, bfloat16]) - The query tensor.</span>
<span class="sd">          Input tensor of shape :math:`(B, S1, H1)` or `(B, N1, S1, D)`.</span>
<span class="sd">        - **key** (Tensor[float16, bfloat16]) - The key tensor.</span>
<span class="sd">          Input tensor of shape :math:`(B, S2, H2)` or `(B, N2, S2, D)`.</span>
<span class="sd">        - **value** (Tensor[float16, bfloat16]) - The value tensor.</span>
<span class="sd">          Input tensor of shape :math:`(B, S2, H2)` or `(B, N2, S2, D)`.</span>
<span class="sd">        - **real_shift** (Union[Tensor[float16, bfloat16], None]) - The position embedding code. If S is greater than</span>
<span class="sd">          1024 and the mask of the lower triangle is used, enter only the inverse 1024 lines of the lower triangle for</span>
<span class="sd">          memory optimization.</span>
<span class="sd">          Input tensor of shape :math: `(B, N1, S1, S2)`, `(1, N1, S1, S2)`, `(B, N1, 1024, S2)`, `(1, N1, 1024, S2)`</span>
<span class="sd">          or (1024, 1024).</span>
<span class="sd">        - **drop_mask** (Union[Tensor[uint8], None]) - The dropout mask tensor.</span>
<span class="sd">          Input tensor of shape :math:`(B, N1, S1, S2 // 8) or None`.</span>
<span class="sd">        - **padding_mask** (None) - Reserved parameter. Not implemented yet.</span>
<span class="sd">        - **attn_mask** (Union[Tensor[uint8], None]) - The attention mask tensor. For each element, 0 indicates</span>
<span class="sd">          retention and 1 indicates discard. Input tensor of shape :math:`(B, N1, S1, S2)`, `(B, 1, S1, S2)`, `(S1, S2)`</span>
<span class="sd">          or (2048, 2048).</span>
<span class="sd">        - **prefix** (Union[Tensor[int64], None]) - N value of each Batch in the prefix sparse calculation scenario.</span>
<span class="sd">          Input tensor of shape :math:`(B,)`.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **softmax_max** (Tensor[float32]) - (B, N1, S1, 8)</span>
<span class="sd">        - **softmax_sum** (Tensor[float32]) - (B, N1, S1, 8)</span>
<span class="sd">        - **softmax_out** (Tensor[float16, bfloat16]) - Useless output, ignore it. Output tensor of shape : `()`</span>
<span class="sd">        - **attention_out** (Tensor[float16, bfloat16]) - The output of attention, its shape, and data type</span>
<span class="sd">          are the same as the query.</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend910B``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">head_num</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale_value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="o">=</span><span class="mi">2147483647</span><span class="p">,</span> <span class="n">next_tokens</span><span class="o">=</span><span class="mi">2147483647</span><span class="p">,</span>
                 <span class="n">inner_precise</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">input_layout</span><span class="o">=</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="n">sparse_mode</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize FlashAttentionScore&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;head_num&#39;</span><span class="p">,</span> <span class="n">head_num</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;keep_prob&#39;</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">GE</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">LE</span><span class="p">,</span> <span class="s2">&quot;keep_prob&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;scale_value&#39;</span><span class="p">,</span> <span class="n">scale_value</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;pre_tokens&#39;</span><span class="p">,</span> <span class="n">pre_tokens</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;next_tokens&#39;</span><span class="p">,</span> <span class="n">next_tokens</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;inner_precise&#39;</span><span class="p">,</span> <span class="n">inner_precise</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;sparse_mode&#39;</span><span class="p">,</span> <span class="n">sparse_mode</span><span class="p">,</span> <span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">valid_sparse_mode</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">sparse_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_sparse_mode</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attribute &#39;sparse_mode&#39; must be one of </span><span class="si">{</span><span class="n">valid_sparse_mode</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">sparse_mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">inner_precise</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attribute &#39;inner_precise&#39; must be 0, but got </span><span class="si">{</span><span class="n">inner_precise</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s1">&#39;input_layout&#39;</span><span class="p">,</span> <span class="n">input_layout</span><span class="p">,</span> <span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">support_layout</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;BSH&quot;</span><span class="p">,</span> <span class="s2">&quot;BNSD&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">input_layout</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">support_layout</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attribute &#39;input_layout&#39; must be one of </span><span class="si">{</span><span class="n">support_layout</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">input_layout</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;query&#39;</span><span class="p">,</span> <span class="s1">&#39;key&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="s1">&#39;real_shift&#39;</span><span class="p">,</span> <span class="s1">&#39;drop_mask&#39;</span><span class="p">,</span> <span class="s1">&#39;padding_mask&#39;</span><span class="p">,</span> <span class="s1">&#39;attn_mask&#39;</span><span class="p">,</span> <span class="s1">&#39;prefix&#39;</span><span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;softmax_max&#39;</span><span class="p">,</span> <span class="s1">&#39;softmax_sum&#39;</span><span class="p">,</span> <span class="s1">&#39;softmax_out&#39;</span><span class="p">,</span> <span class="s1">&#39;attention_out&#39;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">RmsNorm</span><span class="p">(</span><span class="n">Primitive</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The RmsNorm operator is a normalization operation, and its formula is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y=\frac{x_i}{\sqrt{\frac{1}{n}}\sum_{i=1}^{n}{ x_i^2}+\varepsilon  }\gamma_i</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This is an experimental API that is subject to change or deletion.</span>

<span class="sd">    Args:</span>
<span class="sd">        epsilon (float): prevent division by 0, default value is `1e-6`</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - **input_x** (Tensor) - Input data of RmsNorm, support data type: float16, float32, bfloat16.</span>
<span class="sd">        - **gamma** (Tensor) - Support data type: float16, float32, bfloat16.</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - **y** (Tensor) - Has the same type and shape with `input_x`.</span>
<span class="sd">        - **rstd** (Tensor) - Has the same type with `input_x`, used by gradient calculation.</span>

<span class="sd">    Raises:</span>
<span class="sd">        TypeError: If data type of `input_x` is not one of the following: float16, float32, bfloat16.</span>
<span class="sd">        TypeError: If data type of `gamma` is not one of the following: float16, float32, bfloat16.</span>
<span class="sd">        TypeError: If data type of &quot;input_x&quot; is not the same with the data type of &quot;gamma&quot;</span>

<span class="sd">    Supported Platforms:</span>
<span class="sd">        ``Ascend``</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@prim_attr_register</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Dense.&quot;&quot;&quot;</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_type</span><span class="p">(</span><span class="s2">&quot;epsilon&quot;</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_prim_io_names</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;gamma&#39;</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;rstd&quot;</span><span class="p">])</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>