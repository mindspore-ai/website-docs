<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.set_auto_parallel_context &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/mermaid-9.3.0.js"></script><script src="../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="mindspore.get_auto_parallel_context" href="mindspore.get_auto_parallel_context.html" />
    <link rel="prev" title="mindspore.get_context" href="mindspore.get_context.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">分布式并行原生</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore.html">mindspore</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore.html#数据表达">数据表达</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore.html#运行环境">运行环境</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mindspore.set_context.html">mindspore.set_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.get_context.html">mindspore.get_context</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore.set_auto_parallel_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.get_auto_parallel_context.html">mindspore.get_auto_parallel_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.reset_auto_parallel_context.html">mindspore.reset_auto_parallel_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.ParallelMode.html">mindspore.ParallelMode</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.set_ps_context.html">mindspore.set_ps_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.get_ps_context.html">mindspore.get_ps_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.reset_ps_context.html">mindspore.reset_ps_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.set_algo_parameters.html">mindspore.set_algo_parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.get_algo_parameters.html">mindspore.get_algo_parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.reset_algo_parameters.html">mindspore.reset_algo_parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.set_offload_context.html">mindspore.set_offload_context</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore.get_offload_context.html">mindspore.get_offload_context</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.html#随机种子">随机种子</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.html#序列化">序列化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.html#自动微分">自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.html#并行优化">并行优化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.html#即时编译">即时编译</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore.html#工具">工具</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/enveriment_preparation.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/model_development/model_development.html">网络搭建对比</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide/faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/operators.html">静态图语法-运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/statements.html">静态图语法-Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/python_builtin_functions.html">静态图语法-Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore.html">mindspore</a> &raquo;</li>
      <li>mindspore.set_auto_parallel_context</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/api_python/mindspore/mindspore.set_auto_parallel_context.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindspore-set-auto-parallel-context">
<h1>mindspore.set_auto_parallel_context<a class="headerlink" href="#mindspore-set-auto-parallel-context" title="永久链接至标题"></a></h1>
<a class="reference external image-reference" href="https://gitee.com/mindspore/mindspore/blob/r2.3/docs/api/api_python/mindspore/mindspore.set_auto_parallel_context.rst"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source.svg" /></a>
<dl class="py function">
<dt class="sig sig-object py" id="mindspore.set_auto_parallel_context">
<span class="sig-prename descclassname"><span class="pre">mindspore.</span></span><span class="sig-name descname"><span class="pre">set_auto_parallel_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/mindspore/context.html#set_auto_parallel_context"><span class="viewcode-link"><span class="pre">[源代码]</span></span></a><a class="headerlink" href="#mindspore.set_auto_parallel_context" title="打开链接"></a></dt>
<dd><p>配置自动并行，当前CPU仅支持数据并行。</p>
<div class="admonition note">
<p class="admonition-title">说明</p>
<p>配置时，必须输入配置的名称。如果某个程序具有不同并行模式下的任务，需要提前调用 <a class="reference internal" href="mindspore.reset_auto_parallel_context.html#mindspore.reset_auto_parallel_context" title="mindspore.reset_auto_parallel_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">mindspore.reset_auto_parallel_context()</span></code></a> 为下一个任务设置新的并行模式。若要设置或更改并行模式，必须在创建任何Initializer之前调用接口，否则，在编译网络时，可能会出现RuntimeError。</p>
</div>
<p>某些配置适用于特定的并行模式，有关详细信息，请参见下表：</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 48%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Common</p></th>
<th class="head"><p>AUTO_PARALLEL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>device_num</p></td>
<td><p>gradient_fp32_sync</p></td>
</tr>
<tr class="row-odd"><td><p>global_rank</p></td>
<td><p>loss_repeated_mean</p></td>
</tr>
<tr class="row-even"><td><p>gradients_mean</p></td>
<td><p>search_mode</p></td>
</tr>
<tr class="row-odd"><td><p>parallel_mode</p></td>
<td><p>parameter_broadcast</p></td>
</tr>
<tr class="row-even"><td><p>all_reduce_fusion_config</p></td>
<td><p>strategy_ckpt_load_file</p></td>
</tr>
<tr class="row-odd"><td><p>enable_parallel_optimizer</p></td>
<td><p>strategy_ckpt_save_file</p></td>
</tr>
<tr class="row-even"><td><p>parallel_optimizer_config</p></td>
<td><p>dataset_strategy</p></td>
</tr>
<tr class="row-odd"><td><p>enable_alltoall</p></td>
<td><p>pipeline_stages</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>auto_parallel_search_mode</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td><p>comm_fusion</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td><p>strategy_ckpt_config</p></td>
</tr>
</tbody>
</table>
<dl class="simple">
<dt>参数：</dt><dd><ul class="simple">
<li><p><strong>device_num</strong> (int) - 表示可用设备的编号，必须在[1,4096]范围中。默认值： <code class="docutils literal notranslate"><span class="pre">1</span></code> 。</p></li>
<li><p><strong>global_rank</strong> (int) - 表示全局RANK的ID，必须在[0,4095]范围中。默认值： <code class="docutils literal notranslate"><span class="pre">0</span></code> 。</p></li>
<li><p><strong>gradients_mean</strong> (bool) - 表示是否在梯度的 AllReduce后执行平均算子。stand_alone不支持gradients_mean。默认值： <code class="docutils literal notranslate"><span class="pre">False</span></code> 。</p></li>
<li><p><strong>gradient_fp32_sync</strong> (bool) - 在FP32中运行梯度的 AllReduce。stand_alone、data_parallel和hybrid_parallel不支持gradient_fp32_sync。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code> 。</p></li>
<li><p><strong>parallel_mode</strong> (str) - 有五种并行模式，分别是 <code class="docutils literal notranslate"><span class="pre">stand_alone</span></code> 、 <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> 、 <code class="docutils literal notranslate"><span class="pre">hybrid_parallel</span></code> 、 <code class="docutils literal notranslate"><span class="pre">semi_auto_parallel</span></code> 和 <code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code> 。默认值： <code class="docutils literal notranslate"><span class="pre">stand_alone</span></code> 。</p>
<ul>
<li><p>stand_alone：单卡模式。</p></li>
<li><p>data_parallel：数据并行模式。</p></li>
<li><p>hybrid_parallel：手动实现数据并行和模型并行。</p></li>
<li><p>semi_auto_parallel：半自动并行模式。</p></li>
<li><p>auto_parallel：自动并行模式。</p></li>
</ul>
</li>
<li><p><strong>search_mode</strong> (str) - 表示有三种策略搜索模式，分别是 <code class="docutils literal notranslate"><span class="pre">recursive_programming</span></code> ， <code class="docutils literal notranslate"><span class="pre">dynamic_programming</span></code> 和 <code class="docutils literal notranslate"><span class="pre">sharding_propagation</span></code> 。默认值： <code class="docutils literal notranslate"><span class="pre">recursive_programming</span></code> 。</p>
<ul>
<li><p>recursive_programming：表示双递归搜索模式。为了获取最优性能，建议用户设置batch size大于等于设备数与多副本并行数的乘积。</p></li>
<li><p>dynamic_programming：表示动态规划搜索模式。</p></li>
<li><p>sharding_propagation：表示从已配置算子的切分策略传播到所有算子。</p></li>
</ul>
</li>
<li><p><strong>auto_parallel_search_mode</strong> (str) - search_mode参数的兼容接口。将在后续的版本中删除。</p></li>
<li><p><strong>parameter_broadcast</strong> (bool) - 表示在训练前是否广播参数。在训练之前，为了使所有设备的网络初始化参数值相同，请将设备0上的参数广播到其他设备。不同并行模式下的参数广播不同。在 <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> 模式下，除layerwise_parallel属性为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 的参数外，所有参数都会被广播。在 <code class="docutils literal notranslate"><span class="pre">hybrid_parallel</span></code> 、 <code class="docutils literal notranslate"><span class="pre">semi_auto_parallel</span></code> 和 <code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code> 模式下，分段参数不参与广播。默认值： <code class="docutils literal notranslate"><span class="pre">False</span></code> 。</p></li>
<li><p><strong>strategy_ckpt_load_file</strong> (str) - 表示用于加载并行策略checkpoint的路径。目前不建议使用该参数，建议使用strategy_ckpt_config来替代它。默认值： <code class="docutils literal notranslate"><span class="pre">''</span></code> 。</p></li>
<li><p><strong>strategy_ckpt_save_file</strong> (str) - 表示用于保存并行策略checkpoint的路径。目前不建议使用该参数，建议使用strategy_ckpt_config来替代它。默认值： <code class="docutils literal notranslate"><span class="pre">''</span></code> 。</p></li>
<li><p><strong>full_batch</strong> (bool) - 如果在 <code class="docutils literal notranslate"><span class="pre">auto_parallel</span></code> 模式下加载整个batch数据集，则此参数应设置为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 。默认值： <code class="docutils literal notranslate"><span class="pre">False</span></code> 。目前不建议使用该接口，建议使用dataset_strategy来替换它。</p></li>
<li><p><strong>dataset_strategy</strong> (Union[str, tuple]) - 表示数据集分片策略。默认值： <code class="docutils literal notranslate"><span class="pre">data_parallel</span></code> 。dataset_strategy=”data_parallel”等于full_batch=False，dataset_strategy=”full_batch”等于full_batch=True。对于在静态图模式下执行并且通过模型并列策略加载到网络的数据集，如ds_stra ((1, 8)、(1, 8))，需要使用set_auto_parallel_context(dataset_strategy=ds_stra)。</p></li>
<li><p><strong>enable_parallel_optimizer</strong> (bool) - 这是一个开发中的特性，它可以为数据并行训练对权重更新计算进行分片，以节省时间和内存。目前，自动和半自动并行模式支持Ascend和GPU中的所有优化器。数据并行模式仅支持Ascend中的 <cite>Lamb</cite> 和 <cite>AdamWeightDecay</cite> 。默认值： <code class="docutils literal notranslate"><span class="pre">False</span></code> 。</p></li>
<li><p><strong>enable_alltoall</strong> (bool) - 允许在通信期间生成 <cite>AllToAll</cite> 通信算子的开关。如果其值为 False，则将由 <cite>AllGather</cite> 、 <cite>Split</cite> 和 <cite>Concat</cite> 等通信算子的组合来代替 <cite>AllToAll</cite> 。默认值： <code class="docutils literal notranslate"><span class="pre">False</span></code> 。</p></li>
<li><p><strong>all_reduce_fusion_config</strong> (list) - 通过参数索引设置 AllReduce 融合策略。仅支持ReduceOp.SUM和HCCL_WORLD_GROUP/NCCL_WORLD_GROUP。没有默认值。如果不设置，则关闭算子融合。</p></li>
<li><p><strong>pipeline_stages</strong> (int) - 设置pipeline并行的阶段信息。这表明了设备如何单独分布在pipeline上。所有的设备将被划分为pipeline_stags个阶段。默认值： <code class="docutils literal notranslate"><span class="pre">1</span></code> 。</p></li>
<li><p><strong>parallel_optimizer_config</strong> (dict) - 用于开启优化器并行后的行为配置。仅在enable_parallel_optimizer=True的时候生效。目前，它支持关键字如下的关键字：</p>
<ul>
<li><p>gradient_accumulation_shard(bool)：设置累加梯度变量是否在数据并行维度上进行切分。开启后，将进一步减小模型的显存占用，但是会在反向计算梯度时引入额外的通信算子（ReduceScatter）。此配置仅在流水线并行训练和梯度累加模式下生效。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code> 。</p></li>
<li><p>parallel_optimizer_threshold(int)：设置参数切分的阈值。占用内存小于该阈值的参数不做切分。占用内存大小 = shape[0] * … * shape[n] * size(dtype)。该阈值非负。单位：KB。默认值： <code class="docutils literal notranslate"><span class="pre">64</span></code> 。</p></li>
<li><p>optimizer_weight_shard_size(int)：设置指定优化器权重切分通信域的大小。只有当启用优化器并行时生效。数值范围可以是(0, device_num]，若同时开启流水线并行，数值范围则为(0, device_num/stage]。如果参数的数据并行通信域大小不能被 <cite>optimizer_weight_shard_size</cite> 整除，那么指定的优化器权重切分通信域大小就不会生效。默认值为 <code class="docutils literal notranslate"><span class="pre">-1</span></code> ，表示优化器权重切片通信域大小是每个参数的数据并行通信域大小。</p></li>
</ul>
</li>
<li><p><strong>comm_fusion</strong> (dict) - 用于设置通信算子的融合配置。可以同一类型的通信算子按梯度张量的大小或者顺序分块传输。输入格式为{“通信类型”: {“mode”:str, “config”: None int 或者 list}},每种通信算子的融合配置有两个键：”mode”和”config”。支持以下通信类型的融合类型和配置：</p>
<ul>
<li><p>openstate：是否开启通信融合功能。通过 <code class="docutils literal notranslate"><span class="pre">True</span></code> 或 <code class="docutils literal notranslate"><span class="pre">False</span></code> 来开启或关闭通信融合功能。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code> 。</p></li>
<li><p>allreduce：进行AllReduce算子的通信融合。”mode”包含：”auto”、”size”和”index”。在”auto”模式下，融合的是梯度变量的大小，默认值阈值为”64”MB，”config”对应的值为None。在”size”模式下，需要用户在config的字典中指定梯度大小阈值，这个值必须大于”0”MB。在”mode”为”index”时，它与”all_reduce_fusion_config”相同，用户需要给”config”传入一个列表，里面每个值表示梯度的索引。</p></li>
<li><p>allgather：进行AllGather算子的通信融合。”mode”包含：”auto”、”size”。”auto” 和 “size”模式的配置方式与AllReduce相同。</p></li>
<li><p>reducescatter：进行ReduceScatter算子的通信融合。”mode”包含：”auto”、”size”。”auto” 和 “size”模式的配置方式与AllReduce相同。</p></li>
</ul>
</li>
<li><p><strong>strategy_ckpt_config</strong> (dict) - 用于设置并行策略文件的配置。包含 <cite>strategy_ckpt_load_file</cite> 和 <cite>strategy_ckpt_save_file</cite> 两个参数的功能，建议使用此参数替换这两个参数。它包含以下配置：</p>
<ul>
<li><p>load_file(str)：加载并行切分策略的路径。如果文件扩展名为 <cite>.json</cite>，文件以json格式加载。否则，文件以ProtoBuf格式加载。默认值：””。</p></li>
<li><p>save_file(str)：保存并行切分策略的路径。如果文件扩展名为 <cite>.json</cite>，文件以json格式保存。否则，文件以ProtoBuf格式保存。默认值：””。</p></li>
<li><p>only_trainable_params(bool)：仅保存/加载可训练参数的策略信息。默认值： <code class="docutils literal notranslate"><span class="pre">True</span></code> 。</p></li>
</ul>
</li>
</ul>
</dd>
<dt>异常：</dt><dd><ul class="simple">
<li><p><strong>ValueError</strong> - 输入key不是自动并行上下文中的属性。</p></li>
</ul>
</dd>
</dl>
<p><strong>样例：</strong></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">global_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">gradients_mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">gradient_fp32_sync</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_mode</span><span class="o">=</span><span class="s2">&quot;auto_parallel&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">search_mode</span><span class="o">=</span><span class="s2">&quot;dynamic_programming&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">auto_parallel_search_mode</span><span class="o">=</span><span class="s2">&quot;dynamic_programming&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parameter_broadcast</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_load_file</span><span class="o">=</span><span class="s2">&quot;./strategy_stage1.ckpt&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="s2">&quot;./strategy_stage1.ckpt&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">dataset_strategy</span><span class="o">=</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">enable_alltoall</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">all_reduce_fusion_config</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">160</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">pipeline_stages</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">parallel_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;gradient_accumulation_shard&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;parallel_optimizer_threshold&quot;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span>
<span class="gp">... </span>                   <span class="s2">&quot;optimizer_weight_shard_size&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">parallel_optimizer_config</span><span class="o">=</span><span class="n">parallel_config</span><span class="p">,</span> <span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;allreduce&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">},</span> <span class="s2">&quot;allgather&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;size&quot;</span><span class="p">,</span> <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">comm_fusion</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">stra_ckpt_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;load_file&quot;</span><span class="p">:</span> <span class="s2">&quot;./stra0.ckpt&quot;</span><span class="p">,</span> <span class="s2">&quot;save_file&quot;</span><span class="p">:</span> <span class="s2">&quot;./stra1.ckpt&quot;</span><span class="p">,</span> <span class="s2">&quot;only_trainable_params&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ms</span><span class="o">.</span><span class="n">set_auto_parallel_context</span><span class="p">(</span><span class="n">strategy_ckpt_config</span><span class="o">=</span><span class="n">stra_ckpt_dict</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore.get_context.html" class="btn btn-neutral float-left" title="mindspore.get_context" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="mindspore.get_auto_parallel_context.html" class="btn btn-neutral float-right" title="mindspore.get_auto_parallel_context" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>