<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>学习率与优化器 &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/mermaid-9.3.0.js"></script><script src="../../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="梯度求导" href="gradient.html" />
    <link rel="prev" title="损失函数" href="loss_function.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">设计</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../design/overview.html">MindSpore设计概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/programming_paradigm.html">函数式和对象式融合编程范式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/dynamic_graph_and_static_graph.html">动静态图结合</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/distributed_training_design.html">分布式并行原生</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/data_engine.html">高性能数据处理引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/all_scenarios.html">全场景统一架构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/graph_fusion_engine.html">图算融合加速引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/pluggable_device.html">三方硬件对接</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../design/glossary.html">术语</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型库</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/official_models.html">官方模型库</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.nn.html">mindspore.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.ops.html">mindspore.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.ops.primitive.html">mindspore.ops.primitive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.amp.html">mindspore.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.train.html">mindspore.train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.communication.html">mindspore.communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.common.initializer.html">mindspore.common.initializer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.hal.html">mindspore.hal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.html">mindspore.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.dataset.transforms.html">mindspore.dataset.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.mindrecord.html">mindspore.mindrecord</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.nn.probability.html">mindspore.nn.probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.rewrite.html">mindspore.rewrite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.multiprocessing.html">mindspore.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.boost.html">mindspore.boost</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.numpy.html">mindspore.numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.scipy.html">mindspore.scipy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_python/mindspore.experimental.html">mindspore.experimental</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API映射</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/api_mapping/pytorch_api_mapping.html">PyTorch与MindSpore API映射表</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">迁移指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">迁移指南概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../enveriment_preparation.html">环境准备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../analysis_and_preparation.html">模型分析与准备</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="model_development.html">网络搭建对比</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dataset.html">数据处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="model_and_cell.html">网络搭建</a></li>
<li class="toctree-l2"><a class="reference internal" href="loss_function.html">损失函数</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">学习率与优化器</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradient.html">梯度求导</a></li>
<li class="toctree-l2"><a class="reference internal" href="training_and_evaluation.html">训练及推理流程</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../debug_and_tune.html">调试调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../sample_code.html">网络迁移调试实例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">常见问题</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">语法支持</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax_support.html">静态图语法支持</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/operators.html">静态图语法-运算符</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/statements.html">静态图语法-Python语句</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/static_graph_syntax/python_builtin_functions.html">静态图语法-Python内置函数</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../note/index_support.html">Tensor索引支持</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">环境变量</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../note/env_var_list.html">环境变量</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/data_processing.html">数据处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/implement_problem.html">执行问题</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/network_compilation.html">网络编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/operators_compile.html">算子编译</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/usage_migrate_3rd.html">第三方框架迁移使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/performance_tuning.html">性能调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/precision_tuning.html">精度调优</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/distributed_parallel.html">分布式并行</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/inference.html">推理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq/feature_advice.html">特性咨询</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="model_development.html">网络搭建对比</a> &raquo;</li>
      <li>学习率与优化器</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/migration_guide/model_development/learning_rate_and_optimizer.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="学习率与优化器">
<h1>学习率与优化器<a class="headerlink" href="#学习率与优化器" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/mindspore/source_zh_cn/migration_guide/model_development/learning_rate_and_optimizer.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source.png" /></a></p>
<p>在阅读本章节之前，请先阅读MindSpore官网教程<a class="reference external" href="https://mindspore.cn/tutorials/zh-CN/r2.3/advanced/modules/optimizer.html">优化器</a>。</p>
<p>这里就MindSpore的优化器的一些特殊使用方式和学习率衰减策略的原理做一个介绍。</p>
<section id="优化器对比">
<h2>优化器对比<a class="headerlink" href="#优化器对比" title="永久链接至标题"></a></h2>
<section id="优化器支持差异">
<h3>优化器支持差异<a class="headerlink" href="#优化器支持差异" title="永久链接至标题"></a></h3>
<p>PyTorch和MindSpore同时支持的优化器异同比较详见<a class="reference external" href="https://mindspore.cn/docs/zh-CN/r2.3/note/api_mapping/pytorch_api_mapping.html#torch-optim">API映射表</a>。MindSpore暂不支持的优化器：LBFGS，NAdam，RAdam。</p>
</section>
<section id="优化器的执行和使用差异">
<h3>优化器的执行和使用差异<a class="headerlink" href="#优化器的执行和使用差异" title="永久链接至标题"></a></h3>
<p>PyTorch单步执行优化器时，一般需要手动执行 <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> 方法将历史梯度设置为0(或None)，然后使用 <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> 计算当前训练step的梯度，最后调用优化器的 <code class="docutils literal notranslate"><span class="pre">step()</span></code> 方法实现网络权重的更新；</p>
<p>MindSpore中优化器的使用，只需要直接对梯度进行计算，然后使用 <code class="docutils literal notranslate"><span class="pre">optimizer(grads)</span></code> 执行网络权重的更新。</p>
<table class="colwidths-auto docutils align-default">
<tr>
<td style="text-align:center"> PyTorch </td> <td style="text-align:center"> MindSpore </td>
</tr>
<tr>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</pre>
</td>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</pre>
</td>
</tr>
</table>
</section>
<section id="超参差异">
<h3>超参差异<a class="headerlink" href="#超参差异" title="永久链接至标题"></a></h3>
<section id="超参名称">
<h4>超参名称<a class="headerlink" href="#超参名称" title="永久链接至标题"></a></h4>
<p>网络权重和学习率入参名称异同：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>MindSpore</p></th>
<th class="head"><p>差异</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>网络权重</p></td>
<td><p>params</p></td>
<td><p>params</p></td>
<td><p>参数名相同</p></td>
</tr>
<tr class="row-odd"><td><p>学习率</p></td>
<td><p>lr</p></td>
<td><p>learning_rate</p></td>
<td><p>参数名不同</p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto docutils align-default">
<tr>
<td style="text-align:center"> PyTorch </td> <td style="text-align:center"> MindSpore </td>
</tr>
<tr>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</pre>
</td>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</pre>
</td>
</tr>
</table>
</section>
<section id="超参配置方式">
<h4>超参配置方式<a class="headerlink" href="#超参配置方式" title="永久链接至标题"></a></h4>
<ul>
<li><p>参数不分组：</p>
<p><code class="docutils literal notranslate"><span class="pre">params</span></code> 入参支持类型不同： PyTorch入参类型为 <code class="docutils literal notranslate"><span class="pre">iterable(Tensor)</span></code> 和 <code class="docutils literal notranslate"><span class="pre">iterable(dict)</span></code>，支持迭代器类型；
MindSpore入参类型为 <code class="docutils literal notranslate"><span class="pre">list(Parameter)</span></code>，<code class="docutils literal notranslate"><span class="pre">list(dict)</span></code>，不支持迭代器。</p>
<p>其他超参配置及支持差异详见<a class="reference external" href="https://mindspore.cn/docs/zh-CN/r2.3/note/api_mapping/pytorch_api_mapping.html#torch-optim">API映射表</a>。</p>
</li>
<li><p>参数分组：</p>
<p>PyTorch支持所有参数分组；MindSpore仅支持特定key分组：”params”，”lr”，”weight_decay”，”grad_centralization”，”order_params”。</p>
  <div class="wy-table-responsive">
  <table class="colwidths-auto docutils align-default">
  <tr>
  <td style="text-align:center"> PyTorch </td> <td style="text-align:center"> MindSpore </td>
  </tr>
  <tr>
  <td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">parameters</span><span class="p">()},</span>
            <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>
        <span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
  </pre>
  </td>
  <td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">no_conv_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s1">&#39;conv&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()))</span>
<span class="n">group_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_params</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.02</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">no_conv_params</span><span class="p">}]</span>

<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
  </pre>
  </td>
  </tr>
  </table>
  </div>
</li>
</ul>
</section>
<section id="运行时超参修改">
<h4>运行时超参修改<a class="headerlink" href="#运行时超参修改" title="永久链接至标题"></a></h4>
<p>PyTorch支持在训练过程中修改任意的优化器参数，并提供了 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 用于动态修改学习率；</p>
<p>MindSpore当前不支持训练过程中修改优化器参数，但提供了修改学习率和权重衰减的方式，使用方式详见<a class="reference internal" href="#学习率策略对比"><span class="std std-doc">学习率</span></a>和<a class="reference internal" href="#权重衰减"><span class="std std-doc">权重衰减</span></a>章节。</p>
</section>
</section>
<section id="权重衰减">
<h3>权重衰减<a class="headerlink" href="#权重衰减" title="永久链接至标题"></a></h3>
<p>PyTorch中修改 <code class="docutils literal notranslate"><span class="pre">weight_decay</span></code> 示例如下；</p>
<p>MindSpore中实现动态weight decay：用户可以继承 <code class="docutils literal notranslate"><span class="pre">Cell</span></code> 自定义动态weight decay的类，传入优化器中。</p>
<table class="colwidths-auto docutils align-default">
<tr>
<td style="text-align:center"> PyTorch </td> <td style="text-align:center"> MindSpore </td>
</tr>
<tr>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">decay_factor</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">]</span> <span class="o">*=</span> <span class="n">decay_factor</span>
</pre></div>
</div>
</pre>
</td>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExponentialWeightDecay</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span> <span class="n">decay_rate</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExponentialWeightDecay</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span> <span class="o">=</span> <span class="n">decay_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span> <span class="o">=</span> <span class="n">decay_steps</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">global_step</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">decay_steps</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">*</span> <span class="n">ops</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">decay_rate</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>

<span class="n">weight_decay</span> <span class="o">=</span> <span class="n">ExponentialWeightDecay</span><span class="p">(</span><span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">decay_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
</pre></div>
</div>
</pre>
</td>
</tr>
</table>
</section>
<section id="优化器状态的保存与加载">
<h3>优化器状态的保存与加载<a class="headerlink" href="#优化器状态的保存与加载" title="永久链接至标题"></a></h3>
<p>PyTorch的优化器模块提供了 <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> 用于优化器状态的查看及保存，<code class="docutils literal notranslate"><span class="pre">load_state_dict</span></code> 用于优化器状态的加载。</p>
<p>MindSpore的优化器模块继承自 <code class="docutils literal notranslate"><span class="pre">Cell</span></code>，优化器的保存与加载和网络的保存与加载方式相同，通常情况下配合 <code class="docutils literal notranslate"><span class="pre">save_checkpoint</span></code> 与<code class="docutils literal notranslate"><span class="pre">load_checkpoint</span></code> 使用。</p>
<table class="colwidths-auto docutils align-default">
<tr>
<td style="text-align:center"> PyTorch </td> <td style="text-align:center"> MindSpore </td>
</tr>
<tr>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 优化器保存：</span>
<span class="c1"># 使用torch.save()把获取到的state_dict保存到pkl文件中</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 优化器加载：</span>
<span class="c1"># 使用torch.load()加载保存的state_dict，</span>
<span class="c1"># 然后使用load_state_dict将获取到的state_dict加载到优化器中</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>
</pre></div>
</div>
</pre>
</td>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 优化器保存：</span>
<span class="c1"># 使用mindspore.save_checkpoint()将优化器实例保存到ckpt文件中</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">save_path</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 优化器加载：</span>
<span class="c1"># 使用mindspore.load_checkpoint()加载保存的ckpt文件，</span>
<span class="c1"># 然后使用load_param_into_net将获取到的param_dict加载到优化器中</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">param_dict</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
<span class="n">mindspore</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
</pre></div>
</div>
</pre>
</td>
</tr>
</table>
</section>
</section>
<section id="学习率策略对比">
<h2>学习率策略对比<a class="headerlink" href="#学习率策略对比" title="永久链接至标题"></a></h2>
<section id="动态学习率差异">
<h3>动态学习率差异<a class="headerlink" href="#动态学习率差异" title="永久链接至标题"></a></h3>
<p>PyTorch中定义了 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 类用于对学习率进行管理。使用动态学习率时，将 <code class="docutils literal notranslate"><span class="pre">optimizer</span></code> 实例传入 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 子类中，通过循环调用 <code class="docutils literal notranslate"><span class="pre">scheduler.step()</span></code> 执行学习率修改，并将修改同步至优化器中。</p>
<p>MindSpore中的动态学习率有 <code class="docutils literal notranslate"><span class="pre">Cell</span></code> 和 <code class="docutils literal notranslate"><span class="pre">list</span></code> 两种实现方式，两种类型的动态学习率使用方式一致，都是在实例化完成之后传入优化器，前者在内部的 <code class="docutils literal notranslate"><span class="pre">construct</span></code> 中进行每一步学习率的计算，后者直接按照计算逻辑预生成学习率列表，训练过程中内部实现学习率的更新。具体请参考<a class="reference external" href="https://mindspore.cn/docs/zh-CN/r2.3/api_python/mindspore.nn.html#%E5%8A%A8%E6%80%81%E5%AD%A6%E4%B9%A0%E7%8E%87">动态学习率</a>。</p>
<table class="colwidths-auto docutils align-default">
<tr>
<td style="text-align:center"> PyTorch </td> <td style="text-align:center"> MindSpore </td>
</tr>
<tr>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</pre>
</td>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">polynomial_decay_lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PolynomialDecayLR</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">end_learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">decay_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">polynomial_decay_lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="n">grad_fn</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">forward_fn</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">has_aux</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
    <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">(</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</pre>
</td>
</tr>
</table>
</section>
<section id="自定义学习率差异">
<h3>自定义学习率差异<a class="headerlink" href="#自定义学习率差异" title="永久链接至标题"></a></h3>
<p>PyTorch的动态学习率模块 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 提供了<code class="docutils literal notranslate"><span class="pre">LambdaLR</span></code> 接口供用户自定义学习率调整规则，用户通过传入lambda表达式或自定义函数实现学习率指定。</p>
<p>MindSpore未提供类似的lambda接口，自定义学习率调整策略可以通过自定义函数或自定义 <code class="docutils literal notranslate"><span class="pre">LearningRateSchedule</span></code> 来实现。</p>
<table class="colwidths-auto docutils align-default">
<tr>
<td style="text-align:center"> PyTorch </td> <td style="text-align:center"> MindSpore </td>
</tr>
<tr>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">lbd</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="n">epoch</span> <span class="o">//</span> <span class="mi">5</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="o">=</span><span class="n">lbd</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">validate</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</pre>
</td>
<td style="vertical-align:top"><pre>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 方式一：定义python函数指定计算逻辑，返回学习率列表</span>
<span class="k">def</span> <span class="nf">dynamic_lr</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">total_step</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">):</span>
    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_step</span><span class="p">):</span>
        <span class="n">current_epoch</span> <span class="o">=</span> <span class="n">i</span> <span class="o">//</span> <span class="n">step_per_epoch</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="n">current_epoch</span> <span class="o">//</span> <span class="mi">5</span>
        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">factor</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lrs</span>

<span class="n">decay_lr</span> <span class="o">=</span> <span class="n">dynamic_lr</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">total_step</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">decay_lr</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 方式二：继承LearningRateSchedule，在construct方法中定义变化策略</span>
<span class="k">class</span> <span class="nc">DynamicDecayLR</span><span class="p">(</span><span class="n">LearningRateSchedule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DynamicDecayLR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_per_epoch</span> <span class="o">=</span> <span class="n">step_per_epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cast</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">Cast</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">global_step</span><span class="p">):</span>
        <span class="n">current_epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">global_step</span><span class="p">,</span> <span class="n">mstype</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">//</span> <span class="n">step_per_epoch</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">current_epoch</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span>

<span class="n">decay_lr</span> <span class="o">=</span> <span class="n">DynamicDecayLR</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">optim</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">decay_lr</span><span class="p">)</span>
</pre></div>
</div>
</pre>
</td>
</tr>
</table>
</section>
<section id="学习率获取">
<h3>学习率获取<a class="headerlink" href="#学习率获取" title="永久链接至标题"></a></h3>
<p>PyTorch：</p>
<ul class="simple">
<li><p>固定学习率情况下，通常通过 <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()</span></code> 进行学习率的查看和打印，例如参数分组时，对于第n个参数组，使用 <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()['param_groups'][n]['lr']</span></code>，参数不分组时，使用 <code class="docutils literal notranslate"><span class="pre">optimizer.state_dict()['param_groups'][0]['lr']</span></code>；</p></li>
<li><p>动态学习率情况下，可以使用 <code class="docutils literal notranslate"><span class="pre">LRScheduler</span></code> 的 <code class="docutils literal notranslate"><span class="pre">get_lr</span></code> 方法获取当前学习率，或使用 <code class="docutils literal notranslate"><span class="pre">print_lr</span></code> 方法打印学习率。</p></li>
</ul>
<p>MindSpore：</p>
<ul class="simple">
<li><p>目前未提供直接查看学习率的接口，后续版本中会针对此问题进行修复。</p></li>
</ul>
</section>
<section id="学习率更新">
<h3>学习率更新<a class="headerlink" href="#学习率更新" title="永久链接至标题"></a></h3>
<p>PyTorch：</p>
<p>PyTorch提供了<code class="docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler</span></code>包用于动态修改lr，使用的时候需要显式地调用<code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>和<code class="docutils literal notranslate"><span class="pre">scheduler.step()</span></code>来更新lr，详情请参考<a class="reference external" href="https://pytorch.org/docs/1.12/optim.html#how-to-adjust-learning-rate">如何调整学习率</a>。</p>
<p>MindSpore：</p>
<p>MindSpore的学习率是包到优化器里面的，每调用一次优化器，学习率更新的step会自动更新一次。</p>
</section>
</section>
<section id="参数分组">
<h2>参数分组<a class="headerlink" href="#参数分组" title="永久链接至标题"></a></h2>
<p>MindSpore的优化器支持一些特别的操作，比如对网络里所有的可训练的参数可以设置不同的学习率（lr）、权重衰减（weight_decay）和梯度中心化（grad_centralization）策略，如：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># 定义模型</span>
<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">12</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">SequentialCell</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;pad&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveMaxPool2d</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">def</span> <span class="nf">params_not_in</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_list</span><span class="p">):</span>
    <span class="c1"># 利用Parameter的id来判断一个param是否不在param_list中</span>
    <span class="n">param_id</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_list</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">==</span> <span class="n">param_id</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="p">()</span>
<span class="n">trainable_param</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()</span>
<span class="n">conv_weight</span><span class="p">,</span> <span class="n">bn_weight</span><span class="p">,</span> <span class="n">dense_weight</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">cell</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
    <span class="c1"># 判断是什么API，将对应参数加到不同列表里</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
        <span class="n">conv_weight</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
        <span class="n">bn_weight</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
        <span class="n">bn_weight</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">):</span>
        <span class="n">dense_weight</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

<span class="n">other_param</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># 所有分组里的参数不能重复，并且其交集是需要做参数更新的所有参数</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">trainable_param</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">params_not_in</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">conv_weight</span><span class="p">)</span> <span class="ow">and</span> <span class="n">params_not_in</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">bn_weight</span><span class="p">)</span> <span class="ow">and</span> <span class="n">params_not_in</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">dense_weight</span><span class="p">):</span>
        <span class="n">other_param</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

<span class="n">group_param</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;order_params&#39;</span><span class="p">:</span> <span class="n">trainable_param</span><span class="p">}]</span>
<span class="c1"># 每一个分组的参数列表不能是空的</span>

<span class="k">if</span> <span class="n">conv_weight</span><span class="p">:</span>
    <span class="n">conv_weight_lr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">cosine_decay_lr</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">total_step</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">step_per_epoch</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">decay_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">group_param</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">conv_weight</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="n">conv_weight_lr</span><span class="p">})</span>
<span class="k">if</span> <span class="n">bn_weight</span><span class="p">:</span>
    <span class="n">group_param</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">bn_weight</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">0.</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">})</span>
<span class="k">if</span> <span class="n">dense_weight</span><span class="p">:</span>
    <span class="n">group_param</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">dense_weight</span><span class="p">,</span> <span class="s1">&#39;weight_decay&#39;</span><span class="p">:</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">})</span>
<span class="k">if</span> <span class="n">other_param</span><span class="p">:</span>
    <span class="n">group_param</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">other_param</span><span class="p">})</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_param</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>
</div>
<p>需要注意以下几点：</p>
<ol class="arabic simple">
<li><p>每一个分组的参数列表不能是空的；</p></li>
<li><p>如果没有设置<code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>和<code class="docutils literal notranslate"><span class="pre">lr</span></code>则使用优化器里设置的值，设置了的话使用分组参数字典里的值；</p></li>
<li><p>每个分组里的<code class="docutils literal notranslate"><span class="pre">lr</span></code>都可以是静态或动态的，但不能再分组；</p></li>
<li><p>每个分组里的<code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>都需要是符合规范的浮点数；</p></li>
<li><p>所有分组里的参数不能重复，并且其交集是需要做参数更新的所有参数。</p></li>
</ol>
</section>
<section id="mindspore的学习率衰减策略">
<h2>MindSpore的学习率衰减策略<a class="headerlink" href="#mindspore的学习率衰减策略" title="永久链接至标题"></a></h2>
<p>在训练过程中，MindSpore的学习率是以参数的形式存在于网络里的，在执行优化器更新网络可训练参数前，MindSpore会调用<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.3/api_python/nn/mindspore.nn.Optimizer.html#mindspore.nn.Optimizer.get_lr">get_lr</a>
方法获取到当前step需要的学习率的值。</p>
<p>MindSpore的学习率支持静态、动态、分组三种，其中静态学习率在网络里是一个float32类型的Tensor。</p>
<p>动态学习率有两种，一种在网络里是一个长度为训练总的step数，float32类型的Tensor，如<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.3/api_python/mindspore.nn.html#dynamic-lr%E5%87%BD%E6%95%B0">Dynamic LR函数</a>。在优化器里有一个<code class="docutils literal notranslate"><span class="pre">global_step</span></code>的参数，每经过一次优化器更新参数会+1，MindSpore内部会根据<code class="docutils literal notranslate"><span class="pre">global_step</span></code>和<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>这两个参数来获取当前step的学习率的值；
另一种是通过构图来生成学习率的值的，如<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.3/api_python/mindspore.nn.html#learningrateschedule%E7%B1%BB">LearningRateSchedule类</a>。</p>
<p>分组学习率如上一小节参数分组中介绍的。</p>
<p>因为MindSpore的学习率是参数，我们也可以通过给<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>参数赋值的方式修改训练过程中学习率的值，如<a class="reference external" href="https://www.mindspore.cn/docs/zh-CN/r2.3/_modules/mindspore/train/callback/_lr_scheduler_callback.html#LearningRateScheduler">LearningRateScheduler Callback</a>，这种方法只支持优化器中传入静态的学习率。关键代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">ops</span><span class="p">,</span> <span class="n">nn</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">learning_rate</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
<span class="n">new_lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="c1"># 改写learning_rate参数的值</span>
<span class="n">ops</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">new_lr</span><span class="p">,</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">learning_rate</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<p>运行结果：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>0.1
0.01
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="loss_function.html" class="btn btn-neutral float-left" title="损失函数" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="gradient.html" class="btn btn-neutral float-right" title="梯度求导" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>