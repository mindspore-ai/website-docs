<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore.context &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../federated_server.html">Federated-Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../federated_client.html">Federated-Client</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">Module code</a> &raquo;</li>
      <li>mindspore.context</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore.context</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2021 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The context of mindspore, used to configure the current execution environment,</span>
<span class="sd">includes the execution mode, execution backend and other feature switches.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">FunctionType</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">MSContext</span><span class="p">,</span> <span class="n">ms_ctx_param</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">args_type_check</span><span class="p">,</span> <span class="n">Validator</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._auto_parallel_context</span> <span class="kn">import</span> <span class="n">_set_auto_parallel_context</span><span class="p">,</span> <span class="n">_get_auto_parallel_context</span><span class="p">,</span> \
    <span class="n">_reset_auto_parallel_context</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._ps_context</span> <span class="kn">import</span> <span class="n">_set_ps_context</span><span class="p">,</span> <span class="n">_get_ps_context</span><span class="p">,</span> <span class="n">_reset_ps_context</span>
<span class="kn">from</span> <span class="nn">.default_config</span> <span class="kn">import</span> <span class="n">__device_target__</span><span class="p">,</span> <span class="n">__package_name__</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;GRAPH_MODE&#39;</span><span class="p">,</span> <span class="s1">&#39;PYNATIVE_MODE&#39;</span><span class="p">,</span> <span class="s1">&#39;set_context&#39;</span><span class="p">,</span> <span class="s1">&#39;get_context&#39;</span><span class="p">,</span> <span class="s1">&#39;set_auto_parallel_context&#39;</span><span class="p">,</span>
           <span class="s1">&#39;get_auto_parallel_context&#39;</span><span class="p">,</span> <span class="s1">&#39;reset_auto_parallel_context&#39;</span><span class="p">,</span> <span class="s1">&#39;ParallelMode&#39;</span><span class="p">,</span> <span class="s1">&#39;set_ps_context&#39;</span><span class="p">,</span>
           <span class="s1">&#39;get_ps_context&#39;</span><span class="p">,</span> <span class="s1">&#39;reset_ps_context&#39;</span><span class="p">,</span> <span class="s1">&#39;set_fl_context&#39;</span><span class="p">,</span> <span class="s1">&#39;get_fl_context&#39;</span><span class="p">]</span>

<span class="n">GRAPH_MODE</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">PYNATIVE_MODE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">_DEVICE_APP_MEMORY_SIZE</span> <span class="o">=</span> <span class="mi">31</span>  <span class="c1"># The max memory size of graph plus variable.</span>
<span class="n">_re_pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;[1-9][0-9]*(\.)?[0-9]*GB|0\.[0-9]*GB&#39;</span>
<span class="n">_k_context</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_make_directory</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Make directory.&quot;&quot;&quot;</span>
    <span class="n">real_path</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">path</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input path `</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">` is invalid type&quot;</span><span class="p">)</span>

    <span class="c1"># convert the relative paths</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;The absolute path is </span><span class="si">%r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="c1"># check whether the path is already existed and has written permissions</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="n">real_path</span> <span class="o">=</span> <span class="n">path</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># All exceptions need to be caught because create directory maybe have some limit(permissions)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;The directory(</span><span class="si">%s</span><span class="s2">) doesn&#39;t exist, will create it&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="n">real_path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">except</span> <span class="ne">PermissionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No write permission on the directory `</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">, error = </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No write permission on the directory `</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">`.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">real_path</span>


<span class="k">def</span> <span class="nf">_get_print_file_name</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add timestamp suffix to file name. Rename the file name:  file_name + &quot;.&quot; + time(seconds).&quot;&quot;&quot;</span>
    <span class="n">time_second</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()))</span>
    <span class="n">file_name</span> <span class="o">=</span> <span class="n">file_name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">time_second</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
        <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;This file </span><span class="si">{}</span><span class="s2"> already exists.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">file_name</span>


<span class="k">class</span> <span class="nc">_ThreadLocalInfo</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Thread local Info used for store thread local attributes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ThreadLocalInfo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug_runtime</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span>

    <span class="nd">@reserve_class_name_in_scope</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reserve_class_name_in_scope</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Set reserve_class_name_in_scope value must be bool!&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span> <span class="o">=</span> <span class="n">reserve_class_name_in_scope</span>


<span class="n">_ContextRecord</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;_ContextRecord&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;is_pynative_mode&quot;</span><span class="p">,</span> <span class="s2">&quot;switch_context_fn&quot;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">_ContextSwitchInfo</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Record of context switch information.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_pynative (bool): Whether to adopt the PyNative mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_pynative</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ContextSwitchInfo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">is_pynative</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_pynative</span><span class="p">,</span> <span class="n">switch_context_fn</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Push a context switch record onto the stack.</span>

<span class="sd">        Args:</span>
<span class="sd">            is_pynative (bool): Whether context switch to PyNative mode.</span>
<span class="sd">            switch_context_fn (Function): A callable that executes the context switch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">switch_context_fn</span><span class="p">,</span> <span class="n">FunctionType</span><span class="p">):</span>
            <span class="n">switch_context_fn</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">_ContextRecord</span><span class="p">(</span><span class="n">is_pynative</span><span class="p">,</span> <span class="n">switch_context_fn</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_Context</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    _Context is the environment in which operations are executed</span>

<span class="sd">    Note:</span>
<span class="sd">        Create a context through instantiating Context object is not recommended.</span>
<span class="sd">        should use context() to get the context since Context is singleton.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_instance</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_instance_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span> <span class="o">=</span> <span class="n">_ThreadLocalInfo</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span> <span class="o">=</span> <span class="n">_ContextSwitchInfo</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="o">=</span> <span class="n">MSContext</span><span class="o">.</span><span class="n">get_instance</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance_lock</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance_lock</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span>

    <span class="k">def</span> <span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attr</span> <span class="o">==</span> <span class="s2">&quot;_context_handle&quot;</span> <span class="ow">and</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context handle is none in context!!!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">get_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Switch between Graph mode and PyNative mode.</span>

<span class="sd">        Args:</span>
<span class="sd">            mode (int): GRAPH_MODE or PYNATIVE_MODE.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">PYNATIVE_MODE</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;vm&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">GRAPH_MODE</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;ge&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The execution mode </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s1"> is invalid!&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_backend_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
        <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">success</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Backend policy must be one of ge, vm, ms.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_save_graphs_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">save_graphs_path</span><span class="p">,</span> <span class="n">_make_directory</span><span class="p">(</span><span class="n">save_graphs_path</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">set_device_target</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">valid_targets</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="s2">&quot;Davinci&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">valid_targets</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target device name </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2"> is invalid! It must be one of </span><span class="si">{</span><span class="n">valid_targets</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;Davinci&quot;</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;Ascend&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="ow">and</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;vm&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_auto_tune_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tune_mode</span><span class="p">):</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;NO_TUNE&quot;</span><span class="p">,</span> <span class="s2">&quot;RL&quot;</span><span class="p">,</span> <span class="s2">&quot;GA&quot;</span><span class="p">,</span> <span class="s2">&quot;RL,GA&quot;</span><span class="p">,</span> <span class="s2">&quot;GA,RL&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">tune_mode</span> <span class="ow">in</span> <span class="n">candidate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">tune_mode</span><span class="p">,</span> <span class="n">tune_mode</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tune mode must be in [&#39;NO_TUNE&#39;, &#39;RL&#39;, &#39;GA&#39;, &#39;RL,GA&#39;, &#39;GA,RL&#39;], but got </span><span class="si">{</span><span class="n">tune_mode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_device_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">device_id</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">device_id</span> <span class="o">&gt;</span> <span class="mi">4095</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device id must be in [0, 4095], but got </span><span class="si">{</span><span class="n">device_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_id</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_max_call_depth</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">max_call_depth</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max call depth must be greater than 0, but got </span><span class="si">{</span><span class="n">max_call_depth</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">max_call_depth</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_profiling_options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">option</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">option</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;The parameter option must be str.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">profiling_options</span><span class="p">,</span> <span class="n">option</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_variable_memory_max_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable_memory_max_size</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;set values of variable_memory_max_size and graph_memory_max_size&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">,</span> <span class="n">_re_pattern</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context param variable_memory_max_size should be in correct format! Such as </span><span class="se">\&quot;</span><span class="s2">5GB</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">_DEVICE_APP_MEMORY_SIZE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context param variable_memory_max_size should be not greater than 31GB.&quot;</span><span class="p">)</span>
        <span class="n">variable_memory_max_size_</span> <span class="o">=</span> <span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; * 1024 * 1024 * 1024&quot;</span>
        <span class="n">graph_memory_max_size</span> <span class="o">=</span> <span class="n">_DEVICE_APP_MEMORY_SIZE</span> <span class="o">-</span> <span class="nb">int</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">graph_memory_max_size_</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">graph_memory_max_size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; * 1024 * 1024 * 1024&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">variable_memory_max_size</span><span class="p">,</span> <span class="n">variable_memory_max_size_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">_graph_memory_max_size</span><span class="p">,</span> <span class="n">graph_memory_max_size_</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_max_device_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">,</span> <span class="n">_re_pattern</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context param max_device_memory should be in correct format! Such as </span><span class="se">\&quot;</span><span class="s2">3.5GB</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">max_device_memory_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">max_device_memory_value</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context param max_device_memory should be in correct format! Such as </span><span class="se">\&quot;</span><span class="s2">3.5GB</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">max_device_memory</span><span class="p">,</span> <span class="n">max_device_memory_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_print_file_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add timestamp suffix to file name. Sets print file path.&quot;&quot;&quot;</span>
        <span class="n">print_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">IOError</span><span class="p">(</span><span class="s2">&quot;Print_file_path should be file path, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_path</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">):</span>
            <span class="n">_path</span><span class="p">,</span> <span class="n">_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">)</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">_make_directory</span><span class="p">(</span><span class="n">_path</span><span class="p">)</span>
            <span class="n">file_name</span> <span class="o">=</span> <span class="n">_get_print_file_name</span><span class="p">(</span><span class="n">_file_name</span><span class="p">)</span>
            <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">print_file_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">print_file_path</span><span class="p">,</span> <span class="n">full_file_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_env_config_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_config_path</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check and set env_config_path.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">enable_dump_ir</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The &#39;env_config_path&#39; is not supported, please enable ENABLE_DUMP_IR &quot;</span>
                             <span class="s2">&quot;with &#39;-D on&#39; and recompile source.&quot;</span><span class="p">)</span>
        <span class="n">env_config_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The </span><span class="si">%r</span><span class="s2"> set by &#39;env_config_path&#39; should be an existing json file.&quot;</span> <span class="o">%</span> <span class="n">env_config_path</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">exo</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The </span><span class="si">%r</span><span class="s2"> set by &#39;env_config_path&#39; should be a json file. &quot;</span>
                             <span class="s2">&quot;Detail: </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">env_config_path</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">exo</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">env_config_path</span><span class="p">,</span> <span class="n">env_config_path</span><span class="p">)</span>

    <span class="n">setters</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="n">set_mode</span><span class="p">,</span>
        <span class="s1">&#39;save_graphs_path&#39;</span><span class="p">:</span> <span class="n">set_save_graphs_path</span><span class="p">,</span>
        <span class="s1">&#39;device_target&#39;</span><span class="p">:</span> <span class="n">set_device_target</span><span class="p">,</span>
        <span class="s1">&#39;device_id&#39;</span><span class="p">:</span> <span class="n">set_device_id</span><span class="p">,</span>
        <span class="s1">&#39;auto_tune_mode&#39;</span><span class="p">:</span> <span class="n">set_auto_tune_mode</span><span class="p">,</span>
        <span class="s1">&#39;max_call_depth&#39;</span><span class="p">:</span> <span class="n">set_max_call_depth</span><span class="p">,</span>
        <span class="s1">&#39;profiling_options&#39;</span><span class="p">:</span> <span class="n">set_profiling_options</span><span class="p">,</span>
        <span class="s1">&#39;variable_memory_max_size&#39;</span><span class="p">:</span> <span class="n">set_variable_memory_max_size</span><span class="p">,</span>
        <span class="s1">&#39;max_device_memory&#39;</span><span class="p">:</span> <span class="n">set_max_device_memory</span><span class="p">,</span>
        <span class="s1">&#39;print_file_path&#39;</span><span class="p">:</span> <span class="n">set_print_file_path</span><span class="p">,</span>
        <span class="s1">&#39;env_config_path&#39;</span><span class="p">:</span> <span class="n">set_env_config_path</span>
    <span class="p">}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">reserve_class_name_in_scope</span>

    <span class="nd">@reserve_class_name_in_scope</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">reserve_class_name_in_scope</span> <span class="o">=</span> <span class="n">reserve_class_name_in_scope</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">enable_ge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">get_backend_policy</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;ge&#39;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">enable_debug_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">debug_runtime</span>

    <span class="nd">@enable_debug_runtime</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">enable_debug_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enable</span><span class="p">):</span>
        <span class="n">thread_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span>
        <span class="n">thread_info</span><span class="o">.</span><span class="n">debug_runtime</span> <span class="o">=</span> <span class="n">enable</span>


<span class="k">def</span> <span class="nf">_context</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the global _context, if context is not created, create a new one.</span>

<span class="sd">    Returns:</span>
<span class="sd">        _Context, the global context in PyNative mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_k_context</span>
    <span class="k">if</span> <span class="n">_k_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_backend</span> <span class="o">=</span> <span class="s1">&#39;debug&#39;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">default_config</span>
            <span class="n">default_backend</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">__backend__</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;import default config fail&quot;</span><span class="p">)</span>
        <span class="n">_k_context</span> <span class="o">=</span> <span class="n">_Context</span><span class="p">()</span>
        <span class="n">_k_context</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">default_backend</span> <span class="o">==</span> <span class="s1">&#39;debug&#39;</span><span class="p">:</span>
            <span class="n">_k_context</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">default_backend</span> <span class="o">=</span> <span class="s1">&#39;vm&#39;</span>
        <span class="n">_k_context</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="n">default_backend</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_k_context</span>


<span class="nd">@args_type_check</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">global_rank</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">gradient_fp32_sync</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">parallel_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">auto_parallel_search_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">parameter_broadcast</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">strategy_ckpt_load_file</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">full_batch</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">all_reduce_fusion_config</span><span class="o">=</span><span class="nb">list</span><span class="p">,</span> <span class="n">pipeline_stages</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">grad_accumulation_step</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_auto_parallel_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set auto parallel context, which is valid only for Ascend and GPU target.</span>

<span class="sd">    Auto parallel context should be configured before the initialization of your network.</span>

<span class="sd">    Note:</span>
<span class="sd">        Attribute name is required for setting attributes.</span>
<span class="sd">        If a program has tasks with different parallel modes, then before setting new parallel mode for the</span>
<span class="sd">        next task, interface mindspore.context.reset_auto_parallel_context() needs to be called to reset</span>
<span class="sd">        the configuration.</span>
<span class="sd">        Setting or changing parallel modes must be called before any creating Initializer, otherwise,</span>
<span class="sd">        RuntimeError may be raised when compiling the network.</span>

<span class="sd">    Some configurations are parallel mode specific, see the below table for details:</span>

<span class="sd">    ===========================  ===========================</span>
<span class="sd">    Common                       AUTO_PARALLEL</span>
<span class="sd">    ===========================  ===========================</span>
<span class="sd">    device_num                   gradient_fp32_sync</span>
<span class="sd">    global_rank                  loss_repeated_mean</span>
<span class="sd">    gradients_mean               auto_parallel_search_mode</span>
<span class="sd">    parallel_mode                strategy_ckpt_load_file</span>
<span class="sd">    all_reduce_fusion_config     strategy_ckpt_save_file</span>
<span class="sd">    enable_parallel_optimizer    full_batch</span>
<span class="sd">               \                 pipeline_stages</span>
<span class="sd">               \                 grad_accumulation_step</span>
<span class="sd">    ===========================  ===========================</span>

<span class="sd">    Args:</span>
<span class="sd">        device_num (int): Available device number, the value must be in [1, 4096]. Default: 1.</span>
<span class="sd">        global_rank (int): Global rank id, the value must be in [0, 4095]. Default: 0.</span>
<span class="sd">        gradients_mean (bool): Whether to perform mean operator after allreduce of gradients.</span>
<span class="sd">                     &quot;stand_alone&quot; do not support gradients_mean. Default: False.</span>
<span class="sd">        gradient_fp32_sync (bool): Run allreduce of gradients in fp32.</span>
<span class="sd">                     &quot;stand_alone&quot;, &quot;data_parallel&quot; and &quot;hybrid_parallel&quot; do not support</span>
<span class="sd">                     gradient_fp32_sync. Default: True.</span>
<span class="sd">        parallel_mode (str): There are five kinds of parallel modes, &quot;stand_alone&quot;, &quot;data_parallel&quot;,</span>
<span class="sd">                     &quot;hybrid_parallel&quot;, &quot;semi_auto_parallel&quot; and &quot;auto_parallel&quot;. Default: &quot;stand_alone&quot;.</span>

<span class="sd">                     - stand_alone: Only one processor is working.</span>

<span class="sd">                     - data_parallel: Distributes the data across different processors.</span>

<span class="sd">                     - hybrid_parallel: Achieves data parallelism and model parallelism manually.</span>

<span class="sd">                     - semi_auto_parallel: Achieves data parallelism and model parallelism by</span>
<span class="sd">                       setting parallel strategies.</span>

<span class="sd">                     - auto_parallel: Achieving parallelism automatically.</span>
<span class="sd">        auto_parallel_search_mode (str): There are two kinds of shard strategy search modes, &quot;recursive_programming&quot;</span>
<span class="sd">                     and &quot;dynamic_programming&quot;. Default: &quot;dynamic_programming&quot;.</span>

<span class="sd">                     - recursive_programming: Recursive programming search mode.</span>

<span class="sd">                     - dynamic_programming: Dynamic programming search mode.</span>
<span class="sd">        parameter_broadcast (bool): Whether to broadcast parameters before training. Before training, in order to have</span>
<span class="sd">                     the same network initialization parameter values for all devices, broadcast the parameters</span>
<span class="sd">                     on device 0 to other devices. Parameter broadcasting in different parallel modes is different,</span>
<span class="sd">                     data_parallel mode, all parameters are broadcast except for the parameter whose attribute</span>
<span class="sd">                     layerwise_parallel is True. Hybrid_parallel, semi_auto_parallel and auto_parallel mode, the</span>
<span class="sd">                     segmented parameters do not participate in broadcasting. Default: False.</span>
<span class="sd">        strategy_ckpt_load_file (str): The path to load parallel strategy checkpoint. Default: &#39;&#39;</span>
<span class="sd">        strategy_ckpt_save_file (str): The path to save parallel strategy checkpoint. Default: &#39;&#39;</span>
<span class="sd">        full_batch (bool): If you load whole batch datasets in auto_parallel mode, this parameter</span>
<span class="sd">                       should be set with True. Default: False.</span>
<span class="sd">        enable_parallel_optimizer (bool): This is a developing feature, which shards the weight update computation for</span>
<span class="sd">                       data parallel training in the benefit of time and memory saving. Currently, auto and semi auto</span>
<span class="sd">                       parallel mode support all optimizers in both Ascend and GPU. Data parallel mode only supports</span>
<span class="sd">                       `Lamb` and `AdamWeightDecay` in Ascend . Default: False.</span>
<span class="sd">        all_reduce_fusion_config (list): Set allreduce fusion strategy by parameters indices. Only support ReduceOp.SUM</span>
<span class="sd">                       and HCCL_WORLD_GROUP/NCCL_WORLD_GROUP. No Default, if it is not set, the fusion is closed.</span>
<span class="sd">        pipeline_stages (int): Set the stage information for pipeline parallel. This indicates how</span>
<span class="sd">                        the devices are distributed alone the pipeline. The total devices will be divided into</span>
<span class="sd">                        &#39;pipeline_stags&#39; stages. This currently could only be used when</span>
<span class="sd">                        parallel mode semi_auto_parallel is enabled. Default: 1.</span>
<span class="sd">        grad_accumulation_step (int): Set the accumulation steps of gradients in auto and semi auto parallel mode.</span>
<span class="sd">                        This should be a positive int. Default: 1.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(device_num=8)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(global_rank=0)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(gradients_mean=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(gradient_fp32_sync=False)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(parallel_mode=&quot;auto_parallel&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(auto_parallel_search_mode=&quot;dynamic_programming&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(parameter_broadcast=False)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(strategy_ckpt_load_file=&quot;./strategy_stage1.ckpt&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(strategy_ckpt_save_file=&quot;./strategy_stage1.ckpt&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(full_batch=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(enable_parallel_optimizer=False)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(all_reduce_fusion_config=[8, 160])</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(pipeline_stages=2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_auto_parallel_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_auto_parallel_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get auto parallel context attribute value according to the key.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns attribute value according to the key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_auto_parallel_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reset_auto_parallel_context</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reset auto parallel context attributes to the default values:</span>

<span class="sd">    - device_num: 1.</span>
<span class="sd">    - global_rank: 0.</span>
<span class="sd">    - gradients_mean: False.</span>
<span class="sd">    - gradient_fp32_sync: True.</span>
<span class="sd">    - parallel_mode: &#39;stand_alone&#39;.</span>
<span class="sd">    - auto_parallel_search_mode: &#39;dynamic_programming&#39;.</span>
<span class="sd">    - parameter_broadcast: False.</span>
<span class="sd">    - strategy_ckpt_load_file: &#39;&#39;.</span>
<span class="sd">    - strategy_ckpt_save_file: &#39;&#39;.</span>
<span class="sd">    - full_batch: False.</span>
<span class="sd">    - enable_parallel_optimizer: False.</span>
<span class="sd">    - pipeline_stages: 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_reset_auto_parallel_context</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">arg_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Checking whether a config is suitable for a specified device&quot;&quot;&quot;</span>
    <span class="n">device_cfgs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;enable_auto_mixed_precision&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;enable_dump&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;save_dump_path&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;enable_graph_kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;graph_kernel_flags&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;enable_reduce_precision&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;enable_profiling&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;profiling_options&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;print_file_path&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;variable_memory_max_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;auto_tune_mode&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;max_device_memory&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;GPU&#39;</span><span class="p">]</span>
    <span class="p">}</span>
    <span class="c1"># configs not in map device_cfgs are supposed to be suitable for all devices</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">arg_key</span> <span class="ow">in</span> <span class="n">device_cfgs</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="n">supported_devices</span> <span class="o">=</span> <span class="n">device_cfgs</span><span class="p">[</span><span class="n">arg_key</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">supported_devices</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Config &#39;</span><span class="si">{</span><span class="n">arg_key</span><span class="si">}</span><span class="s2">&#39; only supports devices in </span><span class="si">{</span><span class="n">supported_devices</span><span class="si">}</span><span class="s2">, current device is &#39;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&#39;&quot;</span>
                   <span class="s2">&quot;, ignore it.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="nd">@args_type_check</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">precompile_only</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">save_graphs</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">save_graphs_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_dump</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">auto_tune_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">save_dump_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_reduce_precision</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">variable_memory_max_size</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">enable_profiling</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">profiling_options</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_auto_mixed_precision</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">enable_graph_kernel</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">check_bprop</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">print_file_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">enable_sparse</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">env_config_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">graph_kernel_flags</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">save_compile_cache</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">load_compile_cache</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">grad_for_scalar</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set context for running environment.</span>

<span class="sd">    Context should be configured before running your program. If there is no configuration,</span>
<span class="sd">    it will automatic acquisition according to device target by default. GRAPH_MODE or</span>
<span class="sd">    PYNATIVE_MODE can be set by `mode` attribute and both modes support all backends, default</span>
<span class="sd">    mode is GRAPH_MODE.</span>

<span class="sd">    When the `save_graphs` attribute is set to True, attribute of `save_graphs_path` is used to set the</span>
<span class="sd">    intermediate compilation graph storage path. By default, the graphs are saved in the current directory.</span>
<span class="sd">    For other configurations and arguments, please refer to the corresponding module</span>
<span class="sd">    description, the configuration is optional and can be enabled when needed.</span>

<span class="sd">    Note:</span>
<span class="sd">        Attribute name is required for setting attributes.</span>
<span class="sd">        The mode is not recommended to be changed after net was initialized because the implementations of some</span>
<span class="sd">        operations are different in graph mode and pynative mode. Default: GRAPH_MODE.</span>

<span class="sd">    Some configurations are device specific, see the below table for details:</span>

<span class="sd">    ===========================  ===========================  =================</span>
<span class="sd">    Common(CPU/GPU/Ascend)       Ascend                       GPU</span>
<span class="sd">    ===========================  ===========================  =================</span>
<span class="sd">    check_bprop                  print_file_path              max_device_memory</span>
<span class="sd">    device_id                    enable_dump                  enable_graph_kernel</span>
<span class="sd">    device_target                save_dump_path               graph_kernel_flags</span>
<span class="sd">    enable_sparse                enable_graph_kernel</span>
<span class="sd">    max_call_depth               enable_reduce_precision</span>
<span class="sd">    mode                         enable_profiling</span>
<span class="sd">    reserve_class_name_in_scope  profiling_options</span>
<span class="sd">    save_graphs                  variable_memory_max_size</span>
<span class="sd">    save_graphs_path             auto_tune_mode</span>
<span class="sd">    env_config_path              graph_kernel_flags</span>
<span class="sd">    grad_for_scalar</span>
<span class="sd">    save_compile_cache</span>
<span class="sd">    load_compile_cache</span>
<span class="sd">    ===========================  ===========================  =================</span>

<span class="sd">    Args:</span>
<span class="sd">        mode (int): Running in GRAPH_MODE(0) or PYNATIVE_MODE(1). Default: GRAPH_MODE(0).</span>
<span class="sd">        precompile_only (bool): Whether to only precompile the network. If set, the network will only be compiled and</span>
<span class="sd">             not executed. Default: False.</span>
<span class="sd">        device_target (str): The target device to run, support &quot;Ascend&quot;, &quot;GPU&quot;, and &quot;CPU&quot;.</span>
<span class="sd">        device_id (int): ID of the target device, the value must be in [0, device_num_per_host-1],</span>
<span class="sd">                    while device_num_per_host should be no more than 4096. Default: 0.</span>
<span class="sd">        save_graphs (bool): Whether to save graphs. Default: False.</span>
<span class="sd">        save_graphs_path (str): Path to save graphs. Default: &quot;.&quot;.</span>

<span class="sd">            If the program is executed in the parallel mode, `save_graphs_path` should consist of the path and the</span>
<span class="sd">            current device id, to ensure that writing file conflicts won&#39;t happen when the different processes try to</span>
<span class="sd">            create the files in the same directory. For example, the `device_id` can be generated by</span>
<span class="sd">            `device_id = os.getenv(&quot;DEVICE_ID&quot;)` and the `save_graphs_path` can be set by</span>
<span class="sd">            `context.set_context(save_graphs_path=&quot;path/to/ir/files&quot;+device_id)`.</span>
<span class="sd">        enable_graph_kernel (bool): Whether to enable graph kernel fusion to optimize network execution performance.</span>
<span class="sd">             Default: False.</span>
<span class="sd">        graph_kernel_flags (str): Optimization options of graph kernel fusion. Experienced user only.</span>
<span class="sd">            For example, `context.set_context(graph_kernel_flags=&quot;--opt_level=2 --dump_as_text&quot;)`.</span>
<span class="sd">            Some general options:</span>

<span class="sd">            - opt_level: optimization level between 0 and 3. Default: 2. Graph kernel fusion can be enabled</span>
<span class="sd">              equivalently by setting opt_level greater than 0.</span>
<span class="sd">            - dump_as_text: dump detail info as text files. Default: false.</span>

<span class="sd">            More options can be referred from the implementation code.</span>
<span class="sd">            These options can also be set by environment variable `MS_GRAPH_KERNEL_FLAGS`, without modifying</span>
<span class="sd">            network source code. For example, `export MS_GRAPH_KERNEL_FLAGS=&quot;--opt_level=2 --dump_as_text&quot;`.</span>
<span class="sd">        reserve_class_name_in_scope (bool) : Whether to save the network class name in the scope. Default: True.</span>
<span class="sd">             Each node has a scope. A scope of a subnode is the name of its parent node. If reserve_class_name_in_scope</span>
<span class="sd">             is set, the class name will be saved after keyword &#39;net-&#39; in the scope. For example:</span>
<span class="sd">             Default/net-Net1/net-Net2 (reserve_class_name_in_scope=True)</span>
<span class="sd">             Default/net/net (reserve_class_name_in_scope=False)</span>
<span class="sd">        enable_reduce_precision (bool): Whether to enable precision reduction. Default: True.</span>
<span class="sd">        enable_dump (bool): Whether to enable dump. Default: False.</span>
<span class="sd">        save_dump_path (str): When the program is executed on Ascend, operators can dump data in this path.</span>
<span class="sd">            The root dump path is configured in /home/HwHiAiUser/ide_daemon/ide_daemon.cfg.</span>
<span class="sd">            So the real dump path is &quot;{configured root dump path}/{`save_dump_path`}&quot;. Default: &quot;.&quot;.</span>
<span class="sd">        variable_memory_max_size (str): Set the maximum size of the variable memory max size. Default: &quot;0GB&quot;.</span>
<span class="sd">        enable_profiling (bool): Whether to open profiling. Default: False.</span>
<span class="sd">        profiling_options (str): Set profiling collection options, operators can profiling data here.</span>
<span class="sd">            The values of profiling collection options are as follows, supporting the collection of multiple data.</span>

<span class="sd">            - output: the saving the path of the profiling collection result file. The directory spectified by this</span>
<span class="sd">              parameter needs to be created in advance on the training environment (container or host side) and ensure</span>
<span class="sd">              that the running user configured during installation has read and write permissions.It supports the</span>
<span class="sd">              configuration of absolute or relative paths(relative to the current path when executing the command line).</span>
<span class="sd">              The absolute path configuration starts with &#39;/&#39;, for example:/home/data/output.</span>
<span class="sd">              The relative path configuration directly starts with the directory name,for example:output.</span>

<span class="sd">            - training_trace: collect iterative trajectory data, that is, the training task and software information of</span>
<span class="sd">              the AI software stack, to achieve performance analysis of the training task, focusing on data</span>
<span class="sd">              enhancement, forward and backward calculation, gradient aggregation update and other related data.</span>
<span class="sd">              The value is on/off.</span>

<span class="sd">            - task_trace: collect task trajectory data, that is, the hardware information of the HWTS/AICore of</span>
<span class="sd">              the Ascend 910 processor, and analyze the information of beginning and ending of the task.</span>
<span class="sd">              The value is on/off.</span>

<span class="sd">            - aicpu: collect profiling data enhanced by aicpu data. The value is on/off.</span>

<span class="sd">            - fp_point: specify the start position of the forward operator of the training network iteration trajectory,</span>
<span class="sd">              which is used to record the start timestamp of the forward calculation.The configuration value is the name</span>
<span class="sd">              of the first operator specified in the forward direction. when the value is empty,the system will</span>
<span class="sd">              automatically obtain the forward operator name.</span>

<span class="sd">            - bp_point: specify the end position of the iteration trajectory reversal operator of the training network,</span>
<span class="sd">              record the end timestamp of the backward calculation. The configuration value is the name of the operator</span>
<span class="sd">              after the specified reverse. when the value is empty,the system will automatically obtain the backward</span>
<span class="sd">              operator name.</span>

<span class="sd">            - aic_metrics: the values are as follows:</span>
<span class="sd">              ArithmeticUtilization: percentage statistics of various calculation indicators.</span>
<span class="sd">              PipeUtilization: the time-consuming ratio of calculation unit and handling unit,this item is</span>
<span class="sd">              the default value.</span>
<span class="sd">              Memory: percentage of external memory read and write instructions.</span>
<span class="sd">              MemoryL0: percentage of internal memory read and write instructions.</span>
<span class="sd">              ResourceConflictRatio: proportion of pipline queue instructions.</span>

<span class="sd">            The profiling_options is like &#39;{&quot;output&quot;:&#39;/home/data/output&#39;,&#39;training_trace&#39;:&#39;on&#39;}&#39;</span>

<span class="sd">        check_bprop (bool): Whether to check back propagation nodes. The checking ensures that the shape and dtype</span>
<span class="sd">             of back propagation node outputs is the same as input parameters. Default: False.</span>
<span class="sd">        max_device_memory (str): Sets the maximum memory available for devices.</span>
<span class="sd">            Currently, it is only supported on GPU. The format is &quot;xxGB&quot;. Default: &quot;1024GB&quot;.</span>
<span class="sd">        print_file_path (str): The path of saving print data. If this parameter is set, print data is saved to</span>
<span class="sd">            a file by default, and turns off printing to the screen. If the file already exists, add a timestamp</span>
<span class="sd">            suffix to the file. Default: &#39;&#39;.</span>
<span class="sd">        enable_sparse (bool): Whether to enable sparsity feature. Default: False.</span>
<span class="sd">            For details of sparsity and sparse tensor, please check `&lt;https://www.mindspore.cn/docs/programming_guide/zh-CN/r1.3/tensor.html&gt;`_.</span>
<span class="sd">        max_call_depth (int): Specify the maximum depth of function call. Must be positive integer. Default: 1000.</span>
<span class="sd">        env_config_path (str): Config path for DFX.</span>
<span class="sd">        auto_tune_mode (str): The mode of auto tune when op building, get the best tiling performance,</span>
<span class="sd">            default: NO_TUNE. The value must be in [&#39;RL&#39;, &#39;GA&#39;, &#39;RL,GA&#39;].</span>
<span class="sd">            RL: rl_tune;</span>
<span class="sd">            GA: ga_tune;</span>
<span class="sd">            RL,GA: rl_tune/ga_tune(Automatic selection).</span>
<span class="sd">            - rl_tune: Reinforecement Learning tune.</span>
<span class="sd">            - ga_tune: Genetic Algorithm tune.</span>
<span class="sd">        grad_for_scalar (bool): Whether to get gradient for scalar. If set, the gradient of scalar input parameter</span>
<span class="sd">            can be calculated. Now, only part of the scalar operators support this calculation. Default: False.</span>
<span class="sd">        save_compile_cache (bool): Whether to cache the graph compiled by frontend. Default: False.</span>
<span class="sd">            This is an experimental prototype that is subject to change and/or deletion.</span>
<span class="sd">        load_compile_cache (bool): Whether to use the cache of the graph compiled by frontend.</span>
<span class="sd">            When it is true, the graph compilation will skip the frontend compilation process. It means that</span>
<span class="sd">            you should make sure the network has not been changed since the last execution. Currently we have</span>
<span class="sd">            not support automatic checking the changes yet. Default: False.</span>
<span class="sd">            This is an experimental prototype that is subject to change and/or deletion.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not an attribute in context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(mode=context.GRAPH_MODE)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(mode=context.PYNATIVE_MODE)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(device_target=&quot;Ascend&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(device_id=0)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(save_graphs=True, save_graphs_path=&quot;./model.ms&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(enable_reduce_precision=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(enable_dump=True, save_dump_path=&quot;.&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(reserve_class_name_in_scope=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(variable_memory_max_size=&quot;6GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(mode=context.GRAPH_MODE,</span>
<span class="sd">        ...                     device_target=&quot;Ascend&quot;,device_id=0, save_graphs=True,</span>
<span class="sd">        ...                     save_graphs_path=&quot;/mindspore&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(enable_profiling=True,</span>
<span class="sd">        ...                     profiling_options=&#39;{&quot;output&quot;:&quot;/home/data/output&quot;,&quot;training_trace&quot;:&quot;on&quot;}&#39;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(max_device_memory=&quot;3.5GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(print_file_path=&quot;print.pb&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(max_call_depth=80)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(env_config_path=&quot;./env_config.json&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="c1"># set device target first</span>
    <span class="k">if</span> <span class="s1">&#39;device_target&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">set_device_target</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;device_target&#39;</span><span class="p">])</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">device</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">__device_target__</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error, package type </span><span class="si">{</span><span class="n">__package_name__</span><span class="si">}</span><span class="s2"> support device type </span><span class="si">{</span><span class="n">__device_target__</span><span class="si">}</span><span class="s2">, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got device target </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">setters</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">setters</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="n">ctx</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="c1"># enum variables beginning with &#39;_&#39; are for internal use</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span> <span class="ow">and</span> <span class="n">key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Set context keyword </span><span class="si">%s</span><span class="s2"> is not recognized!&quot;</span> <span class="o">%</span> <span class="n">key</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get context attribute value according to the input key.</span>
<span class="sd">    If some attribute are not set, it will be automatically obtained.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Object, The value of given attribute key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not an attribute in context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">)</span>
    <span class="c1"># enum variables beginning with &#39;_&#39; are for internal use</span>
    <span class="k">if</span> <span class="n">attr_key</span> <span class="ow">in</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span> <span class="ow">and</span> <span class="n">attr_key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span><span class="p">[</span><span class="n">attr_key</span><span class="p">])</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Get context keyword </span><span class="si">%s</span><span class="s2"> is not recognized!&quot;</span> <span class="o">%</span> <span class="n">attr_key</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ParallelMode</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parallel mode options.</span>

<span class="sd">    There are five kinds of parallel modes, &quot;STAND_ALONE&quot;, &quot;DATA_PARALLEL&quot;,</span>
<span class="sd">    &quot;HYBRID_PARALLEL&quot;, &quot;SEMI_AUTO_PARALLEL&quot; and &quot;AUTO_PARALLEL&quot;. Default: &quot;STAND_ALONE&quot;.</span>

<span class="sd">    - STAND_ALONE: Only one processor is working.</span>
<span class="sd">    - DATA_PARALLEL: Distributes the data across different processors.</span>
<span class="sd">    - HYBRID_PARALLEL: Achieves data parallelism and model parallelism manually.</span>
<span class="sd">    - SEMI_AUTO_PARALLEL: Achieves data parallelism and model parallelism by setting parallel strategies.</span>
<span class="sd">    - AUTO_PARALLEL: Achieves parallelism automatically.</span>

<span class="sd">    MODE_LIST: The list of all supported parallel modes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">STAND_ALONE</span> <span class="o">=</span> <span class="s2">&quot;stand_alone&quot;</span>
    <span class="n">DATA_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;data_parallel&quot;</span>
    <span class="n">HYBRID_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;hybrid_parallel&quot;</span>
    <span class="n">SEMI_AUTO_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;semi_auto_parallel&quot;</span>
    <span class="n">AUTO_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;auto_parallel&quot;</span>
    <span class="n">MODE_LIST</span> <span class="o">=</span> <span class="p">[</span><span class="n">STAND_ALONE</span><span class="p">,</span> <span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">HYBRID_PARALLEL</span><span class="p">,</span> <span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">AUTO_PARALLEL</span><span class="p">]</span>


<span class="nd">@args_type_check</span><span class="p">(</span><span class="n">enable_ps</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_ps_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set parameter server training mode context.</span>

<span class="sd">    Note:</span>
<span class="sd">        Some other environment variables should also be set for parameter server training mode.</span>
<span class="sd">        These environment variables are listed below:</span>

<span class="sd">    - MS_SERVER_NUM: Server number</span>
<span class="sd">    - MS_WORKER_NUM: Worker number</span>
<span class="sd">    - MS_SCHED_HOST: Scheduler IP address</span>
<span class="sd">    - MS_SCHED_PORT: Scheduler port</span>
<span class="sd">    - MS_ROLE: The role of this process:</span>
<span class="sd">    - MS_SCHED: represents the scheduler,</span>
<span class="sd">    - MS_WORKER: represents the worker,</span>
<span class="sd">    - MS_PSERVER: represents the Server</span>

<span class="sd">    Args:</span>
<span class="sd">        enable_ps (bool): Whether to enable parameter server training mode.</span>
<span class="sd">                          Only after enable_ps is set True, the environment variables will be effective.</span>
<span class="sd">                          Default: False.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not the attribute in parameter server training mode context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; context.set_ps_context(enable_ps=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_ps_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_ps_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get parameter server training mode context attribute value according to the key.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute:</span>
<span class="sd">            - enable_ps (bool): Whether to enable parameter server training mode.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns attribute value according to the key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_ps_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reset_ps_context</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reset parameter server training mode context attributes to the default values:</span>

<span class="sd">    - enable_ps: False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_reset_ps_context</span><span class="p">()</span>

<div class="viewcode-block" id="set_fl_context"><a class="viewcode-back" href="../../federated_server.html#mindspore.context.set_fl_context">[docs]</a><span class="k">def</span> <span class="nf">set_fl_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set federated learning training mode context.</span>

<span class="sd">    Args:</span>
<span class="sd">        enable_fl (bool): Whether to enable federated learning training mode.</span>
<span class="sd">                          Default: False.</span>
<span class="sd">        server_mode (str): Describe the server mode, which must one of &#39;FEDERATED_LEARNING&#39; and &#39;HYBRID_TRAINING&#39;.</span>
<span class="sd">                              Default: &#39;FEDERATED_LEARNING&#39;.</span>
<span class="sd">        ms_role (str): The process&#39;s role in the federated learning mode,</span>
<span class="sd">                          which must be one of &#39;MS_SERVER&#39;, &#39;MS_WORKER&#39; and &#39;MS_SCHED&#39;.</span>
<span class="sd">                          Default: &#39;MS_SERVER&#39;.</span>
<span class="sd">        worker_num (int): The number of workers. For current version, this must be set to 1 or 0.</span>
<span class="sd">        server_num (int): The number of federated learning servers. Default: 0.</span>
<span class="sd">        scheduler_ip (str): The scheduler IP. Default: &#39;0.0.0.0&#39;.</span>
<span class="sd">        scheduler_port (int): The scheduler port. Default: 6667.</span>
<span class="sd">        fl_server_port (int): The http port of the federated learning server.</span>
<span class="sd">                              Normally for each server this should be set to the same value. Default: 6668.</span>
<span class="sd">        enable_fl_client (bool): Whether this process is federated learning client. Default: False.</span>
<span class="sd">        start_fl_job_threshold (int): The threshold count of startFLJob. Default: 1.</span>
<span class="sd">        start_fl_job_time_window (int): The time window duration for startFLJob in millisecond. Default: 3000.</span>
<span class="sd">        share_secrets_ratio (float): The ratio for computing the threshold count of share secrets. Default: 1.0.</span>
<span class="sd">        update_model_ratio (float): The ratio for computing the threshold count of updateModel. Default: 1.0.</span>
<span class="sd">        cipher_time_window (int): The time window duration for each cipher round in millisecond. Default: 300000.</span>
<span class="sd">        reconstruct_secrets_threshold (int): The threshold count of reconstruct threshold. Default: 0.</span>
<span class="sd">        update_model_time_window (int): The time window duration for updateModel in millisecond. Default: 3000.</span>
<span class="sd">        fl_name (string): The federated learning job name. Default: &#39;&#39;.</span>
<span class="sd">        fl_iteration_num (int): Iteration number of federated learning,</span>
<span class="sd">                                which is the number of interactions between client and server. Default: 20.</span>
<span class="sd">        client_epoch_num (int): Client training epoch number. Default: 25.</span>
<span class="sd">        client_batch_size (int): Client training data batch size. Default: 32.</span>
<span class="sd">        client_learning_rate (float): Client training learning rate. Default: 0.001.</span>
<span class="sd">        worker_step_num_per_iteration (int): The worker&#39;s standalone training step number before communicating with</span>
<span class="sd">                                             server. Default: 65.</span>
<span class="sd">        dp_eps (float): Epsilon budget of differential privacy mechanism. The smaller the dp_eps, the better the</span>
<span class="sd">            privacy protection effect. Default: 50.0.</span>
<span class="sd">        dp_delta (float): Delta budget of differential privacy mechanism, which is usually equals the reciprocal of</span>
<span class="sd">            client number. The smaller the dp_delta, the better the privacy protection effect. Default: 0.01.</span>
<span class="sd">        dp_norm_clip (float): A factor used for clipping model&#39;s weights for differential mechanism. Its value is</span>
<span class="sd">            suggested to be 0.5~2. Default: 1.0.</span>
<span class="sd">        encrypt_type (string): Secure schema for federated learning, which can be &#39;NOT_ENCRYPT&#39;, &#39;DP_ENCRYPT&#39; or</span>
<span class="sd">            &#39;PW_ENCRYPT&#39;. If &#39;DP_ENCRYPT&#39;, differential privacy schema would be applied for clients and the privacy</span>
<span class="sd">            protection effect would be determined by dp_eps, dp_delta and dp_norm_clip as described above. If</span>
<span class="sd">            &#39;PW_ENCRYPT&#39;, pairwise secure aggregation would be applied to protect clients&#39; model from stealing.</span>
<span class="sd">            Default: &#39;NOT_ENCRYPT&#39;.</span>
<span class="sd">        config_file_path (string): Configuration file path used by recovery. Default: &#39;&#39;.</span>
<span class="sd">        scheduler_manage_port (int): scheduler manage port used to scale out/in. Default: 11202.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not the attribute in federated learning mode context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; context.set_fl_context(enable_fl=True, server_mode=&#39;FEDERATED_LEARNING&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_ps_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_fl_context"><a class="viewcode-back" href="../../federated_server.html#mindspore.context.get_fl_context">[docs]</a><span class="k">def</span> <span class="nf">get_fl_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get federated learning mode context attribute value according to the key.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute.</span>
<span class="sd">                        Please refer to `set_fl_context`&#39;s parameters to decide what key should be passed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns attribute value according to the key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in federated learning mode context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; context.get_fl_context(&quot;server_mode&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_ps_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">)</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>