<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Vertical Federated - Feature Protection Based on Trusted Execution Environment &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Vertical Federated - Label Protection Based on Differential Privacy" href="secure_vertical_federated_learning_with_DP.html" />
    <link rel="prev" title="Vertical Federated-Feature Protection Based on Information Obfuscation" href="secure_vertical_federated_learning_with_EmbeddingDP.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="federated_install.html">Obtaining MindSpore Federated</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_federated_server.html">Horizontal Federated Cloud-based Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_federated_client.html">Horizontal Federated Device-side Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_vfl.html">Vertical Federated Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Horizontal Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="image_classfication_dataset_process.html">Federated Learning Image Classification Dataset Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_classification_application.html">Implementing an Image Classification Application of Cross-device Federated Learning (x86)</a></li>
<li class="toctree-l1"><a class="reference internal" href="sentiment_classification_application.html">Implementing a Sentiment Classification Application (Android)</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_classification_application_in_cross_silo.html">Implementing a Cloud-Slio Federated Image Classification Application (x86)</a></li>
<li class="toctree-l1"><a class="reference internal" href="object_detection_application_in_cross_silo.html">Implementing a Cross-Silo Federated Target Detection Application (x86)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Vertical Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="data_join.html">Vertical Federated Learning Data Access</a></li>
<li class="toctree-l1"><a class="reference internal" href="split_wnd_application.html">Vertical Federated Learning Model Training - Wide&amp;Deep Recommendation Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="split_pangu_alpha_application.html">Vertical Federated Learning Model Training - Pangu Alpha Large Model Cross-Domain Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Security and Privacy</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="local_differential_privacy_training_noise.html">Horizontal FL-Local Differential Privacy Perturbation Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="local_differential_privacy_training_signds.html">Horizontal FL-Local Differential Privacy SignDS training</a></li>
<li class="toctree-l1"><a class="reference internal" href="pairwise_encryption_training.html">Horizontal FL-Pairwise encryption training</a></li>
<li class="toctree-l1"><a class="reference internal" href="private_set_intersection.html">Vertical Federated-Privacy Set Intersection</a></li>
<li class="toctree-l1"><a class="reference internal" href="secure_vertical_federated_learning_with_EmbeddingDP.html">Vertical Federated-Feature Protection Based on Information Obfuscation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Vertical Federated - Feature Protection Based on Trusted Execution Environment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="#algorithm-introduction">Algorithm Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quick-experience">Quick Experience</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#front-end-needs-and-environment-configuration">Front-End Needs and Environment Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#starting-the-script">Starting the Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="#viewing-results">Viewing Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deep-experience">Deep Experience</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#front-end-needs-and-environment-configuration-1">Front-End Needs and Environment Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-network-model">Defining the Network Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#forward-propagation">Forward Propagation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#backward-propagation">Backward Propagation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#definig-the-optimizer">Definig the Optimizer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#constructing-the-training-script">Constructing the Training Script</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#constructing-the-network">Constructing the Network</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="secure_vertical_federated_learning_with_DP.html">Vertical Federated - Label Protection Based on Differential Privacy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Communication Compression</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="communication_compression.html">Device-Cloud Federated Learning Communication Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="vfl_communication_compress.html">Vertical Federated Learning Communication Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Horizontal Federated API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="horizontal_server.html">Federated Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="cross_device.html">Device-side Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="horizontal/cross_silo.html">Cross-Silo</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Vertical Federated API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Data_Join.html">Data Join</a></li>
<li class="toctree-l1"><a class="reference internal" href="vertical/vertical_communicator.html">Vertical Federated Learning Communicator</a></li>
<li class="toctree-l1"><a class="reference internal" href="vertical_federated_trainer.html">Vertical Federated Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Vertical Federated - Feature Protection Based on Trusted Execution Environment</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/secure_vertical_federated_learning_with_TEE.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="vertical-federated--feature-protection-based-on-trusted-execution-environment">
<h1>Vertical Federated - Feature Protection Based on Trusted Execution Environment<a class="headerlink" href="#vertical-federated--feature-protection-based-on-trusted-execution-environment" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.0/docs/federated/docs/source_en/secure_vertical_federated_learning_with_TEE.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png" /></a></p>
<p>Note: This is an experimental feature and may be modified or removed in the future.</p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline"></a></h2>
<p>Vertical federated learning (vFL) is a major branch of federated learning (FL). When different participants have data from the same batch of users but with different attributes, they can use vFL for collaborative training. In vFL, each participant with attributes holds a bottom model, and they input the attributes into the bottom model to get the intermediate result (embedding), which is sent to the participant with labels (referred to as leader paraticipant, participant B as shown in the figure below, as shown in the figure below, and the participant without labels, called follower, as shown in the figure below, as participant A). The leader side uses the embedding and labels to train the upper layer network, and then passes the calculated gradients back to each participant to train the lower layer network. It can be seen that vFL does not require any participant to upload their own raw data to collaboratively train the model.</p>
<p><img alt="image.png" src="_images/vfl_1_en.png" /></p>
<p>By avoiding direct uploading of raw data, vFL protects privacy security to a certain extent, which is one of the core goals of vFL. However, it is still possible for an attacker to reverse user information from the uploaded embedding, causing privacy security risks. In such a context, we need to provide stronger privacy guarantees for the embedding and gradients transmitted during vFL training to circumvent privacy security risks.</p>
<p>Trusted execution environment (TEE) is a hardware-based trusted computing solution that provides data security of the computing process by making the whole computing process in hardware black-boxed relative to the outside world. By shielding the key layer in the vFL network through TEE, it can make the computation of that layer difficult to be reversed, thus ensuring the data security of the vFL training and inference process.</p>
</section>
<section id="algorithm-introduction">
<h2>Algorithm Introduction<a class="headerlink" href="#algorithm-introduction" title="Permalink to this headline"></a></h2>
<p><img alt="image.png" src="_images/vfl_with_tee_en.png" /></p>
<p>As shown in the figure, if participant A sends the intermediate result <span class="math notranslate nohighlight">\(\alpha^{(A)}\)</span> directly to participant B, it is easy for participant B to use the intermediate result to reverse the original data <span class="math notranslate nohighlight">\(X^{(A)}\)</span> of participant A. To reduce such risk, participant A encrypts the intermediate result <span class="math notranslate nohighlight">\(\alpha^{(A)}\)</span> computed by Bottom Model to get <span class="math notranslate nohighlight">\(E(\alpha^{(A)})\)</span> first, and passes <span class="math notranslate nohighlight">\(E(\alpha^{(A)})\)</span> to participant B. Participant B inputs <span class="math notranslate nohighlight">\(E(\alpha^{(A)})\)</span> into the TEE-based Cut Layer, and then decrypts it into <span class="math notranslate nohighlight">\(\alpha^{(A)}\)</span> for forward propagation inside the TEE, and the whole process is black-boxed for B.</p>
<p>The gradient is passed backward similarly, Cut Layer computes the gradient <span class="math notranslate nohighlight">\(\nabla\alpha^{(A)}\)</span>, encrypts it into <span class="math notranslate nohighlight">\(E(\nabla\alpha^{(A)})\)</span> and then passes it back from participant B to participant A. Then participant A decrypts it into <span class="math notranslate nohighlight">\(\nabla\alpha^{(A)}\)</span> and continues to do backward propagation.</p>
</section>
<section id="quick-experience">
<h2>Quick Experience<a class="headerlink" href="#quick-experience" title="Permalink to this headline"></a></h2>
<p>We use the local case in <a class="reference external" href="https://gitee.com/mindspore/federated/tree/r0.1/example/splitnn_criteo">Wide&amp;Deep Vertical Federated Learning Case</a> as an example of configuring TEE protection.</p>
<section id="front-end-needs-and-environment-configuration">
<h3>Front-End Needs and Environment Configuration<a class="headerlink" href="#front-end-needs-and-environment-configuration" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Environmental requirements.</p>
<ul class="simple">
<li><p>Processor: Intel SGX (Intel Sofrware Guard Extensions) support required</p></li>
<li><p>OS: openEuler 20.03, openEuler 21.03 LTS SP2 or higher</p></li>
</ul>
</li>
<li><p>Install SGX and SecGear (you can refer to <a class="reference external" href="https://gitee.com/openeuler/secGear">secGear official website</a>).</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>yum<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>cmake<span class="w"> </span>ocaml-dune<span class="w"> </span>linux-sgx-driver<span class="w"> </span>sgxsdk<span class="w"> </span>libsgx-launch<span class="w"> </span>libsgx-urts<span class="w"> </span>sgxssl
git<span class="w"> </span>clone<span class="w"> </span>https://gitee.com/openeuler/secGear.git
<span class="nb">cd</span><span class="w"> </span>secGear
<span class="nb">source</span><span class="w"> </span>/opt/intel/sgxsdk/environment<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">source</span><span class="w"> </span>environment
mkdir<span class="w"> </span>debug<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>debug<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>cmake<span class="w"> </span>..<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>make<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>sudo<span class="w"> </span>make<span class="w"> </span>install
</pre></div>
</div>
</li>
<li><p>Install MindSpore 1.8.1 or its higher version, please refer to the <a class="reference external" href="https://www.mindspore.cn/install">MindSpore Official Site Installation Guide</a>.</p></li>
<li><p>Download federated</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://gitee.com/mindspore/federated.git<span class="w"> </span>-b<span class="w"> </span>r0.1
</pre></div>
</div>
</li>
<li><p>For installing MindSpore Federated relies on Python libraries, see <a class="reference external" href="https://gitee.com/mindspore/federated/tree/r0.1/example/splitnn_criteo">Wide&amp;Deep Vertical Federated Learning Case</a>.</p></li>
<li><p>Install MindSpore Federated for TEE compilation (need to additionally set compiler options to indicate whether to use SGX or not).</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>sh<span class="w"> </span>federated/build.sh<span class="w"> </span>-s<span class="w"> </span>on
pip<span class="w"> </span>install<span class="w"> </span>federated/build/packages/mindspore_federated-XXXXX.whl
</pre></div>
</div>
</li>
<li><p>To prepare the criteo dataset, please refer to <a class="reference external" href="https://gitee.com/mindspore/federated/tree/r0.1/example/splitnn_criteo">Wide&amp;Deep Vertical Federated Learning Case</a>.</p></li>
</ol>
</section>
<section id="starting-the-script">
<h3>Starting the Script<a class="headerlink" href="#starting-the-script" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Go to the folder where the script is located</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>federated/example/splitnn_criteo
</pre></div>
</div>
</li>
<li><p>Run the script</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>sh<span class="w"> </span>run_vfl_train_local_tee.sh
</pre></div>
</div>
</li>
</ol>
</section>
<section id="viewing-results">
<h3>Viewing Results<a class="headerlink" href="#viewing-results" title="Permalink to this headline"></a></h3>
<p>Check loss changes of the model training in the training log <code class="docutils literal notranslate"><span class="pre">log_local_cpu_tee.txt</span></code>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">100</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.661822<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.662018
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">100</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.685003<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.685198
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">200</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.649380<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.649381
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">300</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.612189<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.612189
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">400</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.630079<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.630079
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">500</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.602897<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.602897
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">600</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.621647<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.621647
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">700</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.624762<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.624762
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">800</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.622042<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.622042
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">900</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.585274<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.585274
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">1000</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.590947<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.590947
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">1100</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.586775<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.586775
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">1200</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.597362<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.597362
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">1300</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.607390<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.607390
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">1400</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.584204<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.584204
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">1500</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.583618<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.583618
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">1600</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.573294<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.573294
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">1700</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.600686<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.600686
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">1800</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.585533<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.585533
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">1900</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.583466<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.583466
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">2000</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.560188<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.560188
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">2100</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.569232<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.569232
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">2200</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.591643<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.591643
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">2300</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.572473<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.572473
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">2400</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.582825<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.582825
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">2500</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.567196<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.567196
INFO:root:epoch<span class="w"> </span><span class="m">0</span><span class="w"> </span>step<span class="w"> </span><span class="m">2600</span>/41322<span class="w"> </span>wide_loss:<span class="w"> </span><span class="m">0</span>.602022<span class="w"> </span>deep_loss:<span class="w"> </span><span class="m">0</span>.602022
</pre></div>
</div>
</section>
</section>
<section id="deep-experience">
<h2>Deep Experience<a class="headerlink" href="#deep-experience" title="Permalink to this headline"></a></h2>
<p>The forward and backward propagation of the TEE layer requires calling its own functions rather than MindSpore, so there are differences in implementation from the usual vFL model.</p>
<p>Usually, the Top Model and Cut Layer are put together for the backpropagation of the vFL model during training, and are derived and updated in one step by Participant B through MindSpore. When the network containing TEE is back propagated, the Top Model is updated by Participant B based on MindSpore, while the Cut Layer (TEE) is updated within itself after receiving the gradients back from the Top Model. The gradients that need to be passed back to Participant A are encrypted and passed out to Participant B. The whole process is done within the TEE.</p>
<p>Currently in MindSpore Federated, the above function is used to implement a custom backward propagation process by passing <code class="docutils literal notranslate"><span class="pre">grad_network</span></code> into the <code class="docutils literal notranslate"><span class="pre">mindspore_federated.vfl_model.FLModel()</span></code> definition. Therefore, to implement a network containing TEE, the user can define the backward propagation process for Top Model and Cut Layer in <code class="docutils literal notranslate"><span class="pre">grad_network</span></code> and just pass in <code class="docutils literal notranslate"><span class="pre">FLModel</span></code>, and <code class="docutils literal notranslate"><span class="pre">FLModel</span></code> will go through the user-defined training process during backward propagation.</p>
<p>We use the local case in <a class="reference external" href="https://gitee.com/mindspore/federated/tree/r0.1/example/splitnn_criteo">Wide&amp;Deep Vertical Federated Learning Case</a> as an example of how to configure TEE protection in a vertical federated model. The presentation focuses on the differences between the configuration and the usual case when using TEE, and the same points will be skipped (a detailed description of vFL training can be found in <a class="reference external" href="https://mindspore.cn/federated/docs/en/r0.1/split_pangu_alpha_application.html">Vertical Federated Learning Model Training - Pangu Alpha Large Model Cross-Domain Training</a>.</p>
<section id="front-end-needs-and-environment-configuration-1">
<h3>Front-End Needs and Environment Configuration<a class="headerlink" href="#front-end-needs-and-environment-configuration-1" title="Permalink to this headline"></a></h3>
<p>Refer to <a class="reference internal" href="#quick-experience"><span class="std std-doc">Quick Experience</span></a>.</p>
</section>
<section id="defining-the-network-model">
<h3>Defining the Network Model<a class="headerlink" href="#defining-the-network-model" title="Permalink to this headline"></a></h3>
<section id="forward-propagation">
<h4>Forward Propagation<a class="headerlink" href="#forward-propagation" title="Permalink to this headline"></a></h4>
<p>As usual vFL training, users need to define a network model containing TEE based on the <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> provided by MindSpore (see <a class="reference external" href="https://mindspore.cn/docs/en/r2.0/api_python/nn/mindspore.nn.Cell.html#mindspore-nn-cell">mindspore.nn.Cell</a>) to develop the training network. The difference is that at the layer where the TEE is located, the user needs to call the TEE forward propagation function in the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function of the class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_federated._mindspore_federated</span> <span class="kn">import</span> <span class="n">init_tee_cut_layer</span><span class="p">,</span> <span class="n">backward_tee_cut_layer</span><span class="p">,</span> \
    <span class="n">encrypt_client_data</span><span class="p">,</span> <span class="n">secure_forward_tee_cut_layer</span>

<span class="k">class</span> <span class="nc">TeeLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    TEE layer of the leader net.</span>
<span class="sd">    Args:</span>
<span class="sd">        config (class): default config info.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TeeLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">init_tee_cut_layer</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">3.5e-4</span><span class="p">,</span> <span class="mf">1024.0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Reshape</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">wide_out0</span><span class="p">,</span> <span class="n">deep_out0</span><span class="p">,</span> <span class="n">wide_embedding</span><span class="p">,</span> <span class="n">deep_embedding</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert and encrypt the intermediate data&quot;&quot;&quot;</span>
        <span class="n">local_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">wide_out0</span><span class="p">,</span> <span class="n">deep_out0</span><span class="p">))</span>
        <span class="n">remote_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">wide_embedding</span><span class="p">,</span> <span class="n">deep_embedding</span><span class="p">))</span>
        <span class="n">aa</span> <span class="o">=</span> <span class="n">remote_emb</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">bb</span> <span class="o">=</span> <span class="n">local_emb</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="n">enc_aa</span><span class="p">,</span> <span class="n">enc_aa_len</span> <span class="o">=</span> <span class="n">encrypt_client_data</span><span class="p">(</span><span class="n">aa</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">aa</span><span class="p">))</span>
        <span class="n">enc_bb</span><span class="p">,</span> <span class="n">enc_bb_len</span> <span class="o">=</span> <span class="n">encrypt_client_data</span><span class="p">(</span><span class="n">bb</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">bb</span><span class="p">))</span>
        <span class="n">tee_output</span> <span class="o">=</span> <span class="n">secure_forward_tee_cut_layer</span><span class="p">(</span><span class="n">remote_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">remote_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                                  <span class="n">local_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">enc_aa</span><span class="p">,</span> <span class="n">enc_aa_len</span><span class="p">,</span> <span class="n">enc_bb</span><span class="p">,</span> <span class="n">enc_bb_len</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">tee_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">tee_output</span><span class="p">),</span> <span class="p">(</span><span class="n">remote_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">tee_output</span>
</pre></div>
</div>
</section>
<section id="backward-propagation">
<h4>Backward Propagation<a class="headerlink" href="#backward-propagation" title="Permalink to this headline"></a></h4>
<p>In the usual vfl model, backward propagation is automatically configured by the <code class="docutils literal notranslate"><span class="pre">FLModel</span></code> class, but in models containing TEE, the user needs to develop a <code class="docutils literal notranslate"><span class="pre">grad_network</span></code> to define the backward propagation process. <code class="docutils literal notranslate"><span class="pre">grad_network</span></code> is also based on <code class="docutils literal notranslate"><span class="pre">nn.Cell</span></code> and includes a <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function and a <code class="docutils literal notranslate"><span class="pre">construct</span></code> function. When initializing, you need to pass in the network used for training and define in the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function: the derivative operator, the parameters for the network outside Cut Layer, the loss function, the Optimizer for the network outside the Cut Layer. The example is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LeaderGradNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    grad_network of the leader party.</span>
<span class="sd">    Args:</span>
<span class="sd">        net (class): LeaderNet, which is the net of leader party.</span>
<span class="sd">        config (class): default config info.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">LeaderNet</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sens</span> <span class="o">=</span> <span class="mf">1024.0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op_param_sens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_by_list</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_op_input_sens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">GradOperation</span><span class="p">(</span><span class="n">get_all</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sens_param</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">params_head</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">head_layer</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params_bottom_deep</span> <span class="o">=</span> <span class="n">vfl_utils</span><span class="o">.</span><span class="n">get_params_by_name</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">bottom_net</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;deep&#39;</span><span class="p">,</span> <span class="s1">&#39;dense&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params_bottom_wide</span> <span class="o">=</span> <span class="n">vfl_utils</span><span class="o">.</span><span class="n">get_params_by_name</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">bottom_net</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;wide&#39;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_net</span> <span class="o">=</span> <span class="n">HeadLossNet</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">head_layer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_net_l2</span> <span class="o">=</span> <span class="n">L2LossNet</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">bottom_net</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_head</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params_head</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3.5e-4</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">loss_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_bottom_deep</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params_bottom_deep</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">3.5e-4</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">loss_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_bottom_wide</span> <span class="o">=</span> <span class="n">FTRL</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params_bottom_wide</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">,</span> <span class="n">l1</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
                                          <span class="n">initial_accum</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">loss_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sens</span><span class="p">)</span>
</pre></div>
</div>
<p>The input to the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function of <code class="docutils literal notranslate"><span class="pre">grad_network</span></code> is two dictionaries <code class="docutils literal notranslate"><span class="pre">local_data_batch</span></code> and <code class="docutils literal notranslate"><span class="pre">remote_data_batch</span></code>. In the <code class="docutils literal notranslate"><span class="pre">construct</span></code> function, you first need to extract the corresponding data from the dictionaries. Next, the layers other than TEE need to call the MindSpore derivative operators on the parameters and the input for the derivative operation and update with the optimizer respectively. The TEE layer needs to call the built-in functions of TEE for the derivative and update. The examples are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">local_data_batch</span><span class="p">,</span> <span class="n">remote_data_batch</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The backward propagation of the leader net.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># data processing</span>
    <span class="n">id_hldr</span> <span class="o">=</span> <span class="n">local_data_batch</span><span class="p">[</span><span class="s1">&#39;id_hldr&#39;</span><span class="p">]</span>
    <span class="n">wt_hldr</span> <span class="o">=</span> <span class="n">local_data_batch</span><span class="p">[</span><span class="s1">&#39;wt_hldr&#39;</span><span class="p">]</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">local_data_batch</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
    <span class="n">wide_embedding</span> <span class="o">=</span> <span class="n">remote_data_batch</span><span class="p">[</span><span class="s1">&#39;wide_embedding&#39;</span><span class="p">]</span>
    <span class="n">deep_embedding</span> <span class="o">=</span> <span class="n">remote_data_batch</span><span class="p">[</span><span class="s1">&#39;deep_embedding&#39;</span><span class="p">]</span>

    <span class="c1"># forward</span>
    <span class="n">wide_out0</span><span class="p">,</span> <span class="n">deep_out0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">bottom_net</span><span class="p">(</span><span class="n">id_hldr</span><span class="p">,</span> <span class="n">wt_hldr</span><span class="p">)</span>
    <span class="n">local_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">wide_out0</span><span class="p">,</span> <span class="n">deep_out0</span><span class="p">))</span>
    <span class="n">remote_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">((</span><span class="n">wide_embedding</span><span class="p">,</span> <span class="n">deep_embedding</span><span class="p">))</span>
    <span class="n">head_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">cut_layer</span><span class="p">(</span><span class="n">wide_out0</span><span class="p">,</span> <span class="n">deep_out0</span><span class="p">,</span> <span class="n">wide_embedding</span><span class="p">,</span> <span class="n">deep_embedding</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_net</span><span class="p">(</span><span class="n">head_input</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>

    <span class="c1"># update of head net</span>
    <span class="n">sens</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Fill</span><span class="p">()(</span><span class="n">ops</span><span class="o">.</span><span class="n">DType</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="n">ops</span><span class="o">.</span><span class="n">Shape</span><span class="p">()(</span><span class="n">loss</span><span class="p">),</span> <span class="mf">1024.0</span><span class="p">)</span>
    <span class="n">grad_head_input</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op_input_sens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_net</span><span class="p">)(</span><span class="n">head_input</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
    <span class="n">grad_head_param</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op_param_sens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_net</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params_head</span><span class="p">)(</span><span class="n">head_input</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">sens</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_head</span><span class="p">(</span><span class="n">grad_head_param</span><span class="p">)</span>

    <span class="c1"># update of cut layer</span>

    <span class="n">tmp</span> <span class="o">=</span> <span class="n">grad_head_input</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">grad_input</span> <span class="o">=</span> <span class="n">backward_tee_cut_layer</span><span class="p">(</span><span class="n">remote_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">remote_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">local_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tmp</span><span class="p">)</span>
    <span class="n">grad_inputa</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">grad_input</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">remote_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">grad_inputb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">grad_input</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">local_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">grad_cutlayer_input</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad_inputb</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">grad_inputb</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">grad_inputa</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="n">grad_inputa</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>

    <span class="c1"># update of bottom net</span>
    <span class="n">grad_bottom_wide</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op_param_sens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">bottom_net</span><span class="p">,</span>
                                               <span class="bp">self</span><span class="o">.</span><span class="n">params_bottom_wide</span><span class="p">)(</span><span class="n">id_hldr</span><span class="p">,</span> <span class="n">wt_hldr</span><span class="p">,</span>
                                                                        <span class="n">grad_cutlayer_input</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_bottom_wide</span><span class="p">(</span><span class="n">grad_bottom_wide</span><span class="p">)</span>
    <span class="n">grad_bottom_deep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op_param_sens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="o">.</span><span class="n">bottom_net</span><span class="p">,</span>
                                               <span class="bp">self</span><span class="o">.</span><span class="n">params_bottom_deep</span><span class="p">)(</span><span class="n">id_hldr</span><span class="p">,</span> <span class="n">wt_hldr</span><span class="p">,</span>
                                                                        <span class="n">grad_cutlayer_input</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
    <span class="n">grad_bottom_l2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_op_param_sens</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_net_l2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">params_bottom_deep</span><span class="p">)(</span><span class="n">sens</span><span class="p">)</span>
    <span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grad_bottom_deep</span><span class="p">,</span> <span class="n">grad_bottom_l2</span><span class="p">)</span>
    <span class="n">grad_bottom_deep</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">sum</span><span class="p">,</span> <span class="n">zipped</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_bottom_deep</span><span class="p">(</span><span class="n">grad_bottom_deep</span><span class="p">)</span>

    <span class="c1"># output the gradients for follower party</span>
    <span class="n">scales</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">scales</span><span class="p">[</span><span class="s1">&#39;wide_loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="nb">zip</span><span class="p">([</span><span class="s1">&#39;wide_embedding&#39;</span><span class="p">,</span> <span class="s1">&#39;deep_embedding&#39;</span><span class="p">],</span> <span class="n">grad_cutlayer_input</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]))</span>
    <span class="n">scales</span><span class="p">[</span><span class="s1">&#39;deep_loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scales</span><span class="p">[</span><span class="s1">&#39;wide_loss&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">scales</span>
</pre></div>
</div>
</section>
<section id="definig-the-optimizer">
<h4>Definig the Optimizer<a class="headerlink" href="#definig-the-optimizer" title="Permalink to this headline"></a></h4>
<p>When defining the optimizer, there is no need to define the backward propagation part covered by <code class="docutils literal notranslate"><span class="pre">grad_network</span></code> in the yaml file, otherwise there is no difference with the usual vfl model to define the optimizer.</p>
</section>
</section>
<section id="constructing-the-training-script">
<h3>Constructing the Training Script<a class="headerlink" href="#constructing-the-training-script" title="Permalink to this headline"></a></h3>
<section id="constructing-the-network">
<h4>Constructing the Network<a class="headerlink" href="#constructing-the-network" title="Permalink to this headline"></a></h4>
<p>As the usual vFL training, users need to use the classes provided by MindSpore Federated to wrap their constructed networks into a vertical federated network. Detailed API documentation can be found in <a class="reference external" href="https://gitee.com/mindspore/federated/blob/r0.1/docs/api/api_python_en/vertical/vertical_federated_FLModel.rst">Vertical Federated Training Interface</a>. The difference is that when constructing the leader network, you need to add <code class="docutils literal notranslate"><span class="pre">grad_network</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_federated</span> <span class="kn">import</span> <span class="n">FLModel</span><span class="p">,</span> <span class="n">FLYamlData</span>
<span class="kn">from</span> <span class="nn">network_config</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">wide_and_deep</span> <span class="kn">import</span> <span class="n">LeaderNet</span><span class="p">,</span> <span class="n">LeaderLossNet</span><span class="p">,</span> <span class="n">LeaderGradNet</span>


<span class="n">leader_base_net</span> <span class="o">=</span> <span class="n">LeaderNet</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">leader_train_net</span> <span class="o">=</span> <span class="n">LeaderLossNet</span><span class="p">(</span><span class="n">leader_base_net</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">leader_grad_net</span> <span class="o">=</span> <span class="n">LeaderGradNet</span><span class="p">(</span><span class="n">leader_base_net</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

<span class="n">leader_yaml_data</span> <span class="o">=</span> <span class="n">FLYamlData</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">leader_yaml_path</span><span class="p">)</span>
<span class="n">leader_fl_model</span> <span class="o">=</span> <span class="n">FLModel</span><span class="p">(</span><span class="n">yaml_data</span><span class="o">=</span><span class="n">leader_yaml_data</span><span class="p">,</span>
                          <span class="n">network</span><span class="o">=</span><span class="n">leader_base_net</span><span class="p">,</span>
                          <span class="n">grad_network</span><span class="o">=</span><span class="n">Leader_grad_net</span><span class="p">,</span>
                          <span class="n">train_network</span><span class="o">=</span><span class="n">leader_train_net</span><span class="p">)</span>
</pre></div>
</div>
<p>Except for the above, the rest of TEE training is identical to the usual vFL training, and the user can enjoy the security of TEE once the configuration is completed.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="secure_vertical_federated_learning_with_EmbeddingDP.html" class="btn btn-neutral float-left" title="Vertical Federated-Feature Protection Based on Information Obfuscation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="secure_vertical_federated_learning_with_DP.html" class="btn btn-neutral float-right" title="Vertical Federated - Label Protection Based on Differential Privacy" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>