<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Vertical Federated Learning Model Training - Pangu Alpha Large Model Cross-Domain Training &mdash; MindSpore master documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Horizontal FL-Local Differential Privacy Perturbation Training" href="local_differential_privacy_training_noise.html" />
    <link rel="prev" title="Vertical Federated Learning Model Training - Wide&amp;Deep Recommendation Application" href="split_wnd_application.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="federated_install.html">Obtaining MindSpore Federated</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_federated_server.html">Horizontal Federated Cloud-based Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_federated_client.html">Horizontal Federated Device-side Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_vfl.html">Vertical Federated Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Horizontal Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="image_classfication_dataset_process.html">Federated Learning Image Classification Dataset Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_classification_application.html">Implementing an Image Classification Application of Cross-device Federated Learning (x86)</a></li>
<li class="toctree-l1"><a class="reference internal" href="sentiment_classification_application.html">Implementing a Sentiment Classification Application (Android)</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_classification_application_in_cross_silo.html">Implementing a Cloud-Slio Federated Image Classification Application (x86)</a></li>
<li class="toctree-l1"><a class="reference internal" href="object_detection_application_in_cross_silo.html">Implementing a Cross-Silo Federated Target Detection Application (x86)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Vertical Application</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="data_join.html">Vertical Federated Learning Data Access</a></li>
<li class="toctree-l1"><a class="reference internal" href="split_wnd_application.html">Vertical Federated Learning Model Training - Wide&amp;Deep Recommendation Application</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Vertical Federated Learning Model Training - Pangu Alpha Large Model Cross-Domain Training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#preparation">Preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataset-preparation">Dataset Preparation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#defining-the-vertical-federated-learning-training-process">Defining the Vertical Federated Learning Training Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-network-model">Defining the Network Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-optimizer">Defining the Optimizer</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-gradient-weighting-coefficient-calculation">Defining Gradient weighting coefficient Calculation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#executing-the-training">Executing the Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#running-the-example">Running the Example</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#running-a-single-process-example">Running a Single-Process Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-a-multi-process-example">Running a Multi-Process Example</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Security and Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="local_differential_privacy_training_noise.html">Horizontal FL-Local Differential Privacy Perturbation Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="local_differential_privacy_training_signds.html">Horizontal FL-Local Differential Privacy SignDS training</a></li>
<li class="toctree-l1"><a class="reference internal" href="pairwise_encryption_training.html">Horizontal FL-Pairwise encryption training</a></li>
<li class="toctree-l1"><a class="reference internal" href="private_set_intersection.html">Vertical Federated-Privacy Set Intersection</a></li>
<li class="toctree-l1"><a class="reference internal" href="secure_vertical_federated_learning_with_EmbeddingDP.html">Vertical Federated-Feature Protection Based on Information Obfuscation</a></li>
<li class="toctree-l1"><a class="reference internal" href="secure_vertical_federated_learning_with_TEE.html">Vertical Federated - Feature Protection Based on Trusted Execution Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="secure_vertical_federated_learning_with_DP.html">Vertical Federated - Label Protection Based on Differential Privacy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Communication Compression</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="communication_compression.html">Device-Cloud Federated Learning Communication Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="vfl_communication_compress.html">Vertical Federated Learning Communication Compression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Horizontal Federated API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="horizontal_server.html">Federated Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="cross_device.html">Device-side Client</a></li>
<li class="toctree-l1"><a class="reference internal" href="horizontal/cross_silo.html">Cross-Silo</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Vertical Federated API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Data_Join.html">Data Join</a></li>
<li class="toctree-l1"><a class="reference internal" href="vertical/vertical_communicator.html">Vertical Federated Learning Communicator</a></li>
<li class="toctree-l1"><a class="reference internal" href="vertical_federated_trainer.html">Vertical Federated Trainer</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Vertical Federated Learning Model Training - Pangu Alpha Large Model Cross-Domain Training</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/split_pangu_alpha_application.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="vertical-federated-learning-model-training--pangu-alpha-large-model-cross-domain-training">
<h1>Vertical Federated Learning Model Training - Pangu Alpha Large Model Cross-Domain Training<a class="headerlink" href="#vertical-federated-learning-model-training--pangu-alpha-large-model-cross-domain-training" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0/docs/federated/docs/source_en/split_pangu_alpha_application.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>With the advancement of hardware computing power and the continuous expansion of network data size, pre-training large models has increasingly become an important research direction in fields such as natural language processing and graphical multimodality. Take Pangu Alpha, which released a large pre-trained model of Chinese NLP in 2021, as an example, the number of model parameters reaches 200 billion, and the training process relies on massive data and advanced computing centers, which limits its application landing and technology evolution. A feasible solution is to integrate the computing power and data resources of multiple participants based on vertical federated learning or split learning techniques to achieve cross-domain collaborative training of pre-trained large models while ensuring security and privacy.</p>
<p>MindSpore Federated provides a vertical federated learning base functional component based on split learning. This sample provides a federated learning training sample for large NLP models by taking the Pangaea alpha model as an example.</p>
<p><img alt="Implementing cross-domain training for the Pangu Alpha large model" src="_images/splitnn_pangu_alpha_en.png" /></p>
<p>As shown in the figure above, in this case, the Pangaea α model is sliced into three sub-networks, such as Embedding, Backbone and Head. The front-level subnetwork Embedding and the end-level subnetwork Head are deployed in the network domain of participant A, and the Backbone subnetwork containing multi-level Transformer modules is deployed in the network domain of participant B. The Embedding subnetwork and Head subnetwork read the data held by participant A and dominate the training and inference tasks for performing the Pangaea α model.</p>
<ul class="simple">
<li><p>In the forward inference stage, Participant A uses the Embedding subnetwork to process the original data and transmits the output Embedding Feature tensor and Attention Mask Feature tensor to Participant B as the input of Participant B Backbone subnetwork. Then, Participant A reads the Hide State Feature tensor output from the Backbone subnetwork as the input of Participant A Head subnetwork, and finally the predicted result or loss value is output by the Head sub-network.</p></li>
<li><p>In the backward propagation phase, after completing the gradient calculation and parameter update of the Head subnetwork, Participant A transmits the gradient tensor associated with the Hide State Feature tensor to Participant B for the gradient calculation and parameter update of the Backbone subnetwork. Then, Participant B transmits the gradient tensor associated with the Embedding Feature tensor to Participant A for the gradient calculation and parameter update of the Embedding subnetwork after completing the gradient calculation and parameter update of the Backbone subnetwork.</p></li>
</ul>
<p>The feature tensor and gradient tensor exchanged between participant A and participant B during the above forward inference and backward propagation are processed by using privacy security mechanisms and encryption algorithms, so that it is not necessary to transmit the data held by participant A to participant B for implementing the collaboration training of the network model by the two participants. Due to the small number of Embedding and Head subnetwork parameters and the huge number of Backbone subnetwork parameters, this sample application is suitable for the large model collaboration training or deployment between the service side (corresponding to participant A) and the computing center (corresponding to participant B).</p>
<p>For a detailed introduction to the pangu α model principles, please refer to <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.0/official/nlp/Pangu_alpha">MindSpore ModelZoo - pangu_alpha</a>, <a class="reference external" href="https://git.openi.org.cn/PCL-Platform.Intelligence/PanGu-Alpha">Introduction to Pengcheng -pangu α</a>, and its <a class="reference external" href="https://arxiv.org/pdf/2104.12369.pdf">research paper</a>.</p>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<section id="environment-preparation">
<h3>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Refer to <a class="reference external" href="https://mindspore.cn/federated/docs/en/r0.1/federated_install.html">Obtaining MindSpore Federated</a> to install MindSpore version 1.8.1 and above and MindSpore Federated.</p></li>
<li><p>Download the MindSpore Federated code and install the Python packages that this sample application depends on.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>https://gitee.com/mindspore/federated.git
<span class="nb">cd</span><span class="w"> </span>federated/example/splitnn_pangu_alpha/
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
</ol>
</section>
<section id="dataset-preparation">
<h3>Dataset Preparation<a class="headerlink" href="#dataset-preparation" title="Permalink to this headline"></a></h3>
<p>Before running the sample, refer to <a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.0/official/nlp/Pangu_alpha#dataset-generation">MindSpore ModelZoo - pangu_alpha - Dataset Generation</a> and use the preprocess.py script to convert the raw text corpus for training into a dataset that can be used for model training.</p>
</section>
</section>
<section id="defining-the-vertical-federated-learning-training-process">
<h2>Defining the Vertical Federated Learning Training Process<a class="headerlink" href="#defining-the-vertical-federated-learning-training-process" title="Permalink to this headline"></a></h2>
<p>MindSpore Federated Vertical Federated Learning Framework uses FLModel (see <a class="reference external" href="https://mindspore.cn/federated/docs/en/r0.1/vertical/vertical_federated_FLModel.html">Vertical Federated Learning Model Training Interface</a>) and yaml files (see <a class="reference external" href="https://mindspore.cn/federated/docs/en/r0.1/vertical/vertical_federated_yaml.html">Yaml Configuration file for model training of vertical federated learning</a>), to model vertical federated learning training process.</p>
<section id="defining-the-network-model">
<h3>Defining the Network Model<a class="headerlink" href="#defining-the-network-model" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Call the function components provided by MindSpore and take nn.Cell (see <a class="reference external" href="https://mindspore.cn/docs/en/r2.0/api_python/nn/mindspore.nn.Cell.html#mindspore-nn-cell">mindspore.nn.Cell</a>) as a base class to program the training network of this participant to be involved in vertical federated learning. Taking the Embedding subnetwork of participant A in this application practice as an example, <a class="reference external" href="https://gitee.com/mindspore/federated/blob/r0.1/example/splitnn_pangu_alpha/src/split_pangu_alpha.py">sample code</a> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EmbeddingLossNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Train net of the embedding party, or the tail sub-network.</span>
<span class="sd">    Args:</span>
<span class="sd">        net (class): EmbeddingLayer, which is the 1st sub-network.</span>
<span class="sd">        config (class): default config info.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">:</span> <span class="n">EmbeddingLayer</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EmbeddingLossNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">auto_prefix</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">seq_length</span>
        <span class="n">dp</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">data_parallel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eod_token</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">eod_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slice</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">not_equal</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">NotEqual</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">()))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">len</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">seq_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">slice2</span> <span class="o">=</span> <span class="n">P</span><span class="o">.</span><span class="n">StridedSlice</span><span class="p">()</span><span class="o">.</span><span class="n">shard</span><span class="p">(((</span><span class="n">dp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),))</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">position_id</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;forward process of FollowerLossNet&quot;&quot;&quot;</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">embedding_table</span><span class="p">,</span> <span class="n">word_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">position_id</span><span class="p">,</span> <span class="n">batch_valid_length</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embedding_table</span><span class="p">,</span> <span class="n">word_table</span><span class="p">,</span> <span class="n">position_id</span><span class="p">,</span> <span class="n">attention_mask</span>
</pre></div>
</div>
</li>
<li><p>In the yaml configuration file, describe the corresponding name, input, output and other information of the training network. Taking the Embedding subnetwork of Participant A in this application practice, <a class="reference external" href="https://gitee.com/mindspore/federated/blob/r0.1/example/splitnn_pangu_alpha/embedding.yaml">example code</a> is as follows:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">train_net</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">follower_loss_net</span>
<span class="w">    </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">input_ids</span>
<span class="w">        </span><span class="nt">source</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">local</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">position_id</span>
<span class="w">        </span><span class="nt">source</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">local</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">attention_mask</span>
<span class="w">        </span><span class="nt">source</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">local</span>
<span class="w">    </span><span class="nt">outputs</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">embedding_table</span>
<span class="w">        </span><span class="nt">destination</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">remote</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">word_table</span>
<span class="w">        </span><span class="nt">destination</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">remote</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">position_id</span>
<span class="w">        </span><span class="nt">destination</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">remote</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">attention_mask</span>
<span class="w">        </span><span class="nt">destination</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">remote</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">name</span></code> field is the name of the training network and will be used to name the checkpoints file saved during the training process. The <code class="docutils literal notranslate"><span class="pre">inputs</span></code> field is the list of input tensor in the training network, and the <code class="docutils literal notranslate"><span class="pre">outputs</span></code> field is the list of output tensor in the training network.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">name</span></code> fields under the <code class="docutils literal notranslate"><span class="pre">inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">outputs</span></code> fields are the input/output tensor names. The names and order of the input/output tensors need to correspond strictly to the inputs/outputs of the <code class="docutils literal notranslate"><span class="pre">construct</span></code> method in the corresponding Python code of the training network.</p>
<p><code class="docutils literal notranslate"><span class="pre">source</span></code> under the <code class="docutils literal notranslate"><span class="pre">inputs</span></code> field identifies the data source of the input tensor, with <code class="docutils literal notranslate"><span class="pre">local</span></code> representing that the input tensor is loaded from local data and <code class="docutils literal notranslate"><span class="pre">remote</span></code> representing that the input tensor is from network transmission of other participants.</p>
<p><code class="docutils literal notranslate"><span class="pre">destination</span></code> under the <code class="docutils literal notranslate"><span class="pre">outputs</span></code> field identifies the destination of the output tensor, with <code class="docutils literal notranslate"><span class="pre">local</span></code> representing the output tensor for local use only, and <code class="docutils literal notranslate"><span class="pre">remote</span></code> representing that the output tensor is transferred to other participants via networks.</p>
</li>
<li><p>Optionally, a similar approach is used to model the assessment network of vertical federated learning that this participant is to be involved.</p></li>
</ol>
</section>
<section id="defining-the-optimizer">
<h3>Defining the Optimizer<a class="headerlink" href="#defining-the-optimizer" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Call the functional components provided by MindSpore, to program the optimizer for parameter updates of this participant training network. As an example of a custom optimizer used by Participant A for Embedding subnetwork training in this application practice, <a class="reference external" href="https://gitee.com/mindspore/federated/blob/r0.1/example/splitnn_pangu_alpha/src/pangu_optim.py">sample code</a> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PanguAlphaAdam</span><span class="p">(</span><span class="n">TrainOneStepWithLossScaleCell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Customized Adam optimizer for training of pangu_alpha in the splitnn demo system.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">optim_inst</span><span class="p">,</span> <span class="n">scale_update_cell</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">yaml_data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Custom optimizer-related operators</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Define the gradient calculation and parameter update process</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>Developers can customize the input and output of the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method in the optimizer class, but the input of the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method in the optimizer class needs to contain only <code class="docutils literal notranslate"><span class="pre">inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">sens</span></code>. <code class="docutils literal notranslate"><span class="pre">inputs</span></code> is of type <code class="docutils literal notranslate"><span class="pre">list</span></code>, corresponding to the input tensor list of the training network, and its elements are of type <code class="docutils literal notranslate"><span class="pre">mindspore.Tensor</span></code>. <code class="docutils literal notranslate"><span class="pre">sens</span></code> is of type <code class="docutils literal notranslate"><span class="pre">dict</span></code>, which saves the weighting coefficients used to calculate the gradient values of the training network parameters, and its key is a gradient weighting coefficient identifier of type <code class="docutils literal notranslate"><span class="pre">str</span></code>. Value is of type <code class="docutils literal notranslate"><span class="pre">dict</span></code>, whose key is of type <code class="docutils literal notranslate"><span class="pre">str</span></code>, and it is the name of the output tensor of the training network. Value is of type <code class="docutils literal notranslate"><span class="pre">mindspore.Tensor</span></code>, which is the weighting coefficient of the training network parameter gradient values corresponding to this output tensor.</p>
</li>
<li><p>In the yaml configuration file, describe the corresponding gradient calculation, parameter update, and other information of the optimizer. The <a class="reference external" href="https://gitee.com/mindspore/federated/blob/r0.1/example/splitnn_pangu_alpha/embedding.yaml">sample code</a> is as follows:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">opts</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CustomizedAdam</span>
<span class="w">    </span><span class="nt">grads</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">input_ids</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">position_id</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">attention_mask</span>
<span class="w">        </span><span class="nt">output</span><span class="p">:</span>
<span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">embedding_table</span>
<span class="w">        </span><span class="nt">sens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hidden_states</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">input_ids</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">position_id</span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">attention_mask</span>
<span class="w">        </span><span class="nt">output</span><span class="p">:</span>
<span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">word_table</span>
<span class="w">        </span><span class="nt">sens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">word_table</span>
<span class="w">    </span><span class="nt">params</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">word_embedding</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">position_embedding</span>
<span class="w">    </span><span class="nt">hyper_parameters</span><span class="p">:</span>
<span class="w">      </span><span class="nt">learning_rate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5.e-6</span>
<span class="w">      </span><span class="nt">eps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1.e-8</span>
<span class="w">      </span><span class="nt">loss_scale</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1024.0</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">type</span></code> field is of the optimizer type. Here is the developer-defined optimizer.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">grads</span></code> field is a list of <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> associated with the optimizer, which will use the <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> operator in the list to compute the output gradient values and update the training network parameters. The <code class="docutils literal notranslate"><span class="pre">inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">output</span></code> fields are input and output tensor lists of the <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> operator, whose elements are an input/output tensor name, respectively. The <code class="docutils literal notranslate"><span class="pre">sens</span></code> field is the gradient weighting coefficient or the sensitivity identifier of the <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> operator (refer to <a class="reference external" href="https://mindspore.cn/docs/en/r2.0/api_python/ops/mindspore.ops.GradOperation.html?highlight=gradoperation">mindspore.ops.GradOperation</a>).</p>
<p>The <code class="docutils literal notranslate"><span class="pre">params</span></code> field is a list of training network parameter names to be updated by the optimizer, whose elements are the names of one training network parameter each. In this example, the custom optimizer will update the network parameters with the <code class="docutils literal notranslate"><span class="pre">word_embedding</span></code> string and the <code class="docutils literal notranslate"><span class="pre">position_embedding</span></code> string in their names.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">hyper_parameters</span></code> field is a list of hyperparameters for the optimizer.</p>
</li>
</ol>
</section>
<section id="defining-gradient-weighting-coefficient-calculation">
<h3>Defining Gradient weighting coefficient Calculation<a class="headerlink" href="#defining-gradient-weighting-coefficient-calculation" title="Permalink to this headline"></a></h3>
<p>According to the chain rule of gradient calculation, the subnetwork located at the backstream of the global network needs to calculate the gradient value of its output tensor relative to the input tensor, i.e., the gradient weighting coefficient or sensitivity, to be passed to the sub-network located at the upstream of the global network for its training parameter update.</p>
<p>MindSpore Federated uses the <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> operator to complete the above gradient weighting coefficient or sensitivity calculation process. The developer needs to describe the <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> operator used to calculate the gradient weighting coefficients in the yaml configuration file. Taking Head of participant A in this application practice as an example, <a class="reference external" href="https://gitee.com/mindspore/federated/blob/r0.1/example/splitnn_pangu_alpha/head.yaml">sample code</a> is as follows:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">grad_scalers</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">inputs</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hidden_states</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">input_ids</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">word_table</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">position_id</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">attention_mask</span>
<span class="w">    </span><span class="nt">output</span><span class="p">:</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">output</span>
<span class="w">    </span><span class="nt">sens</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1024.0</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">output</span></code> fields are lists of input and output tensors of the <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> operator, whose elements are input/output tensor names, respectively. The <code class="docutils literal notranslate"><span class="pre">sens</span></code> field is the gradient weighting coefficient or sensitivity of this <code class="docutils literal notranslate"><span class="pre">GradOperation</span></code> operator (refer to <a class="reference external" href="https://mindspore.cn/docs/en/r2.0/api_python/ops/mindspore.ops.GradOperation.html?highlight=gradoperation">mindspore.ops.GradOperation</a>). If it is a <code class="docutils literal notranslate"><span class="pre">float</span></code> or <code class="docutils literal notranslate"><span class="pre">int</span></code> type value, a constant tensor will be constructed as the gradient weighting coefficient. If it is a <code class="docutils literal notranslate"><span class="pre">str</span></code> type string, the tensor corresponding to the name will be parsed as a weighting coefficient from the weighting coefficients transmitted by the other participants via the network.</p>
</section>
<section id="executing-the-training">
<h3>Executing the Training<a class="headerlink" href="#executing-the-training" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>After completing the above Python programming development and yaml configuration file, the <code class="docutils literal notranslate"><span class="pre">FLModel</span></code> class and <code class="docutils literal notranslate"><span class="pre">FLYamlData</span></code> class provided by MindSpore Federated are used to build the vertical federated learning process. Taking the Embedding subnetwork of participant A in this application practice as an example, <a class="reference external" href="https://gitee.com/mindspore/federated/blob/r0.1/example/splitnn_pangu_alpha/run_pangu_train_local.py">sample code</a> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_yaml</span> <span class="o">=</span> <span class="n">FLYamlData</span><span class="p">(</span><span class="s1">&#39;./embedding.yaml&#39;</span><span class="p">)</span>
<span class="n">embedding_base_net</span> <span class="o">=</span> <span class="n">EmbeddingLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">embedding_eval_net</span> <span class="o">=</span> <span class="n">embedding_train_net</span> <span class="o">=</span> <span class="n">EmbeddingLossNet</span><span class="p">(</span><span class="n">embedding_base_net</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">embedding_with_loss</span> <span class="o">=</span> <span class="n">_VirtualDatasetCell</span><span class="p">(</span><span class="n">embedding_eval_net</span><span class="p">)</span>
<span class="n">embedding_params</span> <span class="o">=</span> <span class="n">embedding_with_loss</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()</span>
<span class="n">embedding_group_params</span> <span class="o">=</span> <span class="n">set_embedding_weight_decay</span><span class="p">(</span><span class="n">embedding_params</span><span class="p">)</span>
<span class="n">embedding_optim_inst</span> <span class="o">=</span> <span class="n">FP32StateAdamWeightDecay</span><span class="p">(</span><span class="n">embedding_group_params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="n">embedding_optim</span> <span class="o">=</span> <span class="n">PanguAlphaAdam</span><span class="p">(</span><span class="n">embedding_train_net</span><span class="p">,</span> <span class="n">embedding_optim_inst</span><span class="p">,</span> <span class="n">update_cell</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">embedding_yaml</span><span class="p">)</span>

<span class="n">embedding_fl_model</span> <span class="o">=</span> <span class="n">FLModel</span><span class="p">(</span><span class="n">yaml_data</span><span class="o">=</span><span class="n">embedding_yaml</span><span class="p">,</span>
                             <span class="n">network</span><span class="o">=</span><span class="n">embedding_train_net</span><span class="p">,</span>
                             <span class="n">eval_network</span><span class="o">=</span><span class="n">embedding_eval_net</span><span class="p">,</span>
                             <span class="n">optimizers</span><span class="o">=</span><span class="n">embedding_optim</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">FLYamlData</span></code> class mainly completes the parsing and verification of yaml configuration files, and the <code class="docutils literal notranslate"><span class="pre">FLModel</span></code> class mainly provides the control interface for vertical federated learning training, inference and other processes.</p>
</li>
<li><p>Call the interface methods of the <code class="docutils literal notranslate"><span class="pre">FLModel</span></code> class to perform vertical federated learning training. Taking the Embedding subnetwork of participant A in this application practice as an example, <a class="reference external" href="https://gitee.com/mindspore/federated/blob/r0.1/example/splitnn_pangu_alpha/run_pangu_train_local.py">sample code</a> is as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">opt</span><span class="o">.</span><span class="n">resume</span><span class="p">:</span>
    <span class="n">embedding_fl_model</span><span class="o">.</span><span class="n">load_ckpt</span><span class="p">()</span>
    <span class="o">...</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># forward process</span>
        <span class="n">step</span> <span class="o">=</span> <span class="n">epoch</span> <span class="o">*</span> <span class="n">train_size</span> <span class="o">+</span> <span class="n">step</span>
        <span class="n">embedding_out</span> <span class="o">=</span> <span class="n">embedding_fl_model</span><span class="o">.</span><span class="n">forward_one_step</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="o">...</span>
        <span class="c1"># backward process</span>
        <span class="n">embedding_fl_model</span><span class="o">.</span><span class="n">backward_one_step</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">sens</span><span class="o">=</span><span class="n">backbone_scale</span><span class="p">)</span>
        <span class="o">...</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">embedding_fl_model</span><span class="o">.</span><span class="n">save_ckpt</span><span class="p">()</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">forward_one_step</span></code> method and the <code class="docutils literal notranslate"><span class="pre">backward_one_step</span></code> method perform the forward inference and backward propagation operations of a data batch, respectively. The <code class="docutils literal notranslate"><span class="pre">load_ckpt</span></code> method and the <code class="docutils literal notranslate"><span class="pre">save_ckpt</span></code> method perform the checkpoints file loading and saving operations respectively.</p>
</li>
</ol>
</section>
</section>
<section id="running-the-example">
<h2>Running the Example<a class="headerlink" href="#running-the-example" title="Permalink to this headline"></a></h2>
<p>This example provides 2 sample programs, both running as shell scripts to pull up Python programs.</p>
<ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">run_pangu_train_local.sh</span></code>: Single-process example program. Participant A and participant B are trained in the same process, which transmits the feature tensor and gradient tensor directly to the other participant in the form of intra-program variables.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run_pangu_train_leader.sh</span></code> and <code class="docutils literal notranslate"><span class="pre">run_pangu_train_follower.sh</span></code>: Multi-process example program. Participant A and participant B run a separate process, which encapsulates the feature tensor and gradient tensor as protobuf messages, respectively, and transmits them to the other participant via the https communication interface. <code class="docutils literal notranslate"><span class="pre">run_pangu_train_leader.sh</span></code> and <code class="docutils literal notranslate"><span class="pre">run_pangu_train_follower.sh</span></code> can be run on two servers separately to achieve cross-domain collaboration training.</p></li>
<li><p>The current vertical federated distributed training supports https cross-domain encrypted communication. The startup command is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start the leader process in https encrypted communication mode:</span>
bash<span class="w"> </span>run_pangu_train_leader.sh<span class="w"> </span><span class="m">127</span>.0.0.1:10087<span class="w"> </span><span class="m">127</span>.0.0.1:10086<span class="w"> </span>/path/to/train/data_set<span class="w"> </span>/path/to/eval/data_set<span class="w"> </span>True<span class="w"> </span>server_cert_password<span class="w"> </span>client_cert_password<span class="w"> </span>/path/to/server_cert<span class="w"> </span>/path/to/client_cert<span class="w"> </span>/path/to/ca_cert

<span class="c1"># Start the follower process in https encrypted communication mode:</span>
bash<span class="w"> </span>run_pangu_train_follower.sh<span class="w"> </span><span class="m">127</span>.0.0.1:10086<span class="w"> </span><span class="m">127</span>.0.0.1:10087<span class="w"> </span>True<span class="w"> </span>server_cert_password<span class="w"> </span>client_cert_password<span class="w"> </span>/path/to/server_cert<span class="w"> </span>/path/to/client_cert<span class="w"> </span>/path/to/ca_cert
</pre></div>
</div>
</li>
</ol>
<section id="running-a-single-process-example">
<h3>Running a Single-Process Example<a class="headerlink" href="#running-a-single-process-example" title="Permalink to this headline"></a></h3>
<p>Taking <code class="docutils literal notranslate"><span class="pre">run_pangu_train_local.sh</span></code> as an example, run the sample program as follows:</p>
<ol class="arabic">
<li><p>Go to the sample program directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>federated/example/splitnn_pangu_alpha/
</pre></div>
</div>
</li>
<li><p>Taking the wiki dataset as an example, copy the dataset to the sample program directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cp<span class="w"> </span>-r<span class="w"> </span><span class="o">{</span>dataset_dir<span class="o">}</span>/wiki<span class="w"> </span>./
</pre></div>
</div>
</li>
<li><p>Install the dependent Python packages:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Modify <code class="docutils literal notranslate"><span class="pre">src/utils.py</span></code> to configure parameters such as checkpoint file load path, training dataset path, and evaluation dataset path. Examples are as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--load_ckpt_path&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;./checkpoints&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;predict file path.&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--data_url&#39;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;./wiki/train/&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Location of data.&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--eval_data_url&#39;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s1">&#39;./wiki/eval/&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Location of eval data.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Execute the training script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_pangu_train_local.sh
</pre></div>
</div>
</li>
<li><p>View the training loss information recorded in the training log <code class="docutils literal notranslate"><span class="pre">splitnn_pangu_local.txt</span></code>.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>INFO:root:epoch 0 step 10/43391 loss: 10.616087
INFO:root:epoch 0 step 20/43391 loss: 10.424824
INFO:root:epoch 0 step 30/43391 loss: 10.209235
INFO:root:epoch 0 step 40/43391 loss: 9.950026
INFO:root:epoch 0 step 50/43391 loss: 9.712448
INFO:root:epoch 0 step 60/43391 loss: 9.557744
INFO:root:epoch 0 step 70/43391 loss: 9.501564
INFO:root:epoch 0 step 80/43391 loss: 9.326054
INFO:root:epoch 0 step 90/43391 loss: 9.387547
INFO:root:epoch 0 step 100/43391 loss: 8.795234
...
</pre></div>
</div>
<p>The corresponding visualization results are shown below, where the horizontal axis is the number of training steps, the vertical axis is the loss value, the red curve is the Pangu α training loss value, and the blue curve is the Pangu α training loss value based on splitting learning in this example. The trend of decreasing loss values is basically the same, and the correctness of the training process can be verified considering that the initialization of the network parameter values has randomness.</p>
<p><img alt="Cross-domain training results of the Pangu alpha large model" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/docs/federated/docs/source_zh_cn/images/splitnn_pangu_alpha_result.png" /></p>
</li>
</ol>
</section>
<section id="running-a-multi-process-example">
<h3>Running a Multi-Process Example<a class="headerlink" href="#running-a-multi-process-example" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Similar to the single-process example, go to the sample program directory, and install the dependent Python packages:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>federated/example/splitnn_pangu_alpha/
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
<li><p>Copy the dataset to the sample program directory on Server 1:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>cp<span class="w"> </span>-r<span class="w"> </span><span class="o">{</span>dataset_dir<span class="o">}</span>/wiki<span class="w"> </span>./
</pre></div>
</div>
</li>
<li><p>Start the training script for Participant A on Server 1:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_pangu_train_leader.sh<span class="w"> </span><span class="o">{</span>ip_address_server1<span class="o">}</span><span class="w"> </span><span class="o">{</span>ip_address_server2<span class="o">}</span><span class="w"> </span>./wiki/train<span class="w"> </span>./wiki/train
</pre></div>
</div>
<p>The first parameter of the training script is the IP address and port number of the local server (Server 1), and the second parameter is the IP address and port number of the peer server (Server 2). The third parameter is the training dataset file path. The fourth parameter is the evaluation dataset file path, and the fifth parameter identifies whether to load an existing checkpoint file.</p>
</li>
<li><p>Start the training script for Participant B on Server 2.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./run_pangu_train_follower.sh<span class="w"> </span><span class="o">{</span>ip_address_server2<span class="o">}</span><span class="w"> </span><span class="o">{</span>ip_address_server1<span class="o">}</span>
</pre></div>
</div>
<p>The first parameter of the training script is the IP address and port number of the local server (Server 2), and the second parameter is the IP address and port number of the peer server (Server 2). The third parameter identifies whether to load an existing checkpoint file.</p>
</li>
<li><p>Check the training loss information recorded in the training log <code class="docutils literal notranslate"><span class="pre">leader_processs.log</span></code> of Server 1. If the trend of its loss information is consistent with that of the centralized training loss values of Pangaea α, the correctness of the training process can be verified.</p></li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="split_wnd_application.html" class="btn btn-neutral float-left" title="Vertical Federated Learning Model Training - Wide&amp;Deep Recommendation Application" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="local_differential_privacy_training_noise.html" class="btn btn-neutral float-right" title="Horizontal FL-Local Differential Privacy Perturbation Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>