

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Cloud-based Deployment &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="On-Device Deployment" href="deploy_federated_client.html" />
    <link rel="prev" title="Obtaining MindSpore Federated" href="federated_install.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Deployment</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="federated_install.html">Obtaining MindSpore Federated</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Cloud-based Deployment</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#preparations">Preparations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#installing-mindspore">Installing MindSpore</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#defining-a-model">Defining a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuring-parameters">Configuring Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#starting-a-cluster">Starting a Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="#auto-scaling">Auto Scaling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#disaster-recovery">Disaster Recovery</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="deploy_federated_client.html">On-Device Deployment</a></li>
</ul>
<p class="caption"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="image_classification_application.html">Implementing an Image Classification Application of Cross-device Federated Learning (x86)</a></li>
<li class="toctree-l1"><a class="reference internal" href="sentiment_classification_application.html">Implementing a Sentiment Classification Application (Android)</a></li>
</ul>
<p class="caption"><span class="caption-text">Security and Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="security_and_privacy_protection.html">Model Security and Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Cloud-based Deployment</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/deploy_federated_server.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="cloud-based-deployment">
<h1>Cloud-based Deployment<a class="headerlink" href="#cloud-based-deployment" title="Permalink to this headline">Â¶</a></h1>
<!-- TOC -->
<ul class="simple">
<li><p><a class="reference external" href="#cloud-based-deployment">Cloud-based Deployment</a></p>
<ul>
<li><p><a class="reference external" href="#preparations">Preparations</a></p>
<ul>
<li><p><a class="reference external" href="#installing-mindspore">Installing MindSpore</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#defining-a-model">Defining a Model</a></p></li>
<li><p><a class="reference external" href="#configuring-parameters">Configuring Parameters</a></p></li>
<li><p><a class="reference external" href="#starting-a-cluster">Starting a Cluster</a></p></li>
<li><p><a class="reference external" href="#auto-scaling">Auto Scaling</a></p></li>
<li><p><a class="reference external" href="#disaster-recovery">Disaster Recovery</a></p></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p><a href="https://gitee.com/mindspore/docs/blob/r1.5/docs/federated/docs/source_en/deploy_federated_server.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png"></a></p>
<p>The following uses LeNet as an example to describe how to use MindSpore to deploy a federated learning cluster.</p>
<blockquote>
<div><p>You can download the complete demo from <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/tests/st/fl/mobile">here</a>.</p>
</div></blockquote>
<p>The following figure shows the physical architecture of the MindSpore Federated Learning Server cluster:</p>
<p><img alt="mindspore-federated-networking" src="_images/mindspore_federated_networking.png" /></p>
<p>As shown in the preceding figure, in the federated learning cloud cluster, there are two MindSpore process roles: <code class="docutils literal notranslate"><span class="pre">Federated</span> <span class="pre">Learning</span> <span class="pre">Scheduler</span></code> and <code class="docutils literal notranslate"><span class="pre">Federated</span> <span class="pre">Learning</span> <span class="pre">Server</span></code>:</p>
<ul>
<li><p>Federated Learning Scheduler</p>
<p><code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> provides the following functions:</p>
<ol class="simple">
<li><p>Cluster networking assistance: During cluster initialization, the <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> collects server information and ensures cluster consistency.</p></li>
<li><p>Open management plane: You can manage clusters through the <code class="docutils literal notranslate"><span class="pre">RESTful</span></code> APIs.</p></li>
</ol>
<p>In a federated learning task, there is only one <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>, which communicates with the <code class="docutils literal notranslate"><span class="pre">Server</span></code> using the TCP proprietary protocol.</p>
</li>
<li><p>Federated Learning Server</p>
<p><code class="docutils literal notranslate"><span class="pre">Server</span></code> executes federated learning tasks, receives and parses data from devices, and provides capabilities such as secure aggregation, time-limited communication, and model storage. In a federated learning task, users can configure multiple <code class="docutils literal notranslate"><span class="pre">Servers</span></code> which communicate with each other through the TCP proprietary protocol and open HTTP ports for device-side connection.</p>
<blockquote>
<div><p>In the MindSpore federated learning framework, <code class="docutils literal notranslate"><span class="pre">Server</span></code> also supports auto scaling and disaster recovery, and can dynamically schedule hardware resources without interrupting training tasks.</p>
</div></blockquote>
</li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> and <code class="docutils literal notranslate"><span class="pre">Server</span></code> must be deployed on a server or container with a single NIC and in the same network segment. MindSpore automatically obtains the first available IP address as the <code class="docutils literal notranslate"><span class="pre">Server</span></code> IP address.</p>
<div class="section" id="preparations">
<h2>Preparations<a class="headerlink" href="#preparations" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="installing-mindspore">
<h3>Installing MindSpore<a class="headerlink" href="#installing-mindspore" title="Permalink to this headline">Â¶</a></h3>
<p>The MindSpore federated learning cloud cluster supports deployment on x86 CPU and GPU hardware platforms. Run commands provided by the <a class="reference external" href="https://www.mindspore.cn/install">official website</a> to install the latest MindSpore.</p>
</div>
</div>
<div class="section" id="defining-a-model">
<h2>Defining a Model<a class="headerlink" href="#defining-a-model" title="Permalink to this headline">Â¶</a></h2>
<p>To facilitate deployment, the <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> and <code class="docutils literal notranslate"><span class="pre">Server</span></code> processes of MindSpore federated learning can reuse the training script. You can select different startup modes by referring to <a class="reference external" href="#configuring-parameters">Configuring Parameters</a>.</p>
<p>This tutorial uses LeNet as an example. For details about the network structure, loss function, and optimizer definition, see <a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.5/docs/sample_code/lenet/lenet.py">LeNet sample code</a>.</p>
</div>
<div class="section" id="configuring-parameters">
<h2>Configuring Parameters<a class="headerlink" href="#configuring-parameters" title="Permalink to this headline">Â¶</a></h2>
<p>The MindSpore federated learning task process reuses the training script. You only need to use the same script to transfer different parameters through the Python API <code class="docutils literal notranslate"><span class="pre">set_fl_context</span></code> and start different MindSpore process roles. For details about the parameter configuration, see <a class="reference external" href="https://www.mindspore.cn/federated/api/en/r1.5/federated_server.html#mindspore.context.set_fl_context">MindSpore API</a>.</p>
<p>After parameter configuration and before training, call the <code class="docutils literal notranslate"><span class="pre">set_fl_context</span></code> API as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore.context</span> <span class="k">as</span> <span class="nn">context</span>
<span class="o">...</span>

<span class="n">enable_fl</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">server_mode</span> <span class="o">=</span> <span class="s2">&quot;FEDERATED_LEARNING&quot;</span>
<span class="n">ms_role</span> <span class="o">=</span> <span class="s2">&quot;MS_SERVER&quot;</span>
<span class="n">server_num</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">scheduler_ip</span> <span class="o">=</span> <span class="s2">&quot;192.168.216.124&quot;</span>
<span class="n">scheduler_port</span> <span class="o">=</span> <span class="mi">6667</span>
<span class="n">fl_server_port</span> <span class="o">=</span> <span class="mi">6668</span>
<span class="n">fl_name</span> <span class="o">=</span> <span class="s2">&quot;LeNet&quot;</span>
<span class="n">scheduler_manage_port</span> <span class="o">=</span> <span class="mi">11202</span>
<span class="n">config_file_path</span> <span class="o">=</span> <span class="s2">&quot;./config.json&quot;</span>

<span class="n">fl_ctx</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;enable_fl&quot;</span><span class="p">:</span> <span class="n">enable_fl</span><span class="p">,</span>
    <span class="s2">&quot;server_mode&quot;</span><span class="p">:</span> <span class="n">server_mode</span><span class="p">,</span>
    <span class="s2">&quot;ms_role&quot;</span><span class="p">:</span> <span class="n">ms_role</span><span class="p">,</span>
    <span class="s2">&quot;server_num&quot;</span><span class="p">:</span> <span class="n">server_num</span><span class="p">,</span>
    <span class="s2">&quot;scheduler_ip&quot;</span><span class="p">:</span> <span class="n">scheduler_ip</span><span class="p">,</span>
    <span class="s2">&quot;scheduler_port&quot;</span><span class="p">:</span> <span class="n">scheduler_port</span><span class="p">,</span>
    <span class="s2">&quot;fl_server_port&quot;</span><span class="p">:</span> <span class="n">fl_server_port</span><span class="p">,</span>
    <span class="s2">&quot;fl_name&quot;</span><span class="p">:</span> <span class="n">fl_name</span><span class="p">,</span>
    <span class="s2">&quot;scheduler_manage_port&quot;</span><span class="p">:</span> <span class="n">scheduler_manage_port</span><span class="p">,</span>
    <span class="s2">&quot;config_file_path&quot;</span><span class="p">:</span> <span class="n">config_file_path</span>
<span class="p">}</span>
<span class="n">context</span><span class="o">.</span><span class="n">set_fl_context</span><span class="p">(</span><span class="o">**</span><span class="n">fl_ctx</span><span class="p">)</span>
<span class="o">...</span>

<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>In this example, the training task mode is set to <code class="docutils literal notranslate"><span class="pre">federated</span> <span class="pre">learning</span></code>, and the training process role is <code class="docutils literal notranslate"><span class="pre">Server</span></code>. In this task, <code class="docutils literal notranslate"><span class="pre">4</span></code> <code class="docutils literal notranslate"><span class="pre">Servers</span></code> need to be started to complete the cluster networking. The IP address of the cluster <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> is <code class="docutils literal notranslate"><span class="pre">192.168.216.124</span></code>, the cluster <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> port number is <code class="docutils literal notranslate"><span class="pre">6667</span></code>, the <code class="docutils literal notranslate"><span class="pre">HTTP</span> <span class="pre">service</span> <span class="pre">port</span> <span class="pre">number</span></code> of federated learning is <code class="docutils literal notranslate"><span class="pre">6668</span></code> (connected by the device), the task name is <code class="docutils literal notranslate"><span class="pre">LeNet</span></code>, and the cluster <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> management port number is <code class="docutils literal notranslate"><span class="pre">11202</span></code>.</p>
<blockquote>
<div><p>Some parameters are used by either <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> (for example, scheduler_manage_port) or <code class="docutils literal notranslate"><span class="pre">Server</span></code> (for example, fl_server_port). To facilitate deployment, transfer these parameters together to MindSpore. MindSpore reads different parameters based on process roles.
You are advised to import the parameter configuration through the Python <code class="docutils literal notranslate"><span class="pre">argparse</span></code> module:</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>

<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--server_mode&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;FEDERATED_LEARNING&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--ms_role&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;MS_SERVER&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--server_num&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--scheduler_ip&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;192.168.216.124&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--scheduler_port&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">6667</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--fl_server_port&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">6668</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--fl_name&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;LeNet&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--scheduler_manage_port&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">11202</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--config_file_path&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

<span class="n">args</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_known_args</span><span class="p">()</span>
<span class="n">server_mode</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">server_mode</span>
<span class="n">ms_role</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">ms_role</span>
<span class="n">server_num</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">server_num</span>
<span class="n">scheduler_ip</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">scheduler_ip</span>
<span class="n">scheduler_port</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">scheduler_port</span>
<span class="n">fl_server_port</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">fl_server_port</span>
<span class="n">fl_name</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">fl_name</span>
<span class="n">scheduler_manage_port</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">scheduler_manage_port</span>
<span class="n">config_file_path</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">config_file_path</span>
</pre></div>
</div>
<blockquote>
<div><p>Each Python script corresponds to a process. If multiple <code class="docutils literal notranslate"><span class="pre">Server</span></code> roles need to be deployed on different hosts, you can use shell commands and Python to quickly start multiple <code class="docutils literal notranslate"><span class="pre">Server</span></code> processes. You can refer to the <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/tests/st/fl/mobile">examples</a>.</p>
<p>Each <code class="docutils literal notranslate"><span class="pre">Server</span></code> process needs a unique identifier <code class="docutils literal notranslate"><span class="pre">MS_NODE_ID</span></code> which should be set by environment variable. In this tutorial, this environment variable has been set in the script <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/tests/st/fl/mobile/run_mobile_server.py">run_mobile_server.py</a>.</p>
</div></blockquote>
</div>
<div class="section" id="starting-a-cluster">
<h2>Starting a Cluster<a class="headerlink" href="#starting-a-cluster" title="Permalink to this headline">Â¶</a></h2>
<p>Start the cluster by referring to the <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/tests/st/fl/mobile">examples</a>. An example directory structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mobile/
âââ config.json
âââ finish_mobile.py
âââ run_mobile_sched.py
âââ run_mobile_server.py
âââ src
â   âââ model.py
âââ test_mobile_lenet.py
</pre></div>
</div>
<p>Descriptions of the documents:</p>
<ul class="simple">
<li><p>config.json: The config file, which is used to configure security, disaster recovery, etc.</p></li>
<li><p>finish_mobile.py: This script is used to stop the cluster.</p></li>
<li><p>run_mobile_sched.py: Launch scheduler.</p></li>
<li><p>run_mobile_server.py: Launch server.</p></li>
<li><p>model.py: The model.</p></li>
<li><p>test_mobile_lenet.py: Training script.</p></li>
</ul>
<ol>
<li><p>Start the <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">run_mobile_sched.py</span></code> is a Python script provided for you to start <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> and supports configuration modification through passing the <code class="docutils literal notranslate"><span class="pre">argparse</span></code> parameter. Run the following command to start the <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> of the federated learning task. The TCP port number is <code class="docutils literal notranslate"><span class="pre">6667</span></code>, the HTTP service port number is <code class="docutils literal notranslate"><span class="pre">6668</span></code>, the number of <code class="docutils literal notranslate"><span class="pre">Server</span></code> is <code class="docutils literal notranslate"><span class="pre">4</span></code>, and the management port number of the cluster <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> is <code class="docutils literal notranslate"><span class="pre">11202</span></code>:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python run_mobile_sched.py --scheduler_ip<span class="o">=</span><span class="m">192</span>.168.216.124 --scheduler_port<span class="o">=</span><span class="m">6667</span> --fl_server_port<span class="o">=</span><span class="m">6668</span> --server_num<span class="o">=</span><span class="m">4</span> --scheduler_manage_port<span class="o">=</span><span class="m">11202</span>
</pre></div>
</div>
</li>
<li><p>Start the <code class="docutils literal notranslate"><span class="pre">Servers</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">run_mobile_server.py</span></code> is a Python script provided for you to start multiple <code class="docutils literal notranslate"><span class="pre">Servers</span></code> and supports configuration modification through passing the <code class="docutils literal notranslate"><span class="pre">argparse</span></code> parameter. Run the following command to start the <code class="docutils literal notranslate"><span class="pre">Servers</span></code> of the federated learning task. The TCP port number is <code class="docutils literal notranslate"><span class="pre">6667</span></code>, the HTTP service port number is <code class="docutils literal notranslate"><span class="pre">6668</span></code>, the number of <code class="docutils literal notranslate"><span class="pre">Server</span></code> is <code class="docutils literal notranslate"><span class="pre">4</span></code>, and the number of devices required for the federated learning task is <code class="docutils literal notranslate"><span class="pre">8</span></code>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python run_mobile_server.py --scheduler_ip<span class="o">=</span><span class="m">192</span>.168.216.124 --scheduler_port<span class="o">=</span><span class="m">6667</span> --fl_server_port<span class="o">=</span><span class="m">6668</span> --server_num<span class="o">=</span><span class="m">4</span> --start_fl_job_threshold<span class="o">=</span><span class="m">8</span>
</pre></div>
</div>
<p>The preceding command is equivalent to starting four <code class="docutils literal notranslate"><span class="pre">Server</span></code> processes, of which the federated learning service port numbers are <code class="docutils literal notranslate"><span class="pre">6668</span></code>, <code class="docutils literal notranslate"><span class="pre">6669</span></code>, <code class="docutils literal notranslate"><span class="pre">6670</span></code>, and <code class="docutils literal notranslate"><span class="pre">6671</span></code>. For details, see <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/tests/st/fl/mobile/run_mobile_server.py">run_mobile_server.py</a>.</p>
<blockquote>
<div><p>If you only want to deploy <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> and <code class="docutils literal notranslate"><span class="pre">Server</span></code> in a standalone system, change the <code class="docutils literal notranslate"><span class="pre">scheduler_ip</span></code> to <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code>.</p>
</div></blockquote>
<p>To distribute the <code class="docutils literal notranslate"><span class="pre">Servers</span></code> on different physical nodes, you can use the <code class="docutils literal notranslate"><span class="pre">local_server_num</span></code> parameter to specify the number of <code class="docutils literal notranslate"><span class="pre">Server</span></code> processes to be executed on <strong>the current node</strong>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="c1">#Start three `Server` processes on node 1.</span>
python run_mobile_server.py --scheduler_ip<span class="o">=</span><span class="m">192</span>.168.216.124 --scheduler_port<span class="o">=</span><span class="m">6667</span> --fl_server_port<span class="o">=</span><span class="m">6668</span> --server_num<span class="o">=</span><span class="m">4</span> --start_fl_job_threshold<span class="o">=</span><span class="m">8</span> --local_server_num<span class="o">=</span><span class="m">3</span>
</pre></div>
</div>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="c1">#Start one `Server` process on node 2.</span>
python run_mobile_server.py --scheduler_ip<span class="o">=</span><span class="m">192</span>.168.216.124 --scheduler_port<span class="o">=</span><span class="m">6667</span> --fl_server_port<span class="o">=</span><span class="m">6668</span> --server_num<span class="o">=</span><span class="m">4</span> --start_fl_job_threshold<span class="o">=</span><span class="m">8</span> --local_server_num<span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
<p>The log is displayed as follows:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>Server started successfully.
</pre></div>
</div>
<p>If the preceding information is displayed, it indicates that the startup is successful.</p>
<blockquote>
<div><p>In the preceding commands for distributed deployment, all values of <code class="docutils literal notranslate"><span class="pre">server_num</span></code> are set to 4. This is because this parameter indicates the number of global <code class="docutils literal notranslate"><span class="pre">Servers</span></code> in the cluster and should not change with the number of physical nodes. <code class="docutils literal notranslate"><span class="pre">Servers</span></code> on different nodes do not need to be aware of their own IP addresses. The cluster consistency and node discovery are scheduled by <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>.</p>
</div></blockquote>
</li>
<li><p>Stop federated learning.</p>
<p>Currently, <code class="docutils literal notranslate"><span class="pre">finish_mobile.py</span></code> is used to stop the federated learning server. Run the following command to stop the federated learning cluster. The value of the <code class="docutils literal notranslate"><span class="pre">scheduler_port</span></code> parameter is the same as that passed when the server is started.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python finish_mobile.py --scheduler_port<span class="o">=</span><span class="m">6667</span>
</pre></div>
</div>
<p>The result is as follows:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>killed <span class="nv">$PID1</span>
killed <span class="nv">$PID2</span>
killed <span class="nv">$PID3</span>
killed <span class="nv">$PID4</span>
killed <span class="nv">$PID5</span>
killed <span class="nv">$PID6</span>
killed <span class="nv">$PID7</span>
killed <span class="nv">$PID8</span>
</pre></div>
</div>
<p>The services are stopped successfully.</p>
</li>
</ol>
</div>
<div class="section" id="auto-scaling">
<h2>Auto Scaling<a class="headerlink" href="#auto-scaling" title="Permalink to this headline">Â¶</a></h2>
<p>The MindSpore federated learning framework supports auto scaling of <code class="docutils literal notranslate"><span class="pre">Server</span></code> and provides the <code class="docutils literal notranslate"><span class="pre">RESTful</span></code> service through the <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code> management port. In this way, you can dynamically schedule hardware resources without interrupting training tasks. Currently, MindSpore supports only horizontal scaling (scale-out or scale-in) and does not support vertical scaling (scale-up or scale-down). In the auto scaling scenario, the number of <code class="docutils literal notranslate"><span class="pre">Server</span></code> processes either increases or decreases.</p>
<p>The following describes how to control cluster scale-in and scale-out using the native RESTful APIs.</p>
<ol>
<li><p>Scale-out</p>
<p>After the cluster is started, send a scale-out request to <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>. Use the <code class="docutils literal notranslate"><span class="pre">curl</span></code> instruction to construct a <code class="docutils literal notranslate"><span class="pre">RESTful</span></code> scale-out request, indicating that two <code class="docutils literal notranslate"><span class="pre">Server</span></code> nodes need to be added to the cluster.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>curl -i -X POST <span class="se">\</span>
-H <span class="s2">&quot;Content-Type:application/json&quot;</span> <span class="se">\</span>
-d <span class="se">\</span>
<span class="s1">&#39;{</span>
<span class="s1">&quot;worker_num&quot;:0,</span>
<span class="s1">&quot;server_num&quot;:2</span>
<span class="s1">}&#39;</span> <span class="se">\</span>
<span class="s1">&#39;http://192.168.216.124:11202/scaleout&#39;</span>
</pre></div>
</div>
<p>Start <code class="docutils literal notranslate"><span class="pre">2</span></code> new <code class="docutils literal notranslate"><span class="pre">Server</span></code> processes and add up the values of <code class="docutils literal notranslate"><span class="pre">server_num</span></code> to ensure that the global networking information is correct. After the scale-out, the value of <code class="docutils literal notranslate"><span class="pre">server_num</span></code> should be <code class="docutils literal notranslate"><span class="pre">6</span></code>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python run_mobile_server.py --scheduler_ip<span class="o">=</span><span class="m">192</span>.168.216.124 --scheduler_port<span class="o">=</span><span class="m">6667</span> --fl_server_port<span class="o">=</span><span class="m">6672</span> --server_num<span class="o">=</span><span class="m">6</span> --start_fl_job_threshold<span class="o">=</span><span class="m">8</span> --local_server_num<span class="o">=</span><span class="m">2</span>
</pre></div>
</div>
<p>This command is used to start two <code class="docutils literal notranslate"><span class="pre">Server</span></code> nodes. The port numbers of the federated learning services are <code class="docutils literal notranslate"><span class="pre">6672</span></code> and <code class="docutils literal notranslate"><span class="pre">6673</span></code>, and the total number of <code class="docutils literal notranslate"><span class="pre">Servers</span></code> is <code class="docutils literal notranslate"><span class="pre">6</span></code>.</p>
</li>
<li><p>Scale-in</p>
<p>After the cluster is started, send a scale-in request to <code class="docutils literal notranslate"><span class="pre">Scheduler</span></code>. Obtain the node information to perform the scale-in operation on specific nodes.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>curl -i -X GET <span class="se">\</span>
<span class="s1">&#39;http://192.168.216.124:11202/nodes&#39;</span>
</pre></div>
</div>
<p>The result in the <code class="docutils literal notranslate"><span class="pre">json</span></code> format is returned:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;message&quot;</span><span class="p">:</span> <span class="s2">&quot;Get nodes info successful.&quot;</span><span class="p">,</span>
    <span class="nt">&quot;node_ids&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nt">&quot;node_id&quot;</span><span class="p">:</span> <span class="s2">&quot;40d56ffe-f8d1-4960-85fa-fdf88820402a&quot;</span><span class="p">,</span>
            <span class="nt">&quot;rank_id&quot;</span><span class="p">:</span> <span class="s2">&quot;3&quot;</span><span class="p">,</span>
            <span class="nt">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;SERVER&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="nt">&quot;node_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1ba06348-f2e2-4ad2-be83-0d41fcb53228&quot;</span><span class="p">,</span>
            <span class="nt">&quot;rank_id&quot;</span><span class="p">:</span> <span class="s2">&quot;2&quot;</span><span class="p">,</span>
            <span class="nt">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;SERVER&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="nt">&quot;node_id&quot;</span><span class="p">:</span> <span class="s2">&quot;997967bb-c1ab-4916-8697-dcfaaf0354e5&quot;</span><span class="p">,</span>
            <span class="nt">&quot;rank_id&quot;</span><span class="p">:</span> <span class="s2">&quot;1&quot;</span><span class="p">,</span>
            <span class="nt">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;SERVER&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="nt">&quot;node_id&quot;</span><span class="p">:</span> <span class="s2">&quot;4b8d5bdf-eafd-4f5c-8cae-79008f19298a&quot;</span><span class="p">,</span>
            <span class="nt">&quot;rank_id&quot;</span><span class="p">:</span> <span class="s2">&quot;0&quot;</span><span class="p">,</span>
            <span class="nt">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;SERVER&quot;</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Select <code class="docutils literal notranslate"><span class="pre">Rank3</span></code> and <code class="docutils literal notranslate"><span class="pre">Rank2</span></code> for scale-in.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>curl -i -X POST <span class="se">\</span>
-H <span class="s2">&quot;Content-Type:application/json&quot;</span> <span class="se">\</span>
-d <span class="se">\</span>
<span class="s1">&#39;{</span>
<span class="s1">&quot;node_ids&quot;: [&quot;40d56ffe-f8d1-4960-85fa-fdf88820402a&quot;, &quot;1ba06348-f2e2-4ad2-be83-0d41fcb53228&quot;]</span>
<span class="s1">}&#39;</span> <span class="se">\</span>
<span class="s1">&#39;http://10.113.216.124:11202/scalein&#39;</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>After the cluster scale-out or scale-in is successful, the training task is automatically restored. No manual intervention is required.</p></li>
<li><p>You can use a cluster management tool (such as Kubernetes) to create or release <code class="docutils literal notranslate"><span class="pre">Server</span></code> resources.</p></li>
<li><p>After scale-in, the process scaled in will not exit. You need to use the cluster management tool (such as Kubernetes) or command <code class="docutils literal notranslate"><span class="pre">kill</span> <span class="pre">-15</span> <span class="pre">$PID</span></code> to control the process to exit.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="disaster-recovery">
<h2>Disaster Recovery<a class="headerlink" href="#disaster-recovery" title="Permalink to this headline">Â¶</a></h2>
<p>After a node in the MindSpore federated learning cluster goes offline, you can keep the cluster online without exiting the training task. After the node is restarted, you can resume the training task. Currently, MindSpore supports disaster recovery for <code class="docutils literal notranslate"><span class="pre">Server</span></code> nodes (except Server 0).</p>
<p>To enable disaster recovery, the fields below should be added to the config.json set by config_file_path:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;recovery&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;storage_type&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="nt">&quot;storge_file_path&quot;</span><span class="p">:</span> <span class="s2">&quot;config.json&quot;</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>recovery: If this field is set, the disaster recovery feature is enabled.</p></li>
<li><p>storage_type: Persistent storage type. Only <code class="docutils literal notranslate"><span class="pre">1</span></code> is supported currently which represents file storage.</p></li>
<li><p>storage_file_path: The recovery file path.</p></li>
</ul>
<p>The node restart command is similar to the scale-out command. After the node is manually brought offline, run the following command:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>python run_mobile_server.py --scheduler_ip<span class="o">=</span><span class="m">192</span>.168.216.124 --scheduler_port<span class="o">=</span><span class="m">6667</span> --fl_server_port<span class="o">=</span><span class="m">6673</span> --server_num<span class="o">=</span><span class="m">6</span> --start_fl_job_threshold<span class="o">=</span><span class="m">8</span> --local_server_num<span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
<p>This command indicates that the <code class="docutils literal notranslate"><span class="pre">Server</span></code> is restarted. The federated learning service port number is <code class="docutils literal notranslate"><span class="pre">6673</span></code>.</p>
<blockquote>
<div><p>MindSpore does not support disaster recovery after the auto scaling command is successfully delivered and before the scaling service is complete.</p>
<p>After recovery, the restarted nodeâs <code class="docutils literal notranslate"><span class="pre">MS_NODE_ID</span></code> variable should be the same as the one which exited in exception to ensure the networking recovery.</p>
</div></blockquote>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="deploy_federated_client.html" class="btn btn-neutral float-right" title="On-Device Deployment" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="federated_install.html" class="btn btn-neutral float-left" title="Obtaining MindSpore Federated" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>