

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore_federated.trainer._fl_manager &mdash; MindSpore master 文档</title>
  

  
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
   
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/translations.js"></script>
        
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../federated_install.html">获取MindSpore Federated</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy_federated_server.html">横向联邦云侧部署</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy_federated_client.html">横向联邦端侧部署</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../deploy_vfl.html">纵向联邦部署</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">横向应用实践</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../image_classfication_dataset_process.html">联邦学习图像分类数据集处理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../image_classification_application.html">实现一个端云联邦的图像分类应用(x86)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sentiment_classification_application.html">实现一个端云情感分类应用(Android)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../image_classification_application_in_cross_silo.html">实现一个云云联邦的图像分类应用(x86)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../object_detection_application_in_cross_silo.html">实现一个云云联邦的目标检测应用(x86)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">纵向应用实践</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../data_join.html">纵向联邦学习数据接入</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../split_wnd_application.html">纵向联邦学习模型训练 - Wide&amp;Deep推荐应用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../split_pangu_alpha_application.html">纵向联邦学习模型训练 - 盘古α大模型跨域训练</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">安全和隐私</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../local_differential_privacy_training_noise.html">横向联邦-局部差分隐私加噪训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../local_differential_privacy_training_signds.html">横向联邦-局部差分隐私SignDS训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../local_differential_privacy_eval_laplace.html">横向联邦-局部差分隐私推理结果保护</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pairwise_encryption_training.html">横向联邦-安全聚合训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../private_set_intersection.html">纵向联邦-隐私集合求交</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../secure_vertical_federated_learning_with_EmbeddingDP.html">纵向联邦-基于信息混淆的特征保护</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../secure_vertical_federated_learning_with_TEE.html">纵向联邦-基于可信执行环境的特征保护</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../secure_vertical_federated_learning_with_DP.html">纵向联邦-基于差分隐私的标签保护</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">通信压缩</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../communication_compression.html">端云联邦学习通信压缩</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../vfl_communication_compress.html">纵向联邦学习通信压缩</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">横向联邦API参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../horizontal_server.html">联邦服务器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cross_device.html">端侧客户端</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../horizontal/cross_silo.html">云侧客户端</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">纵向联邦API参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Data_Join.html">数据求交</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../vertical/vertical_communicator.html">纵向联邦学习通信器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../vertical_federated_trainer.html">纵向联邦训练器</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">模块代码</a> &raquo;</li>
      <li>mindspore_federated.trainer._fl_manager</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>mindspore_federated.trainer._fl_manager 源代码</h1><div class="highlight"><pre>
<span></span><span class="c1"># pylint: disable=missing-docstring</span>
<span class="c1"># Copyright 2023 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;FederatedLearningManager related class and functions.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore.ops</span> <span class="k">as</span> <span class="nn">ops</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">mindspore.nn</span> <span class="kn">import</span> <span class="n">Cell</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">load_param_into_net</span>
<span class="kn">from</span> <span class="nn">mindspore.communication.management</span> <span class="kn">import</span> <span class="n">init</span><span class="p">,</span> <span class="n">get_rank</span>
<span class="kn">from</span> <span class="nn">mindspore.common.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">mindspore.common.parameter</span> <span class="kn">import</span> <span class="n">Parameter</span><span class="p">,</span> <span class="n">ParameterTuple</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">Callback</span>
<span class="kn">from</span> <span class="nn">mindspore_federated.common</span> <span class="kn">import</span> <span class="n">_checkparam</span> <span class="k">as</span> <span class="n">validator</span>
<span class="kn">from</span> <span class="nn">mindspore_federated._mindspore_federated</span> <span class="kn">import</span> <span class="n">Federated_</span><span class="p">,</span> <span class="n">FLContext</span>
<span class="kn">from</span> <span class="nn">mindspore_federated</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>

<span class="kn">from</span> <span class="nn">..startup.ssl_config</span> <span class="kn">import</span> <span class="n">init_ssl_config</span>
<span class="kn">from</span> <span class="nn">..startup.yaml_config</span> <span class="kn">import</span> <span class="n">load_yaml_config</span>
<span class="kn">from</span> <span class="nn">..common</span> <span class="kn">import</span> <span class="n">_fl_context</span><span class="p">,</span> <span class="n">check_type</span>

<span class="n">TRAIN_BEGIN_STEP_NUM</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">TRAIN_END_STEP_NUM</span> <span class="o">=</span> <span class="mi">0</span>


<span class="k">class</span> <span class="nc">_StartFLJob</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    StartFLJob for Federated Learning Worker.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_size</span> <span class="o">=</span> <span class="n">data_size</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Federated_</span><span class="o">.</span><span class="n">start_fl_job</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_size</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_UpdateAndGetModel</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update and Get Model for Federated Learning Worker.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_UpdateAndGetModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">weights</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Federated_</span><span class="o">.</span><span class="n">update_and_get_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_ExchangeKeys</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Exchange Keys for Stable PW Encrypt.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">Federated_</span><span class="o">.</span><span class="n">exchange_keys</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_GetKeys</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get Keys for Stable PW Encrypt.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">Federated_</span><span class="o">.</span><span class="n">get_keys</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_PullWeight</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pull Weight for Federated Learning Worker.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pull_weight_params</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pull_weight_params</span> <span class="o">=</span> <span class="n">pull_weight_params</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Federated_</span><span class="o">.</span><span class="n">pull_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pull_weight_params</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_PushWeight</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Push Weight for Federated Learning Worker.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">weights</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Federated_</span><span class="o">.</span><span class="n">push_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PushMetrics</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Push Metrics for Federated Learning Worker.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Federated_</span><span class="o">.</span><span class="n">push_metrics</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BroadcastNet</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct of weight input for Broadcast.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_broadcast</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Broadcast</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_broadcast</span><span class="p">((</span><span class="n">input_x</span><span class="p">,))</span>


<span class="k">def</span> <span class="nf">_get_fl_param_names</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">fl_param_names</span><span class="p">,</span> <span class="n">requires_aggr</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">sub_cell</span> <span class="ow">in</span> <span class="n">network</span><span class="o">.</span><span class="n">cells</span><span class="p">():</span>
        <span class="n">fl_param_names</span> <span class="o">=</span> <span class="n">_get_fl_param_names</span><span class="p">(</span><span class="n">sub_cell</span><span class="p">,</span> <span class="n">fl_param_names</span><span class="p">,</span> <span class="n">requires_aggr</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sub_cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">sub_cell</span><span class="o">.</span><span class="n">parameters</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">requires_aggr</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">k</span><span class="o">.</span><span class="n">requires_aggr</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">fl_param_names</span><span class="p">:</span>
                    <span class="n">fl_param_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fl_param_names</span>


<span class="k">def</span> <span class="nf">_get_lr</span><span class="p">(</span><span class="n">network</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">sub_cell</span> <span class="ow">in</span> <span class="n">network</span><span class="o">.</span><span class="n">cells</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sub_cell</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">sub_cell</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">_get_lr</span><span class="p">(</span><span class="n">sub_cell</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">lr</span>
    <span class="k">return</span> <span class="kc">None</span>


<div class="viewcode-block" id="FederatedLearningManager"><a class="viewcode-back" href="../../../horizontal/cross_silo.html#mindspore_federated.FederatedLearningManager">[文档]</a><span class="k">class</span> <span class="nc">FederatedLearningManager</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Manage Federated Learning during training.</span>

<span class="sd">    Args:</span>
<span class="sd">        yaml_config (str): The yaml file path. For more detail see `federated_server_yaml &lt;https://gitee.com/mindspore/federated/blob/master/docs/api/api_python_en/horizontal/federated_server_yaml.md&gt;`_.</span>
<span class="sd">        model (nn.Cell): A model for Federated Training.</span>
<span class="sd">        sync_frequency (int): Synchronization frequency of parameters in Federated Learning. Indicating the number</span>
<span class="sd">                              of steps between two adjacent synchronization operations when `dataset_sink_mode` is</span>
<span class="sd">                              set to False. If `sync_type` is set to &quot;fixed&quot;, it serves as a fixed number of steps.</span>
<span class="sd">                              If `sync_type` is set to &quot;adaptive&quot;, it serves as the initial value of the adaptive</span>
<span class="sd">                              synchronization frequency. Note that its function is changed in dataset sink mode.</span>
<span class="sd">                              If `dataset_sink_mode` is set to True and `sink_size` is set to a non-positive value,</span>
<span class="sd">                              the synchronization operation will execute once every `sync_frequency` epochs. If</span>
<span class="sd">                              `dataset_sink_mode` is set to True and `sink_size` is set to a positive value, the</span>
<span class="sd">                              synchronization operation will execute once every `sink_size` * `sync_frequency` steps.</span>
<span class="sd">                              The `dataset_sink_mode` and the `sink_size` is set by users in `mindspore.train.Model` .</span>
<span class="sd">        http_server_address (str): The http server address used for communicating. Default: ``&quot;&quot;``.</span>
<span class="sd">        data_size (int): The data size to be reported to the worker. Default: ``1``.</span>
<span class="sd">        sync_type (str): The synchronization type of parameter in Federated Learning.</span>
<span class="sd">                         Supports ``&quot;fixed&quot;``, ``&quot;adaptive&quot;``. Default: ``&quot;fixed&quot;``.</span>

<span class="sd">                         - ``fixed``: The frequency of parameter synchronization is fixed.</span>

<span class="sd">                         - ``adaptive``: The frequency of parameter synchronization changes adaptively.</span>

<span class="sd">        run_distribute (bool): Whether to open distribute training. Default: ``False``.</span>
<span class="sd">        ssl_config (Union(None, SSLConfig)): Config of ssl. Default: ``None``.</span>
<span class="sd">        min_consistent_rate (float): Minimum consistency ratio threshold. The greater the value, the more</span>
<span class="sd">                                     difficult it is to improve the synchronization frequency.</span>
<span class="sd">                                     Value range: greater than or equal to 0.0. Default: ``1.1``.</span>
<span class="sd">        min_consistent_rate_at_round (int): The number of rounds of the minimum consistency ratio threshold.</span>
<span class="sd">                                            The greater the value, the more difficult it is to improve the</span>
<span class="sd">                                            synchronization frequency.</span>
<span class="sd">                                            Value range: greater than or equal to 0. Default: ``0``.</span>
<span class="sd">        ema_alpha (float): Gradient consistency smoothing coefficient. The smaller the value, the more the</span>
<span class="sd">                           frequency will be judged according to the gradient bifurcation of the current round</span>
<span class="sd">                           more. Otherwise it will be judged according to the historical gradient bifurcation</span>
<span class="sd">                           more.</span>
<span class="sd">                           Value range: (0.0, 1.0). Default: ``0.5``.</span>
<span class="sd">        observation_window_size (int): The number of rounds in the observation time window. The greater the</span>
<span class="sd">                                       value, the more difficult it is to reduce the synchronization frequency.</span>
<span class="sd">                                       Value range: greater than 0. Default: ``5``.</span>
<span class="sd">        frequency_increase_ratio (int): Frequency increase amplitude. The greater the value, the greater the</span>
<span class="sd">                                        frequency increase amplitude.</span>
<span class="sd">                                        Value range: greater than 0. Default: ``2``.</span>
<span class="sd">        unchanged_round (int): The number of rounds whose frequency does not change. The frequency is unchanged</span>
<span class="sd">                               before unchanged_round rounds.</span>
<span class="sd">                               Value range: greater than or equal to 0. Default: ``0``.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore_federated import FederatedLearningManager</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import nn, Model</span>
<span class="sd">        &gt;&gt;&gt; from network.lenet import LeNet5, create_dataset_from_folder</span>
<span class="sd">        &gt;&gt;&gt; network = LeNet5(62, 3)</span>
<span class="sd">        &gt;&gt;&gt; federated_learning_manager = FederatedLearningManager(</span>
<span class="sd">        ...     yaml_config=&quot;default_yaml_config.yaml&quot;,</span>
<span class="sd">        ...     model=network,</span>
<span class="sd">        ...     sync_frequency=100,</span>
<span class="sd">        ...     http_server_address=&quot;127.0.0.1:10086&quot;,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=&#39;mean&#39;)</span>
<span class="sd">        &gt;&gt;&gt; net_opt = nn.Momentum(network.trainable_params(), 0.001, 0.9)</span>
<span class="sd">        &gt;&gt;&gt; model = Model(network, net_loss, net_opt)</span>
<span class="sd">        &gt;&gt;&gt; dataset = create_dataset_from_folder(&quot;0/train/&quot;, 32, 16, 1)</span>
<span class="sd">        &gt;&gt;&gt; model.train(100, dataset, callbacks=[federated_learning_manager], dataset_sink_mode=False)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">yaml_config</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">sync_frequency</span><span class="p">,</span> <span class="n">http_server_address</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">data_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sync_type</span><span class="o">=</span><span class="s1">&#39;fixed&#39;</span><span class="p">,</span>
                 <span class="n">run_distribute</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ssl_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FederatedLearningManager</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">check_type</span><span class="o">.</span><span class="n">check_str</span><span class="p">(</span><span class="s2">&quot;yaml_config&quot;</span><span class="p">,</span> <span class="n">yaml_config</span><span class="p">)</span>
        <span class="n">init_ssl_config</span><span class="p">(</span><span class="n">ssl_config</span><span class="p">)</span>
        <span class="n">load_yaml_config</span><span class="p">(</span><span class="n">yaml_config</span><span class="p">,</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">ROLE_OF_SERVER</span><span class="p">)</span>

        <span class="n">ctx</span> <span class="o">=</span> <span class="n">FLContext</span><span class="o">.</span><span class="n">get_instance</span><span class="p">()</span>
        <span class="n">server_mode</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">server_mode</span><span class="p">()</span>
        <span class="n">aggregation_type</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">aggregation_type</span><span class="p">()</span>
        <span class="n">encrypt_type</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">encrypt_type</span><span class="p">()</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">set_http_server_address</span><span class="p">(</span><span class="n">http_server_address</span><span class="p">)</span>

        <span class="n">initial_model</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="n">param_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">initial_model</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_data</span>
        <span class="n">Federated_</span><span class="o">.</span><span class="n">init_federated_worker</span><span class="p">(</span><span class="n">initial_model</span><span class="p">)</span>

        <span class="n">validator</span><span class="o">.</span><span class="n">check_isinstance</span><span class="p">(</span><span class="s1">&#39;model&#39;</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="n">sync_frequency</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_string</span><span class="p">(</span><span class="n">sync_type</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;fixed&quot;</span><span class="p">,</span> <span class="s2">&quot;adaptive&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_server_mode</span> <span class="o">=</span> <span class="n">server_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span> <span class="o">=</span> <span class="n">sync_frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_next_begin_sync_iter</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_next_end_sync_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data_size</span> <span class="o">=</span> <span class="n">data_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sync_type</span> <span class="o">=</span> <span class="n">sync_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_run_distribute</span> <span class="o">=</span> <span class="n">run_distribute</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_distribute</span><span class="p">:</span>
            <span class="n">init</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_broadcast</span> <span class="o">=</span> <span class="n">BroadcastNet</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rank id is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_rank_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation_type</span> <span class="o">=</span> <span class="n">aggregation_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_prefix</span> <span class="o">=</span> <span class="s2">&quot;global_weights&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">SUPPORT_AGG_TYPES</span> <span class="ow">and</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">_server_mode</span> <span class="o">==</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">SERVER_MODE_CLOUD</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;aggregation_type must be in </span><span class="si">{}</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">_fl_context</span><span class="o">.</span><span class="n">SUPPORT_AGG_TYPES</span><span class="p">,</span>
                                                                     <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation_type</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation_type</span> <span class="ow">in</span> <span class="p">(</span><span class="n">_fl_context</span><span class="o">.</span><span class="n">FEDPROX</span><span class="p">,</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">FEDNOVA</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_global_weights</span> <span class="o">=</span> <span class="n">ParameterTuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">())</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_prefix</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_weights</span><span class="p">:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">insert_param_to_cell</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_encrypt_type</span> <span class="o">=</span> <span class="n">encrypt_type</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encrypt_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">SUPPORT_ENC_TYPES_CLOUD</span> <span class="ow">and</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">_server_mode</span> <span class="o">==</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">SERVER_MODE_CLOUD</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;encrypt_mode must be in </span><span class="si">{}</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">_fl_context</span><span class="o">.</span><span class="n">SUPPORT_ENC_TYPES_CLOUD</span><span class="p">,</span>
                                                                 <span class="bp">self</span><span class="o">.</span><span class="n">_encrypt_type</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_adaptive_sync</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_as_set_init_state</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_as_wrap_cell</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Step number needs to run per iteration </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_next_end_sync_iter</span><span class="si">}</span><span class="s2">,&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;server mode </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_server_mode</span><span class="si">}</span><span class="s2">, aggregation type </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_aggregation_type</span><span class="si">}</span><span class="s2">,&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;encrypt type </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_encrypt_type</span><span class="si">}</span><span class="s2">, http server address </span><span class="si">{</span><span class="n">http_server_address</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fl_param_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_fl_param_names</span> <span class="o">=</span> <span class="n">_get_fl_param_names</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fl_param_names</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fl_param_names</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fl_param_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">_</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_local_control_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_control_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_scaffold_prefix</span> <span class="o">=</span> <span class="s2">&quot;control.&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_scaffold</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fl_param_names</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_last_params</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_local_control_params</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_global_control_params</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">Federated_</span><span class="o">.</span><span class="n">stop_federated_worker</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_is_adaptive_sync</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Determine whether adaptive frequency synchronization is required.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_type</span> <span class="o">==</span> <span class="s2">&quot;adaptive&quot;</span>

    <span class="k">def</span> <span class="nf">_is_scaffold</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Determine whether scaffold is required.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation_type</span> <span class="o">==</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">SCAFFOLD</span>

    <span class="k">def</span> <span class="nf">_is_fednova</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Determine whether FedNova is required.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation_type</span> <span class="o">==</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">FEDNOVA</span>

    <span class="k">def</span> <span class="nf">_as_set_init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setting the initial state for adaptive synchronization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span> <span class="o">=</span> <span class="s2">&quot;as_abs_grad.&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_min_consistent_rate</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;min_consistent_rate&quot;</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_min_consistent_rate</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_min_consistent_rate_at_round</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;min_consistent_rate_at_round&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_min_consistent_rate_at_round</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ema_alpha</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ema_alpha&quot;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_float_range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ema_alpha</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">validator</span><span class="o">.</span><span class="n">INC_NEITHER</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_observation_window_size</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;observation_window_size&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_observation_window_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_increase_ratio</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;frequency_increase_ratio&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_positive_int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_frequency_increase_ratio</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_unchanged_round</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;unchanged_round&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_non_negative_int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_unchanged_round</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_round_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_param</span> <span class="o">=</span> <span class="p">{</span><span class="n">_</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">_</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()</span>
                            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_</span><span class="o">.</span><span class="n">name</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_size</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grads_ema</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_abs_grads_ema</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_model_size</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_grads_ema</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_abs_grads_ema</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_model_size</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_as_wrap_cell</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Wrap Cell for adaptive synchronization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">param_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="n">new_param</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">new_param</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span> <span class="o">+</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span>
            <span class="n">param_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_param</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">param_list</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">insert_param_to_cell</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_as_set_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the absolute value of the gradient for adaptive synchronization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">abs_grads</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
                <span class="n">abs_grads</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span> <span class="o">+</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_param</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">Parameter</span><span class="p">(</span><span class="n">abs_grads</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">_as_analyze_gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Analysis of relevant statistics based on gradient for adaptive synchronization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">FLContext</span><span class="o">.</span><span class="n">get_instance</span><span class="p">()</span>
        <span class="n">worker_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">start_fl_job_threshold</span><span class="p">()</span> <span class="o">*</span> <span class="n">ctx</span><span class="o">.</span><span class="n">update_model_ratio</span><span class="p">())</span>
        <span class="n">ema_alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ema_alpha</span>
        <span class="n">consistent_rate_sum</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">abs_grads</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
                <span class="n">abs_grads</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">*</span> <span class="n">worker_num</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grads</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_param</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">])</span> <span class="o">*</span> <span class="n">worker_num</span>
        <span class="k">for</span> <span class="n">last_p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_param</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_grads_ema</span><span class="p">[</span><span class="n">last_p</span><span class="p">]</span> <span class="o">=</span> <span class="n">ema_alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grads_ema</span><span class="p">[</span><span class="n">last_p</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ema_alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="n">last_p</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_abs_grads_ema</span><span class="p">[</span><span class="n">last_p</span><span class="p">]</span> <span class="o">=</span> <span class="n">ema_alpha</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_abs_grads_ema</span><span class="p">[</span><span class="n">last_p</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ema_alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">abs_grads</span><span class="p">[</span><span class="n">last_p</span><span class="p">]</span>
            <span class="n">divide_base</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_abs_grads_ema</span><span class="p">[</span><span class="n">last_p</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
                                   <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_abs_grads_ema</span><span class="p">[</span><span class="n">last_p</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_abs_grads_ema</span><span class="p">[</span><span class="n">last_p</span><span class="p">])</span>
            <span class="n">layer_consistent_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_grads_ema</span><span class="p">[</span><span class="n">last_p</span><span class="p">])</span> <span class="o">/</span> <span class="n">divide_base</span>
            <span class="n">consistent_rate_sum</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">layer_consistent_rate</span><span class="p">)</span>

        <span class="n">consistent_rate</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">consistent_rate_sum</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model_size</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_min_consistent_rate</span> <span class="o">&gt;</span> <span class="n">consistent_rate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_consistent_rate</span> <span class="o">=</span> <span class="n">consistent_rate</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_consistent_rate_at_round</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round_id</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round_id</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_min_consistent_rate_at_round</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_observation_window_size</span> <span class="ow">and</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round_id</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_unchanged_round</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_increase_ratio</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> \
                                   <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_increase_ratio</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_consistent_rate</span> <span class="o">=</span> <span class="mf">1.1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_min_consistent_rate_at_round</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_round_id</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_observation_window_size</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_increase_ratio</span>

            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_grads_ema</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_abs_grads_ema</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_as_set_last_param</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the value of last parameters for adaptive synchronization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_last_param</span> <span class="o">=</span> <span class="p">{</span><span class="n">_</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">_</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()</span>
                            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_as_prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">_</span><span class="o">.</span><span class="n">name</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">_start_pull_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Pull weight from server in hybrid training mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Try to pull weights. Local step number: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">))</span>
        <span class="c1"># The worker has to train self._sync_frequency standalone iterations before it communicates with server.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span> <span class="o">!=</span> <span class="n">TRAIN_BEGIN_STEP_NUM</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">pull_weight_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="n">pull_weight_params</span> <span class="o">=</span> <span class="n">_get_fl_param_names</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="n">pull_weight_params</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">pull_weight_params</span><span class="p">:</span>
            <span class="n">pull_weight_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">_</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()]</span>
        <span class="n">weight_infos</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">pull_weight_params</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">param_np</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">param_np</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">weight_infos</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">param_np</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">param_np</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">pull_weight</span> <span class="o">=</span> <span class="n">_PullWeight</span><span class="p">(</span><span class="n">pull_weight_params</span><span class="p">)</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">pull_weight</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">weights</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Weights from pulling weight is empty!&quot;</span><span class="p">)</span>
        <span class="n">parameter_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">weights</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">weight_infos</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">weight_infos</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="n">param_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">parameter_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">param_data</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>
        <span class="n">load_param_into_net</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="n">parameter_dict</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_model_with_distribute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">weight_infos</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update model with distributed training mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rank_id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">update_and_get_model</span> <span class="o">=</span> <span class="n">_UpdateAndGetModel</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
            <span class="n">feature_map</span> <span class="o">=</span> <span class="n">update_and_get_model</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">feature_map</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Feature map from getting model is empty!&quot;</span><span class="p">)</span>

            <span class="n">parameter_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">weight_info</span> <span class="ow">in</span> <span class="n">weight_infos</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">feature_map</span><span class="p">[</span><span class="n">key</span><span class="p">]:</span>
                    <span class="k">continue</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">feature_map</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">weight_info</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">weight_info</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">param_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">tensor</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">param_data</span><span class="p">)</span>
                <span class="n">parameter_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_broadcast</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
            <span class="n">load_param_into_net</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="n">parameter_dict</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">parameter_dict</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">weight_info</span> <span class="ow">in</span> <span class="n">weight_infos</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">weight_info</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">weight_info</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">param_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">received_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_broadcast</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">param_data</span><span class="p">))</span>
                <span class="n">parameter_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">received_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>
            <span class="n">load_param_into_net</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="n">parameter_dict</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">weight_infos</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update and get model without distributed training mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">update_and_get_model</span> <span class="o">=</span> <span class="n">_UpdateAndGetModel</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">feature_map</span> <span class="o">=</span> <span class="n">update_and_get_model</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">feature_map</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Feature map from getting model is empty!&quot;</span><span class="p">)</span>

        <span class="n">parameter_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">parameter_dict_global</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">feature_map</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">weight_infos</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">weight_infos</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
            <span class="n">param_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">parameter_dict</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">param_data</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="n">key</span><span class="p">)</span>
            <span class="n">parameter_dict_global</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_prefix</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">key</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">Parameter</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">param_data</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_prefix</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">key</span><span class="p">)</span>
        <span class="n">load_param_into_net</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="n">parameter_dict</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_aggregation_type</span> <span class="ow">in</span> <span class="p">(</span><span class="n">_fl_context</span><span class="o">.</span><span class="n">FEDPROX</span><span class="p">,</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">FEDNOVA</span><span class="p">):</span>
            <span class="n">load_param_into_net</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="n">parameter_dict_global</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_start_push_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Push weight to server in hybrid training mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Try to push weights. Local step number: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span> <span class="o">!=</span> <span class="n">TRAIN_END_STEP_NUM</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">push_weight_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="n">push_weight_params</span> <span class="o">=</span> <span class="n">_get_fl_param_names</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="n">push_weight_params</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">push_weight_params</span><span class="p">:</span>
            <span class="n">push_weight_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">_</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">()]</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">push_weight_params</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="n">weights</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="n">push_weight</span> <span class="o">=</span> <span class="n">_PushWeight</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">push_weight</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_scaffold_set_global_control_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">flattened_control_params</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_control_params</span><span class="p">:</span>
            <span class="n">control_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scaffold_prefix</span> <span class="o">+</span> <span class="n">name</span>
            <span class="k">if</span> <span class="n">control_name</span> <span class="ow">in</span> <span class="n">flattened_control_params</span><span class="p">:</span>
                <span class="n">global_control_param</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">flattened_control_params</span><span class="p">[</span><span class="n">control_name</span><span class="p">])</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_control_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_global_control_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">global_control_param</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">{}</span><span class="s2">&#39; is not in control parameters sent by server&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">control_name</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_scaffold_update_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Using control parameters to update parameters every step.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fl_param_names</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_control_params</span><span class="p">:</span>
                    <span class="n">global_control_param</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_control_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">{}</span><span class="s2">&#39; is not in global_control_params&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_control_params</span><span class="p">:</span>
                    <span class="n">local_control_param</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_control_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">{}</span><span class="s2">&#39; is not in local_control_params&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
                <span class="n">control_params</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">global_control_param</span> <span class="o">-</span> <span class="n">local_control_param</span><span class="p">)</span>
                <span class="n">param</span><span class="o">.</span><span class="n">set_data</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">-</span> <span class="n">control_params</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_scaffold_get_control_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get updated control parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">control_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fl_param_names</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_control_params</span><span class="p">:</span>
                    <span class="n">local_control_param</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_local_control_params</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">{}</span><span class="s2">&#39; is not in local_control_params&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_control_params</span><span class="p">:</span>
                    <span class="n">global_control_param</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_control_params</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">{}</span><span class="s2">&#39; is not in global_control_params&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
                <span class="n">temp1</span> <span class="o">=</span> <span class="n">local_control_param</span> <span class="o">-</span> <span class="n">global_control_param</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_last_params</span><span class="p">:</span>
                    <span class="n">temp2</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_last_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">-</span> <span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span> <span class="o">*</span> <span class="n">lr</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">{}</span><span class="s2">&#39; is not in last_params&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
                <span class="n">control_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp1</span> <span class="o">+</span> <span class="n">temp2</span>
        <span class="k">return</span> <span class="n">control_params</span>

    <span class="k">def</span> <span class="nf">_scaffold_set_last_params_and_local_control_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">control_params</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fl_param_names</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_last_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">control_params</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_local_control_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">control_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;&#39;</span><span class="si">{}</span><span class="s2">&#39; is not in control_params&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_model_params_to_weights_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">weight_infos</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Exact trainable params from model, then fill into weights and weights_infos&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">trainable_params</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">:</span>
                <span class="n">param_np</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">param_np</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">weight_infos</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">param_np</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">param_np</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">weights</span><span class="p">[</span><span class="n">param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_model_params_to_weights_diff_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">weight_infos</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Exact trainable params from model, then calculate diff value for FedNova&quot;&quot;&quot;</span>
        <span class="n">local_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_prefix</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                                   <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()))</span>
        <span class="n">global_params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_prefix</span> <span class="ow">in</span> <span class="n">x</span><span class="o">.</span><span class="n">name</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">_model</span><span class="o">.</span><span class="n">get_parameters</span><span class="p">()))</span>
        <span class="k">for</span> <span class="n">local_param</span><span class="p">,</span> <span class="n">global_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">local_params</span><span class="p">,</span> <span class="n">global_params</span><span class="p">):</span>
            <span class="n">param</span> <span class="o">=</span> <span class="n">local_param</span> <span class="o">-</span> <span class="n">global_param</span>
            <span class="n">param_np</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">()</span>
            <span class="n">weight_infos</span><span class="p">[</span><span class="n">local_param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">param_np</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">param_np</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">weights</span><span class="p">[</span><span class="n">local_param</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">param_np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">on_train_step_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">is_cloud</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_server_mode</span> <span class="o">==</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">SERVER_MODE_CLOUD</span>
        <span class="n">is_sync</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_begin_sync_iter</span>
        <span class="n">is_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rank_id</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_distribute</span> <span class="k">else</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_distribute</span>

        <span class="k">if</span> <span class="n">is_cloud</span> <span class="ow">and</span> <span class="n">is_sync</span> <span class="ow">and</span> <span class="n">is_dist</span><span class="p">:</span>
            <span class="c1"># In FedNova mode, the upload _data_size will be reset to the number of training steps</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_fednova</span><span class="p">():</span>
                <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_data_size</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">batch_num</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span> \
                        <span class="k">if</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">dataset_sink_mode</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span>
            <span class="n">start_fl_job</span> <span class="o">=</span> <span class="n">_StartFLJob</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data_size</span><span class="p">)</span>
            <span class="n">flattened_control_params</span> <span class="o">=</span> <span class="n">start_fl_job</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_scaffold</span><span class="p">()</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_scaffold_set_global_control_params</span><span class="p">(</span><span class="n">flattened_control_params</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;run_context is </span><span class="si">%r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">run_context</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_train_step_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_scaffold</span><span class="p">():</span>
            <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
            <span class="n">train_network</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">train_network</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">_get_lr</span><span class="p">(</span><span class="n">train_network</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Can not find optimizer in train network!&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_scaffold_update_params</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_server_mode</span> <span class="o">==</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">SERVER_MODE_CLOUD</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_end_sync_iter</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_adaptive_sync</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_as_set_grads</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encrypt_type</span> <span class="o">==</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">ENCRYPT_STABLE_PW</span><span class="p">:</span>
                    <span class="n">exchange_keys</span> <span class="o">=</span> <span class="n">_ExchangeKeys</span><span class="p">()</span>
                    <span class="n">exchange_keys</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>
                    <span class="n">get_keys</span> <span class="o">=</span> <span class="n">_GetKeys</span><span class="p">()</span>
                    <span class="n">get_keys</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>

                <span class="n">control_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_scaffold</span><span class="p">():</span>
                    <span class="n">control_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scaffold_get_control_params</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

                <span class="n">weights</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="n">weight_infos</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_fednova</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_model_params_to_weights_diff_dict</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">weight_infos</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_model_params_to_weights_dict</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">weight_infos</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_scaffold</span><span class="p">():</span>
                    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">control_params</span><span class="p">:</span>
                        <span class="n">delta_control_param</span> <span class="o">=</span> <span class="n">control_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_local_control_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                        <span class="n">weights</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_scaffold_prefix</span> <span class="o">+</span> <span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_control_param</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_distribute</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_update_model_with_distribute</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">weight_infos</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_update_model</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">weight_infos</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_scaffold</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_scaffold_set_last_params_and_local_control_params</span><span class="p">(</span><span class="n">control_params</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Load params from getting model into net, global step is </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_next_end_sync_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sync_frequency</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_next_begin_sync_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_global_step</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_adaptive_sync</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_as_analyze_gradient</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_round_id</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_as_set_last_param</span><span class="p">()</span>
                <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;total epoch num:</span><span class="si">{}</span><span class="s2">, batch num:</span><span class="si">{}</span><span class="s2">, Current epoch num is: </span><span class="si">{}</span><span class="s2">, Current step num is: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">cb_params</span><span class="o">.</span><span class="n">epoch_num</span><span class="p">,</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">batch_num</span><span class="p">,</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_epoch_num</span><span class="p">,</span>
                        <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_step_num</span><span class="p">))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_server_mode</span> <span class="o">==</span> <span class="n">_fl_context</span><span class="o">.</span><span class="n">SERVER_MODE_HYBRID</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_start_pull_weight</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_start_push_weight</span><span class="p">()</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2022, MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>