

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore.context &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../federated_install.html">获取MindSpore Federated</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy_federated_server.html">云侧部署</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy_federated_client.html">端侧部署</a></li>
</ul>
<p class="caption"><span class="caption-text">应用实践</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../image_classification_application.html">实现一个端云联邦的图像分类应用(x86)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sentiment_classification_application.html">实现一个情感分类应用(Android)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../image_classification_application_in_cross_silo.html">实现一个云云联邦的图像分类应用(x86)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../object_detection_application_in_cross_silo.html">实现一个云云联邦的目标检测应用(x86)</a></li>
</ul>
<p class="caption"><span class="caption-text">安全和隐私</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../local_differential_privacy_training_noise.html">局部差分隐私加噪训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../local_differential_privacy_training_signds.html">局部差分隐私SignDS训练</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pairwise_encryption_training.html">安全聚合训练</a></li>
</ul>
<p class="caption"><span class="caption-text">API参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../federated_server.html">Federated-Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../federated_client.html">Federated-Client</a></li>
</ul>
<p class="caption"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">FAQ</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>mindspore.context</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for mindspore.context</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2020-2021 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">The context of mindspore, used to configure the current execution environment,</span>
<span class="sd">includes the execution mode, execution backend and other feature switches.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">FunctionType</span>

<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">log</span> <span class="k">as</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">mindspore._c_expression</span> <span class="kn">import</span> <span class="n">MSContext</span><span class="p">,</span> <span class="n">ms_ctx_param</span>
<span class="kn">from</span> <span class="nn">mindspore._checkparam</span> <span class="kn">import</span> <span class="n">args_type_check</span><span class="p">,</span> <span class="n">Validator</span><span class="p">,</span> <span class="n">args_unreset_check</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._auto_parallel_context</span> <span class="kn">import</span> <span class="n">_set_auto_parallel_context</span><span class="p">,</span> <span class="n">_get_auto_parallel_context</span><span class="p">,</span> \
    <span class="n">_reset_auto_parallel_context</span>
<span class="kn">from</span> <span class="nn">mindspore.parallel._ps_context</span> <span class="kn">import</span> <span class="n">_set_ps_context</span><span class="p">,</span> <span class="n">_get_ps_context</span><span class="p">,</span> <span class="n">_reset_ps_context</span>
<span class="kn">from</span> <span class="nn">.default_config</span> <span class="kn">import</span> <span class="n">__device_target__</span><span class="p">,</span> <span class="n">__package_name__</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;GRAPH_MODE&#39;</span><span class="p">,</span> <span class="s1">&#39;PYNATIVE_MODE&#39;</span><span class="p">,</span> <span class="s1">&#39;set_context&#39;</span><span class="p">,</span> <span class="s1">&#39;get_context&#39;</span><span class="p">,</span> <span class="s1">&#39;set_auto_parallel_context&#39;</span><span class="p">,</span>
           <span class="s1">&#39;get_auto_parallel_context&#39;</span><span class="p">,</span> <span class="s1">&#39;reset_auto_parallel_context&#39;</span><span class="p">,</span> <span class="s1">&#39;ParallelMode&#39;</span><span class="p">,</span> <span class="s1">&#39;set_ps_context&#39;</span><span class="p">,</span>
           <span class="s1">&#39;get_ps_context&#39;</span><span class="p">,</span> <span class="s1">&#39;reset_ps_context&#39;</span><span class="p">,</span> <span class="s1">&#39;set_fl_context&#39;</span><span class="p">,</span> <span class="s1">&#39;get_fl_context&#39;</span><span class="p">]</span>

<span class="n">GRAPH_MODE</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">PYNATIVE_MODE</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">_DEVICE_APP_MEMORY_SIZE</span> <span class="o">=</span> <span class="mi">31</span>  <span class="c1"># The max memory size of graph plus variable.</span>
<span class="n">_re_pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;[1-9][0-9]*(\.)?[0-9]*GB|0\.[0-9]*GB&#39;</span>
<span class="n">_k_context</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">_make_directory</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Make directory.&quot;&quot;&quot;</span>
    <span class="n">real_path</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="n">path</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the &#39;save_graphs_path&#39; or the &#39;print_file_path&#39; is invalid &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;type, it should be Non-empty string, but got &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>

    <span class="c1"># convert the relative paths</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;The absolute path is </span><span class="si">%r</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="c1"># check whether the path is already existed and has written permissions</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="n">real_path</span> <span class="o">=</span> <span class="n">path</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># All exceptions need to be caught because create directory maybe have some limit(permissions)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;The directory(</span><span class="si">%s</span><span class="s2">) doesn&#39;t exist, will create it&quot;</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="n">real_path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">except</span> <span class="ne">PermissionError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">critical</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No write permission on the directory &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;&#39;, error = </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">No write permission on the directory &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">real_path</span>


<span class="k">def</span> <span class="nf">_get_print_file_name</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add timestamp suffix to file name. Rename the file name:  file_name + &quot;.&quot; + time(seconds).&quot;&quot;&quot;</span>
    <span class="n">time_second</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()))</span>
    <span class="n">file_name</span> <span class="o">=</span> <span class="n">file_name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">time_second</span>
    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_name</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;print_file_path&#39; </span><span class="si">{}</span><span class="s2"> already exists, &quot;</span>
                         <span class="s2">&quot;please check it&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_name</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">file_name</span>


<span class="k">class</span> <span class="nc">_ThreadLocalInfo</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Thread local Info used for store thread local attributes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ThreadLocalInfo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug_runtime</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span>

    <span class="nd">@reserve_class_name_in_scope</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reserve_class_name_in_scope</span> <span class="o">=</span> <span class="n">reserve_class_name_in_scope</span>


<span class="n">_ContextRecord</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;_ContextRecord&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;is_pynative_mode&quot;</span><span class="p">,</span> <span class="s2">&quot;switch_context_fn&quot;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">_ContextSwitchInfo</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">local</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Record of context switch information.</span>

<span class="sd">    Args:</span>
<span class="sd">        is_pynative (bool): Whether to adopt the PyNative mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_pynative</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ContextSwitchInfo</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">is_pynative</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">push</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_pynative</span><span class="p">,</span> <span class="n">switch_context_fn</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Push a context switch record onto the stack.</span>

<span class="sd">        Args:</span>
<span class="sd">            is_pynative (bool): Whether context switch to PyNative mode.</span>
<span class="sd">            switch_context_fn (Function): A callable that executes the context switch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">switch_context_fn</span><span class="p">,</span> <span class="n">FunctionType</span><span class="p">):</span>
            <span class="n">switch_context_fn</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">_ContextRecord</span><span class="p">(</span><span class="n">is_pynative</span><span class="p">,</span> <span class="n">switch_context_fn</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">pop</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">_Context</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    _Context is the environment in which operations are executed</span>

<span class="sd">    Note:</span>
<span class="sd">        Create a context through instantiating Context object is not recommended.</span>
<span class="sd">        should use context() to get the context since Context is a singleton.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_instance</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_instance_lock</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Lock</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span> <span class="o">=</span> <span class="n">_ThreadLocalInfo</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span> <span class="o">=</span> <span class="n">_ContextSwitchInfo</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span> <span class="o">=</span> <span class="n">MSContext</span><span class="o">.</span><span class="n">get_instance</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enable_compile_cache</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance_lock</span><span class="o">.</span><span class="n">acquire</span><span class="p">()</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance_lock</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span>

    <span class="k">def</span> <span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__getattribute__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attr</span> <span class="o">==</span> <span class="s2">&quot;_context_handle&quot;</span> <span class="ow">and</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context handle is none in context!!!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">value</span>

    <span class="k">def</span> <span class="nf">get_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get current mode.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Switch between Graph mode and PyNative mode.</span>

<span class="sd">        Args:</span>
<span class="sd">            mode (int): GRAPH_MODE or PYNATIVE_MODE.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">PYNATIVE_MODE</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;vm&quot;</span><span class="p">)</span>
            <span class="n">parallel_mode</span> <span class="o">=</span> <span class="n">_get_auto_parallel_context</span><span class="p">(</span><span class="s2">&quot;parallel_mode&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">parallel_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ParallelMode</span><span class="o">.</span><span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">ParallelMode</span><span class="o">.</span><span class="n">STAND_ALONE</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pynative Only support STAND_ALONE and DATA_PARALLEL for ParallelMode,&quot;</span>
                                 <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">parallel_mode</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="n">GRAPH_MODE</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;ge&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_context_switches</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;mode&#39; should be context.GRAPH_MODE (0) &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;or context.PYNATIVE_MODE (1), but got </span><span class="si">{</span><span class="n">mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_backend_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
        <span class="n">success</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">success</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Backend policy must be one of values in [&#39;ge&#39;, &#39;vm&#39;, &#39;ms&#39;]. &quot;</span>
                               <span class="s2">&quot;But got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">policy</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">set_save_graphs_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_graphs_path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">save_graphs_path</span><span class="p">,</span> <span class="n">_make_directory</span><span class="p">(</span><span class="n">save_graphs_path</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">set_device_target</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">valid_targets</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="s2">&quot;Davinci&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">valid_targets</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;device_target&#39; must be one of &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">valid_targets</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;Davinci&quot;</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;Ascend&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="ow">and</span> <span class="n">target</span> <span class="o">==</span> <span class="s2">&quot;CPU&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="s2">&quot;vm&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_auto_tune_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tune_mode</span><span class="p">):</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;NO_TUNE&quot;</span><span class="p">,</span> <span class="s2">&quot;RL&quot;</span><span class="p">,</span> <span class="s2">&quot;GA&quot;</span><span class="p">,</span> <span class="s2">&quot;RL,GA&quot;</span><span class="p">,</span> <span class="s2">&quot;GA,RL&quot;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">tune_mode</span> <span class="ow">in</span> <span class="n">candidate</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">tune_mode</span><span class="p">,</span> <span class="n">tune_mode</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;auto_tune_mode&#39; must be in &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;[&#39;NO_TUNE&#39;, &#39;RL&#39;, &#39;GA&#39;, &#39;RL,GA&#39;, &#39;GA,RL&#39;], but got </span><span class="si">{</span><span class="n">tune_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_device_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">device_id</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">device_id</span> <span class="o">&gt;</span> <span class="mi">4095</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;device_id&#39; must be in range [0, 4095], &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">device_id</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_id</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_max_call_depth</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">max_call_depth</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;max_call_depth&#39; must be greater than 0, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;but got </span><span class="si">{</span><span class="n">max_call_depth</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">max_call_depth</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_profiling_options</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">option</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">option</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;profiling_option&#39; must be string, &quot;</span>
                            <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">option</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">profiling_options</span><span class="p">,</span> <span class="n">option</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_variable_memory_max_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">variable_memory_max_size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;set values of variable_memory_max_size and graph_memory_max_size&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;The parameter &#39;variable_memory_max_size&#39; is deprecated, and will be removed in a future &quot;</span>
                       <span class="s2">&quot;version. Please use parameter &#39;max_device_memory&#39; instead.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">,</span> <span class="n">_re_pattern</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;variable_memory_max_size&#39; should be in correct&quot;</span>
                             <span class="s2">&quot; format! It must be a string ending with &#39;GB&#39;, in addition to that, it must contain &quot;</span>
                             <span class="s2">&quot;only numbers or decimal points, such as </span><span class="se">\&quot;</span><span class="s2">5GB</span><span class="se">\&quot;</span><span class="s2"> or </span><span class="se">\&quot;</span><span class="s2">3.5GB</span><span class="se">\&quot;</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">float</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">_DEVICE_APP_MEMORY_SIZE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;variable_memory_max_size&#39; should not be &quot;</span>
                             <span class="s2">&quot;greater than 31GB, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">))</span>
        <span class="n">variable_memory_max_size_</span> <span class="o">=</span> <span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot; * 1024 * 1024 * 1024&quot;</span>
        <span class="n">graph_memory_max_size</span> <span class="o">=</span> <span class="n">_DEVICE_APP_MEMORY_SIZE</span> <span class="o">-</span> <span class="nb">int</span><span class="p">(</span><span class="n">variable_memory_max_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">graph_memory_max_size_</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">graph_memory_max_size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; * 1024 * 1024 * 1024&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">variable_memory_max_size</span><span class="p">,</span> <span class="n">variable_memory_max_size_</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">_graph_memory_max_size</span><span class="p">,</span> <span class="n">graph_memory_max_size_</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_max_device_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">,</span> <span class="n">_re_pattern</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;max_device_memory&#39; should be in correct &quot;</span>
                             <span class="s2">&quot; format! It must be a string ending with &#39;GB&#39;, in addition to that, it must contain &quot;</span>
                             <span class="s2">&quot;only numbers or decimal points, such as </span><span class="se">\&quot;</span><span class="s2">5GB</span><span class="se">\&quot;</span><span class="s2"> or </span><span class="se">\&quot;</span><span class="s2">3.5GB</span><span class="se">\&quot;</span><span class="s2">, but got </span><span class="si">{}</span><span class="s2">.&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">))</span>
        <span class="n">max_device_memory_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">max_device_memory</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">max_device_memory_value</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;max_device_memory&#39; should not be </span><span class="se">\&quot;</span><span class="s2">0GB</span><span class="se">\&quot;</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">max_device_memory</span><span class="p">,</span> <span class="n">max_device_memory_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_mempool_block_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mempool_block_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">_get_mode</span><span class="p">()</span> <span class="o">==</span> <span class="n">GRAPH_MODE</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Graph mode not support mempool_block_size context currently&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">Validator</span><span class="o">.</span><span class="n">check_str_by_regular</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="p">,</span> <span class="n">_re_pattern</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context param mempool_block_size should be in correct format! Such as </span><span class="se">\&quot;</span><span class="s2">10GB</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">mempool_block_size_value</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">mempool_block_size</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">mempool_block_size_value</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Context param mempool_block_size should be greater or equal to </span><span class="se">\&quot;</span><span class="s2">1GB</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">mempool_block_size</span><span class="p">,</span> <span class="n">mempool_block_size_value</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_print_file_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Add timestamp suffix to file name. Sets print file path.&quot;&quot;&quot;</span>
        <span class="n">print_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">IOError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;print_file_path&#39; should be file path, &quot;</span>
                          <span class="s2">&quot;but got directory </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">file_path</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">):</span>
            <span class="n">_path</span><span class="p">,</span> <span class="n">_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">print_file_path</span><span class="p">)</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">_make_directory</span><span class="p">(</span><span class="n">_path</span><span class="p">)</span>
            <span class="n">file_name</span> <span class="o">=</span> <span class="n">_get_print_file_name</span><span class="p">(</span><span class="n">_file_name</span><span class="p">)</span>
            <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">file_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">full_file_name</span> <span class="o">=</span> <span class="n">print_file_path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">print_file_path</span><span class="p">,</span> <span class="n">full_file_name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_env_config_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_config_path</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check and set env_config_path.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">enable_dump_ir</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the argument &#39;env_config_path&#39; is not supported, please &quot;</span>
                             <span class="s2">&quot;enable ENABLE_DUMP_IR with &#39;-D on&#39; and recompile source firstly.&quot;</span><span class="p">)</span>
        <span class="n">env_config_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the &#39;env_config_path&#39; file </span><span class="si">%r</span><span class="s2"> is not exists, &quot;</span>
                             <span class="s2">&quot;please check whether &#39;env_config_path&#39; is correct.&quot;</span> <span class="o">%</span> <span class="n">env_config_path</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">TypeError</span><span class="p">,</span> <span class="ne">ValueError</span><span class="p">)</span> <span class="k">as</span> <span class="n">exo</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">exo</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">For &#39;context.set_context&#39;, open or load the &#39;env_config_path&#39; file </span><span class="si">{}</span><span class="s2"> &quot;</span>
                             <span class="s2">&quot;failed, please check whether &#39;env_config_path&#39; is json file and correct, or may not &quot;</span>
                             <span class="s2">&quot;have permission to read it.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_config_path</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">env_config_path</span><span class="p">,</span> <span class="n">env_config_path</span><span class="p">)</span>

    <span class="n">setters</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;mode&#39;</span><span class="p">:</span> <span class="n">set_mode</span><span class="p">,</span>
        <span class="s1">&#39;save_graphs_path&#39;</span><span class="p">:</span> <span class="n">set_save_graphs_path</span><span class="p">,</span>
        <span class="s1">&#39;device_target&#39;</span><span class="p">:</span> <span class="n">set_device_target</span><span class="p">,</span>
        <span class="s1">&#39;device_id&#39;</span><span class="p">:</span> <span class="n">set_device_id</span><span class="p">,</span>
        <span class="s1">&#39;auto_tune_mode&#39;</span><span class="p">:</span> <span class="n">set_auto_tune_mode</span><span class="p">,</span>
        <span class="s1">&#39;max_call_depth&#39;</span><span class="p">:</span> <span class="n">set_max_call_depth</span><span class="p">,</span>
        <span class="s1">&#39;profiling_options&#39;</span><span class="p">:</span> <span class="n">set_profiling_options</span><span class="p">,</span>
        <span class="s1">&#39;variable_memory_max_size&#39;</span><span class="p">:</span> <span class="n">set_variable_memory_max_size</span><span class="p">,</span>
        <span class="s1">&#39;max_device_memory&#39;</span><span class="p">:</span> <span class="n">set_max_device_memory</span><span class="p">,</span>
        <span class="s1">&#39;mempool_block_size&#39;</span><span class="p">:</span> <span class="n">set_mempool_block_size</span><span class="p">,</span>
        <span class="s1">&#39;print_file_path&#39;</span><span class="p">:</span> <span class="n">set_print_file_path</span><span class="p">,</span>
        <span class="s1">&#39;env_config_path&#39;</span><span class="p">:</span> <span class="n">set_env_config_path</span>
    <span class="p">}</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">reserve_class_name_in_scope</span>

    <span class="nd">@reserve_class_name_in_scope</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">reserve_class_name_in_scope</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set whether to save the network class name in the scope.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">reserve_class_name_in_scope</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;For &#39;context.set_context&#39;, the type of the property &#39;reserve_class_name_in_scope&#39; must &quot;</span>
                             <span class="s2">&quot;be bool, but got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">reserve_class_name_in_scope</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">reserve_class_name_in_scope</span> <span class="o">=</span> <span class="n">reserve_class_name_in_scope</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">enable_ge</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_context_handle</span><span class="o">.</span><span class="n">get_backend_policy</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;ge&#39;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">enable_debug_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">debug_runtime</span>

    <span class="nd">@enable_debug_runtime</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">enable_debug_runtime</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enable</span><span class="p">):</span>
        <span class="n">thread_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_thread_local_info</span>
        <span class="n">thread_info</span><span class="o">.</span><span class="n">debug_runtime</span> <span class="o">=</span> <span class="n">enable</span>


<span class="k">def</span> <span class="nf">_context</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the global _context, if context is not created, create a new one.</span>

<span class="sd">    Returns:</span>
<span class="sd">        _Context, the global context in PyNative mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_k_context</span>
    <span class="k">if</span> <span class="n">_k_context</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">default_backend</span> <span class="o">=</span> <span class="s1">&#39;debug&#39;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">default_config</span>
            <span class="n">default_backend</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">__backend__</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;import default config fail&quot;</span><span class="p">)</span>
        <span class="n">_k_context</span> <span class="o">=</span> <span class="n">_Context</span><span class="p">()</span>
        <span class="n">_k_context</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">default_backend</span> <span class="o">==</span> <span class="s1">&#39;debug&#39;</span><span class="p">:</span>
            <span class="n">_k_context</span><span class="o">.</span><span class="n">enable_debug_runtime</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">default_backend</span> <span class="o">=</span> <span class="s1">&#39;vm&#39;</span>
        <span class="n">_k_context</span><span class="o">.</span><span class="n">set_backend_policy</span><span class="p">(</span><span class="n">default_backend</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">_k_context</span>


<span class="nd">@args_type_check</span><span class="p">(</span><span class="n">device_num</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">global_rank</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">gradients_mean</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">gradient_fp32_sync</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">parallel_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">auto_parallel_search_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">search_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">parameter_broadcast</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">strategy_ckpt_load_file</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">strategy_ckpt_save_file</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">full_batch</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">enable_parallel_optimizer</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">all_reduce_fusion_config</span><span class="o">=</span><span class="nb">list</span><span class="p">,</span> <span class="n">pipeline_stages</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">grad_accumulation_step</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                 <span class="n">parallel_optimizer_config</span><span class="o">=</span><span class="nb">dict</span><span class="p">,</span> <span class="n">comm_fusion</span><span class="o">=</span><span class="nb">dict</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_auto_parallel_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set auto parallel context, which is valid only for Ascend and GPU target.</span>

<span class="sd">    Auto parallel context should be configured before the initialization of your network.</span>

<span class="sd">    Note:</span>
<span class="sd">        Attribute name is required for setting attributes.</span>
<span class="sd">        If a program has tasks on different parallel modes, before setting a new parallel mode for the</span>
<span class="sd">        next task, interface mindspore.context.reset_auto_parallel_context() should be called to reset</span>
<span class="sd">        the configuration.</span>
<span class="sd">        Setting or changing parallel modes must be called before creating any Initializer, otherwise,</span>
<span class="sd">        it may have RuntimeError when compiling the network.</span>

<span class="sd">    Some configurations are parallel mode specific, see the below table for details:</span>

<span class="sd">    ===========================  ===========================</span>
<span class="sd">    Common                       AUTO_PARALLEL</span>
<span class="sd">    ===========================  ===========================</span>
<span class="sd">    device_num                   gradient_fp32_sync</span>
<span class="sd">    global_rank                  loss_repeated_mean</span>
<span class="sd">    gradients_mean               search_mode</span>
<span class="sd">    parallel_mode                strategy_ckpt_load_file</span>
<span class="sd">    all_reduce_fusion_config     strategy_ckpt_save_file</span>
<span class="sd">    enable_parallel_optimizer    dataset_strategy</span>
<span class="sd">    parallel_optimizer_config    pipeline_stages</span>
<span class="sd">               \                 grad_accumulation_step</span>
<span class="sd">               \                 auto_parallel_search_mode</span>
<span class="sd">               \                 comm_fusion</span>
<span class="sd">    ===========================  ===========================</span>

<span class="sd">    Args:</span>
<span class="sd">        device_num (int): Available device number, the value must be in [1, 4096]. Default: 1.</span>
<span class="sd">        global_rank (int): Global rank id, the value must be in [0, 4095]. Default: 0.</span>
<span class="sd">        gradients_mean (bool): Whether to perform mean operator after allreduce of gradients.</span>
<span class="sd">                     &quot;stand_alone&quot; do not support gradients_mean. Default: False.</span>
<span class="sd">        gradient_fp32_sync (bool): Run allreduce of gradients in fp32. &quot;stand_alone&quot;, &quot;data_parallel&quot;</span>
<span class="sd">                     and &quot;hybrid_parallel&quot; do not support gradient_fp32_sync. Default: True.</span>
<span class="sd">        parallel_mode (str): There are five kinds of parallel modes, &quot;stand_alone&quot;, &quot;data_parallel&quot;,</span>
<span class="sd">                     &quot;hybrid_parallel&quot;, &quot;semi_auto_parallel&quot; and &quot;auto_parallel&quot;. Note the pynative mode only supports</span>
<span class="sd">                     the &quot;stand_alone&quot; and &quot;data_parallel&quot; mode. Default: &quot;stand_alone&quot;.</span>

<span class="sd">                     - stand_alone: Only one processor is working.</span>

<span class="sd">                     - data_parallel: Distributes the data across different processors.</span>

<span class="sd">                     - hybrid_parallel: Achieves data parallelism and model parallelism manually.</span>

<span class="sd">                     - semi_auto_parallel: Achieves data and model parallelism by setting parallel strategies.</span>

<span class="sd">                     - auto_parallel: Achieving parallelism automatically.</span>
<span class="sd">        search_mode (str): There are three kinds of shard strategy search modes: &quot;recursive_programming&quot;,</span>
<span class="sd">                     &quot;dynamic_programming&quot; and &quot;sharding_propagation&quot;. Default: &quot;dynamic_programming&quot;.</span>

<span class="sd">                     - recursive_programming: Recursive programming search mode.</span>

<span class="sd">                     - dynamic_programming: Dynamic programming search mode.</span>

<span class="sd">                     - sharding_propagation: Propagate shardings from configured ops to non-configured ops.</span>
<span class="sd">        auto_parallel_search_mode (str): This is the old version of &#39;search_mode&#39;. Here, remaining this attribute is</span>
<span class="sd">                     for forward compatibility, and this attribute will be deleted in a future MindSpore version.</span>
<span class="sd">        parameter_broadcast (bool): Whether to broadcast parameters before training. Before training, in order to have</span>
<span class="sd">                     the same network initialization parameter values for all devices, broadcast the parameters</span>
<span class="sd">                     on device 0 to other devices. Parameter broadcasting in different parallel modes is different,</span>
<span class="sd">                     data_parallel mode, all parameters are broadcast except for the parameter whose attribute</span>
<span class="sd">                     layerwise_parallel is True. Hybrid_parallel, semi_auto_parallel and auto_parallel mode, the</span>
<span class="sd">                     segmented parameters do not participate in broadcasting. Default: False.</span>
<span class="sd">        strategy_ckpt_load_file (str): The path to load parallel strategy checkpoint. Default: &#39;&#39;</span>
<span class="sd">        strategy_ckpt_save_file (str): The path to save parallel strategy checkpoint. Default: &#39;&#39;</span>
<span class="sd">        full_batch (bool): If you load whole batch datasets in auto_parallel mode, this parameter</span>
<span class="sd">                       should be set as True. Default: False. The interface is not to be recommended currently,</span>
<span class="sd">                       it is better using &#39;dataset_strategy&#39; to replace it.</span>
<span class="sd">        dataset_strategy (Union[str, tuple]): Dataset sharding strategy. Default: &quot;data_parallel&quot;.</span>
<span class="sd">                       dataset_strategy=&quot;data_parallel&quot; is equal to full_batch=False, dataset_strategy=&quot;full_batch&quot; is</span>
<span class="sd">                       equal to full_batch=True. For dataset load into net by model parallel strategy likes</span>
<span class="sd">                       ds_stra ((1, 8), (1, 8)), it requires using set_auto_parallel_context(dataset_strategy=ds_stra).</span>
<span class="sd">        enable_parallel_optimizer (bool): This is a developing feature, which shards the weight update computation for</span>
<span class="sd">                       data parallel training in the benefit of time and memory saving. Currently, auto and semi auto</span>
<span class="sd">                       parallel mode support all optimizers in both Ascend and GPU. Data parallel mode only supports</span>
<span class="sd">                       `Lamb` and `AdamWeightDecay` in Ascend . Default: False.</span>
<span class="sd">        all_reduce_fusion_config (list): Set allreduce fusion strategy by parameters indices. Only support ReduceOp.SUM</span>
<span class="sd">                       and HCCL_WORLD_GROUP/NCCL_WORLD_GROUP. No Default, if it is not set, the fusion is closed.</span>
<span class="sd">        pipeline_stages (int): Set the stage information for pipeline parallel. This indicates how the devices are</span>
<span class="sd">                        distributed alone in the pipeline. The total devices will be divided into &#39;pipeline_stags&#39;</span>
<span class="sd">                        stages. Currently, this could only be used when parallel mode semi_auto_parallel is enabled.</span>
<span class="sd">                        Default: 1.</span>
<span class="sd">        grad_accumulation_step (int): Set the accumulation steps of gradients in auto and semi auto parallel mode.</span>
<span class="sd">                        This should be a positive int. Default: 1.</span>
<span class="sd">        parallel_optimizer_config (dict): A dict contains the keys and values for setting the parallel optimizer</span>
<span class="sd">                        configure. The configure provides more detailed behavior control about parallel training</span>
<span class="sd">                        when parallel optimizer is enabled. Currently it supports the key `gradient_accumulation_shard`.</span>
<span class="sd">                        The configure will be effective when we use</span>
<span class="sd">                        context.set_auto_parallel_context(enable_parallel_optimizer=True).</span>
<span class="sd">                        It supports the following keys.</span>

<span class="sd">                        - gradient_accumulation_shard: If true, the accumulation gradient parameters will be</span>
<span class="sd">                          sharded across the data parallel devices. This will</span>
<span class="sd">                          introduce additional communication(ReduceScatter) at</span>
<span class="sd">                          each step when accumulate the gradients, but saves a</span>
<span class="sd">                          lot of device memories, thus can make model be trained</span>
<span class="sd">                          with larger batch size. This configure is effective only</span>
<span class="sd">                          when the model runs on pipeline training or gradient</span>
<span class="sd">                          accumulation with data parallel. Default True.</span>
<span class="sd">        comm_fusion (dict): A dict contains the types and configurations for setting the communication fusion. each</span>
<span class="sd">                        communication fusion config has two keys: &quot;mode&quot; and &quot;config&quot;.</span>
<span class="sd">                        It supports following communication fusion types and configurations:</span>

<span class="sd">                        - allreduce: If communication fusion type is `allreduce`. The `mode` contains: `auto`, `size`</span>
<span class="sd">                          and `index`. In `auto` mode, allreduce fusion is configured by gradients size, and the default</span>
<span class="sd">                          fusion threshold is `64` MB. In &#39;size&#39; mode, allreduce fusion is configured by gradients size</span>
<span class="sd">                          manually, and the fusion threshold must be larger than `0` MB. In `index` mode, it is same as</span>
<span class="sd">                          `all_reduce_fusion_config`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import context</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(device_num=8)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(global_rank=0)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(gradients_mean=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(gradient_fp32_sync=False)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(parallel_mode=&quot;auto_parallel&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(search_mode=&quot;dynamic_programming&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(auto_parallel_search_mode=&quot;dynamic_programming&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(parameter_broadcast=False)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(strategy_ckpt_load_file=&quot;./strategy_stage1.ckpt&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(strategy_ckpt_save_file=&quot;./strategy_stage1.ckpt&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(dataset_strategy=((1, 8), (1, 8)))</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(enable_parallel_optimizer=False)</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(all_reduce_fusion_config=[8, 160])</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(pipeline_stages=2)</span>
<span class="sd">        &gt;&gt;&gt; parallel_config = {&quot;gradient_accumulation_shard&quot;: True}</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(parallel_optimizer_config=parallel_config, enable_parallel_optimizer=True)</span>
<span class="sd">        &gt;&gt;&gt; comm_fusion_config = {&quot;allreduce&quot;: {&quot;mode&quot;: &quot;size&quot;, &quot;config&quot;: 32}}</span>
<span class="sd">        &gt;&gt;&gt; context.set_auto_parallel_context(comm_fusion=comm_fusion_config)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_auto_parallel_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_auto_parallel_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get auto parallel context attribute value according to the key.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns attribute value according to the key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import context</span>
<span class="sd">        &gt;&gt;&gt; parallel_mode = context.get_auto_parallel_context(&quot;parallel_mode&quot;)</span>
<span class="sd">        &gt;&gt;&gt; dataset_strategy = context.get_auto_parallel_context(&quot;dataset_strategy&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_auto_parallel_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reset_auto_parallel_context</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reset auto parallel context attributes to the default values:</span>

<span class="sd">    - device_num: 1.</span>
<span class="sd">    - global_rank: 0.</span>
<span class="sd">    - gradients_mean: False.</span>
<span class="sd">    - gradient_fp32_sync: True.</span>
<span class="sd">    - parallel_mode: &#39;stand_alone&#39;.</span>
<span class="sd">    - search_mode: &#39;dynamic_programming&#39;.</span>
<span class="sd">    - auto_parallel_search_mode: &#39;dynamic_programming&#39;.</span>
<span class="sd">    - parameter_broadcast: False.</span>
<span class="sd">    - strategy_ckpt_load_file: &#39;&#39;.</span>
<span class="sd">    - strategy_ckpt_save_file: &#39;&#39;.</span>
<span class="sd">    - full_batch: False.</span>
<span class="sd">    - enable_parallel_optimizer: False.</span>
<span class="sd">    - pipeline_stages: 1.</span>
<span class="sd">    - fusion_threshold: 64.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_reset_auto_parallel_context</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">arg_key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Checking whether a config is suitable for a specified device&quot;&quot;&quot;</span>
    <span class="n">device_cfgs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;enable_dump&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;save_dump_path&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;enable_graph_kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="s1">&#39;CPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;graph_kernel_flags&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="s1">&#39;CPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;enable_reduce_precision&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;enable_profiling&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;profiling_options&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;print_file_path&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;variable_memory_max_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;auto_tune_mode&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">],</span>
        <span class="s1">&#39;max_device_memory&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Ascend&#39;</span><span class="p">,</span> <span class="s1">&#39;GPU&#39;</span><span class="p">],</span>
        <span class="s1">&#39;mempool_block_size&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;GPU&#39;</span><span class="p">,</span> <span class="s1">&#39;Ascend&#39;</span><span class="p">]</span>
    <span class="p">}</span>
    <span class="c1"># configs not in map device_cfgs are supposed to be suitable for all devices</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">arg_key</span> <span class="ow">in</span> <span class="n">device_cfgs</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="n">supported_devices</span> <span class="o">=</span> <span class="n">device_cfgs</span><span class="p">[</span><span class="n">arg_key</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">supported_devices</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Config &#39;</span><span class="si">{</span><span class="n">arg_key</span><span class="si">}</span><span class="s2">&#39; only supports devices in </span><span class="si">{</span><span class="n">supported_devices</span><span class="si">}</span><span class="s2">, current device is &#39;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&#39;&quot;</span>
                   <span class="s2">&quot;, ignore it.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">False</span>


<span class="nd">@args_unreset_check</span><span class="p">(</span><span class="n">device_id</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">variable_memory_max_size</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">max_device_memory</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">mempool_block_size</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
<span class="nd">@args_type_check</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">precompile_only</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">device_target</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">save_graphs</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">save_graphs_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_dump</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">auto_tune_mode</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">save_dump_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_reduce_precision</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">variable_memory_max_size</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
                 <span class="n">enable_profiling</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">profiling_options</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_auto_mixed_precision</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">enable_graph_kernel</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">reserve_class_name_in_scope</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">check_bprop</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">max_device_memory</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">print_file_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">enable_sparse</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">max_call_depth</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
                 <span class="n">env_config_path</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">graph_kernel_flags</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">save_compile_cache</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span>
                 <span class="n">load_compile_cache</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">grad_for_scalar</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">pynative_synchronize</span><span class="o">=</span><span class="nb">bool</span><span class="p">,</span> <span class="n">mempool_block_size</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set context for running environment.</span>

<span class="sd">    Context should be configured before running your program. If there is no configuration,</span>
<span class="sd">    it will be automatically set according to the device target by default.</span>

<span class="sd">    Note:</span>
<span class="sd">        Attribute name is required for setting attributes.</span>
<span class="sd">        The mode is not recommended to be changed after net was initialized because the implementations of some</span>
<span class="sd">        operations are different in graph mode and pynative mode. Default: GRAPH_MODE.</span>

<span class="sd">    Some configurations are device specific, see the below table for details:</span>

<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>
<span class="sd">    | Function Classification |   Configuration Parameters   |   Hardware Platform Support|</span>
<span class="sd">    +=========================+==============================+============================+</span>
<span class="sd">    | System Configuration    |   device_id                  |   CPU/GPU/Ascend           |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |   device_target              |   CPU/GPU/Ascend           |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  max_device_memory           |  GPU/Ascend                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  variable_memory_max_size    |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  mempool_block_size          |  GPU/Ascend                |</span>
<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>
<span class="sd">    | Debug Configuration     |  save_graphs                 |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  save_graphs_path            |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_dump                 |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  save_dump_path              |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_profiling            |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  profiling_options           |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  print_file_path             |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  env_config_path             |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  precompile_only             |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  reserve_class_name_in_scope |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  pynative_synchronize        |  GPU/Ascend                |</span>
<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>
<span class="sd">    | Executive Control       |   mode                       |   CPU/GPU/Ascend           |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_graph_kernel         |  Ascend/GPU                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  graph_kernel_flags          |  Ascend/GPU                |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_reduce_precision     |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  auto_tune_mode              |  Ascend                    |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  check_bprop                 |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  max_call_depth              |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_sparse               |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  grad_for_scalar             |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  enable_compile_cache        |  CPU/GPU/Ascend            |</span>
<span class="sd">    |                         +------------------------------+----------------------------+</span>
<span class="sd">    |                         |  compile_cache_path          |  CPU/GPU/Ascend            |</span>
<span class="sd">    +-------------------------+------------------------------+----------------------------+</span>

<span class="sd">    Args:</span>
<span class="sd">        device_id (int): ID of the target device, the value must be in [0, device_num_per_host-1],</span>
<span class="sd">            while device_num_per_host should be no more than 4096. Default: 0.</span>
<span class="sd">        device_target (str): The target device to run, support &quot;Ascend&quot;, &quot;GPU&quot;, and &quot;CPU&quot;.</span>
<span class="sd">            If device target is not set, the version of MindSpore package is used.</span>
<span class="sd">        max_device_memory (str): Set the maximum memory available for devices. The format is &quot;xxGB&quot;. Default: &quot;1024GB&quot;.</span>
<span class="sd">            The actual used memory size is the minimum of the available memory of the device and max_device_memory.</span>
<span class="sd">        variable_memory_max_size (str): This parameter is deprecated, and will be removed in a future version.</span>
<span class="sd">            Please use parameter &#39;max_device_memory&#39; instead.</span>
<span class="sd">        mempool_block_size (str): Set the size of the memory pool block in PyNative mode for devices.</span>
<span class="sd">            The format is &quot;xxGB&quot;. Default: &quot;1GB&quot;. Minimum size is &quot;1G&quot;. The actual used memory block size is the minimum</span>
<span class="sd">            of the available memory of the device and mempool_block_size.</span>
<span class="sd">        save_graphs (bool): Whether to save graphs. Default: False.</span>
<span class="sd">            When the `save_graphs` attribute is set as True, attribute of `save_graphs_path` is used to set the</span>
<span class="sd">            intermediate compilation graph storage path. By default, the graphs are saved in the current directory.</span>
<span class="sd">        save_graphs_path (str): Path to save graphs. Default: &quot;.&quot;.</span>
<span class="sd">            If the specified directory does not exist, the system will automatically create the directory.</span>
<span class="sd">            During distributed training, graphs will be saved to the directory of</span>
<span class="sd">            `save_graphs_path/rank_${rank_id}/`. `rank_id` is the ID of the current device in the cluster.</span>
<span class="sd">        enable_dump (bool): This parameters is deprecated, and will be deleted in the next version.</span>
<span class="sd">        save_dump_path (str): This parameters is deprecated, and will be deleted in the next version.</span>
<span class="sd">        enable_profiling (bool): This parameters is deprecated, and will be deleted in the next version.</span>
<span class="sd">            Please use mindspore.profiler.Profiler api instead.</span>
<span class="sd">        profiling_options (str): This parameters is deprecated, and will be deleted in the next version.</span>
<span class="sd">            Please use mindspore.profiler.Profiler api instead.</span>
<span class="sd">        print_file_path (str): The path of saving print data. If this parameter is set, print data is saved to</span>
<span class="sd">            a file by default, and print_file_path is not set, the screen will be displayed.</span>
<span class="sd">            If the saved file already exists, the timestamp suffix will be added to the file. Saving data to a file</span>
<span class="sd">            solves the problem of data loss in screen printing when a large amount of data is generated.</span>
<span class="sd">            If it is not set, an error will be reported: prompt to set the upper absolute path.</span>
<span class="sd">        env_config_path (str): Config path for DFX.</span>
<span class="sd">            Through context.set_context(env_config_path=&quot;./mindspore_config.json&quot;)</span>

<span class="sd">            configure RDR:</span>

<span class="sd">            - enable: controls whether the RDR is enabled to collect the key data during training and</span>
<span class="sd">              save key data in the fault scenario. When set to true, the RDR will be turned on.</span>
<span class="sd">              When set to false, the RDR will be turned off.</span>
<span class="sd">            - mode: sets the mode of RDR on exporting data. When set to 1, the RDR only exports data</span>
<span class="sd">              in the fault scenario. When set to 2, the RDR exports data in the fault scenario and the</span>
<span class="sd">              normal end scenario. Default is 1.</span>
<span class="sd">            - path: sets the path where RDR saves data. The current path must be absolute.</span>

<span class="sd">            Memory reuse:</span>

<span class="sd">            - mem_Reuse: controls whether the memory reuse function is turned on. When set to True,</span>
<span class="sd">            - the memory reuse function is turned on. When set to False, the memory reuse function is turned off.</span>

<span class="sd">        precompile_only (bool): Whether to only precompile the network. Default: False.</span>
<span class="sd">            If set to True, the network will only be compiled, not executed.</span>
<span class="sd">        reserve_class_name_in_scope (bool) : Whether to save the network class name in the scope. Default: True.</span>
<span class="sd">            Each node has a scope. A scope of a subnode is the name of its parent node. If reserve_class_name_in_scope</span>
<span class="sd">            is set to True, the class name will be saved after keyword &#39;net-&#39; in the scope.</span>
<span class="sd">            For example:</span>

<span class="sd">            Default/net-Net1/net-Net2 (reserve_class_name_in_scope=True)</span>

<span class="sd">            Default/net/net (reserve_class_name_in_scope=False)</span>

<span class="sd">        pynative_synchronize (bool): Whether to enable synchronous execution of the device in PyNative mode.</span>
<span class="sd">            Default: False. When the value is set to False, the operator is executed asynchronously on the device.</span>
<span class="sd">            When an error occurs in the execution of the operator, the specific error script code location cannot</span>
<span class="sd">            be located, when the value is set to True, the operator is executed synchronously on the device. It will</span>
<span class="sd">            reduce the execution performance of the program. At this time, when an error occurs in the execution of</span>
<span class="sd">            the operator, the location of the error script code can be located according to the call stack of the error.</span>
<span class="sd">        mode (int): Running in GRAPH_MODE(0) or PYNATIVE_MODE(1). Default: GRAPH_MODE(0).</span>
<span class="sd">            GRAPH_MODE or PYNATIVE_MODE can be set by `mode` attribute and both modes support all backends, default</span>
<span class="sd">            mode is GRAPH_MODE.</span>
<span class="sd">        enable_graph_kernel (bool): Whether to enable graph kernel fusion to optimize network execution performance.</span>
<span class="sd">            Default: False.</span>
<span class="sd">            Indicates whether to enable image-computing convergence to optimize network execution performance.</span>
<span class="sd">            If enable_graph_kernel is set to True, acceleration can be enabled.</span>
<span class="sd">            For details of graph kernel fusion, please check</span>
<span class="sd">            `Enabling Graph Kernel Fusion &lt;https://www.mindspore.cn/docs/programming_guide</span>
<span class="sd">            /en/master/enable_graph_kernel_fusion.html&gt;`_.</span>
<span class="sd">        graph_kernel_flags (str) –</span>
<span class="sd">            Optimization options of graph kernel fusion, and the priority is higher when it conflicts</span>
<span class="sd">            with enable_graph_kernel. Only for experienced users.</span>
<span class="sd">            For example, context.set_context(graph_kernel_flags=&quot;--opt_level=2 --dump_as_text&quot;). Some general options:</span>

<span class="sd">            - opt_level: Set the optimization level.</span>
<span class="sd">              Default: 2. Graph kernel fusion can be enabled equivalently by setting opt_level greater than 0.</span>
<span class="sd">              Available values are:</span>

<span class="sd">              - 0: disables graph kernel fusion;</span>
<span class="sd">              - 1: enables the basic fusion of operators;</span>
<span class="sd">              - 2: includes all optimizations of level 1,</span>
<span class="sd">                and turns on more optimizations such as CSE, arithmetic simplification and so on;</span>
<span class="sd">              - 3: includes all optimizations of level 2, and turns on more optimizations such as SitchingFusion,</span>
<span class="sd">                ParallelFusion and so on. Optimizations of this level are radical and unstable in some scenarios.</span>
<span class="sd">                Be caution when using this level.</span>

<span class="sd">            - dump_as_text: dumps detail info as text files. Default: false.</span>

<span class="sd">            More options can refer to the implementation code.</span>
<span class="sd">        enable_reduce_precision (bool): Whether to enable precision reduction.</span>
<span class="sd">            If the operator does not support the user-specified precision, the precision will</span>
<span class="sd">            be changed automatically. Default: True.</span>
<span class="sd">        auto_tune_mode (str): The mode of auto tune when op building, get the best tiling performance.</span>
<span class="sd">            Default: NO_TUNE. The value must be in [&#39;RL&#39;, &#39;GA&#39;, &#39;RL,GA&#39;].</span>

<span class="sd">            - RL: Reinforcement Learning tune.</span>
<span class="sd">            - GA: Genetic Algorithm tune.</span>
<span class="sd">            - RL,GA: When both RL and GA optimization are enabled, the tool automatically selects RL or GA based on</span>
<span class="sd">              different types of operators in the network model. The sequence of RL and GA is not differentiated.</span>
<span class="sd">              (Automatic selection).</span>

<span class="sd">            For more information about the enable operator tuning tool settings, please check</span>
<span class="sd">            `Enable the operator optimization tool &lt;https://www.mindspore.cn/docs/programming_guide/en</span>
<span class="sd">            /master/enable_auto_tune.html&gt;`_.</span>
<span class="sd">        check_bprop (bool): Whether to check back propagation nodes. The checking ensures that the shape and dtype</span>
<span class="sd">            of back propagation node outputs is the same as input parameters. Default: False.</span>
<span class="sd">        max_call_depth (int): Specify the maximum depth of function call. Must be positive integer. Default: 1000.</span>
<span class="sd">            The max_call_depth parameter needs to be set when the nested call is too deep or the number</span>
<span class="sd">            of subgraphs is too large. If max_call_depth is set larger than before, the system max stack depth should be</span>
<span class="sd">            set larger too, otherwise a `core dumped` exception may be raised because of system stack overflow.</span>
<span class="sd">        enable_sparse (bool): Whether to enable sparsity feature. Default: False.</span>
<span class="sd">            For details of sparsity and sparse tensor, please check</span>
<span class="sd">            `sparse tensor &lt;https://www.mindspore.cn/docs/programming_guide/en/r1.6/tensor.html#sparse-tensor&gt;`_.</span>
<span class="sd">        grad_for_scalar (bool):  Whether to get gradient for scalar. Default: False.</span>
<span class="sd">            When grad_for_scalar is set to True, the function&#39;s scalar input can be derived.</span>
<span class="sd">            The default value is False. Because the back-end does not support scaling operations currently,</span>
<span class="sd">            this interface only supports simple operations that can be deduced by the front-end.</span>
<span class="sd">        enable_compile_cache (bool): Whether to save or load the cache of the graph compiled by front-end.</span>
<span class="sd">            After enable_compile_cache is set to True, during the first execution, a hardware-independent</span>
<span class="sd">            compilation cache is generated and exported to a MINDIR file. When the network is executed again,</span>
<span class="sd">            if enable_compile_cache is still set to True and the network scripts are not changed,</span>
<span class="sd">            the compile cache is loaded. Note that only limited automatic detection for the changes of</span>
<span class="sd">            python scripts is supported by now, which means that there is a correctness risk. Default: False.</span>
<span class="sd">            Note that it isn&#39;t yet supported in PS mode.</span>
<span class="sd">            This is an experimental prototype that is subject to change and/or deletion.</span>
<span class="sd">        compile_cache_path (str): Path to save the cache of the graph compiled by front-end. Default: &quot;.&quot;.</span>
<span class="sd">            If the specified directory does not exist, the system will automatically create the directory.</span>
<span class="sd">            The cache will be saved to the directory of `compile_cache_path/rank_${rank_id}/`. The `rank_id` is</span>
<span class="sd">            the ID of the current device in the cluster.</span>
<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not an attribute in context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import context</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(mode=context.PYNATIVE_MODE)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(precompile_only=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(device_target=&quot;Ascend&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(device_id=0)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(save_graphs=True, save_graphs_path=&quot;./model.ms&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(enable_reduce_precision=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(enable_dump=True, save_dump_path=&quot;.&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(enable_graph_kernel=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(graph_kernel_flags=&quot;--opt_level=2 --dump_as_text&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(reserve_class_name_in_scope=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(variable_memory_max_size=&quot;6GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(enable_profiling=True,</span>
<span class="sd">        ...                     profiling_options=&#39;{&quot;output&quot;:&quot;/home/data/output&quot;,&quot;training_trace&quot;:&quot;on&quot;}&#39;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(check_bprop=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(max_device_memory=&quot;3.5GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(mempool_block_size=&quot;1GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(print_file_path=&quot;print.pb&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(enable_sparse=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(max_call_depth=80)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(env_config_path=&quot;./env_config.json&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(auto_tune_mode=&quot;GA,RL&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(grad_for_scalar=True)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(enable_compile_cache=True, compile_cache_path=&quot;./cache.ms&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.set_context(pynative_synchronize=True)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="c1"># set device target first</span>
    <span class="k">if</span> <span class="s1">&#39;device_target&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">set_device_target</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;device_target&#39;</span><span class="p">])</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">device</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="n">__device_target__</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, package type </span><span class="si">{</span><span class="n">__package_name__</span><span class="si">}</span><span class="s2"> support &#39;device_target&#39; &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;type </span><span class="si">{</span><span class="n">__device_target__</span><span class="si">}</span><span class="s2">, but got </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;enable_profiling&#39;</span><span class="p">,</span> <span class="s1">&#39;profiling_options&#39;</span><span class="p">,</span> <span class="s1">&#39;enable_auto_mixed_precision&#39;</span><span class="p">,</span>
                   <span class="s1">&#39;enable_dump&#39;</span><span class="p">,</span> <span class="s1">&#39;save_dump_path&#39;</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; &#39;</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&#39; parameters will be deprecated.&quot;</span>
                           <span class="s2">&quot;For details, please see the interface parameter API comments&quot;</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">setters</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">setters</span><span class="p">[</span><span class="n">key</span><span class="p">](</span><span class="n">ctx</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="c1"># enum variables beginning with &#39;_&#39; are for internal use</span>
        <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span> <span class="ow">and</span> <span class="n">key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">set_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">value</span><span class="p">)</span>
            <span class="k">continue</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.set_context&#39;, the keyword argument </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2"> is not recognized! For detailed &quot;</span>
                         <span class="sa">f</span><span class="s2">&quot;usage of &#39;set_context&#39;, please refer to the Mindspore official website.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get context attribute value according to the input key.</span>
<span class="sd">    If some attributes are not set, they will be automatically obtained.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Object, The value of given attribute key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not an attribute in context.</span>
<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import context</span>
<span class="sd">        &gt;&gt;&gt; context.get_context(&quot;device_target&quot;)</span>
<span class="sd">        &gt;&gt;&gt; context.get_context(&quot;device_id&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">_check_target_specific_cfgs</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">attr_key</span><span class="p">)</span>
    <span class="c1"># enum variables beginning with &#39;_&#39; are for internal use</span>
    <span class="k">if</span> <span class="n">attr_key</span> <span class="ow">in</span> <span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span> <span class="ow">and</span> <span class="n">attr_key</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s1">&#39;_&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_param</span><span class="p">(</span><span class="n">ms_ctx_param</span><span class="o">.</span><span class="n">__members__</span><span class="p">[</span><span class="n">attr_key</span><span class="p">])</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;For &#39;context.get_context&#39;, the argument </span><span class="si">{</span><span class="n">attr_key</span><span class="si">}</span><span class="s2"> is not recognized! For detailed &quot;</span>
                     <span class="sa">f</span><span class="s2">&quot;usage of &#39;get_context&#39;, please refer to the Mindspore official website.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_mode</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get execution mode. Only for internal using.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Object: The Value of execution mode.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="n">_context</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get_mode</span><span class="p">()</span>


<span class="k">class</span> <span class="nc">ParallelMode</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Parallel mode options.</span>

<span class="sd">    There are five kinds of parallel modes, &quot;STAND_ALONE&quot;, &quot;DATA_PARALLEL&quot;,</span>
<span class="sd">    &quot;HYBRID_PARALLEL&quot;, &quot;SEMI_AUTO_PARALLEL&quot; and &quot;AUTO_PARALLEL&quot;. Default: &quot;STAND_ALONE&quot;.</span>

<span class="sd">    - STAND_ALONE: Only one processor is working.</span>
<span class="sd">    - DATA_PARALLEL: Distributes the data across different processors.</span>
<span class="sd">    - HYBRID_PARALLEL: Achieves data parallelism and model parallelism manually.</span>
<span class="sd">    - SEMI_AUTO_PARALLEL: Achieves data parallelism and model parallelism by setting parallel strategies.</span>
<span class="sd">    - AUTO_PARALLEL: Achieves parallelism automatically.</span>

<span class="sd">    MODE_LIST: The list of all supported parallel modes.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">STAND_ALONE</span> <span class="o">=</span> <span class="s2">&quot;stand_alone&quot;</span>
    <span class="n">DATA_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;data_parallel&quot;</span>
    <span class="n">HYBRID_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;hybrid_parallel&quot;</span>
    <span class="n">SEMI_AUTO_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;semi_auto_parallel&quot;</span>
    <span class="n">AUTO_PARALLEL</span> <span class="o">=</span> <span class="s2">&quot;auto_parallel&quot;</span>
    <span class="n">MODE_LIST</span> <span class="o">=</span> <span class="p">[</span><span class="n">STAND_ALONE</span><span class="p">,</span> <span class="n">DATA_PARALLEL</span><span class="p">,</span> <span class="n">HYBRID_PARALLEL</span><span class="p">,</span> <span class="n">SEMI_AUTO_PARALLEL</span><span class="p">,</span> <span class="n">AUTO_PARALLEL</span><span class="p">]</span>


<span class="nd">@args_type_check</span><span class="p">(</span><span class="n">enable_ps</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">set_ps_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set parameter server training mode context.</span>

<span class="sd">    Note:</span>
<span class="sd">        Some other environment variables should also be set for parameter server training mode.</span>
<span class="sd">        These environment variables are listed below:</span>

<span class="sd">        MS_SERVER_NUM: Server number</span>

<span class="sd">        MS_WORKER_NUM: Worker number</span>

<span class="sd">        MS_SCHED_HOST: Scheduler IP address</span>

<span class="sd">        MS_SCHED_PORT: Scheduler port</span>

<span class="sd">        MS_ROLE: The role of this process:</span>

<span class="sd">        MS_SCHED: represents the scheduler,</span>

<span class="sd">        MS_WORKER: represents the worker,</span>

<span class="sd">        MS_PSERVER/MS_SERVER: represents the Server</span>

<span class="sd">    Args:</span>
<span class="sd">        enable_ps (bool): Whether to enable parameter server training mode.</span>
<span class="sd">                          Only after enable_ps is set True, the environment variables will be effective.</span>
<span class="sd">                          Default: False.</span>
<span class="sd">        config_file_path (string): Configuration file path used by recovery, parameter server training mode only</span>
<span class="sd">                                   supports Server disaster recovery currently. Default: &#39;&#39;.</span>
<span class="sd">        scheduler_manage_port (int): Scheduler manage port used to scale out/in. Default: 11202.</span>
<span class="sd">        enable_ssl (bool): Set PS SSL mode enabled or disabled. Default: False.</span>
<span class="sd">        client_password (str): Password to decrypt the secret key stored in the client certificate. Default: &#39;&#39;.</span>
<span class="sd">        server_password (str): Password to decrypt the secret key stored in the server certificate. Default: &#39;&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not the attribute in parameter server training mode context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import context</span>
<span class="sd">        &gt;&gt;&gt; context.set_ps_context(enable_ps=True, enable_ssl=True, client_password=&#39;123456&#39;, server_password=&#39;123456&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_ps_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_ps_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get parameter server training mode context attribute value according to the key.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute:</span>

<span class="sd">            - enable_ps (bool): Whether to enable parameter server training mode.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns attribute value according to the key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in auto parallel context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from mindspore import context</span>
<span class="sd">        &gt;&gt;&gt; context.get_ps_context(&quot;enable_ps&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_ps_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reset_ps_context</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reset parameter server training mode context attributes to the default values:</span>

<span class="sd">    - enable_ps: False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_reset_ps_context</span><span class="p">()</span>


<div class="viewcode-block" id="set_fl_context"><a class="viewcode-back" href="../../federated_server.html#mindspore.context.set_fl_context">[docs]</a><span class="k">def</span> <span class="nf">set_fl_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Set federated learning training mode context.</span>

<span class="sd">    Args:</span>
<span class="sd">        enable_fl (bool): Whether to enable federated learning training mode.</span>
<span class="sd">                          Default: False.</span>
<span class="sd">        server_mode (str): Describe the server mode, which must one of &#39;FEDERATED_LEARNING&#39; and &#39;HYBRID_TRAINING&#39;.</span>
<span class="sd">                              Default: &#39;FEDERATED_LEARNING&#39;.</span>
<span class="sd">        ms_role (str): The process&#39;s role in the federated learning mode,</span>
<span class="sd">                          which must be one of &#39;MS_SERVER&#39;, &#39;MS_WORKER&#39; and &#39;MS_SCHED&#39;.</span>
<span class="sd">                          Default: &#39;MS_SERVER&#39;.</span>
<span class="sd">        worker_num (int): The number of workers. For current version, this must be set to 1 or 0.</span>
<span class="sd">        server_num (int): The number of federated learning servers. Default: 0.</span>
<span class="sd">        scheduler_ip (str): The scheduler IP. Default: &#39;0.0.0.0&#39;.</span>
<span class="sd">        scheduler_port (int): The scheduler port. Default: 6667.</span>
<span class="sd">        fl_server_port (int): The http port of the federated learning server.</span>
<span class="sd">                              Normally for each server this should be set to the same value. Default: 6668.</span>
<span class="sd">        enable_fl_client (bool): Whether this process is federated learning client. Default: False.</span>
<span class="sd">        start_fl_job_threshold (int): The threshold count of startFLJob. Default: 1.</span>
<span class="sd">        start_fl_job_time_window (int): The time window duration for startFLJob in millisecond. Default: 3000.</span>
<span class="sd">        share_secrets_ratio (float): The ratio for computing the threshold count of share secrets. Default: 1.0.</span>
<span class="sd">        update_model_ratio (float): The ratio for computing the threshold count of updateModel. Default: 1.0.</span>
<span class="sd">        cipher_time_window (int): The time window duration for each cipher round in millisecond. Default: 300000.</span>
<span class="sd">        reconstruct_secrets_threshold (int): The threshold count of reconstruct threshold. Default: 0.</span>
<span class="sd">        update_model_time_window (int): The time window duration for updateModel in millisecond. Default: 3000.</span>
<span class="sd">        fl_name (string): The federated learning job name. Default: &#39;&#39;.</span>
<span class="sd">        fl_iteration_num (int): Iteration number of federated learning,</span>
<span class="sd">                                which is the number of interactions between client and server. Default: 20.</span>
<span class="sd">        client_epoch_num (int): Client training epoch number. Default: 25.</span>
<span class="sd">        client_batch_size (int): Client training data batch size. Default: 32.</span>
<span class="sd">        client_learning_rate (float): Client training learning rate. Default: 0.001.</span>
<span class="sd">        worker_step_num_per_iteration (int): The worker&#39;s standalone training step number before communicating with</span>
<span class="sd">                                             server. Default: 65.</span>
<span class="sd">        dp_eps (float): Epsilon budget of differential privacy mechanism. The smaller the dp_eps, the better the</span>
<span class="sd">            privacy protection effect. Default: 50.0.</span>
<span class="sd">        dp_delta (float): Delta budget of differential privacy mechanism, which is usually equals the reciprocal of</span>
<span class="sd">            client number. The smaller the dp_delta, the better the privacy protection effect. Default: 0.01.</span>
<span class="sd">        dp_norm_clip (float): A factor used for clipping model&#39;s weights for differential mechanism. Its value is</span>
<span class="sd">            suggested to be 0.5~2. Default: 1.0.</span>
<span class="sd">        encrypt_type (string): Secure schema for federated learning, which can be &#39;NOT_ENCRYPT&#39;, &#39;DP_ENCRYPT&#39;,</span>
<span class="sd">            &#39;PW_ENCRYPT&#39;, &#39;STABLE_PW_ENCRYPT&#39; or &#39;SIGNDS&#39;. If &#39;DP_ENCRYPT&#39;, differential privacy schema would be applied</span>
<span class="sd">            for clients and the privacy protection effect would be determined by dp_eps, dp_delta and dp_norm_clip</span>
<span class="sd">            as described above. If &#39;PW_ENCRYPT&#39;, pairwise secure aggregation would be applied to protect clients&#39;</span>
<span class="sd">            model from stealing in cross-device scenario. If &#39;STABLE_PW_ENCRYPT&#39;, pairwise secure aggregation would</span>
<span class="sd">            be applied to protect clients&#39; model from stealing in cross-silo scenario. If &#39;SIGNDS&#39;, SignDS schema would</span>
<span class="sd">            be applied for clients. Default: &#39;NOT_ENCRYPT&#39;.</span>
<span class="sd">        sign_k (float): SignDS: Top-k ratio, namely the number of top-k dimensions divided by the total number of</span>
<span class="sd">            dimensions. Default: 0.01.</span>
<span class="sd">        sign_eps (float): SignDS: Privacy budget. Default: 100.</span>
<span class="sd">        sign_thr_ratio (float): SignDS: Threshold of the expected topk dimension. Default: 0.6.</span>
<span class="sd">        sign_global_lr (float): SignDS: The constant value assigned to the selected dimension. Default: 1.</span>
<span class="sd">        sign_dim_out (int): SignDS: Number of output dimensions. Default: 0.</span>
<span class="sd">        config_file_path (string): Configuration file path used by recovery. Default: &#39;&#39;.</span>
<span class="sd">        scheduler_manage_port (int): scheduler manage port used to scale out/in. Default: 11202.</span>
<span class="sd">        enable_ssl (bool): Set PS SSL mode enabled or disabled. Default: False.</span>
<span class="sd">        client_password (str): Password to decrypt the secret key stored in the client certificate. Default: &#39;&#39;.</span>
<span class="sd">        server_password (str): Password to decrypt the secret key stored in the server certificate. Default: &#39;&#39;.</span>
<span class="sd">        pki_verify (bool): If True, the identity verification between server and clients would be turned on.</span>
<span class="sd">            You should also download Root CA certificate, Root CA G2 certificate and Mobile Equipment CRL certificate</span>
<span class="sd">            from https://pki.consumer.huawei.com/ca/. It should be noted that only when the client is an Android</span>
<span class="sd">            environment with HUKS service, pki_verify can be True. Default: False.</span>
<span class="sd">        root_first_ca_path (str): The file path of the Root CA certificate. It should be given when pki_verify</span>
<span class="sd">            is True. Default: &quot;&quot;.</span>
<span class="sd">        root_second_ca_path (str): The file path of the Root CA G2 certificate. It should be given when</span>
<span class="sd">            pki_verify is True. Default: &quot;&quot;.</span>
<span class="sd">        equip_crl_path (str): The file path of the Mobile Equipment CRL certificate. It should be given when</span>
<span class="sd">            pki_verify is True. Default: &quot;&quot;.</span>
<span class="sd">        replay_attack_time_diff (int): The maximum tolerable error of certificate timestamp verification (ms).</span>
<span class="sd">            Default: 600000.</span>
<span class="sd">        http_url_prefix (string): The http url prefix for http server.</span>
<span class="sd">            Default: &quot;&quot;.</span>
<span class="sd">        global_iteration_time_window (unsigned long): The global iteration time window for one iteration</span>
<span class="sd">            with rounds(ms). Default: 3600000.</span>
<span class="sd">        checkpoint_dir (string): The Server model checkpoint directory. If no checkpoint dir is set,</span>
<span class="sd">            the startup script directory is used by default. Default: &quot;&quot;.</span>
<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not the attribute in federated learning mode context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; context.set_fl_context(enable_fl=True, server_mode=&#39;FEDERATED_LEARNING&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_set_ps_context</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="get_fl_context"><a class="viewcode-back" href="../../federated_server.html#mindspore.context.get_fl_context">[docs]</a><span class="k">def</span> <span class="nf">get_fl_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get federated learning mode context attribute value according to the key.</span>

<span class="sd">    Args:</span>
<span class="sd">        attr_key (str): The key of the attribute.</span>
<span class="sd">                        Please refer to `set_fl_context`&#39;s parameters to decide what key should be passed.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Returns attribute value according to the key.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If input key is not attribute in federated learning mode context.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; context.get_fl_context(&quot;server_mode&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_get_ps_context</span><span class="p">(</span><span class="n">attr_key</span><span class="p">)</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>