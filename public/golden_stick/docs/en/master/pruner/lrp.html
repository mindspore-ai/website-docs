<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LRP Head Pruning &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="LRP Deployment tutorial" href="lrp_tutorial.html" />
    <link rel="prev" title="Applying the SCOP Algorithm" href="scop.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation and Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing MindSpore Golden Stick</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quantization Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantization/overview.html">Quantization Algorithm Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization/simqat.html">Applying the SimQAT Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization/slb.html">Applying the SLB Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Pruning Algorithms</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Pruning Algorithm Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="scop.html">Applying the SCOP Algorithm</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">LRP Head Pruning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pruning-attention-heads">Pruning Attention Heads</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lrp-method">LRP method</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#l0-penalty-graph"><span class="math notranslate nohighlight">\(L\)</span>0_penalty graph</a></li>
<li class="toctree-l4"><a class="reference internal" href="#link-to-the-article">Link to the article</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#headpruning-api">HeadPruning API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#train-gated-model">Train Gated Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#run-sample">Run sample</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#bert-on-mnli">BERT on MNLI</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="lrp_tutorial.html">LRP Deployment tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/overview.html">Model Deployment Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/convert.html">MindSpore Golden Stick Network Conversion</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.html">mindspore_gs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.quantization.html">mindspore_gs.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.pruner.html">mindspore_gs.pruner</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>LRP Head Pruning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/pruner/lrp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="lrp-head-pruning">
<h1>LRP Head Pruning<a class="headerlink" href="#lrp-head-pruning" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/golden_stick/docs/source_en/pruner/lrp.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<p>MindSpore Golden Stick LRP module is an implementation of the LRP method for head pruning first presented in the article
<a class="reference external" href="https://www.aclweb.org/anthology/P19-1580">Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a> and explained below.</p>
<p>For a complete tutorial of how to use this module please refer to the <a class="reference internal" href="lrp_tutorial.html"><span class="doc std std-doc">tutorial page</span></a>.</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<section id="pruning-attention-heads">
<h3>Pruning Attention Heads<a class="headerlink" href="#pruning-attention-heads" title="Permalink to this headline"></a></h3>
<p>The attention mechanism is a crucial component of the transformer architecture.
Different studies have shown that most attention heads are not confident in their decisions and can be pruned.
Since then, several algorithms have been proposed on how to intelligently prune attention heads.</p>
</section>
<section id="lrp-method">
<h3>LRP method<a class="headerlink" href="#lrp-method" title="Permalink to this headline"></a></h3>
<p>The LRP method starts from a converged model that was not necessarily specially trained to support head pruning.
Then, it fine-tunes the model to support head pruning, and at the end of the fine-tuning phase some heads are pruned.</p>
<p>During the fine-tuning, the original Transformer architecture is modified by multiplying the representation computed by each head with a scalar gate.
That is, when moving from the standard Transformer to the modified one, the representation computed by each <span class="math notranslate nohighlight">\({\rm head}_i\)</span> is multiplied by a scalar gate <span class="math notranslate nohighlight">\(g_i\)</span> before being concatenated with results of other attention heads in a layer:
<span class="math notranslate nohighlight">\({\rm MultiHead}(Q, K, V) = {\rm Concat}_i({\rm head}_i)W^O \rightarrow {\rm Concat}_i(g_i*{\rm head}_i)W^O \)</span></p>
<p>Note, <span class="math notranslate nohighlight">\(g_i\)</span> are parameters specific to heads and are independent of the input (i.e. the sentence).</p>
<p>Ideally, the values of the gates are either 1 (“open”) or 0 (“closed”), and the sparsity of the gates is encouraged by applying <span class="math notranslate nohighlight">\(L0\)</span> regularization to the scalar gates <span class="math notranslate nohighlight">\(g_i\)</span>.
The <span class="math notranslate nohighlight">\(L0\)</span> norm equals the number of non-zero components and would push the model to switch off less important heads.</p>
<p>Unfortunately, a direct implementation of this idea leads to an objective function that is non-differentiable with respect to the added gates.
So instead, the algorithm uses a stochastic relaxation in which each gate is modeled as a continuous random variable drawn independently of a head-specific
<a class="reference external" href="https://openreview.net/pdf?id=H1Y8hhg0b">Hard Concrete distribution</a>. The distributions have non-zero probability mass at 0 and 1; look at the illustration.</p>
<p><img alt="concrete" src="../_images/concrete_crop.gif" /></p>
<p>The non-differentiable <span class="math notranslate nohighlight">\(L0\)</span> norm is replaced by the sum of the probabilities of heads being non-zero (<span class="math notranslate nohighlight">\(L_C\)</span>) as a stochastic relaxation,
and the resulting training objective is</p>
<p><span class="math notranslate nohighlight">\(L = L_{xent} + \lambda L_C\)</span></p>
<p>By varying the coefficient <span class="math notranslate nohighlight">\(\lambda\)</span> in the optimized objective, we obtain models with different numbers of retained heads. Below is shown how the probabilities of encoder heads being completely closed <span class="math notranslate nohighlight">\((P(g_i)=0)\)</span> change in training for different values of <span class="math notranslate nohighlight">\(\lambda\)</span> (pruning starts from a converged model). 
White color denotes <span class="math notranslate nohighlight">\(P(g_i=0) = 1\)</span>, which means that a head is completely removed from the model.</p>
<p><img alt="enc_head_gif" src="../_images/enc_head_gif_delay7-min.gif" /></p>
<p>(Gif is for the model trained on EN-RU WMT. For other datasets, values of <span class="math notranslate nohighlight">\(\lambda\)</span> can be different.)</p>
<p>Empirically, the model converges to solutions where gates are either almost completely closed or completely open.
This means that at test time we can treat the model as a standard Transformer and use only a subset of heads.</p>
<p>Overall, there are 3 main parameters that the user needs to know in order to get a general understanding of the algorithm backstage:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">l0_penalty</span></code></p>
<ul>
<li><p>The regularization coefficient through which we control the number of pruned heads</p></li>
<li><p>The larger the coefficient is, the more heads are pruned</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code></p>
<ul>
<li><p>Controls the relaxation of the ideal objective function</p></li>
<li><p>As the temperature decreases, the degree of approximation improves (at the expense of the derivatives’ stability)</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_a</span></code></p>
<ul>
<li><p>A per head parameter indicating the “openness” of the gate associated with the head</p></li>
<li><p>These parameters are learned by the algorithm in the fine-tuning phase (unlike the previous two parameters that are set by the user)</p></li>
</ul>
</li>
</ul>
<section id="l0-penalty-graph">
<h4><span class="math notranslate nohighlight">\(L\)</span>0_penalty graph<a class="headerlink" href="#l0-penalty-graph" title="Permalink to this headline"></a></h4>
<p>(From Differentiable Subset Pruning of Transformer Heads article)</p>
<p><img alt="l0_graph" src="../_images/l0_graph.png" /></p>
</section>
<section id="link-to-the-article">
<h4>Link to the article<a class="headerlink" href="#link-to-the-article" title="Permalink to this headline"></a></h4>
<p><a class="reference external" href="https://arxiv.org/abs/2108.04657">Differentiable Subset Pruning of Transformer Heads</a></p>
</section>
</section>
</section>
<section id="headpruning-api">
<h2>HeadPruning API<a class="headerlink" href="#headpruning-api" title="Permalink to this headline"></a></h2>
<p>Our headPruning algorithm is integrated within the standard training loop,
and returns a pruned model.</p>
<p><strong>Create Dataset:</strong></p>
<p>Create an NLP Dataset for your model.</p>
<p><strong>Create optimizer:</strong></p>
<p>The model parameter ‘log_a’ (which is introduced by the algorithm) should be defined with a high learning rate.</p>
<p>To define a different learning rate especially for this parameter, use the following script</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>params = network.trainable_params()
pruning_params = list(filter(lambda x: &#39;log_a&#39; in x.name, params))
other_params = list(filter(lambda x: &#39;log_a&#39; not in x.name, params))
group_params = [{&#39;params&#39;: other_params},
                {&#39;params&#39;: pruning_params, &#39;lr&#39;: 0.1},
                {&#39;order_params&#39;: params}]
</pre></div>
</div>
<p><strong>Load Model:</strong></p>
<p>Load trained model from mindspore.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bert_model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">(</span><span class="n">bert_net_cfg</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Create HeadPruner:</strong></p>
<p>Initiate HeadPruner</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_gs.prune_heads</span> <span class="kn">import</span> <span class="n">HeadPruningFactory</span><span class="p">,</span> <span class="n">PruningType</span><span class="p">,</span> <span class="n">SupportedModels</span>

<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;config&#39;</span><span class="p">:</span> <span class="n">bert_net_config</span><span class="p">}</span>

<span class="n">l0_penalty</span> <span class="o">=</span> <span class="mf">0.0015</span>
<span class="n">head_pruner_config</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;model_params&#39;</span><span class="p">:</span> <span class="n">model_params</span><span class="p">,</span>
                      <span class="s1">&#39;l0_penalty&#39;</span><span class="p">:</span> <span class="n">l0_penalty</span> <span class="p">}</span>

<span class="n">factory_dic</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;prune_type&quot;</span><span class="p">:</span> <span class="n">PruningType</span><span class="o">.</span><span class="n">LRP</span><span class="p">,</span>
               <span class="s2">&quot;arch_type&quot;</span><span class="p">:</span> <span class="n">SupportedModels</span><span class="o">.</span><span class="n">BERT</span><span class="p">,</span>
               <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="n">head_pruner_config</span><span class="p">}</span>

<span class="n">pruner</span> <span class="o">=</span> <span class="n">HeadPruningFactory</span><span class="o">.</span><span class="n">get_pruner</span><span class="p">(</span><span class="n">pruner_config</span><span class="p">)</span>
</pre></div>
</div>
<p>The parameters required for HeadPruner initialization are:</p>
<ul class="simple">
<li><p>prune_type (Enum) – Type of pruning method.</p></li>
<li><p>arch_type (Enum) – Type of model to prune</p></li>
<li><p>config (Dictionary) - Head pruner config, contain:</p>
<ul>
<li><p>model_params (Dictionary) - Parameters of model.</p></li>
<li><p>l0_penalty (float) – penalty value for gate calculation.</p></li>
</ul>
</li>
</ul>
<p><strong>Apply model:</strong></p>
<p>Repackage the model with additional functionality that supports the pruning.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bert_model_gated</span> <span class="o">=</span> <span class="n">pruner</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">bert_model</span><span class="p">)</span>
</pre></div>
</div>
<p>The function input is:</p>
<ul class="simple">
<li><p>model - model to prune, possible with/without head.</p></li>
</ul>
<section id="train-gated-model">
<h3>Train Gated Model<a class="headerlink" href="#train-gated-model" title="Permalink to this headline"></a></h3>
<p>For the fine-tuning of the model, we add a penalty factor to the loss expression</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">sequence_output</span><span class="p">,</span> <span class="n">pooled_output</span><span class="p">,</span> <span class="n">embedding_table</span><span class="p">,</span> <span class="n">total_reg</span> <span class="o">=</span> \
                                <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">token_type_id</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">)</span>
 <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
 <span class="n">loss</span> <span class="o">+=</span> <span class="n">total_reg</span>
</pre></div>
</div>
<p>Train the model between 1-3 epoch.</p>
<p><strong>Get mask:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">mask</span> <span class="o">=</span> <span class="n">bert_model_gated</span><span class="o">.</span><span class="n">get_gate_values</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Prune model:</strong></p>
<p>After training / fine-tuning, prune the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">prune_model</span> <span class="o">=</span> <span class="n">pruner</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">bert_model_gated</span><span class="p">)</span>
</pre></div>
</div>
<p>The function has an option to get another parameter:</p>
<ul class="simple">
<li><p>save_dir_path (string) - path for saving the model before pruning, and pickle file with heads to prune dictionary.</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> save_dir_path = &#39;...&#39;
 prune_model = pruner.convert(bert_model_gated, save_dir_path)
</pre></div>
</div>
</section>
</section>
<section id="run-sample">
<h2>Run sample<a class="headerlink" href="#run-sample" title="Permalink to this headline"></a></h2>
<section id="bert-on-mnli">
<h3>BERT on MNLI<a class="headerlink" href="#bert-on-mnli" title="Permalink to this headline"></a></h3>
<p>We provide a code example for LRP pruning of Bert model in <a class="reference external" href="https://gitee.com/mindspore/golden-stick/blob/master/mindspore_gs/pruner/heads/lrp/bert/samples/run_sample_bert.py">run_sample_bert.py</a> file. It can be run using the following shell command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">DEVICE_TARGET</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;GPU&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DATA_DIR</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>data/MNLI/
<span class="nb">export</span><span class="w"> </span><span class="nv">TRAINING_MODEL_CKPT</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;...&quot;</span>
python<span class="w"> </span>run_sample_bert.py<span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--device_target<span class="w"> </span><span class="nv">$DEVICE_TARGET</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--distribute<span class="w"> </span><span class="s2">&quot;false&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--epoch_size<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--enable_save_ckpt<span class="w"> </span><span class="s2">&quot;false&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--enable_lossscale<span class="w"> </span><span class="s2">&quot;true&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--do_shuffle<span class="w"> </span><span class="s2">&quot;true&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--enable_data_sink<span class="w"> </span><span class="s2">&quot;true&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--data_sink_steps<span class="w"> </span><span class="m">20</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--load_checkpoint_path<span class="w"> </span><span class="nv">$TRAINING_MODEL_CKPT</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--save_checkpoint_path<span class="w"> </span><span class="s2">&quot;&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--save_checkpoint_steps<span class="w"> </span><span class="m">10000</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--save_checkpoint_num<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">          </span>--data_dir<span class="w"> </span><span class="nv">$DATA_DIR</span><span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="scop.html" class="btn btn-neutral float-left" title="Applying the SCOP Algorithm" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lrp_tutorial.html" class="btn btn-neutral float-right" title="LRP Deployment tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>