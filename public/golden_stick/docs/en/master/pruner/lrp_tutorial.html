<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LRP Deployment tutorial &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Model Deployment Overview" href="../deployment/overview.html" />
    <link rel="prev" title="LRP Head Pruning" href="lrp.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation and Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing MindSpore Golden Stick</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quantization Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantization/overview.html">Quantization Algorithm Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization/simqat.html">Applying the SimQAT Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization/slb.html">Applying the SLB Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Pruning Algorithms</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Pruning Algorithm Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="scop.html">Applying the SCOP Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="lrp.html">LRP Head Pruning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">LRP Deployment tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#provided-classes">Provided Classes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#concrete-gate-class">Concrete-Gate Class</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pruner-classes">Pruner class(es)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#additions-to-the-original-model">Additions to the Original Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-flow-within-the-model">Data Flow Within the Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#attention-level-class">Attention Level Class</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#intermediate-level-classes">Intermediate Level Classes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#loss-level-class">Loss Level Class</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-control">Model Control</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/overview.html">Model Deployment Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/convert.html">MindSpore Golden Stick Network Conversion</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.html">mindspore_gs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.quantization.html">mindspore_gs.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.pruner.html">mindspore_gs.pruner</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>LRP Deployment tutorial</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/pruner/lrp_tutorial.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="lrp-deployment-tutorial">
<h1>LRP Deployment tutorial<a class="headerlink" href="#lrp-deployment-tutorial" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/golden_stick/docs/source_en/pruner/lrp_tutorial.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>The LRP algorithm performs head pruning by:</p>
<ol class="arabic simple">
<li><p>Adding gate modules to a model</p></li>
<li><p>Fine-tuning the model where a penalty term is added to the original loss term</p></li>
<li><p>At the end of the training, hardening the gates or pruning the heads (according to the learned values of the gates)</p></li>
</ol>
<p>To support this scheme, we define two new classes and insert few additions to the original model.</p>
</section>
<section id="provided-classes">
<h2>Provided Classes<a class="headerlink" href="#provided-classes" title="Permalink to this headline"></a></h2>
<p>We introduce two main new classes. The first class modules the gates and is planted into the model itself. The second is in fact a main class that uses several other classes to prune heads of a gated fine-tuned model, according to the gates values.</p>
<section id="concrete-gate-class">
<h3>Concrete-Gate Class<a class="headerlink" href="#concrete-gate-class" title="Permalink to this headline"></a></h3>
<p><em><strong>The Concrete–Gate class is a class that we provide and needs no adjustments when the model to be pruned is replaced. The information below is provided only for the purpose of understanding the changes and additions to be made elsewhere in the code.</strong></em></p>
<p>Conceptually a gate is a simple scalar that determines whether to let an input keep flowing in the neural network, or not. Yet, the list of features we want the gate to have (e.g., being distinctly either closed or open during evaluation mode, being differentiable during training mode, encouraged to be zero during training), requires building a whole module around these scalars. The name of the gate module class is “ConcreteGate”,</p>
<p>A single Concrete-Gate class is in charge of all the gates within a single Attention mechanism. Besides the initialization method, it has the following three methods:</p>
<ul class="simple">
<li><p>get_gates</p>
<ul>
<li><p>Returns the gates’ scalar values, given the operation mode (training/evaluation)</p></li>
</ul>
</li>
<li><p>construct</p>
<ul>
<li><p>Given an input and, optionally, the operating mode, apply the gates on the input (using the method get_gates)</p></li>
</ul>
</li>
<li><p>get_penalty</p>
<ul>
<li><p>calculating a penalty term to be added to the final loss</p></li>
</ul>
</li>
</ul>
<p>ConcreteGate code from <a class="reference external" href="https://gitee.com/mindspore/golden-stick/blob/master/mindspore_gs/pruner/heads/lrp/utils.py">utils.py</a> file as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="k">class</span> <span class="nc">ConcreteGate</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">stretch_limits</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">),</span>
                 <span class="n">l0_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConcreteGate</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stretch_limits</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">stretch_limits</span><span class="p">,</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l0_penalty</span> <span class="o">=</span> <span class="n">l0_penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_a</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">initializer</span><span class="p">(</span><span class="s1">&#39;xavier_uniform&#39;</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;log_a&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">Log</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">op</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">ReduceSum</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">is_train</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">is_train</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">if</span> <span class="n">is_train</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">is_train</span>
        <span class="n">gates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_gates</span><span class="p">(</span><span class="n">is_train</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">values</span> <span class="o">*</span> <span class="n">gates</span>

    <span class="k">def</span> <span class="nf">get_gates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">is_train</span><span class="p">):</span>
        <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stretch_limits</span>

        <span class="k">if</span> <span class="n">is_train</span><span class="p">:</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_a</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">uniformReal</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">UniformReal</span><span class="p">()</span>
            <span class="n">noise</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">uniformReal</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span>
            <span class="n">concrete</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">noise</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_a</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">concrete</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_a</span><span class="p">)</span>

        <span class="n">stretched_concrete</span> <span class="o">=</span> <span class="n">concrete</span> <span class="o">*</span> <span class="p">(</span><span class="n">high</span> <span class="o">-</span> <span class="n">low</span><span class="p">)</span> <span class="o">+</span> <span class="n">low</span>
        <span class="n">clipped_concrete</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">stretched_concrete</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">clipped_concrete</span>

    <span class="k">def</span> <span class="nf">get_penalty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stretch_limits</span>
        <span class="k">assert</span> <span class="n">low</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s2">&quot;p_gate_closed can be computed only if lower stretch limit is negative&quot;</span>
        <span class="c1"># compute p(gate_is_closed) = cdf(stretched_sigmoid &lt; 0)</span>
        <span class="n">p_open</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_a</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Tensor</span><span class="p">(</span><span class="o">-</span><span class="n">low</span><span class="p">)</span> <span class="o">/</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">high</span><span class="p">)))</span>
        <span class="n">p_open</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">p_open</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

        <span class="n">total_reg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l0_penalty</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">op</span><span class="p">(</span><span class="n">p_open</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">total_reg</span>
</pre></div>
</div>
</section>
<section id="pruner-classes">
<h3>Pruner class(es)<a class="headerlink" href="#pruner-classes" title="Permalink to this headline"></a></h3>
<p><em><strong>Unlike the Concrete-Gate class, which is agnostic to the model, the pruner class is defined slightly different for each model.</strong></em></p>
<p>The pruning is executed as follows:</p>
<ol class="arabic simple">
<li><p>A model to prune and pruning configuration are received</p></li>
<li><p>A gated version of the model (that is a model that includes and supports gates) is generated, and the variables values of the original model are copied to the gated model</p></li>
<li><p>The gated model is trained</p></li>
<li><p>The variables values of the gated model are copied to the original model, and some heads of the original model are pruned according to the values of the trained gates.</p></li>
</ol>
<p>The pruner class purpose is to supports these actions. These actions require reaching different blocks in the model, and since different models concatenating blocks differently and named them differently, we need to write different pruner class for each model.</p>
<p>A pruner class inherits the class “AbstructHeadPrunerLRP”, which is a class that we provide. Besides the initialization method (that is performed by the “AbstractHeadPrunerLRP”), the pruner has three more methods that are written differently depending on the model one wants to prune:</p>
<ul class="simple">
<li><p>init_head</p>
<ul>
<li><p>The pruner is initialized with a model to prune and stored it in the attribute “original_model”. The method init_head rearrange the attribute “original_model” and adds a “head” attribute, such that “head” contains the full model and “original_model” contains only the backbone part. Additionally, the method set the value of the Boolean attribute “has_head” to be True if the received model is a full model, and False if the received model is only the backbone part - both cases are supported.</p></li>
</ul>
</li>
<li><p>decorate_model</p>
<ul>
<li><p>The method creates and returns the gated version of the model (with weights copied from the received model).</p></li>
</ul>
</li>
<li><p>get_penalty</p>
<ul>
<li><p>The method receives a trained gated model and use it to correct and output the model stored in the class. The correction is done by coping gated model variables values, and pruning heads that are needed to be pruned.</p></li>
</ul>
</li>
</ul>
<p>We provide two examples of this class - “HeadPrunerBertLRP” that is designed to prune Bert model, and “HeadPrunerGPTLRP” that is designed to prune GPT model.</p>
</section>
</section>
<section id="additions-to-the-original-model">
<h2>Additions to the Original Model<a class="headerlink" href="#additions-to-the-original-model" title="Permalink to this headline"></a></h2>
<p>As implied by the description of the pruner class, we need to form a class that is the gated version of the model to prune class. This is done by taking the classes that compose the original model, and insert some additions.</p>
<p>To ease the review of the added code lines to the original model, we divide the additions into two groups – additions related to the normal flow of the data in the model, and additions that enables controlling and changing the model.</p>
<section id="data-flow-within-the-model">
<h3>Data Flow Within the Model<a class="headerlink" href="#data-flow-within-the-model" title="Permalink to this headline"></a></h3>
<p>The abilities we want to the add to the original model are:</p>
<ul class="simple">
<li><p>Applying gates and calculate contribution to the penalty term at the Attention level</p></li>
<li><p>Transferring and accumulating the penalty terms from the Attention level to the level where the loss is calculated, through the concatenated and nested classes that are the building blocks of the model.</p></li>
<li><p>Adding the accumulated penalty term to the loss</p></li>
</ul>
<p>To enable these abilities, we distinguish between three types of classes – the class that calculate the Attention, the class that calculate the loss, and classes that connect between these two above-mentioned classes</p>
<section id="attention-level-class">
<h4>Attention Level Class<a class="headerlink" href="#attention-level-class" title="Permalink to this headline"></a></h4>
<p>We introduce two attributes to the class - a “ConcreteGate” class type attribute that is named “gate”, and a Boolean type attribute named “has_gate”. The first is obviously the added gate, and the second is an indicator of whether to address or ignore the gate. These two attributes are defined in the <strong>init</strong> method of the class</p>
<p>Code example from <a class="reference external" href="https://gitee.com/mindspore/golden-stick/blob/master/mindspore_gs/pruner/heads/lrp/bert/archs/gated_bert_model.py">gated_bert_model.py</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BertAttention</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">has_gate</span> <span class="o">=</span> <span class="kc">False</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;Creation of the gate calculation object&#39;&#39;&#39;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="o">=</span> <span class="n">ConcreteGate</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<p>Another example is given for GPT model in <a class="reference external" href="https://gitee.com/mindspore/golden-stick/blob/master/mindspore_gs/pruner/heads/lrp/gpt/archs/gated_transformer.py">gated_transformer.py</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MultiHeadAttention</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;Creation of the gate calculation object&#39;&#39;&#39;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gate</span> <span class="o">=</span> <span class="n">ConcreteGate</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">has_gates</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>While performing the forward path of the class, if the gates are enabled, we apply the gates and calculate the contribution of the Attention to the penalty term. These two actions are performed just before we multiply the Attention probabilities by the “values vectors” of the Attention mechanism</p>
<p>Code example from BertAttention forward function in <a class="reference external" href="https://gitee.com/mindspore/golden-stick/blob/master/mindspore_gs/pruner/heads/lrp/bert/archs/gated_bert_model.py">gated_bert_model.py</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BertAttention</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>
    <span class="o">.</span>
    <span class="o">.</span>
    <span class="n">reg</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_gates</span><span class="p">:</span>
        <span class="n">attention_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="p">(</span><span class="n">attention_probs</span><span class="p">)</span>
        <span class="n">reg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate</span><span class="o">.</span><span class="n">get_penalty</span><span class="p">()</span>
    <span class="o">.</span>
    <span class="o">.</span>
</pre></div>
</div>
<p>Finally, we need to output the calculated the penalty term so it could climb up to the loss level</p>
<p>Code example from BertAttention forward function in <a class="reference external" href="https://gitee.com/mindspore/golden-stick/blob/master/mindspore_gs/pruner/heads/lrp/bert/archs/gated_bert_model.py">gated_bert_model.py</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BertAttention</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>
    <span class="o">.</span>
    <span class="o">.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_gates</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reg</span><span class="p">,)</span>
    <span class="o">.</span>
    <span class="o">.</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="intermediate-level-classes">
<h2>Intermediate Level Classes<a class="headerlink" href="#intermediate-level-classes" title="Permalink to this headline"></a></h2>
<p>Intermediate level class are responsible for transferring and accumulating penalty terms. We distinguish between two types of such classes – classes that receive a single penalty term and only need to transfer it, and classes that receive more than one penalty term and need to accumulate them into a single term before the transfer.</p>
<p>Generally, in the forward path each class –</p>
<ol class="arabic simple">
<li><p>Receives a package of output variables from the classes to which it calls</p></li>
<li><p>Extract from the package variable that it needs to perform its forward path</p></li>
<li><p>Update and add variables to the package</p></li>
<li><p>Output the package</p></li>
</ol>
<p>Thus, a class that only transfers the penalty onward needs no additional code lines. On the other hand, class that accumulate the penalties needs additional code lines that execute actions 2 and 3, if the gated are enabled.</p>
<p>Overall, the second type classes need an attribute “has_gate”, similar to the Attention level class, and some additions to the forward path</p>
<p>Code example from BertTransformer in <a class="reference external" href="https://gitee.com/mindspore/golden-stick/blob/master/mindspore_gs/pruner/heads/lrp/bert/archs/gated_bert_model.py">gated_bert_model.py</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BertTransformer</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="o">.</span>
    <span class="o">.</span>
    <span class="n">selg</span><span class="o">.</span><span class="n">has_gates</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="o">.</span>
    <span class="o">.</span>

<span class="n">BertTransformer</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>
    <span class="o">.</span>
    <span class="o">.</span>
    <span class="n">total_reg</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">layer_module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_gates</span>
            <span class="n">total_reg</span> <span class="o">+=</span> <span class="n">layer_output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="o">.</span>
    <span class="o">.</span>
    <span class="k">return</span> <span class="n">all_encoder_layers</span><span class="p">,</span> <span class="n">total_reg</span>
</pre></div>
</div>
<section id="loss-level-class">
<h3>Loss Level Class<a class="headerlink" href="#loss-level-class" title="Permalink to this headline"></a></h3>
<p>The total_reg that comes back from the model is added to Loss calculation.</p>
<p>From Bert code example in <a class="reference external" href="https://gitee.com/mindspore/golden-stick/blob/master/mindspore_gs/pruner/heads/lrp/bert/samples/bert_pretrain_gates_sample.py">bert_pretrain_gates_sample.py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BertPreTrainingForGates</span><span class="o">.</span><span class="n">construct</span><span class="p">()</span>
    <span class="o">.</span>
    <span class="o">.</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">pooled_output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">total_reg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">token_type_id</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">)</span>
    <span class="o">.</span>
    <span class="o">.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_gates</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">total_reg</span>
</pre></div>
</div>
</section>
<section id="model-control">
<h3>Model Control<a class="headerlink" href="#model-control" title="Permalink to this headline"></a></h3>
<p>The pruning of the model is done according to the values of the trained gates. The values of the gates are stored at the Attention level, and to get them we need to transfer this request from the external class of the gated model down to the attention class. This is done using the functions get_gate_values
Another two functions that transfer instruction from the external class of the model down to its components are “apply_gates” and “remove_gates”, that enable and disable the gate using the attributes “has_gate” in the internal classes.</p>
<p>Code additions to BertAttention as can be seen in <a class="reference external" href="https://gitee.com/mindspore/golden-stick/blob/master/mindspore_gs/pruner/heads/lrp/bert/archs/gated_bert_model.py">gated_bert_model.py</a>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>class BertAttention
    .
    .
    def get_gate_value (self):
        gate_values = None
        if self.gate is not None:
            gate_values = self.gate.get_gates(False).flatten()
        return gate_values
    def apply_gates(self, l0_penalty):
        if not self.has_gates:
            self.has_gates = True
            self.gate.l0_penalty = l0_penalty
    def remove_gates(self):
        self.has_gates = False
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lrp.html" class="btn btn-neutral float-left" title="LRP Head Pruning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../deployment/overview.html" class="btn btn-neutral float-right" title="Model Deployment Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>