<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore_gs.quantization.SlbQuantAwareTraining &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore_gs.pruner" href="../mindspore_gs.pruner.html" />
    <link rel="prev" title="mindspore_gs.quantization.SimulatedQuantizationAwareTraining" href="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation and Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing MindSpore Golden Stick</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quantization Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Quantization Algorithm Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="simqat.html">Applying the SimQAT Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="slb.html">Applying the SLB Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Pruning Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pruner/overview.html">Pruning Algorithm Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pruner/scop.html">Applying the SCOP Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/overview.html">Model Deployment Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/convert.html">MindSpore Golden Stick Network Conversion</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.html">mindspore_gs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore_gs.quantization.html">mindspore_gs.quantization</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore_gs.quantization.html#simulated-quantization-algorithm">Simulated Quantization Algorithm</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore_gs.quantization.html#slb-algorithm">SLB Algorithm</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore_gs.quantization.SlbQuantAwareTraining</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.pruner.html">mindspore_gs.pruner</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore_gs.quantization.html">mindspore_gs.quantization</a> &raquo;</li>
      <li>mindspore_gs.quantization.SlbQuantAwareTraining</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quantization/mindspore_gs.quantization.SlbQuantAwareTraining.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="mindspore-gs-quantization-slbquantawaretraining">
<h1>mindspore_gs.quantization.SlbQuantAwareTraining<a class="headerlink" href="#mindspore-gs-quantization-slbquantawaretraining" title="Permalink to this headline"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore_gs.quantization.</span></span><span class="sig-name descname"><span class="pre">SlbQuantAwareTraining</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining" title="Permalink to this definition"></a></dt>
<dd><p>Implementation of slb quantization algorithm, this algorithm regards the discrete weights
in an arbitrary quantized neural network as searchable variables, and utilize a differential method
to search them accurately. In particular, each weight is represented as a probability distribution
over the discrete value set. The probabilities are optimized during training and the values
with the highest probability are selected to establish the desired quantized network.
See more details in <a class="reference external" href="https://arxiv.org/pdf/2009.08695.pdf">Searching for Low-Bit Weights in Quantized Neural
Networks</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method will call other set functions to set special values, please refer to the set function about the error.
For example, <cite>quant_dtype</cite> need refer to <cite>set_weight_quant_dtype</cite> and <cite>set_act_quant_dtype</cite>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – <p>store attributes for quantization aware training, keys are attribute names,
values are attribute values. Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>. Supported attribute are listed below:</p>
<ul class="simple">
<li><p>quant_dtype (Union[QuantDtype, list(QuantDtype), tuple(QuantDtype)]): Datatype used to quantize weights and
activations. The type is a QuantDtype, a list of two QuantDtype or a tuple of two QuantDtype. If quant_dtype is a
QuantDtype, it will be duplicated to a list of two QuantDtype. The first element represents the type of activations
and the second element represents the type of weights. It is necessary to consider the precision support of
hardware devices in the practical quantization infer scenaries. Weights quantization support int4|int2|int1,
and activations quantization support int8 now.
Default: <code class="docutils literal notranslate"><span class="pre">(QuantDtype.INT8,</span> <span class="pre">QuantDtype.INT1)</span></code>.</p></li>
<li><p>enable_act_quant (bool): Whether apply activation quantization while training.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>enable_bn_calibration (bool): Whether apply batchnorm calibration while training.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p>epoch_size (int): Total training epochs.</p></li>
<li><p>has_trained_epoch (int): The trained epochs.</p></li>
<li><p>t_start_val (float): Initial value of temperature hyperparameters. Default: <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p></li>
<li><p>t_start_time (float): Fraction of epochs after which temperature hyperparameters starting changing.
Default: <code class="docutils literal notranslate"><span class="pre">0.2</span></code>.</p></li>
<li><p>t_end_time (float): Fraction of epochs after which temperature hyperparameters stopping changing.
Default: <code class="docutils literal notranslate"><span class="pre">0.6</span></code>.</p></li>
<li><p>t_factor (float): Multiplicative factor of temperature hyperparameters changing.
Default: <code class="docutils literal notranslate"><span class="pre">1.2</span></code>.</p></li>
</ul>
</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>quant_dtype</cite> is not <cite>QuantDtype</cite>, or every element of <cite>quant_dtype</cite> is not <cite>QuantDtype</cite>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>enable_act_quant</cite> or <cite>enable_bn_calibration</cite> is not bool.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the length of <cite>quant_dtype</cite> is greater than 2.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>epoch_size</cite> or <cite>has_trained_epoch</cite> is not an int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>t_start_val</cite>, <cite>t_start_time</cite>, <cite>t_end_time</cite> or <cite>t_factor</cite> is not float.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>epoch_size</cite> is not greater than 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>has_trained_epoch</cite> is less than 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>t_start_val</cite> or <cite>t_factor</cite> is not greater than 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>t_start_time</cite> or <cite>t_end_time</cite> is less than 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>t_start_time</cite> or <cite>t_end_time</cite> is greater than 1.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore_gs.quantization</span> <span class="kn">import</span> <span class="n">SlbQuantAwareTraining</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore.common.dtype</span> <span class="kn">import</span> <span class="n">QuantDtype</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">NetToQuant</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">NetToQuant</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_channel</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 1) Define network to be quantized</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">NetToQuant</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 2) Define SLB QAT-Algorithm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span> <span class="o">=</span> <span class="n">SlbQuantAwareTraining</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3) Use set functions to change config</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3.1) set_weight_quant_dtype is used to set the weight quantization bit, and support QuantDtype.INT4, QuantDtype.INT2,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## QuantDtype.INT1 now.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span><span class="o">.</span><span class="n">set_weight_quant_dtype</span><span class="p">(</span><span class="n">QuantDtype</span><span class="o">.</span><span class="n">INT1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3.2) set_act_quant_dtype is used to set the activation quantization bit, and support QuantDtype.INT8 now.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span><span class="o">.</span><span class="n">set_act_quant_dtype</span><span class="p">(</span><span class="n">QuantDtype</span><span class="o">.</span><span class="n">INT8</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3.3) set_enable_act_quant is used to set whether apply activation quantization.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span><span class="o">.</span><span class="n">set_enable_act_quant</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3.4) set_enable_bn_calibration is used to set whether apply batchnorm calibration.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span><span class="o">.</span><span class="n">set_enable_bn_calibration</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3.5) set_epoch_size is used to set the epoch size of training.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span><span class="o">.</span><span class="n">set_epoch_size</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3.6) set_has_trained_epoch is used to set the trained epoch size of training.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span><span class="o">.</span><span class="n">set_has_trained_epoch</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3.7) set_t_start_val is used to set the initial value of temperature hyperparameters.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span><span class="o">.</span><span class="n">set_t_start_val</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3.8) set_t_start_time is used to set the fraction of epochs after which temperature hyperparameters starting changing.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span><span class="o">.</span><span class="n">set_t_start_time</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3.9) set_t_end_time is used to set the fraction of epochs after which temperature hyperparameters stopping changing.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span><span class="o">.</span><span class="n">set_t_end_time</span><span class="p">(</span><span class="mf">0.6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3.10) set_t_factor is used to set the multiplicative factor of temperature hyperparameters changing.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">slb_quantization</span><span class="o">.</span><span class="n">set_t_factor</span><span class="p">(</span><span class="mf">1.2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 4) Print SLB QAT-Algorithm object and check the config setting result</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set weight_quant_dtype to be QuantDtype.INT1, the value of the attribute weight_quant_dtype is INT1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set act_quant_dtype to be QuantDtype.INT8, the value of the attribute weight_quant_dtype is INT8</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set enable_act_quant to be True, the value of the attribute enable_act_quant is True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set enable_bn_calibration to be True, the value of the attribute enable_bn_calibration is True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set epoch_size to be 100, the value of the attribute epoch_size is 100</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set has_trained_epoch to be 0, the value of the attribute has_trained_epoch is 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set t_start_val to be 1.0, the value of the attribute t_start_val is 1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set t_start_time to be 0.2, the value of the attribute t_start_time is 0.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set t_end_time to be 0.6, the value of the attribute t_end_time is 0.6</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set t_factor to be 1.2, the value of the attribute t_factor is 1.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">slb_quantization</span><span class="p">)</span>
<span class="go">SlbQuantAwareTraining&lt;weight_quant_dtype=INT1, act_quant_dtype=INT8, enable_act_quant=True, enable_bn_calibration=True, epoch_size=100, has_trained_epoch=0, t_start_val=1.0, t_start_time=0.2, t_end_time=0.6, t_factor=1.2&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 5) Apply SLB QAT-algorithm to origin network</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_qat</span> <span class="o">=</span> <span class="n">slb_quantization</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 6) Print network and check the result. Conv2d should be transformed to QuantizeWrapperCells.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set weight_quant_dtype to be QuantDtype.INT1, the bit_num value of fake_quant_weight</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## should be 1, and the weight_bit_num value of Conv2dSlbQuant should be 1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">net_qat</span><span class="p">)</span>
<span class="go">NetToQuantOpt&lt;</span>
<span class="go">  (_handler): NetToQuant&lt;</span>
<span class="go">    (conv): Conv2d&lt;input_channels=1, output_channels=6, kernel_size=(5, 5), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW&gt;</span>
<span class="go">    (bn): BatchNorm2d&lt;num_features=6, eps=1e-05, momentum=0.9, gamma=Parameter(name=bn.gamma, requires_grad=True, shape=[6], dtype=Float32, value= [1., 1., 1., 1., 1., 1.]), beta=Parameter(name=bn.beta, requires_grad=True, shape=[6], dtype=Float32, value= [0., 0., 0., 0., 0., 0.]), moving_mean=Parameter(name=bn.moving_mean, requires_grad=False, shape=[6], dtype=Float32, value= [0., 0., 0., 0., 0., 0.]), moving_variance=Parameter(name=bn.moving_variance, requires_grad=False, shape=[6], dtype=Float32, value= [1., 1., 1., 1., 1., 1.])&gt;</span>
<span class="go">    &gt;</span>
<span class="go">  (bn): BatchNorm2d&lt;num_features=6, eps=1e-05, momentum=0.9, gamma=Parameter(name=bn.gamma, requires_grad=True, shape=[6], dtype=Float32, value= [1., 1., 1., 1., 1., 1.]), beta=Parameter(name=bn.beta, requires_grad=True, shape=[6], dtype=Float32, value= [0., 0., 0., 0., 0., 0.]), moving_mean=Parameter(name=bn.moving_mean, requires_grad=False, shape=[6], dtype=Float32, value= [0., 0., 0., 0., 0., 0.]), moving_variance=Parameter(name=bn.moving_variance, requires_grad=False, shape=[6], dtype=Float32, value= [1., 1., 1., 1., 1., 1.])&gt;</span>
<span class="go">  (Conv2dSlbQuant): QuantizeWrapperCell&lt;</span>
<span class="go">    (_handler): Conv2dSlbQuant&lt;</span>
<span class="go">      in_channels=1, out_channels=6, kernel_size=(5, 5), weight_bit_num=1, stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False</span>
<span class="go">      (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;</span>
<span class="go">      &gt;</span>
<span class="go">    (_input_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;</span>
<span class="go">    (_output_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;</span>
<span class="go">    &gt;</span>
<span class="go">  &gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 7) convert a compressed network to a standard network before exporting to MindIR.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_qat</span> <span class="o">=</span> <span class="n">slb_quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">net_qat</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_in</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]),</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">file_name</span> <span class="o">=</span> <span class="s2">&quot;./conv.mindir&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mindspore</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">net_qat</span><span class="p">,</span> <span class="n">data_in</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="n">file_name</span><span class="p">,</span> <span class="n">file_format</span><span class="o">=</span><span class="s2">&quot;MINDIR&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mindspore</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">GraphCell</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Cell</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.apply"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.apply" title="Permalink to this definition"></a></dt>
<dd><p>Apply SLB quantization Algorithm on <cite>network</cite>, use the following steps to make <cite>network</cite> available for
quantization aware training:</p>
<ol class="arabic simple">
<li><p>Fuse certain cells in <cite>network</cite> using pattern engine which is defined by net policy.</p></li>
<li><p>Propagate layer policies defined through cells.</p></li>
<li><p>Reduce redundant fake quantizers when they are redundant.</p></li>
<li><p>Apply layer policies to convert normal cell to <cite>QuantizeWrapperCell</cite>.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>network</strong> (<em>Cell</em>) – Network to be quantized.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Quantized network.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.callbacks">
<span class="sig-name descname"><span class="pre">callbacks</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dataset</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.callbacks"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.callbacks" title="Permalink to this definition"></a></dt>
<dd><p>Define TemperatureScheduler callback for SLB QAT-algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>Model</em>) – Model to be used.</p></li>
<li><p><strong>dataset</strong> (<em>Dataset</em>) – Dataset to be used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of instance of Callbacks.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If <cite>epoch_size</cite> is not initialized.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If <cite>has_trained_epoch</cite> is not initialized.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>epoch_size</cite> is not greater than <cite>has_trained_epoch</cite>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>t_end_time</cite> is less than <cite>t_start_time</cite>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>model</cite> is not <cite>mindspore.train.Model</cite>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>dataset</cite> is not mindspore.dataset.Dataset.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.convert">
<span class="sig-name descname"><span class="pre">convert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">net_opt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Cell</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ckpt_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.convert"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.convert" title="Permalink to this definition"></a></dt>
<dd><p>Define how to convert a compressed network to a standard network before exporting to MindIR.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>net_opt</strong> (<em>Cell</em>) – Network to be converted which is transformed by <cite>SlbQuantAwareTraining.apply</cite>.</p></li>
<li><p><strong>ckpt_path</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Path to checkpoint file for <cite>net_opt</cite>. Default is <code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code>, which means not loading
checkpoint file to <cite>net_opt</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An instance of Cell represents converted network.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>net_opt</cite> is not Cell.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>ckpt_path</cite> is not string.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>ckpt_path</cite> is not empty and invalid.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – If <cite>ckpt_path</cite> is a valid file and load checkpoint file failed.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.set_act_quant_dtype">
<span class="sig-name descname"><span class="pre">set_act_quant_dtype</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act_quant_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">QuantDtype.INT8</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.set_act_quant_dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.set_act_quant_dtype" title="Permalink to this definition"></a></dt>
<dd><p>Set value of act_quant_dtype of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>act_quant_dtype</strong> (<em>QuantDtype</em>) – Datatype used to quantize activations. Default: <code class="docutils literal notranslate"><span class="pre">QuantDtype.INT8</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>act_quant_dtype</cite> is not QuantDtype.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – Only supported if <cite>act_quant_dtype</cite> is <code class="docutils literal notranslate"><span class="pre">QuantDtype.INT8</span></code> yet.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.set_enable_act_quant">
<span class="sig-name descname"><span class="pre">set_enable_act_quant</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enable_act_quant</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.set_enable_act_quant"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.set_enable_act_quant" title="Permalink to this definition"></a></dt>
<dd><p>Set value of enable_act_quant of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enable_act_quant</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether apply activation quantization while training, default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>enable_act_quant</cite> is not bool.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.set_enable_bn_calibration">
<span class="sig-name descname"><span class="pre">set_enable_bn_calibration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enable_bn_calibration</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.set_enable_bn_calibration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.set_enable_bn_calibration" title="Permalink to this definition"></a></dt>
<dd><p>Set value of enable_bn_calibration of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enable_bn_calibration</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether apply batchnorm calibration while training, default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>enable_bn_calibration</cite> is not bool.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.set_epoch_size">
<span class="sig-name descname"><span class="pre">set_epoch_size</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch_size</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.set_epoch_size"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.set_epoch_size" title="Permalink to this definition"></a></dt>
<dd><p>Set value of epoch_size of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>epoch_size</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – the epoch size of training.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>epoch_size</cite> is not int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>epoch_size</cite> is not greater than 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.set_has_trained_epoch">
<span class="sig-name descname"><span class="pre">set_has_trained_epoch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">has_trained_epoch</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.set_has_trained_epoch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.set_has_trained_epoch" title="Permalink to this definition"></a></dt>
<dd><p>Set value of has_trained_epoch of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>has_trained_epoch</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – the trained epochs of training.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>has_trained_epoch</cite> is not int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>has_trained_epoch</cite> is less than 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.set_t_end_time">
<span class="sig-name descname"><span class="pre">set_t_end_time</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t_end_time</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.6</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.set_t_end_time"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.set_t_end_time" title="Permalink to this definition"></a></dt>
<dd><p>Set value of t_end_time of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t_end_time</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Fraction of epochs after which temperature hyperparameters stopping changing,
default: <code class="docutils literal notranslate"><span class="pre">0.6</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>t_end_time</cite> is not float.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>t_end_time</cite> is less than 0. or greater than 1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.set_t_factor">
<span class="sig-name descname"><span class="pre">set_t_factor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t_factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.set_t_factor"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.set_t_factor" title="Permalink to this definition"></a></dt>
<dd><p>Set value of t_factor of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t_factor</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Multiplicative factor of temperature hyperparameters changing, default: <code class="docutils literal notranslate"><span class="pre">1.2</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>t_factor</cite> is not float.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>t_factor</cite> is not greater than 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.set_t_start_time">
<span class="sig-name descname"><span class="pre">set_t_start_time</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t_start_time</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.set_t_start_time"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.set_t_start_time" title="Permalink to this definition"></a></dt>
<dd><p>Set value of t_start_time of quantization aware training <cite>config</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t_start_time</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Fraction of epochs after which temperature hyperparameters starting changing,
default: <code class="docutils literal notranslate"><span class="pre">0.2</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>t_start_time</cite> is not float.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>t_start_time</cite> is less than 0. or greater than 1.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.set_t_start_val">
<span class="sig-name descname"><span class="pre">set_t_start_val</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t_start_val</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.set_t_start_val"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.set_t_start_val" title="Permalink to this definition"></a></dt>
<dd><p>Set value of t_start_val of quantization aware training <cite>config</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>t_start_val</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#float" title="(in Python v3.8)"><em>float</em></a>) – Initial value of temperature hyperparameters, default: <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>t_start_val</cite> is not float.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>t_start_val</cite> is not greater than 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_gs.quantization.SlbQuantAwareTraining.set_weight_quant_dtype">
<span class="sig-name descname"><span class="pre">set_weight_quant_dtype</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight_quant_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">QuantDtype.INT1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/slb/slb_quant_aware_training.html#SlbQuantAwareTraining.set_weight_quant_dtype"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_gs.quantization.SlbQuantAwareTraining.set_weight_quant_dtype" title="Permalink to this definition"></a></dt>
<dd><p>Set value of weight_quant_dtype of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weight_quant_dtype</strong> (<em>QuantDtype</em>) – Datatype used to quantize weights. Default: <code class="docutils literal notranslate"><span class="pre">QuantDtype.INT1</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>weight_quant_dtype</cite> is not QuantDtype.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – Only supported if <cite>weight_quant_dtype</cite> is <code class="docutils literal notranslate"><span class="pre">QuantDtype.INT1</span></code>, <code class="docutils literal notranslate"><span class="pre">QuantDtype.INT2</span></code>
    or <code class="docutils literal notranslate"><span class="pre">QuantDtype.INT4</span></code> yet.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.html" class="btn btn-neutral float-left" title="mindspore_gs.quantization.SimulatedQuantizationAwareTraining" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../mindspore_gs.pruner.html" class="btn btn-neutral float-right" title="mindspore_gs.pruner" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>