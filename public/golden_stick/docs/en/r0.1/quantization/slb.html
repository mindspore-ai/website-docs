<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Applying the SLB Algorithm &mdash; MindSpore master documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Pruning Algorithm Overview" href="../pruner/overview.html" />
    <link rel="prev" title="Applying the SimQAT Algorithm" href="simqat.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Installation and Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing MindSpore Golden Stick</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quantization Algorithms</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Quantization Algorithm Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="simqat.html">Applying the SimQAT Algorithm</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Applying the SLB Algorithm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">Background</a></li>
<li class="toctree-l2"><a class="reference internal" href="#algorithm-principles">Algorithm Principles</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#temperature-factor">Temperature Factor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#algorithm-features">Algorithm Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="#slb-quantization-training">SLB Quantization Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#slb-quantization-training-example">SLB Quantization Training Example</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#loading-a-dataset">Loading a Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-original-network">Defining the Original Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#applying-the-quantization-algorithm">Applying the Quantization Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#defining-the-optimizer,-loss-function,-and-training-callbacks">Defining the Optimizer, Loss Function, and Training Callbacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-the-model-and-saving-the-model-file">Training the Model and Saving the Model File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-the-model-and-comparing-the-accuracy">Loading the Model and Comparing the Accuracy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Pruning Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pruner/overview.html">Pruning Algorithm Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pruner/scop.html">Applying the SCOP Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.html">mindspore_gs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Applying the SLB Algorithm</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quantization/slb.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="applying-the-slb-algorithm">
<h1>Applying the SLB Algorithm<a class="headerlink" href="#applying-the-slb-algorithm" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/docs/golden_stick/docs/source_en/quantization/slb.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png"></a></p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline"></a></h2>
<p>In a conventional quantization method, a gradient is usually calculated by using straight through estimator (STE) [1] or a self-designed gradient calculation manner [2]. Due to that the quantization function is not differentiable, an error usually occurs in a calculated gradient. An inaccurate optimization direction results in relatively poor final inference performance. Therefore, there is an urgent need for a quantitative neural network learning method that can avoid this inaccurate gradient estimation.</p>
</section>
<section id="algorithm-principles">
<h2>Algorithm Principles<a class="headerlink" href="#algorithm-principles" title="Permalink to this headline"></a></h2>
<p>Searching for low-bit weights (SLB) [3] is a weight quantization algorithm developed by Huawei Noah’s Ark Lab. It provides a low-bit quantization algorithm based on weight search to avoid inaccurate gradient estimation. For quantization of a low-bit network, the number of effective solutions for quantizing the network weight is small. Therefore, the quantization of the network may be implemented through weight search, that is, the quantization process is converted into a weight search process. A group of quantization weights are preset for the quantization network, and then a probability matrix is defined to represent the probability that different quantization weights are retained. In the training phase, the network weights are quantized by optimizing the probability matrix.</p>
<p>The figure on the left shows the traditional quantization algorithm used to do binary quantization. During training, the floating-point weights are updated with inaccurate gradients. Finally, the floating-point weights are processed by binarization (sigmoid function) to obtain the quantized weights. The figure on the right shows the SLB quantization algorithm used to do binary quantization. It uses the continuous relaxation strategy to search for discrete weights, optimizes the weight probability matrix of discrete weights during training, and selects discrete weights based on the probability to implement quantization. The single value corresponding to the red dots in the left figure is obtained by the sigmoid function, which represents the probability that the weight is quantized to -1. The single value corresponding to the blue dots is obtained by the sigmoid function, which represents the probability that the weight is quantized to +1. Inaccurate gradient update in traditional quantization algorithm will affect the update of floating point weight, resulting in a large deviation in the probability here. The two values corresponding to the red and blue dots in the right figure are obtained by the softmax function and represent the probability that the weight is quantized to -1 or +1. By avoiding inaccurate gradient updates, the probability is more accurate.</p>
<p><img alt="SLB algorithm comparison" src="../_images/slb_1.png" /></p>
<section id="temperature-factor">
<h3>Temperature Factor<a class="headerlink" href="#temperature-factor" title="Permalink to this headline"></a></h3>
<p>In classification tasks, softmax distribution is used to calculate the probability that the output is classified into different classes. Therefore, the SLB also uses softmax distribution to calculate the probability that a weight is quantized into each quantized weight, and finally selects a corresponding weight as a quantization result based on the maximum probability. To improve the confidence of the quantization result, the SLB introduces a temperature factor. By gradually adjusting the temperature factor, the softmax distribution gradually becomes steep and gradually approaches the one-hot distribution, thereby maximizing the confidence of the quantization result and reducing the error.</p>
<p>The formula on the left is a standard softmax function, and the formula on the right is the softmax function after the temperature factor is introduced in the SLB algorithm.</p>
<p><img alt="Softmax function" src="../_images/slb_2.png" /></p>
<p>The following figure shows the change process of softmax distribution when the temperature factor is gradually adjusted. The rightmost figure shows the one-hot distribution.</p>
<p><img alt="Softmax distribution change" src="../_images/slb_3.png" /></p>
</section>
</section>
<section id="algorithm-features">
<h2>Algorithm Features<a class="headerlink" href="#algorithm-features" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>A new weight search method is proposed for training quantization deep neural networks, which can avoid inaccurate gradient estimation.</p></li>
<li><p>The continuous relaxation strategy is used to search for discrete weights, optimize the probability distribution of discrete weights during training, and finally select discrete weights according to the probability to realize quantization.</p></li>
<li><p>In order to further eliminate the inference performance gap after search and ensure the consistency of training and testing, a strategy of gradually adjusting the temperature factor is proposed.</p></li>
<li><p>Compared with the traditional quantization algorithm, this algorithm avoids the inaccurate gradient updating process, obtains better inference performance, and has more advantages in very low bit quantization.</p></li>
</ul>
</section>
<section id="slb-quantization-training">
<h2>SLB Quantization Training<a class="headerlink" href="#slb-quantization-training" title="Permalink to this headline"></a></h2>
<p>The training specifications of SLB quantization algorithm are shown in the following table.</p>
<p>Table 1: SLB quantization training specifications</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Specifications</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Hardware</p></td>
<td><p>GPU</p></td>
</tr>
<tr class="row-odd"><td><p>Networks</p></td>
<td><p>ResNet-18. For details, see <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/README.md#apply-algorithm-in-mindspore-golden-stick">https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/README.md#apply-algorithm-in-mindspore-golden-stick</a>.</p></td>
</tr>
<tr class="row-even"><td><p>Solutions</p></td>
<td><p>Supports 1-, 2-, and 4-bit weight quantization solutions.</p></td>
</tr>
<tr class="row-odd"><td><p>Data types</p></td>
<td><p>The GPU platform supports FP32.</p></td>
</tr>
<tr class="row-even"><td><p>Running modes</p></td>
<td><p>Graph mode and PyNative mode</p></td>
</tr>
</tbody>
</table>
</section>
<section id="slb-quantization-training-example">
<h2>SLB Quantization Training Example<a class="headerlink" href="#slb-quantization-training-example" title="Permalink to this headline"></a></h2>
<p>The procedure of SLB quantization training is the same as that of common training. Additional operations need to be performed in the phases of defining a quantization network and generating a quantization model. The complete process is as follows:</p>
<ol class="arabic simple">
<li><p>Load the dataset and process data.</p></li>
<li><p>Define a network.</p></li>
<li><p>Define the SLB quantization algorithm and use the algorithm to generate a quantization model.</p></li>
<li><p>Define the optimizer, loss function, and callbacks.</p></li>
<li><p>Train the network and save the model file.</p></li>
<li><p>Load the model file and compare the accuracy after quantization.</p></li>
</ol>
<p>The following uses the LeNet-18 as an example to describe these steps.</p>
<blockquote>
<div><p>For details about the complete code, see <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/README.md#apply-algorithm-in-mindspore-golden-stick">ResNet model repository</a>. <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/golden_stick/quantization/slb/train.py">train.py</a> is the complete training code, and <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/golden_stick/quantization/slb/eval.py">eval.py</a> is the accuracy verification code.</p>
</div></blockquote>
<section id="loading-a-dataset">
<h3>Loading a Dataset<a class="headerlink" href="#loading-a-dataset" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">do_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">train_image_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">train_image_size</span><span class="p">,</span>
                         <span class="n">eval_image_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">eval_image_size</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device_target</span><span class="p">,</span>
                         <span class="n">distribute</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">run_distribute</span><span class="p">)</span>
</pre></div>
</div>
<p>In the code, <code class="docutils literal notranslate"><span class="pre">create_dataset</span></code> is referenced from <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/src/dataset.py">dataset.py</a>, and <code class="docutils literal notranslate"><span class="pre">config.data_path</span></code> and <code class="docutils literal notranslate"><span class="pre">config.batch_size</span></code> are configured in the <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/golden_stick/quantization/slb/resnet18_cifar10_config.yaml">configuration file</a>.</p>
</section>
<section id="defining-the-original-network">
<h3>Defining the Original Network<a class="headerlink" href="#defining-the-original-network" title="Permalink to this headline"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">src.resnet</span> <span class="kn">import</span> <span class="n">resnet18</span> <span class="k">as</span> <span class="n">resnet</span>

<span class="o">...</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">class_num</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
<p>The original network structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ResNet&lt;
  (conv1): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
  (bn1): BatchNorm2d&lt;num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
  (pad): Pad&lt;&gt;
  (maxpool): MaxPool2d&lt;kernel_size=3, stride=2, pad_mode=VALID&gt;
  (layer1): SequentialCell&lt;
    (0): ResidualBlockBase&lt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.0.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.0.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.0.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.0.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.0.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.0.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.0.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.0.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      &gt;
    (1): ResidualBlockBase&lt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.1.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.1.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.1.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.1.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.1.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.1.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.1.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.1.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      &gt;
    &gt;
  (layer2): SequentialCell&lt;...&gt;
  (layer3): SequentialCell&lt;...&gt;
  (layer4): SequentialCell&lt;...&gt;
  (flatten): Flatten&lt;&gt;
  (end_point): Dense&lt;input_channels=512, output_channels=10, has_bias=True&gt;
  &gt;
</pre></div>
</div>
<p>For details about the ResNet-18 definition, see <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/src/resnet.py">resnet.py</a>.</p>
</section>
<section id="applying-the-quantization-algorithm">
<h3>Applying the Quantization Algorithm<a class="headerlink" href="#applying-the-quantization-algorithm" title="Permalink to this headline"></a></h3>
<p>After a network layer to be quantized is modified based on the original network definition, a network with fake quantization nodes is generated. This network is a quantization network. The <code class="docutils literal notranslate"><span class="pre">SlbQuantAwareTraining</span></code> class under the MindSpore Golden Stick is constructed and applied to the original network to convert the original network into a quantization network. <code class="docutils literal notranslate"><span class="pre">QuantDtype</span></code> is a class that defines various quantization bits. You can customize the weight quantization bits by calling the <code class="docutils literal notranslate"><span class="pre">set_weight_quant_dtype</span></code> API of the <code class="docutils literal notranslate"><span class="pre">SlbQuantAwareTraining</span></code> class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_gs</span> <span class="kn">import</span> <span class="n">SlbQuantAwareTraining</span> <span class="k">as</span> <span class="n">SlbQAT</span>
<span class="kn">from</span> <span class="nn">mindspore_gs.quantization.constant</span> <span class="kn">import</span> <span class="n">QuantDtype</span>

<span class="o">...</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">SlbQAT</span><span class="p">()</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_weight_quant_dtype</span><span class="p">(</span><span class="n">QuantDtype</span><span class="o">.</span><span class="n">INT1</span><span class="p">)</span>
<span class="n">quant_net</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">quant_net</span><span class="p">)</span>
</pre></div>
</div>
<p>The quantized network structure is as follows, QuantizeWrapperCell is the encapsulation class of SLB quantization to the original Conv2d, including the pseudo-quantization node of the original operator and weight. Users can modify the algorithm configuration by referring to <a class="reference external" href="https://www.mindspore.cn/golden_stick/docs/en/r0.1/mindspore_gs.html#mindspore_gs.SlbQuantAwareTraining">API</a> and confirm whether the algorithm is configured successfully by checking the attributes of the QuantizeWrapperCell.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ResNetOpt&lt;
  (_handler): ResNet&lt;...&gt;
  (conv1): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
  (bn1): BatchNorm2d&lt;num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
  (pad): Pad&lt;&gt;
  (maxpool): MaxPool2d&lt;kernel_size=3, stride=2, pad_mode=VALID&gt;
  (layer1): SequentialCellOpt&lt;
    (_handler): SequentialCell&lt;...&gt;
    (cell_list_0): ResidualBlockBaseOpt&lt;
      (_handler): ResidualBlockBase&lt;...&gt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.0.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.0.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.0.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.0.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.0.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.0.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.0.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.0.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      (Conv2dSlbQuant): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        &gt;
      (Conv2dSlbQuant_1): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        &gt;
      &gt;
    (cell_list_1): ResidualBlockBaseOpt_1&lt;
    (_handler): ResidualBlockBase&lt;...&gt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.1.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.1.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.1.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.1.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.1.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.1.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.1.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.1.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      (Conv2dSlbQuant): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        &gt;
      (Conv2dSlbQuant_1): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        &gt;
      &gt;
    &gt;
  (layer2): SequentialCellOpt_1&lt;...&gt;
  (layer3): SequentialCellOpt_3&lt;...&gt;
  (layer4): SequentialCellOpt_5&lt;...&gt;
  (flatten): Flatten&lt;&gt;
  (end_point): Dense&lt;input_channels=512, output_channels=10, has_bias=True&gt;
  (Conv2dSlbQuant): QuantizeWrapperCell&lt;
    (_handler): Conv2dSlbQuant&lt;
      in_channels=3, out_channels=64, kernel_size=(7, 7), weight_bit_num=1, stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False
      (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
      &gt;
    &gt;
  &gt;
</pre></div>
</div>
<p>Compared with the original network, conv in the quantized network is replaced with Conv2dSlbQuant.</p>
</section>
<section id="defining-the-optimizer,-loss-function,-and-training-callbacks">
<h3>Defining the Optimizer, Loss Function, and Training Callbacks<a class="headerlink" href="#defining-the-optimizer,-loss-function,-and-training-callbacks" title="Permalink to this headline"></a></h3>
<p>For the SLB quantization algorithm, in addition to the callbacks commonly used in training, a callback class <code class="docutils literal notranslate"><span class="pre">TemperatureScheduler</span></code> that supports dynamic adjustment of the temperature factor needs to be defined.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.train.callback</span> <span class="k">as</span> <span class="nn">callback</span>
<span class="kn">from</span> <span class="nn">mindspore.train.loss_scale_manager</span> <span class="kn">import</span> <span class="n">FixedLossScaleManager</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">LossMonitor</span><span class="p">,</span> <span class="n">TimeMonitor</span>

<span class="k">class</span> <span class="nc">TemperatureScheduler</span><span class="p">(</span><span class="n">callback</span><span class="o">.</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">epoch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">has_trained_epoch</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">t_start_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">t_start_time</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">t_end_time</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">t_factor</span><span class="o">=</span><span class="mf">1.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epoch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_trained_epoch</span> <span class="o">=</span> <span class="n">has_trained_epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_start_val</span> <span class="o">=</span> <span class="n">t_start_val</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_start_time</span> <span class="o">=</span> <span class="n">t_start_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_end_time</span> <span class="o">=</span> <span class="n">t_end_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_factor</span> <span class="o">=</span> <span class="n">t_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">epoch_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
        <span class="n">epoch</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_epoch_num</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_trained_epoch</span>
        <span class="c1"># Compute temperature value</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_start_val</span>
        <span class="n">t_start_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">t_start_time</span><span class="p">)</span>
        <span class="n">t_end_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">t_end_time</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="n">t_start_epoch</span><span class="p">:</span>
            <span class="n">t</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_factor</span><span class="o">**</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">t_end_epoch</span><span class="p">)</span> <span class="o">-</span> <span class="n">t_start_epoch</span><span class="p">)</span>
        <span class="c1"># Assign new value to temperature parameter</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">cell</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train_network</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">cls_name</span> <span class="o">==</span> <span class="s1">&#39;SlbFakeQuantizerPerLayer&#39;</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">set_temperature</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;=</span> <span class="n">t_end_epoch</span><span class="p">:</span>
                    <span class="n">cell</span><span class="o">.</span><span class="n">set_temperature_end_flag</span><span class="p">()</span>

<span class="n">step_size</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">()</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">get_lr</span><span class="p">(</span><span class="n">lr_init</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_init</span><span class="p">,</span>
            <span class="n">lr_end</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_end</span><span class="p">,</span>
            <span class="n">lr_max</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_max</span><span class="p">,</span>
            <span class="n">warmup_epochs</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">warmup_epochs</span><span class="p">,</span>
            <span class="n">total_epochs</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span>
            <span class="n">steps_per_epoch</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span>
            <span class="n">lr_decay_mode</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_decay_mode</span><span class="p">)</span>
<span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">pre_trained</span><span class="p">:</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">:]</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
<span class="c1"># define optimizer</span>
<span class="n">group_params</span> <span class="o">=</span> <span class="n">init_group_params</span><span class="p">(</span><span class="n">quant_net</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
                  <span class="n">loss_scale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">init_loss_scale</span><span class="p">()</span>
<span class="n">loss_scale</span> <span class="o">=</span> <span class="n">FixedLossScaleManager</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">drop_overflow_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;acc&quot;</span><span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">quant_net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
                 <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O0&quot;</span><span class="p">,</span> <span class="n">boost_level</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">boost_mode</span><span class="p">,</span> <span class="n">keep_batchnorm_fp32</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">boost_config_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;total_steps&quot;</span><span class="p">:</span> <span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">}})</span>

<span class="c1"># define callbacks</span>
<span class="n">time_cb</span> <span class="o">=</span> <span class="n">TimeMonitor</span><span class="p">(</span><span class="n">data_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">)</span>
<span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossCallBack</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">)</span>

<span class="n">cb</span> <span class="o">=</span> <span class="p">[</span><span class="n">time_cb</span><span class="p">,</span> <span class="n">loss_cb</span><span class="p">]</span>
<span class="n">algo_cb</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">callback</span><span class="p">()</span>
<span class="n">cb</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">algo_cb</span><span class="p">)</span>
<span class="n">cb</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TemperatureScheduler</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">t_start_val</span><span class="p">,</span>
                                   <span class="n">config</span><span class="o">.</span><span class="n">t_start_time</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">t_end_time</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">t_factor</span><span class="p">))</span>
<span class="n">ckpt_append_info</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;epoch_num&quot;</span><span class="p">:</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">,</span> <span class="s2">&quot;step_num&quot;</span><span class="p">:</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_step</span><span class="p">}]</span>
<span class="n">config_ck</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">save_checkpoint_epochs</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span>
                             <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">keep_checkpoint_max</span><span class="p">,</span>
                             <span class="n">append_info</span><span class="o">=</span><span class="n">ckpt_append_info</span><span class="p">)</span>
<span class="n">ckpt_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;resnet&quot;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="s2">&quot;./ckpt&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_ck</span><span class="p">)</span>
<span class="n">cb</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ckpt_cb</span><span class="p">]</span>
</pre></div>
</div>
<p>In the code, <code class="docutils literal notranslate"><span class="pre">get_lr</span></code> is referenced from <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/src/lr_generator.py">lr_generator.py</a>, and <code class="docutils literal notranslate"><span class="pre">init_group_params</span></code> and <code class="docutils literal notranslate"><span class="pre">init_loss_scale</span></code> are referenced from <a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/golden_stick/quantization/slb/train.py">train.py</a>.</p>
</section>
<section id="training-the-model-and-saving-the-model-file">
<h3>Training the Model and Saving the Model File<a class="headerlink" href="#training-the-model-and-saving-the-model-file" title="Permalink to this headline"></a></h3>
<p>Once the model is defined, the training begins.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_sink_mode</span> <span class="o">=</span> <span class="n">target</span> <span class="o">!=</span> <span class="s2">&quot;CPU&quot;</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">cb</span><span class="p">,</span>
            <span class="n">sink_size</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">(),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="n">dataset_sink_mode</span><span class="p">)</span>
</pre></div>
</div>
<p>The running result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1562, loss is 1.4536957
Train epoch time: 101539.306 ms, per step time: 65.006 ms
epoch: 2 step: 1562, loss is 1.3616204
Train epoch time: 94238.882 ms, per step time: 60.332 ms
epoch: 3 step: 1562, loss is 1.2128768
Train epoch time: 94237.197 ms, per step time: 60.331 ms
epoch: 4 step: 1562, loss is 0.99068344
Train epoch time: 94084.353 ms, per step time: 60.233 ms
epoch: 5 step: 1562, loss is 0.89842224
Train epoch time: 94498.564 ms, per step time: 60.498 ms
epoch: 6 step: 1562, loss is 0.8985137
Train epoch time: 94106.722 ms, per step time: 60.248 ms
</pre></div>
</div>
</section>
<section id="loading-the-model-and-comparing-the-accuracy">
<h3>Loading the Model and Comparing the Accuracy<a class="headerlink" href="#loading-the-model-and-comparing-the-accuracy" title="Permalink to this headline"></a></h3>
<p>Obtain the accuracy of the common training model according to the steps in the <a class="reference external" href="https://gitee.com/mindspore/models/tree/r1.8/official/cv/resnet">ResNet model repository</a>.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;top_1_accuracy&#39;: 0.9544270833333334, &#39;top_5_accuracy&#39;: 0.9969951923076923
</pre></div>
</div>
<p>Load the model file obtained in the previous step and import the quantized model for accuracy evaluation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_file_path</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">quant_net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">do_train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                         <span class="n">eval_image_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">eval_image_size</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">quant_net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;top_1_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;top_5_accuracy&#39;</span><span class="p">})</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">ds_eval</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;top_1_accuracy&#39;: 0.9485176282051282, &#39;top_5_accuracy&#39;: 0.9965945512820513.
</pre></div>
</div>
<p>In graph mode, apply SLB quantization to ResNet-18 and use the CIFAR-10 dataset for evaluation. The following table lists the experiment results. W32 indicates a full-precision model. W4 indicates that the weight is 4 bits, W2 indicates that the weight is 2 bits, and W1 indicates that the weight is 1 bit. It can be found that, in the current task, compared with the full-precision model, the top 1 accuracy of the model after 4-bit weight quantization has no loss, and the top 1 accuracy loss of the model after 1-bit weight quantization is within 0.6%. SLB quantization greatly reduces model parameters, making it easier to deploy models on devices with limited resources. The model here is not the final deployment model. Due to the addition of pseudo-quantization nodes and weight probability matrix, the checkpoint size increases compared with the original model. The increase amplitude is affected by the weight quantization bits. The final quantization model, that is, the final deployment model, is obtained by selecting the preset quantization weights according to the weight probability matrix.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Quantization Type</p></th>
<th class="head"><p>Top 1 Accuracy</p></th>
<th class="head"><p>Top 5 Accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>W32</p></td>
<td><p>0.9544</p></td>
<td><p>0.9970</p></td>
</tr>
<tr class="row-odd"><td><p>W4</p></td>
<td><p>0.9534</p></td>
<td><p>0.9970</p></td>
</tr>
<tr class="row-even"><td><p>W2</p></td>
<td><p>0.9503</p></td>
<td><p>0.9967</p></td>
</tr>
<tr class="row-odd"><td><p>W1</p></td>
<td><p>0.9485</p></td>
<td><p>0.9966</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline"></a></h2>
<p>[1] Bengio, Yoshua, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. 2013.</p>
<p>[2] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. ICLR, 2019.</p>
<p>[3] Yang Z, Wang Y, Han K, et al. Searching for low-bit weights in quantized neural networks. NIPS, 2020.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="simqat.html" class="btn btn-neutral float-left" title="Applying the SimQAT Algorithm" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../pruner/overview.html" class="btn btn-neutral float-right" title="Pruning Algorithm Overview" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>