

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore_gs.quantization.SimulatedQuantizationAwareTraining &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore_gs.quantization.SlbQuantAwareTraining" href="mindspore_gs.quantization.SlbQuantAwareTraining.html" />
    <link rel="prev" title="mindspore_gs.quantization" href="../mindspore_gs.quantization.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Installation and Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing MindSpore Golden Stick</a></li>
</ul>
<p class="caption"><span class="caption-text">Quantization Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">Quantization Algorithm Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="simqat.html">Applying the SimQAT Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="slb.html">Applying the SLB Algorithm</a></li>
</ul>
<p class="caption"><span class="caption-text">Pruning Algorithms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pruner/overview.html">Pruning Algorithm Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pruner/scop.html">Applying the SCOP Algorithm</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/overview.html">Model Deployment Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/convert.html">MindSpore Golden Stick Network Conversion</a></li>
</ul>
<p class="caption"><span class="caption-text">API References</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.html">mindspore_gs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore_gs.quantization.html">mindspore_gs.quantization</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore_gs.quantization.html#simulated-quantization-algorithm">Simulated Quantization Algorithm</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore_gs.quantization.SimulatedQuantizationAwareTraining</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore_gs.quantization.html#slb-algorithm">SLB Algorithm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.pruner.html">mindspore_gs.pruner</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindspore_gs.quantization.html">mindspore_gs.quantization</a> &raquo;</li>
        
      <li>mindspore_gs.quantization.SimulatedQuantizationAwareTraining</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/quantization/mindspore_gs.quantization.SimulatedQuantizationAwareTraining.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mindspore-gs-quantization-simulatedquantizationawaretraining">
<h1>mindspore_gs.quantization.SimulatedQuantizationAwareTraining<a class="headerlink" href="#mindspore-gs-quantization-simulatedquantizationawaretraining" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining">
<em class="property">class </em><code class="sig-prename descclassname">mindspore_gs.quantization.</code><code class="sig-name descname">SimulatedQuantizationAwareTraining</code><span class="sig-paren">(</span><em class="sig-param">config=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining" title="Permalink to this definition">¶</a></dt>
<dd><p>Basic implementation of simulated quantization aware training, this algorithm adopts fake quantizer to simulate
the loss of quantization calculation, and network parameters are updated through backpropagation, so that the
network parameters can better adapt to the loss caused by quantization. See more details in <a class="reference external" href="https://arxiv.org/pdf/2106.08295.pdf">A White Paper on
Neural Network Quantization</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#dict" title="(in Python v3.8)"><em>dict</em></a>) – <p>store attributes for quantization aware training, keys are attribute names,
values are attribute values. The Default value is None, supported attribute are listed below:</p>
<ul class="simple">
<li><p>quant_delay (Union[int, list, tuple]): Number of steps after which weights and activations are quantized
during train and eval. The first element represents activations and the second element represents weights.
Default: (0, 0).</p></li>
<li><p>quant_dtype (Union[QuantDtype, list, tuple]): The target data type for quantization. It is necessary to
consider the precision support of hardware devices when setting <cite>quant_dtype</cite>. The first element
represents activations and the second element represents weights.
Default: (QuantDtype.INT8, QuantDtype.INT8).</p></li>
<li><p>per_channel (Union[bool, list, tuple]):  Quantization granularity based on layer or on channel. If True
then base on per channel, otherwise base on per layer. The first element represents activations and the
second element represents weights, and the first element must be False now.
Default: (False, False).</p></li>
<li><p>symmetric (Union[bool, list, tuple]): Whether the quantization algorithm is symmetric or not. If True
then base on symmetric, otherwise base on asymmetric. The first element represents activations and the
second element represents weights.
Default: (False, False).</p></li>
<li><p>narrow_range (Union[bool, list, tuple]): Whether the quantization algorithm uses narrow range or not.
The first element represents activations and the second element represents weights.
Default: (False, False).</p></li>
<li><p>enable_fusion (bool): Whether apply fusion before applying quantization.
Default: False.</p></li>
<li><p>freeze_bn (int): Number of steps after which BatchNorm OP parameters fixed to global mean and variance.
Default: 10000000.</p></li>
<li><p>bn_fold (bool): Whether to use bn fold ops for simulation inference operation.
Default: False.</p></li>
<li><p>one_conv_fold (bool): Whether to use one conv bn fold ops for simulation inference operation.
Default: True.</p></li>
</ul>
</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>bn_fold</cite>, <cite>one_conv_fold</cite> or <cite>enable_fusion</cite> is not bool.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>freeze_bn</cite> is not int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>quant_delay</cite> is not int, or every element of <cite>quant_delay</cite> is not int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>quant_dtype</cite> is not <cite>QuantDtype</cite>, or every element of <cite>quant_dtype</cite> is not <cite>QuantDtype</cite>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>per_channel</cite> is not bool, or every element of <cite>per_channel</cite> is not bool.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>symmetric</cite> is not bool, or every element of <cite>symmetric</cite> is not bool.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>narrow_range</cite> is not bool, or every element of <cite>narrow_range</cite> is not bool.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>freeze_bn</cite> is less than 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If the length of <cite>quant_delay</cite>, <cite>quant_dtype</cite>, <cite>per_channel</cite>, <cite>symmetric</cite> or <cite>narrow_range</cite> is more
    than 2.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>quant_delay</cite> is less than 0, or any element of <cite>quant_delay</cite> is less than 0.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>quant_dtype</cite> is not <cite>QuantDtype.INT8</cite>, or any element of <cite>quant_dtype</cite> is not
    <cite>QuantDtype.INT8</cite>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>per_channel</cite> is True, or the first element of <cite>per_channel</cite> is True.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Supported Platforms:</dt><dd><p><code class="docutils literal notranslate"><span class="pre">GPU</span></code></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore_gs.quantization</span> <span class="kn">import</span> <span class="n">SimulatedQuantizationAwareTraining</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="gp">... </span><span class="k">class</span> <span class="nc">NetToQuant</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Cell</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_channel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">(</span><span class="n">NetToQuant</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">num_channel</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s1">&#39;valid&#39;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">... </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 1) Define network to be quantized</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">NetToQuant</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 2) Define SimQAT Algorithm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">simulated_quantization</span> <span class="o">=</span> <span class="n">SimulatedQuantizationAwareTraining</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 3) Use set functions to change config</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">simulated_quantization</span><span class="o">.</span><span class="n">set_enable_fusion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">simulated_quantization</span><span class="o">.</span><span class="n">set_bn_fold</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">simulated_quantization</span><span class="o">.</span><span class="n">set_act_quant_delay</span><span class="p">(</span><span class="mi">900</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">simulated_quantization</span><span class="o">.</span><span class="n">set_weight_quant_delay</span><span class="p">(</span><span class="mi">900</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">simulated_quantization</span><span class="o">.</span><span class="n">set_act_per_channel</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">simulated_quantization</span><span class="o">.</span><span class="n">set_weight_per_channel</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">simulated_quantization</span><span class="o">.</span><span class="n">set_act_narrow_range</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">simulated_quantization</span><span class="o">.</span><span class="n">set_weight_narrow_range</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 4) Apply SimQAT algorithm to origin network</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net_qat</span> <span class="o">=</span> <span class="n">simulated_quantization</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## 5) Print network and check the result. Conv2d and Dense should be transformed to QuantizeWrapperCells.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set enable_fusion to be True, bn_fold to be False, the Conv2d and BatchNorm2d Cells are</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## fused and converted to Conv2dBnWithoutFoldQuant.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set act_quant_delay to be 900, the quant_delay value of _input_quantizer and _output_quantizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## are set to be 900.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set weight_quant_delay to be 900, the quant_delay value of fake_quant_weight are set to be 900.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set act_per_channel to be False, the per_channel value of _input_quantizer and</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## _output_quantizer are set to be False.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set weight_per_channel to be True, the per_channel value of fake_quant_weight are set to be</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## True.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set act_narrow_range to be False, the narrow_range value of _input_quantizer and</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## _output_quantizer are set to be False.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## Since we set weight_narrow_range to be False, the narrow_range value of fake_quant_weight are set to be</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">## True.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">net_qat</span><span class="p">)</span>
<span class="go">NetToQuantOpt&lt;</span>
<span class="go">  (_handler): NetToQuant&lt;</span>
<span class="go">    (conv): Conv2d&lt;input_channels=1, output_channels=6, kernel_size=(5, 5), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, weight_init=normal, bias_init=zeros, format=NCHW&gt;</span>
<span class="go">    (bn): BatchNorm2d&lt;num_features=6, eps=1e-05, momentum=0.09999999999999998, gamma=Parameter (name=_handler.bn.gamma, shape=(6,), dtype=Float32, requires_grad=True), beta=Parameter (name=_handler.bn.beta, shape=(6,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=_handler.bn.moving_mean, shape=(6,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=_handler.bn.moving_variance, shape=(6,), dtype=Float32, requires_grad=False)&gt;</span>
<span class="go">    &gt;</span>
<span class="go">  (Conv2dBnWithoutFoldQuant): QuantizeWrapperCell&lt;</span>
<span class="go">    handler: in_channels=1, out_channels=6, kernel_size=(5, 5), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False, input quantizer: bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900, output quantizer: bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900</span>
<span class="go">    (_handler): Conv2dBnWithoutFoldQuant&lt;</span>
<span class="go">      in_channels=1, out_channels=6, kernel_size=(5, 5), stride=(1, 1), pad_mode=valid, padding=0, dilation=(1, 1), group=1, has_bias=False</span>
<span class="go">      (fake_quant_weight): SimulatedFakeQuantizerPerChannel&lt;bit_num=8, symmetric=True, narrow_range=False, ema=False(0.999), per_channel=True(0, 6), quant_delay=900&gt;</span>
<span class="go">      (batchnorm): BatchNorm2d&lt;num_features=6, eps=1e-05, momentum=0.0030000000000000027, gamma=Parameter (name=Conv2dBnWithoutFoldQuant._handler.batchnorm.gamma, shape=(6,), dtype=Float32, requires_grad=True), beta=Parameter (name=Conv2dBnWithoutFoldQuant._handler.batchnorm.beta, shape=(6,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=Conv2dBnWithoutFoldQuant._handler.batchnorm.moving_mean, shape=(6,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=Conv2dBnWithoutFoldQuant._handler.batchnorm.moving_variance, shape=(6,), dtype=Float32, requires_grad=False)&gt;</span>
<span class="go">      &gt;</span>
<span class="go">    (_input_quantizer): SimulatedFakeQuantizerPerLayer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;</span>
<span class="go">    (_output_quantizer): SimulatedFakeQuantizerPerLayer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;</span>
<span class="go">    &gt;</span>
<span class="go">  &gt;</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.apply">
<code class="sig-name descname">apply</code><span class="sig-paren">(</span><em class="sig-param">network: Cell</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.apply"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply SimQAT Algorithm on <cite>network</cite>, use the following steps to make <cite>network</cite> available for quantization aware
training:</p>
<ol class="arabic simple">
<li><p>Fuse certain cells in <cite>network</cite> using pattern engine which is defined by net policy. Default fuse pattern:
Conv2d + BatchNorm2d + ReLU, Conv2d + ReLU, Dense + BatchNorm2d + ReLU, Dense + BatchNorm2d, Dense + ReLU.</p></li>
<li><p>Propagate LayerPolicies defined in NetPolicy through network.</p></li>
<li><p>Reduce redundant fake quantizers which means two or more fake quantizers existing on one tensor.</p></li>
<li><p>Apply LayerPolicies to convert normal cell to <cite>QuantizeWrapperCell</cite>. We will insert real fake quantizer
into network in this step.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>network</strong> (<em>Cell</em>) – Network to be quantized.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Quantized network.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.convert">
<code class="sig-name descname">convert</code><span class="sig-paren">(</span><em class="sig-param">net_opt: Cell</em>, <em class="sig-param">ckpt_path=&quot;&quot;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.convert"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.convert" title="Permalink to this definition">¶</a></dt>
<dd><p>Define how to convert a compressed network to a standard network before exporting to MindIR.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>net_opt</strong> (<em>Cell</em>) – Network to be converted which is transformed by <cite>SimulatedQuantizationAwareTraining.apply</cite>.</p></li>
<li><p><strong>ckpt_path</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Path to checkpoint file for <cite>net_opt</cite>. Default is a empty string which means not loading
checkpoint file to <cite>net_opt</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An instance of Cell represents converted network.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>net_opt</cite> is not Cell.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>ckpt_path</cite> is not string.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – If <cite>ckpt_path</cite> is not empty and invalid.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_act_narrow_range">
<code class="sig-name descname">set_act_narrow_range</code><span class="sig-paren">(</span><em class="sig-param">act_narrow_range</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_act_narrow_range"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_act_narrow_range" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of act_narrow_range of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>act_narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the quantization algorithm use act narrow_range or not. If True then
base on narrow_range, otherwise base on not narrow_range.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>act_narrow_range</cite> is not bool.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_act_per_channel">
<code class="sig-name descname">set_act_per_channel</code><span class="sig-paren">(</span><em class="sig-param">act_per_channel</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_act_per_channel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_act_per_channel" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of act_per_channel of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>act_per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Quantization granularity based on layer or on channel. If True then base on
per channel, otherwise base on per layer. Only support False now.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>act_per_channel</cite> is not bool.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – Only supported if <cite>act_per_channel</cite> is False yet.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_act_quant_delay">
<code class="sig-name descname">set_act_quant_delay</code><span class="sig-paren">(</span><em class="sig-param">act_quant_delay</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_act_quant_delay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_act_quant_delay" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of act_quant_delay of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>act_quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of steps after which activation is quantized during train and eval.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>act_quant_delay</cite> is not int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – act_quant_delay is less than 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_act_quant_dtype">
<code class="sig-name descname">set_act_quant_dtype</code><span class="sig-paren">(</span><em class="sig-param">act_quant_dtype</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_act_quant_dtype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_act_quant_dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of act_quant_dtype of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>act_quant_dtype</strong> (<em>QuantDtype</em>) – Datatype used to quantize activations.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>act_quant_dtype</cite> is not QuantDtype.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – Only supported if <cite>act_quant_dtype</cite> is <cite>QuantDtype.INT8</cite> yet.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_act_symmetric">
<code class="sig-name descname">set_act_symmetric</code><span class="sig-paren">(</span><em class="sig-param">act_symmetric</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_act_symmetric"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_act_symmetric" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of act_symmetric of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>act_symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the quantization algorithm use act symmetric or not. If True then base on
symmetric, otherwise base on asymmetric.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>act_symmetric</cite> is not bool.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_bn_fold">
<code class="sig-name descname">set_bn_fold</code><span class="sig-paren">(</span><em class="sig-param">bn_fold</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_bn_fold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_bn_fold" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of bn_fold of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>bn_fold</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether quantization algorithm use bn_fold or not.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>bn_fold</cite> is not bool.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_enable_fusion">
<code class="sig-name descname">set_enable_fusion</code><span class="sig-paren">(</span><em class="sig-param">enable_fusion</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_enable_fusion"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_enable_fusion" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of enable_fusion of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>enable_fusion</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether apply fusion before applying quantization.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>enable_fusion</cite> is not bool.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_freeze_bn">
<code class="sig-name descname">set_freeze_bn</code><span class="sig-paren">(</span><em class="sig-param">freeze_bn</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_freeze_bn"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_freeze_bn" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of freeze_bn of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>freeze_bn</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of steps after which BatchNorm OP parameters fixed to global mean and variance.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>freeze_bn</cite> is not int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – <cite>freeze_bn</cite> is less than 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_one_conv_fold">
<code class="sig-name descname">set_one_conv_fold</code><span class="sig-paren">(</span><em class="sig-param">one_conv_fold</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_one_conv_fold"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_one_conv_fold" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of one_conv_fold of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>one_conv_fold</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether quantization algorithm use one_conv_fold or not.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>one_conv_fold</cite> is not bool.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_weight_narrow_range">
<code class="sig-name descname">set_weight_narrow_range</code><span class="sig-paren">(</span><em class="sig-param">weight_narrow_range</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_weight_narrow_range"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_weight_narrow_range" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of weight_narrow_range of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weight_narrow_range</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the quantization algorithm use weight narrow_range or not. If
True then base on narrow_range, otherwise base on not narrow_range.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>weight_narrow_range</cite> is not bool.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_weight_per_channel">
<code class="sig-name descname">set_weight_per_channel</code><span class="sig-paren">(</span><em class="sig-param">weight_per_channel</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_weight_per_channel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_weight_per_channel" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of weight_per_channel of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weight_per_channel</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Quantization granularity based on layer or on channel. If True then base on
per channel, otherwise base on per layer.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>weight_per_channel</cite> is not bool.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_weight_quant_delay">
<code class="sig-name descname">set_weight_quant_delay</code><span class="sig-paren">(</span><em class="sig-param">weight_quant_delay</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_weight_quant_delay"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_weight_quant_delay" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of weight_quant_delay of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weight_quant_delay</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – Number of steps after which weight is quantized during train and eval.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>weight_quant_delay</cite> is not int.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – weight_quant_delay is less than 0.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_weight_quant_dtype">
<code class="sig-name descname">set_weight_quant_dtype</code><span class="sig-paren">(</span><em class="sig-param">weight_quant_dtype</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_weight_quant_dtype"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_weight_quant_dtype" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of weight_quant_dtype of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weight_quant_dtype</strong> (<em>QuantDtype</em>) – Datatype used to quantize weight.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>weight_quant_dtype</cite> is not QuantDtype.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#ValueError" title="(in Python v3.8)"><strong>ValueError</strong></a> – Only supported if <cite>weight_quant_dtype</cite> is <cite>QuantDtype.INT8</cite> yet.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_weight_symmetric">
<code class="sig-name descname">set_weight_symmetric</code><span class="sig-paren">(</span><em class="sig-param">weight_symmetric</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_gs/quantization/simulated_quantization/simulated_quantization_aware_training.html#SimulatedQuantizationAwareTraining.set_weight_symmetric"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_gs.quantization.SimulatedQuantizationAwareTraining.set_weight_symmetric" title="Permalink to this definition">¶</a></dt>
<dd><p>Set value of weight_symmetric of quantization aware training <cite>config</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>weight_symmetric</strong> (<a class="reference external" href="https://docs.python.org/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – Whether the quantization algorithm use weight symmetric or not. If True then
base on symmetric, otherwise base on asymmetric.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – If <cite>weight_symmetric</cite> is not bool.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindspore_gs.quantization.SlbQuantAwareTraining.html" class="btn btn-neutral float-right" title="mindspore_gs.quantization.SlbQuantAwareTraining" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../mindspore_gs.quantization.html" class="btn btn-neutral float-left" title="mindspore_gs.quantization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>