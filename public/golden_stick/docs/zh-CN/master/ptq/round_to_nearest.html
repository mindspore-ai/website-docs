<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>应用RoundToNearest后量化算法 &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script><script async="async" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/mathjax/MathJax-3.2.2/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="剪枝算法概述" href="../pruner/overview.html" />
    <link rel="prev" title="应用SLB算法" href="../quantization/slb.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">安装MindSpore Golden Stick</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">量化感知训练算法</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quantization/overview.html">量化算法概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization/simqat.html">应用SimQAT算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quantization/slb.html">应用SLB算法</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">训练后量化算法</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">应用RoundToNearest后量化算法</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#roundtonearest后量化算法简介">RoundToNearest后量化算法简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="#示例">示例</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#步骤1-环境准备">步骤1. 环境准备</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#11-ascend环境">1.1. Ascend环境</a></li>
<li class="toctree-l4"><a class="reference internal" href="#12-mindspore环境">1.2. MindSpore环境</a></li>
<li class="toctree-l4"><a class="reference internal" href="#13-mindspore-transformers环境">1.3. MindSpore Transformers环境</a></li>
<li class="toctree-l4"><a class="reference internal" href="#14-金箍棒环境">1.4. 金箍棒环境</a></li>
<li class="toctree-l4"><a class="reference internal" href="#15-相关文件准备">1.5. 相关文件准备</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#步骤2-模型量化">步骤2. 模型量化</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#21-实例化mindformerconfig">2.1. 实例化MindFormerConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="#22-实例化llama2网络">2.2. 实例化Llama2网络</a></li>
<li class="toctree-l4"><a class="reference internal" href="#23-实例化rtn算法">2.3. 实例化RTN算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="#24-量化llama2网络并保存ckpt">2.4. 量化Llama2网络并保存ckpt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#步骤3-模型部署">步骤3. 模型部署</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#31-实例化mindformerconfig和llama2网络">3.1. 实例化MindFormerConfig和Llama2网络</a></li>
<li class="toctree-l4"><a class="reference internal" href="#32-加载量化后的ckpt">3.2. 加载量化后的ckpt</a></li>
<li class="toctree-l4"><a class="reference internal" href="#33-评估量化后的网络">3.3. 评估量化后的网络</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#步骤4-效果分析">步骤4. 效果分析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#41-评估fp16网络的perplexity指标">4.1. 评估FP16网络的Perplexity指标</a></li>
<li class="toctree-l4"><a class="reference internal" href="#42-比较结果">4.2. 比较结果</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">剪枝算法</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pruner/overview.html">剪枝算法概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pruner/scop.html">应用SCOP算法</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/overview.html">模型部署概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/convert.html">使用MindSpore Golden Stick进行模型转换</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.html">mindspore_gs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.common.html">mindspore_gs.common</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.quantization.html">mindspore_gs.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.ptq.html">mindspore_gs.ptq</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.pruner.html">mindspore_gs.pruner</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>应用RoundToNearest后量化算法</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/ptq/round_to_nearest.ipynb.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="应用roundtonearest后量化算法">
<h1>应用RoundToNearest后量化算法<a class="headerlink" href="#应用roundtonearest后量化算法" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/golden-stick/blob/r0.4/mindspore_gs/ptq/round_to_nearest/README.ipynb"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.svg" /></a></p>
<section id="roundtonearest后量化算法简介">
<h2>RoundToNearest后量化算法简介<a class="headerlink" href="#roundtonearest后量化算法简介" title="永久链接至标题"></a></h2>
<p>RoundToNearest算法是一类较朴素的后量化算法，其取整方式使用了Round to nearest，即四舍五入的方式。</p>
<p>当前金箍棒中的RoundToNearest后量化（后面使用RTN来简称）主要针对LLM（大语言模型场景），使用MinMax校正器对线性层（Linear）进行量化。伪量化的网络结构示意如下：</p>
<p><img alt="fakequantizer" src="https://gitee.com/mindspore/golden-stick/raw/r0.4/docs/images/ptq/rtn/rtn-fakequantizer.png" /></p>
<p>表1：RTN算法规格</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 9%" />
<col style="width: 91%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>规格</p></th>
<th class="head"><p>规格说明</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>硬件支持</p></td>
<td><p>量化阶段运行在CPU，量化模型推理仅支持Ascend</p></td>
</tr>
<tr class="row-odd"><td><p>网络支持</p></td>
<td><p>Llama2 13B/70B，具体请参见<a class="reference external" href="https://gitee.com/hangangqiang/mindformers/tree/dev/mindformers/models/llama">Llama2网络</a></p></td>
</tr>
<tr class="row-even"><td><p>运行模式支持</p></td>
<td><p>Graph模式和PyNative模式</p></td>
</tr>
</tbody>
</table>
</section>
<section id="示例">
<h2>示例<a class="headerlink" href="#示例" title="永久链接至标题"></a></h2>
<p>跟金箍棒仓所有算法一样，RTN算法的应用主要可以分为两个阶段：量化阶段和部署阶段。</p>
<p>量化阶段是部署前提前完成的，主要的工作是：收集权重的分布、计算量化参数、量化权重数据、插入反量化节点。</p>
<p>部署阶段通常是指用户在生产环境，使用MindSpore框架对量化后的模型进行推理的过程。</p>
<p>本用例使用Llama2网络进行演示，主要分四个步骤：环境准备、模型量化、模型部署评估、效果分析。</p>
<section id="步骤1-环境准备">
<h3>步骤1. 环境准备<a class="headerlink" href="#步骤1-环境准备" title="永久链接至标题"></a></h3>
<section id="11-ascend环境">
<h4>1.1. Ascend环境<a class="headerlink" href="#11-ascend环境" title="永久链接至标题"></a></h4>
<p>RTN算法需要运行在Ascend硬件上，Ascend的环境配置可以参考<a class="reference external" href="https://www.mindspore.cn/install">MindSpore安装指南</a>安装昇腾AI处理器配套软件包小节和配置环境变量小节。</p>
</section>
<section id="12-mindspore环境">
<h4>1.2. MindSpore环境<a class="headerlink" href="#12-mindspore环境" title="永久链接至标题"></a></h4>
<p>金箍棒依赖于MindSpore，需要提前安装合适的MindSpore。可以从MindSpore官网下载预编译好的<a class="reference external" href="https://www.mindspore.cn/versions">v2.3.0-rc1版本安装包</a>并安装。</p>
</section>
<section id="13-mindspore-transformers环境">
<h4>1.3. MindSpore Transformers环境<a class="headerlink" href="#13-mindspore-transformers环境" title="永久链接至标题"></a></h4>
<p>金箍棒依赖于MindSpore Transformers，需要提前安装合适的MindSpore Transformers。可以从MindSpore官网下载预编译好的<a class="reference external" href="https://www.mindspore.cn/versions">v1.1.0-rc1版本安装包</a>并安装。</p>
</section>
<section id="14-金箍棒环境">
<h4>1.4. 金箍棒环境<a class="headerlink" href="#14-金箍棒环境" title="永久链接至标题"></a></h4>
<p>从MindSpore官网下载预编译好的<a class="reference external" href="https://www.mindspore.cn/versions">MindSpore GoldenStick v0.4.0 版本安装包</a>并安装。</p>
</section>
<section id="15-相关文件准备">
<h4>1.5. 相关文件准备<a class="headerlink" href="#15-相关文件准备" title="永久链接至标题"></a></h4>
<p>需要预先下载MindSpore Transformers Llama2网络相关的文件以及评估使用的数据集，包括：wikitext2数据集和Llama2 7B网络相关文件。</p>
<p>第一步创建工作目录：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>mkdir<span class="w"> </span>workspace
</pre></div>
</div>
</div>
<p>第二步准备数据集，由于权限限制，需要手动下载wikitext2数据集：</p>
<p>数据集下载地址：<a class="reference external" href="https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip">WikiText2数据集</a></p>
<p>下载好数据集后，需要将数据集文件拷贝到上一步中创建的workspace目录下，并确保数据集文件名为wikitext-2-v1.zip，然后运行解压代码：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="nb">cd</span><span class="w"> </span>workspace<span class="p">;</span><span class="w"> </span>unzip<span class="w"> </span>wikitext-2-v1.zip
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Archive:  wikitext-2-v1.zip
   creating: wikitext-2/
  inflating: wikitext-2/wiki.test.tokens
  inflating: wikitext-2/wiki.valid.tokens
  inflating: wikitext-2/wiki.train.tokens
</pre></div></div>
</div>
<p>第三步准备Llama2 7B网络checkpoint文件，Llama2分词器文件，Llama2模型配置文件：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="nb">cd</span><span class="w"> </span>workspace<span class="p">;</span><span class="w"> </span>wget<span class="w"> </span>--no-check-certificate<span class="w"> </span>-O<span class="w"> </span>llama2_7b.ckpt<span class="w"> </span>https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/llama2/llama2_7b.ckpt
<span class="o">!</span><span class="nb">cd</span><span class="w"> </span>workspace<span class="p">;</span><span class="w"> </span>wget<span class="w"> </span>--no-check-certificate<span class="w"> </span>-O<span class="w"> </span>tokenizer.model<span class="w"> </span>https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/llama2/tokenizer.model
<span class="o">!</span><span class="nb">cd</span><span class="w"> </span>workspace<span class="p">;</span><span class="w"> </span>cp<span class="w"> </span>../configs/run_llama2_7b_910b.yaml<span class="w"> </span>./
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
--2024-03-19 17:29:17--  https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/llama2/llama2_7b.ckpt
Length: 13476850247 (13G) [binary/octet-stream]
Saving to: ‘llama2_7b.ckpt’

llama2_7b.ckpt      100%[===================&gt;]  12.55G  27.5MB/s    in 7m 39s

2024-03-19 17:36:57 (28.0 MB/s) - ‘llama2_7b.ckpt’ saved [13476850247/13476850247]

--2024-03-19 17:36:57--  https://ascend-repo-modelzoo.obs.cn-east-2.myhuaweicloud.com/MindFormers/llama2/tokenizer.model
Length: 499723 (488K) [binary/octet-stream]
Saving to: ‘tokenizer.model’

tokenizer.model     100%[===================&gt;] 488.01K  --.-KB/s    in 0.1s

2024-03-19 17:36:57 (3.37 MB/s) - ‘tokenizer.model’ saved [499723/499723]

</pre></div></div>
</div>
<blockquote>
<div><p>下载时如果遇到网络问题，可以尝试使用浏览器手动下载相应文件，并放到相应目录下</p>
</div></blockquote>
<p>完成上述准备后，检查目录结构：</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="nb">cd</span><span class="w"> </span>workspace<span class="p">;</span><span class="w"> </span>tree<span class="w"> </span>-L<span class="w"> </span><span class="m">2</span><span class="w"> </span>-U
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-blue-intense-fg ansi-bold">.</span>
├── llama2_7b.ckpt
├── <span class="ansi-blue-intense-fg ansi-bold">wikitext-2</span>
│   ├── wiki.train.tokens
│   ├── wiki.test.tokens
│   └── wiki.valid.tokens
├── tokenizer.model
├── <span class="ansi-red-intense-fg ansi-bold">wikitext-2-v1.zip</span>
└── run_llama2_7b_910b.yaml

1 directory, 7 files
</pre></div></div>
</div>
</section>
</section>
<section id="步骤2-模型量化">
<h3>步骤2. 模型量化<a class="headerlink" href="#步骤2-模型量化" title="永久链接至标题"></a></h3>
<section id="21-实例化mindformerconfig">
<h4>2.1. 实例化MindFormerConfig<a class="headerlink" href="#21-实例化mindformerconfig" title="永久链接至标题"></a></h4>
<p>构造MindSpore Transformers仓的Llama2网络，首先需要构造MindFormerConfig配置项，这里先实现一个创建MindFormerConfig的工具函数，并定义了一些常量：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">mindformers</span> <span class="kn">import</span> <span class="n">LlamaForCausalLM</span><span class="p">,</span> <span class="n">MindFormerConfig</span><span class="p">,</span> <span class="n">LlamaConfig</span><span class="p">,</span> <span class="n">init_context</span><span class="p">,</span> <span class="n">TransformerOpParallelConfig</span>


<span class="k">def</span> <span class="nf">_set_config</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">device_target</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;setup MindFormerConfig&quot;&quot;&quot;</span>
    <span class="n">mfconfig</span> <span class="o">=</span> <span class="n">MindFormerConfig</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">device_id</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
        <span class="n">mfconfig</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="n">device_id</span>
    <span class="n">mfconfig</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">device_target</span> <span class="o">=</span> <span class="n">device_target</span>
    <span class="n">mfconfig</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span> <span class="o">=</span> <span class="n">LlamaConfig</span><span class="p">(</span><span class="o">**</span><span class="n">mfconfig</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="p">)</span>

    <span class="n">init_context</span><span class="p">(</span><span class="n">use_parallel</span><span class="o">=</span><span class="n">mfconfig</span><span class="o">.</span><span class="n">use_parallel</span><span class="p">,</span> <span class="n">context_config</span><span class="o">=</span><span class="n">mfconfig</span><span class="o">.</span><span class="n">context</span><span class="p">,</span> <span class="n">parallel_config</span><span class="o">=</span><span class="n">mfconfig</span><span class="o">.</span><span class="n">parallel</span><span class="p">)</span>

    <span class="n">parallel_config</span> <span class="o">=</span> <span class="n">TransformerOpParallelConfig</span><span class="p">(</span><span class="o">**</span><span class="n">mfconfig</span><span class="o">.</span><span class="n">parallel_config</span><span class="p">)</span>
    <span class="n">mfconfig</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">parallel_config</span> <span class="o">=</span> <span class="n">parallel_config</span>
    <span class="n">mfconfig</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">checkpoint_name_or_path</span> <span class="o">=</span> <span class="n">mfconfig</span><span class="o">.</span><span class="n">load_checkpoint</span>
    <span class="k">return</span> <span class="n">mfconfig</span>


<span class="k">def</span> <span class="nf">create_mfconfig</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">device_target</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">tokenizer_path</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">model_parallel</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create mindformers config for llama2 network for example.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">model_parallel</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">use_parallel</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">use_parallel</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">model_parallel</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">_set_config</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">device_target</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>
    <span class="n">config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">bs</span>
    <span class="n">config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">seq_length</span> <span class="o">=</span> <span class="n">seq_len</span>
    <span class="n">config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">compute_dtype</span> <span class="o">=</span> <span class="n">compute_dtype</span>
    <span class="n">config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">layernorm_compute_type</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">softmax_compute_type</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
    <span class="n">config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">rotary_dtype</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
    <span class="n">config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">param_init_type</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">float16</span>
    <span class="n">config</span><span class="o">.</span><span class="n">processor</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_file</span> <span class="o">=</span> <span class="n">tokenizer_path</span>
    <span class="n">config</span><span class="o">.</span><span class="n">load_checkpoint</span> <span class="o">=</span> <span class="n">ckpt_path</span>
    <span class="n">config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">checkpoint_name_or_path</span> <span class="o">=</span> <span class="n">ckpt_path</span>
    <span class="n">config</span><span class="o">.</span><span class="n">use_parallel</span> <span class="o">=</span> <span class="n">use_parallel</span>
    <span class="n">config</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="n">model_parallel</span>
    <span class="k">return</span> <span class="n">config</span>

<span class="n">llama2_config_file</span> <span class="o">=</span> <span class="s2">&quot;./workspace/run_llama2_7b_910b.yaml&quot;</span>
<span class="n">llama2_w16a16_ckpt_file</span> <span class="o">=</span> <span class="s2">&quot;./workspace/llama2_7b.ckpt&quot;</span>
<span class="n">llama2_w8a16_ckpt_file</span> <span class="o">=</span> <span class="s2">&quot;./workspace/llama2-7b-w8a16.ckpt&quot;</span>
<span class="n">vocab_file</span> <span class="o">=</span> <span class="s2">&quot;./workspace/tokenizer.model&quot;</span>
<span class="n">wikitext2_ds_path</span> <span class="o">=</span> <span class="s2">&quot;./workspace/wikitext-2/wiki.valid.tokens&quot;</span>
<span class="n">bs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">device_id</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<blockquote>
<div><p>代码中的device_id变量可以根据运行环境中Ascend硬件空闲情况修改。</p>
</div></blockquote>
<p>实例化一个MindFormerConfig对象：</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">context</span>

<span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">GRAPH_MODE</span><span class="p">)</span>
<span class="n">quant_network_config</span> <span class="o">=</span> <span class="n">create_mfconfig</span><span class="p">(</span><span class="n">llama2_config_file</span><span class="p">,</span> <span class="s2">&quot;CPU&quot;</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="o">=</span><span class="n">llama2_w16a16_ckpt_file</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="22-实例化llama2网络">
<h4>2.2. 实例化Llama2网络<a class="headerlink" href="#22-实例化llama2网络" title="永久链接至标题"></a></h4>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindformers</span> <span class="kn">import</span> <span class="n">LlamaForCausalLM</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">LlamaForCausalLM</span><span class="p">(</span><span class="n">quant_network_config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="p">)</span>
<span class="n">network</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">network</span><span class="o">.</span><span class="n">phase</span> <span class="o">=</span> <span class="s1">&#39;predict&#39;</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
2024-03-19 18:52:23,380 - mindformers[mindformers/version_control.py:62] - INFO - The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages &lt;= 1
2024-03-19 18:52:23,382 - mindformers[mindformers/version_control.py:66] - INFO -
The current ENABLE_CELL_REUSE=0, please set the environment variable as follows:
export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.
2024-03-19 18:52:23,383 - mindformers[mindformers/version_control.py:72] - INFO - The Cell Reuse compilation acceleration feature does not support single-card mode.This feature is disabled by default. ENABLE_CELL_REUSE=1 does not take effect.
2024-03-19 18:52:23,384 - mindformers[mindformers/version_control.py:75] - INFO - The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage&gt;1).Current pipeline stage=1, the feature is disabled by default.
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
......
[WARNING] ME(1746443:281473469290880,MainProcess):2024-03-19-18:55:07.118.525 [mindspore/train/serialization.py:185] The type of model.layers.31.attention_norm.weight:Float16 in &#39;parameter_dict&#39; is different from the type of it in &#39;net&#39;:Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1746443:281473469290880,MainProcess):2024-03-19-18:55:07.123.086 [mindspore/train/serialization.py:185] The type of model.layers.31.ffn_norm.weight:Float16 in &#39;parameter_dict&#39; is different from the type of it in &#39;net&#39;:Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(1746443:281473469290880,MainProcess):2024-03-19-18:55:07.751.733 [mindspore/train/serialization.py:185] The type of model.norm_out.weight:Float16 in &#39;parameter_dict&#39; is different from the type of it in &#39;net&#39;:Float32, then the type convert from Float16 to Float32 in the network.
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
2024-03-19 18:55:08,105 - mindformers[mindformers/models/modeling_utils.py:1413] - INFO - weights in ./workspace/llama2_7b.ckpt are loaded
</pre></div></div>
</div>
</section>
<section id="23-实例化rtn算法">
<h4>2.3. 实例化RTN算法<a class="headerlink" href="#23-实例化rtn算法" title="永久链接至标题"></a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_gs.common</span> <span class="kn">import</span> <span class="n">BackendTarget</span>
<span class="kn">from</span> <span class="nn">mindspore_gs.ptq</span> <span class="kn">import</span> <span class="n">PTQConfig</span><span class="p">,</span> <span class="n">PTQMode</span>
<span class="kn">from</span> <span class="nn">mindspore_gs.ptq</span> <span class="kn">import</span> <span class="n">RoundToNearest</span> <span class="k">as</span> <span class="n">RTN</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">PTQConfig</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">PTQMode</span><span class="o">.</span><span class="n">QUANTIZE</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">BackendTarget</span><span class="o">.</span><span class="n">ASCEND</span><span class="p">)</span>
<span class="n">ptq</span> <span class="o">=</span> <span class="n">RTN</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">cfg</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="24-量化llama2网络并保存ckpt">
<h4>2.4. 量化Llama2网络并保存ckpt<a class="headerlink" href="#24-量化llama2网络并保存ckpt" title="永久链接至标题"></a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">qnet</span> <span class="o">=</span> <span class="n">ptq</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">network</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">qnet</span> <span class="o">=</span> <span class="n">ptq</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">qnet</span><span class="p">)</span>
<span class="n">network</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">qnet</span>
<span class="n">ms</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">llama2_w8a16_ckpt_file</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>成功运行后，会在当前目录下生成<code class="docutils literal notranslate"><span class="pre">llama2-7b-w8a16.ckpt</span></code>文件。</p>
</section>
</section>
<section id="步骤3-模型部署">
<h3>步骤3. 模型部署<a class="headerlink" href="#步骤3-模型部署" title="永久链接至标题"></a></h3>
<section id="31-实例化mindformerconfig和llama2网络">
<h4>3.1. 实例化MindFormerConfig和Llama2网络<a class="headerlink" href="#31-实例化mindformerconfig和llama2网络" title="永久链接至标题"></a></h4>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">context</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_target</span><span class="o">=</span><span class="s2">&quot;Ascend&quot;</span><span class="p">)</span>
<span class="n">deploy_network_config</span> <span class="o">=</span> <span class="n">create_mfconfig</span><span class="p">(</span><span class="n">llama2_config_file</span><span class="p">,</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">deploy_network</span> <span class="o">=</span> <span class="n">LlamaForCausalLM</span><span class="p">(</span><span class="n">deploy_network_config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="p">)</span>
<span class="n">deploy_network</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">deploy_network</span><span class="o">.</span><span class="n">phase</span> <span class="o">=</span> <span class="s1">&#39;predict&#39;</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
2024-03-19 19:19:37,710 - mindformers[mindformers/version_control.py:62] - INFO - The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages &lt;= 1
2024-03-19 19:19:37,710 - mindformers[mindformers/version_control.py:66] - INFO -
The current ENABLE_CELL_REUSE=0, please set the environment variable as follows:
export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.
2024-03-19 19:19:37,711 - mindformers[mindformers/version_control.py:72] - INFO - The Cell Reuse compilation acceleration feature does not support single-card mode.This feature is disabled by default. ENABLE_CELL_REUSE=1 does not take effect.
2024-03-19 19:19:37,712 - mindformers[mindformers/version_control.py:75] - INFO - The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage&gt;1).Current pipeline stage=1, the feature is disabled by default.
2024-03-19 19:21:07,859 - mindformers[mindformers/models/modeling_utils.py:1415] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
</pre></div></div>
</div>
</section>
<section id="32-加载量化后的ckpt">
<h4>3.2. 加载量化后的ckpt<a class="headerlink" href="#32-加载量化后的ckpt" title="永久链接至标题"></a></h4>
<p>由于MindSpore当前不支持保存修改后的网络，所以在加载量化ckpt之前，需要先用算法恢复带量化结构的网络，然后再加载ckpt到网络。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">deploy_cfg</span> <span class="o">=</span> <span class="n">PTQConfig</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">PTQMode</span><span class="o">.</span><span class="n">DEPLOY</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="n">BackendTarget</span><span class="o">.</span><span class="n">ASCEND</span><span class="p">)</span>
<span class="n">deploy_ptq</span> <span class="o">=</span> <span class="n">RTN</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">deploy_cfg</span><span class="p">)</span>
<span class="n">deploy_network</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">deploy_ptq</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">deploy_network</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">deploy_network</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">deploy_ptq</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">deploy_network</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">llama2_w8a16_ckpt_file</span><span class="p">,</span> <span class="n">deploy_network</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
{&#39;model.tok_embeddings.embedding_weight&#39;: Parameter (name=model.tok_embeddings.embedding_weight, shape=(32000, 4096), dtype=Float16, requires_grad=True),
 ......
 &#39;model.layers.31.feed_forward.w3._weight_quantizer.scale&#39;: Parameter (name=model.layers.31.feed_forward.w3._weight_quantizer.scale, shape=(11008,), dtype=Float16, requires_grad=True),
 &#39;model.layers.31.feed_forward.w3._weight_quantizer.zp_neg&#39;: Parameter (name=model.layers.31.feed_forward.w3._weight_quantizer.zp_neg, shape=(11008,), dtype=Float16, requires_grad=True),
 &#39;model.norm_out.weight&#39;: Parameter (name=model.norm_out.weight, shape=(4096,), dtype=Float32, requires_grad=True),
 &#39;lm_head.weight&#39;: Parameter (name=lm_head.weight, shape=(32000, 4096), dtype=Float16, requires_grad=True)}
</pre></div></div>
</div>
</section>
<section id="33-评估量化后的网络">
<h4>3.3. 评估量化后的网络<a class="headerlink" href="#33-评估量化后的网络" title="永久链接至标题"></a></h4>
<p>本示例对Llama2在wikitext2数据集上评估Perplexity指标。使用步骤1中下载好的分词器和数据集文件分别实例化分词器对象和数据集对象，并实例化PerplexityMetric对象作为metric。</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindformers</span> <span class="kn">import</span> <span class="n">LlamaTokenizer</span>
<span class="kn">from</span> <span class="nn">mindformers.core.metric</span> <span class="kn">import</span> <span class="n">PerplexityMetric</span>
<span class="kn">from</span> <span class="nn">mindspore_gs.datasets</span> <span class="kn">import</span> <span class="n">create_wikitext_dataset</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">LlamaTokenizer</span><span class="p">(</span><span class="n">vocab_file</span><span class="o">=</span><span class="n">vocab_file</span><span class="p">)</span>
<span class="n">deploy_ds</span> <span class="o">=</span> <span class="n">create_wikitext_dataset</span><span class="p">(</span><span class="n">wikitext2_ds_path</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">deploy_metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;PerplexityMetric&quot;</span><span class="p">:</span> <span class="n">PerplexityMetric</span><span class="p">()}</span>
<span class="n">deploy_model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">deploy_network</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">deploy_metrics</span><span class="p">,</span> <span class="n">eval_network</span><span class="o">=</span><span class="n">deploy_network</span><span class="p">)</span>
<span class="n">quant_ppl</span> <span class="o">=</span> <span class="n">deploy_model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">deploy_ds</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="n">deploy_network_config</span><span class="o">.</span><span class="n">runner_config</span><span class="o">.</span><span class="n">sink_mode</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;W8A16 Perplexity: </span><span class="si">{</span><span class="n">quant_ppl</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[INFO] GE(1746443,python):2024-03-19-19:25:18.990.947 [ge_api.cc:523][status:INIT]1746443 AddGraph:Start to add graph in Session. graph_id: 1, graph_name: kernel_graph224, session_id: 0.
[INFO] GE(1746443,python):2024-03-19-19:25:18.991.481 [ge_api.cc:1154][status:INIT]1746443 CompileGraph:Start to compile graph, graph_id: 1
[INFO] GE(1746443,python):2024-03-19-19:25:18.991.586 [graph_manager.cc:1264][EVENT]1746443 PreRun:PreRun start: graph node size 1, session id 0, graph id 1, graph name kernel_graph224.
[INFO] GE(1746443,python):2024-03-19-19:25:19.065.657 [ge_api.cc:1160][status:STOP]1746443 CompileGraph:Compile graph success.
[INFO] GE(1746443,python):2024-03-19-19:25:19.067.797 [ge_api.cc:787][status:INIT]2453595 RunGraphWithStreamAsync:Session run graph with stream async, session_id: 0, graph_id: 1, input size 0, output size 0
[INFO] GE(1746443,python):2024-03-19-19:25:19.079.152 [ge_api.cc:799][status:STOP]2453595 RunGraphWithStreamAsync:Session run graph with stream async finished.
[INFO] GE(1746443,python):2024-03-19-19:26:40.520.923 [ge_api.cc:523][status:INIT]1746443 AddGraph:Start to add graph in Session. graph_id: 2, graph_name: kernel_graph225, session_id: 0.
[INFO] GE(1746443,python):2024-03-19-19:26:40.581.045 [ge_api.cc:1154][status:INIT]1746443 CompileGraph:Start to compile graph, graph_id: 2
[INFO] GE(1746443,python):2024-03-19-19:26:40.633.523 [graph_manager.cc:1264][EVENT]1746443 PreRun:PreRun start: graph node size 3025, session id 0, graph id 2, graph name kernel_graph225.
[INFO] GE(1746443,python):2024-03-19-19:28:24.659.856 [ge_api.cc:799][status:STOP]2453595 RunGraphWithStreamAsync:Session run graph with stream async finished.
[INFO] GE(1746443,python):2024-03-19-19:28:24.665.855 [ge_api.cc:787][status:INIT]2453595 RunGraphWithStreamAsync:Session run graph with stream async, session_id: 0, graph_id: 2, input size 739, output size 3
[INFO] GE(1746443,python):2024-03-19-19:28:24.667.497 [ge_api.cc:799][status:STOP]2453595 RunGraphWithStreamAsync:Session run graph with stream async finished.
[INFO] GE(1746443,python):2024-03-19-19:28:25.267.844 [ge_api.cc:787][status:INIT]2453595 RunGraphWithStreamAsync:Session run graph with stream async, session_id: 0, graph_id: 3, input size 3, output size 1
......
[INFO] GE(1746443,python):2024-03-19-19:29:18.708.299 [ge_api.cc:799][status:STOP]2453595 RunGraphWithStreamAsync:Session run graph with stream async finished.
W8A16 Perplexity: {&#39;PerplexityMetric&#39;: {&#39;loss&#39;: 2.910757654840339, &#39;PPL&#39;: 18.370711954412435}}
</pre></div></div>
</div>
</section>
</section>
<section id="步骤4-效果分析">
<h3>步骤4. 效果分析<a class="headerlink" href="#步骤4-效果分析" title="永久链接至标题"></a></h3>
<section id="41-评估fp16网络的perplexity指标">
<h4>4.1. 评估FP16网络的Perplexity指标<a class="headerlink" href="#41-评估fp16网络的perplexity指标" title="永久链接至标题"></a></h4>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">from</span> <span class="nn">mindformers</span> <span class="kn">import</span> <span class="n">LlamaForCausalLM</span><span class="p">,</span> <span class="n">LlamaTokenizer</span>
<span class="kn">from</span> <span class="nn">mindformers.core.metric</span> <span class="kn">import</span> <span class="n">PerplexityMetric</span>
<span class="kn">from</span> <span class="nn">mindspore_gs.datasets</span> <span class="kn">import</span> <span class="n">create_wikitext_dataset</span>

<span class="n">fp16_network_config</span> <span class="o">=</span> <span class="n">create_mfconfig</span><span class="p">(</span><span class="n">llama2_config_file</span><span class="p">,</span> <span class="s2">&quot;Ascend&quot;</span><span class="p">,</span> <span class="n">device_id</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="o">=</span><span class="n">llama2_w16a16_ckpt_file</span><span class="p">)</span>
<span class="n">fp16_network</span> <span class="o">=</span> <span class="n">LlamaForCausalLM</span><span class="p">(</span><span class="n">fp16_network_config</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">model_config</span><span class="p">)</span>
<span class="n">fp16_network</span><span class="o">.</span><span class="n">set_train</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">fp16_network</span><span class="o">.</span><span class="n">phase</span> <span class="o">=</span> <span class="s1">&#39;predict&#39;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">LlamaTokenizer</span><span class="p">(</span><span class="n">vocab_file</span><span class="o">=</span><span class="n">vocab_file</span><span class="p">)</span>
<span class="n">fp16_ds</span> <span class="o">=</span> <span class="n">create_wikitext_dataset</span><span class="p">(</span><span class="n">wikitext2_ds_path</span><span class="p">,</span> <span class="n">bs</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">fp16_metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;PerplexityMetric&quot;</span><span class="p">:</span> <span class="n">PerplexityMetric</span><span class="p">()}</span>
<span class="n">fp16_model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">fp16_network</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">fp16_metrics</span><span class="p">,</span> <span class="n">eval_network</span><span class="o">=</span><span class="n">fp16_network</span><span class="p">)</span>
<span class="n">fp16_ppl</span> <span class="o">=</span> <span class="n">fp16_model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">fp16_ds</span><span class="p">,</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="n">fp16_network_config</span><span class="o">.</span><span class="n">runner_config</span><span class="o">.</span><span class="n">sink_mode</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;FP16 Perplexity: </span><span class="si">{</span><span class="n">fp16_ppl</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
......
[WARNING] ME(2617230:281472981539200,MainProcess):2024-03-19-19:41:51.626.9 [mindspore/train/serialization.py:185] The type of model.layers.31.attention_norm.weight:Float16 in &#39;parameter_dict&#39; is different from the type of it in &#39;net&#39;:Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(2617230:281472981539200,MainProcess):2024-03-19-19:41:51.881.3 [mindspore/train/serialization.py:185] The type of model.layers.31.ffn_norm.weight:Float16 in &#39;parameter_dict&#39; is different from the type of it in &#39;net&#39;:Float32, then the type convert from Float16 to Float32 in the network.
[WARNING] ME(2617230:281472981539200,MainProcess):2024-03-19-19:41:51.695.148 [mindspore/train/serialization.py:185] The type of model.norm_out.weight:Float16 in &#39;parameter_dict&#39; is different from the type of it in &#39;net&#39;:Float32, then the type convert from Float16 to Float32 in the network.
2024-03-19 19:41:52,132 - mindformers[mindformers/models/modeling_utils.py:1413] - INFO - weights in ./workspace/llama2_7b.ckpt are loaded
[INFO] GE(2617230,python):2024-03-19-19:42:00.316.847 [ge_api.cc:523][status:INIT]2617230 AddGraph:Start to add graph in Session. graph_id: 1, graph_name: kernel_graph0, session_id: 0.
[INFO] GE(2617230,python):2024-03-19-19:42:00.317.200 [ge_api.cc:1154][status:INIT]2617230 CompileGraph:Start to compile graph, graph_id: 1
[INFO] GE(2617230,python):2024-03-19-19:42:00.317.282 [graph_manager.cc:1264][EVENT]2617230 PreRun:PreRun start: graph node size 1, session id 0, graph id 1, graph name kernel_graph0.
......
[INFO] GE(2617230,python):2024-03-19-19:43:17.424.380 [ge_api.cc:787][status:INIT]2654383 RunGraphWithStreamAsync:Session run graph with stream async, session_id: 0, graph_id: 2, input size 291, output size 3
[INFO] GE(2617230,python):2024-03-19-19:43:17.424.812 [ge_api.cc:799][status:STOP]2654383 RunGraphWithStreamAsync:Session run graph with stream async finished.
[INFO] GE(2617230,python):2024-03-19-19:43:17.464.158 [ge_api.cc:787][status:INIT]2654383 RunGraphWithStreamAsync:Session run graph with stream async, session_id: 0, graph_id: 3, input size 3, output size 1
[INFO] GE(2617230,python):2024-03-19-19:43:17.464.296 [ge_api.cc:799][status:STOP]2654383 RunGraphWithStreamAsync:Session run graph with stream async finished.
FP16 Perplexity: {&#39;PerplexityMetric&#39;: {&#39;loss&#39;: 2.909490694278072, &#39;PPL&#39;: 18.347451724873594}}
</pre></div></div>
</div>
</section>
<section id="42-比较结果">
<h4>4.2. 比较结果<a class="headerlink" href="#42-比较结果" title="永久链接至标题"></a></h4>
<p>表2：Llama2 7B网络RTN算法量化前后对比</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 55%" />
<col style="width: 16%" />
<col style="width: 16%" />
<col style="width: 13%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>指标</p></th>
<th class="head"><p>FP16</p></th>
<th class="head"><p>W8A16</p></th>
<th class="head"><p>收益</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ckpt-size(GB)↓</p></td>
<td><p>13</p></td>
<td><p>7.1</p></td>
<td><p>-5.9</p></td>
</tr>
<tr class="row-odd"><td><p>wikitext2-Perplexity↓</p></td>
<td><p>18.347</p></td>
<td><p>18.370</p></td>
<td><p>0.023</p></td>
</tr>
</tbody>
</table>
<p>可以看到，经过RTN量化算法处理后：</p>
<ol class="arabic simple">
<li><p>量化后网络的参数量缩减了5.9GB，只剩下原Float16时的54.6%，即网络部署时，用于静态权重存储的显存下降到Float16时的54.6%。因而量化后的网络可以在资源更紧张的环境上部署，或者在相同的环境中提供更大的吞吐量。</p></li>
<li><p>量化后网络的在wikitext2数据集上的混淆度有极微小的上升，即量化后网络在wikitext2上生成式任务的效果几乎无损。</p></li>
</ol>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../quantization/slb.html" class="btn btn-neutral float-left" title="应用SLB算法" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="../pruner/overview.html" class="btn btn-neutral float-right" title="剪枝算法概述" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>