

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>应用SLB算法 &mdash; MindSpore master documentation</title>
  

  
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="剪枝算法概述" href="../pruner/overview.html" />
    <link rel="prev" title="应用SimQAT算法" href="simqat.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">安装MindSpore Golden Stick</a></li>
</ul>
<p class="caption"><span class="caption-text">量化算法</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">量化算法概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="simqat.html">应用SimQAT算法</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">应用SLB算法</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#背景">背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="#算法原理介绍">算法原理介绍</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#温度因子">温度因子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#算法特点">算法特点</a></li>
<li class="toctree-l2"><a class="reference internal" href="#slb量化训练">SLB量化训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="#slb量化训练示例">SLB量化训练示例</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#加载数据集">加载数据集</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义原网络">定义原网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="#应用量化算法">应用量化算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义优化器损失函数和训练的callbacks">定义优化器、损失函数和训练的callbacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#训练模型保存模型文件">训练模型，保存模型文件</a></li>
<li class="toctree-l3"><a class="reference internal" href="#加载模型对比精度">加载模型，对比精度</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#参考文献">参考文献</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">剪枝算法</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pruner/overview.html">剪枝算法概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pruner/scop.html">应用SCOP算法</a></li>
</ul>
<p class="caption"><span class="caption-text">API参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.html">mindspore_gs</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>应用SLB算法</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/quantization/slb.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="应用slb算法">
<h1>应用SLB算法<a class="headerlink" href="#应用slb算法" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/docs/golden_stick/docs/source_zh_cn/quantization/slb.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source.png"></a></p>
<div class="section" id="背景">
<h2>背景<a class="headerlink" href="#背景" title="Permalink to this headline">¶</a></h2>
<p>传统的量化方法在计算梯度时，通常使用STE(Straight Through Estimator) [1]或者自行设计的梯度计算方式[2]。量化函数的不可微往往会导致计算出来的梯度有误差，从而提供不准确的优化方向，导致最终推理精度比较差。因此，迫切需要一种能规避这种不准确梯度估计的量化神经网络学习方法。</p>
</div>
<div class="section" id="算法原理介绍">
<h2>算法原理介绍<a class="headerlink" href="#算法原理介绍" title="Permalink to this headline">¶</a></h2>
<p>SLB(Searching for low-bit weights) [3]是华为诺亚自研的权重量化算法，提供了一种基于权值搜索的低比特量化算法，能避开不准确的梯度估计。针对低比特网络量化，由于量化网络权值的有效解数量比较少，因此，对网络的量化可以通过对权值搜索实现，即将量化过程转换成权值搜索的过程。对给定量化网络预设一组量化权值，然后定义一个概率矩阵来表示不同量化权值被保留的概率，在训练阶段通过优化概率矩阵实现网络权重的量化。</p>
<p>下面左边图是用传统量化算法做二值量化，训练时用不准确的梯度更新浮点权重，最后对浮点权重做二值化(用sigmoid函数)处理得到量化权重。右边图是用SLB量化算法做二值量化，利用连续松弛策略搜索离散权重，训练时优化离散权重的权值概率矩阵，最后根据概率挑选离散权重实现量化。左边图中红色点对应的单个值是由sigmoid函数得到，表示权重被量化为-1的概率。蓝色点对应的单个值是由sigmoid函数得到，表示权重被量化为+1的概率。传统量化算法中不准确的梯度更新会影响浮点权重的更新，从而导致这里的概率出现较大的偏差。右边图中红蓝相间的点对应的2个值是由softmax函数得到，表示权重被量化为-1或+1的概率。由于避开了不准确的梯度更新，这里的概率会更精准。</p>
<p><img alt="SLB算法对比" src="../_images/slb_1.png" /></p>
<div class="section" id="温度因子">
<h3>温度因子<a class="headerlink" href="#温度因子" title="Permalink to this headline">¶</a></h3>
<p>在分类任务中，softmax分布通常用于计算输出被分为各个类的概率。因此，SLB也使用softmax分布来计算权重被量化为各个量化权值的概率，并最终根据最大概率挑选对应权值作为量化结果。为了提高量化结果的置信度，SLB引入了温度因子，通过逐步调整温度因子，能使softmax分布逐渐变得陡峭，慢慢趋近于one-hot分布，从而最大化量化结果的置信度，缩减量化误差。</p>
<p>下面左边公式是标准的softmax函数，右边是SLB算法中引入了温度因子后的softmax函数。</p>
<p><img alt="softmax函数" src="../_images/slb_2.png" /></p>
<p>下图展示了逐步调整温度因子时，softmax分布的变化过程，最右侧是one-hot分布。</p>
<p><img alt="softmax分布变化" src="../_images/slb_3.png" /></p>
</div>
</div>
<div class="section" id="算法特点">
<h2>算法特点<a class="headerlink" href="#算法特点" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>提出了一种新的权值搜索方法，用于训练量化深度神经网络，能规避不准确梯度估计。</p></li>
<li><p>利用连续松弛策略搜索离散权重，训练时优化离散权重的概率分布，最后根据概率挑选离散权重实现量化。</p></li>
<li><p>为了进一步消除搜索后的推理精度差距，保证训练和测试的一致性，提出了逐步调整温度因子的策略。</p></li>
<li><p>与传统的量化算法相比，规避了不准确的梯度更新过程，能获得更高的推理精度，在极低比特量化中更有优势。</p></li>
</ul>
</div>
<div class="section" id="slb量化训练">
<h2>SLB量化训练<a class="headerlink" href="#slb量化训练" title="Permalink to this headline">¶</a></h2>
<p>SLB量化算法的训练规格如下表所示。</p>
<p>表1：SLB量化训练规格</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>规格</th>
<th>规格说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>硬件支持</td>
<td>GPU</td>
</tr>
<tr>
<td>网络支持</td>
<td>ResNet18，具体请参见<a href="https://gitee.com/mindspore/models/tree/r1.8/official/cv/resnet#应用mindspore-golden-stick模型压缩算法">https://gitee.com/mindspore/models/tree/r1.8/official/cv/resnet#应用mindspore-golden-stick模型压缩算法</a>。</td>
</tr>
<tr>
<td>方案支持</td>
<td>支持1、2、4比特的权重量化方案。</td>
</tr>
<tr>
<td>数据类型支持</td>
<td>GPU平台支持FP32。</td>
</tr>
<tr>
<td>运行模式支持</td>
<td>Graph模式和PyNative模式。</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="slb量化训练示例">
<h2>SLB量化训练示例<a class="headerlink" href="#slb量化训练示例" title="Permalink to this headline">¶</a></h2>
<p>SLB量化训练与一般训练步骤一致，在定义量化网络和生成量化模型阶段需要进行额外的操作，完整流程如下：</p>
<ol class="simple">
<li><p>加载数据集，处理数据。</p></li>
<li><p>定义网络。</p></li>
<li><p>定义SLB量化算法，应用算法生成量化模型。</p></li>
<li><p>定义优化器、损失函数和callbacks。</p></li>
<li><p>训练网络，保存模型文件。</p></li>
<li><p>加载模型文件，对比量化后精度。</p></li>
</ol>
<p>接下来以ResNet18网络为例，分别叙述这些步骤。</p>
<blockquote>
<div><p>完整代码见<a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/README_CN.md#%E5%BA%94%E7%94%A8mindspore-golden-stick%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95">resnet模型仓</a>，其中<a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/golden_stick/quantization/slb/train.py">train.py</a>为完整的训练代码，<a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/golden_stick/quantization/slb/eval.py">eval.py</a>为精度验证代码。</p>
</div></blockquote>
<div class="section" id="加载数据集">
<h3>加载数据集<a class="headerlink" href="#加载数据集" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">do_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">train_image_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">train_image_size</span><span class="p">,</span>
                         <span class="n">eval_image_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">eval_image_size</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device_target</span><span class="p">,</span>
                         <span class="n">distribute</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">run_distribute</span><span class="p">)</span>
</pre></div>
</div>
<p>代码中create_dataset引用自<a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/src/dataset.py">dataset.py</a>，config.data_path和config.batch_size分别在<a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/golden_stick/quantization/slb/resnet18_cifar10_config.yaml">配置文件</a>中配置，下同。</p>
</div>
<div class="section" id="定义原网络">
<h3>定义原网络<a class="headerlink" href="#定义原网络" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">src.resnet</span> <span class="kn">import</span> <span class="n">resnet18</span> <span class="k">as</span> <span class="n">resnet</span>

<span class="o">...</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">class_num</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
<p>原始网络结构如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ResNet&lt;
  (conv1): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
  (bn1): BatchNorm2d&lt;num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
  (pad): Pad&lt;&gt;
  (maxpool): MaxPool2d&lt;kernel_size=3, stride=2, pad_mode=VALID&gt;
  (layer1): SequentialCell&lt;
    (0): ResidualBlockBase&lt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.0.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.0.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.0.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.0.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.0.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.0.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.0.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.0.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      &gt;
    (1): ResidualBlockBase&lt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.1.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.1.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.1.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.1.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.1.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.1.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.1.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.1.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      &gt;
    &gt;
  (layer2): SequentialCell&lt;...&gt;
  (layer3): SequentialCell&lt;...&gt;
  (layer4): SequentialCell&lt;...&gt;
  (flatten): Flatten&lt;&gt;
  (end_point): Dense&lt;input_channels=512, output_channels=10, has_bias=True&gt;
  &gt;
</pre></div>
</div>
<p>ResNet18网络定义见<a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/src/resnet.py">resnet.py</a>。</p>
</div>
<div class="section" id="应用量化算法">
<h3>应用量化算法<a class="headerlink" href="#应用量化算法" title="Permalink to this headline">¶</a></h3>
<p>量化网络是指在原网络定义的基础上，修改需要量化的网络层后生成的带有伪量化节点的网络，通过构造MindSpore Golden Stick下的<code class="docutils literal notranslate"><span class="pre">SlbQuantAwareTraining</span></code>类，并将其应用到原网络上将原网络转换为量化网络。<code class="docutils literal notranslate"><span class="pre">QuantDtype</span></code>是定义了各种量化比特的类，通过调用<code class="docutils literal notranslate"><span class="pre">SlbQuantAwareTraining</span></code>类的<code class="docutils literal notranslate"><span class="pre">set_weight_quant_dtype</span></code>接口可以实现权重量化比特的自定义。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_gs</span> <span class="kn">import</span> <span class="n">SlbQuantAwareTraining</span> <span class="k">as</span> <span class="n">SlbQAT</span>
<span class="kn">from</span> <span class="nn">mindspore_gs.quantization.constant</span> <span class="kn">import</span> <span class="n">QuantDtype</span>

<span class="o">...</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">SlbQAT</span><span class="p">()</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_weight_quant_dtype</span><span class="p">(</span><span class="n">QuantDtype</span><span class="o">.</span><span class="n">INT1</span><span class="p">)</span>
<span class="n">quant_net</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">quant_net</span><span class="p">)</span>
</pre></div>
</div>
<p>量化后的网络结构如下，其中QuantizeWrapperCell为SLB量化对原有Conv2d的封装类，包括了原有的算子和权重的伪量化节点，用户可以参考<a class="reference external" href="https://www.mindspore.cn/golden_stick/docs/zh-CN/r0.1/mindspore_gs.html#mindspore_gs.SlbQuantAwareTraining">API</a> 修改算法配置，并通过检查QuantizeWrapperCell的属性确认算法是否配置成功。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ResNetOpt&lt;
  (_handler): ResNet&lt;...&gt;
  (conv1): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
  (bn1): BatchNorm2d&lt;num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
  (pad): Pad&lt;&gt;
  (maxpool): MaxPool2d&lt;kernel_size=3, stride=2, pad_mode=VALID&gt;
  (layer1): SequentialCellOpt&lt;
    (_handler): SequentialCell&lt;...&gt;
    (cell_list_0): ResidualBlockBaseOpt&lt;
      (_handler): ResidualBlockBase&lt;...&gt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.0.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.0.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.0.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.0.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.0.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.0.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.0.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.0.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      (Conv2dSlbQuant): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        &gt;
      (Conv2dSlbQuant_1): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        &gt;
      &gt;
    (cell_list_1): ResidualBlockBaseOpt_1&lt;
    (_handler): ResidualBlockBase&lt;...&gt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.1.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.1.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.1.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.1.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.1.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.1.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.1.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.1.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      (Conv2dSlbQuant): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        &gt;
      (Conv2dSlbQuant_1): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        &gt;
      &gt;
    &gt;
  (layer2): SequentialCellOpt_1&lt;...&gt;
  (layer3): SequentialCellOpt_3&lt;...&gt;
  (layer4): SequentialCellOpt_5&lt;...&gt;
  (flatten): Flatten&lt;&gt;
  (end_point): Dense&lt;input_channels=512, output_channels=10, has_bias=True&gt;
  (Conv2dSlbQuant): QuantizeWrapperCell&lt;
    (_handler): Conv2dSlbQuant&lt;
      in_channels=3, out_channels=64, kernel_size=(7, 7), weight_bit_num=1, stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False
      (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
      &gt;
    &gt;
  &gt;
</pre></div>
</div>
<p>与原网络相比，量化后的网络里面的conv被替换成了Conv2dSlbQuant。</p>
</div>
<div class="section" id="定义优化器损失函数和训练的callbacks">
<h3>定义优化器、损失函数和训练的callbacks<a class="headerlink" href="#定义优化器损失函数和训练的callbacks" title="Permalink to this headline">¶</a></h3>
<p>对于SLB量化算法，除了要定义训练中常用的callbacks，还需要定义一个支持温度因子动态调整的callback类<code class="docutils literal notranslate"><span class="pre">TemperatureScheduler</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.train.callback</span> <span class="k">as</span> <span class="nn">callback</span>
<span class="kn">from</span> <span class="nn">mindspore.train.loss_scale_manager</span> <span class="kn">import</span> <span class="n">FixedLossScaleManager</span>
<span class="kn">from</span> <span class="nn">mindspore.train.callback</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">LossMonitor</span><span class="p">,</span> <span class="n">TimeMonitor</span>

<span class="k">class</span> <span class="nc">TemperatureScheduler</span><span class="p">(</span><span class="n">callback</span><span class="o">.</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">epoch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">has_trained_epoch</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">t_start_val</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">t_start_time</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">t_end_time</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">t_factor</span><span class="o">=</span><span class="mf">1.2</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epoch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">has_trained_epoch</span> <span class="o">=</span> <span class="n">has_trained_epoch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_start_val</span> <span class="o">=</span> <span class="n">t_start_val</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_start_time</span> <span class="o">=</span> <span class="n">t_start_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_end_time</span> <span class="o">=</span> <span class="n">t_end_time</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_factor</span> <span class="o">=</span> <span class="n">t_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">epoch_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">run_context</span><span class="p">):</span>
        <span class="n">cb_params</span> <span class="o">=</span> <span class="n">run_context</span><span class="o">.</span><span class="n">original_args</span><span class="p">()</span>
        <span class="n">epoch</span> <span class="o">=</span> <span class="n">cb_params</span><span class="o">.</span><span class="n">cur_epoch_num</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">has_trained_epoch</span>
        <span class="c1"># Compute temperature value</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_start_val</span>
        <span class="n">t_start_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">t_start_time</span><span class="p">)</span>
        <span class="n">t_end_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">t_end_time</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="n">t_start_epoch</span><span class="p">:</span>
            <span class="n">t</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_factor</span><span class="o">**</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">t_end_epoch</span><span class="p">)</span> <span class="o">-</span> <span class="n">t_start_epoch</span><span class="p">)</span>
        <span class="c1"># Assign new value to temperature parameter</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">cell</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train_network</span><span class="o">.</span><span class="n">cells_and_names</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">cell</span><span class="o">.</span><span class="n">cls_name</span> <span class="o">==</span> <span class="s1">&#39;SlbFakeQuantizerPerLayer&#39;</span><span class="p">:</span>
                <span class="n">cell</span><span class="o">.</span><span class="n">set_temperature</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;=</span> <span class="n">t_end_epoch</span><span class="p">:</span>
                    <span class="n">cell</span><span class="o">.</span><span class="n">set_temperature_end_flag</span><span class="p">()</span>

<span class="n">step_size</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">()</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">get_lr</span><span class="p">(</span><span class="n">lr_init</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_init</span><span class="p">,</span>
            <span class="n">lr_end</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_end</span><span class="p">,</span>
            <span class="n">lr_max</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_max</span><span class="p">,</span>
            <span class="n">warmup_epochs</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">warmup_epochs</span><span class="p">,</span>
            <span class="n">total_epochs</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span>
            <span class="n">steps_per_epoch</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span>
            <span class="n">lr_decay_mode</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_decay_mode</span><span class="p">)</span>
<span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">pre_trained</span><span class="p">:</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">:]</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
<span class="c1"># define optimizer</span>
<span class="n">group_params</span> <span class="o">=</span> <span class="n">init_group_params</span><span class="p">(</span><span class="n">quant_net</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
                  <span class="n">loss_scale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">init_loss_scale</span><span class="p">()</span>
<span class="n">loss_scale</span> <span class="o">=</span> <span class="n">FixedLossScaleManager</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">drop_overflow_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;acc&quot;</span><span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">quant_net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
                 <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O0&quot;</span><span class="p">,</span> <span class="n">boost_level</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">boost_mode</span><span class="p">,</span> <span class="n">keep_batchnorm_fp32</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">boost_config_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;total_steps&quot;</span><span class="p">:</span> <span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">}})</span>

<span class="c1"># define callbacks</span>
<span class="n">time_cb</span> <span class="o">=</span> <span class="n">TimeMonitor</span><span class="p">(</span><span class="n">data_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">)</span>
<span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossCallBack</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">)</span>

<span class="n">cb</span> <span class="o">=</span> <span class="p">[</span><span class="n">time_cb</span><span class="p">,</span> <span class="n">loss_cb</span><span class="p">]</span>
<span class="n">algo_cb</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">callback</span><span class="p">()</span>
<span class="n">cb</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">algo_cb</span><span class="p">)</span>
<span class="n">cb</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TemperatureScheduler</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">t_start_val</span><span class="p">,</span>
                                   <span class="n">config</span><span class="o">.</span><span class="n">t_start_time</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">t_end_time</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">t_factor</span><span class="p">))</span>
<span class="n">ckpt_append_info</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;epoch_num&quot;</span><span class="p">:</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">,</span> <span class="s2">&quot;step_num&quot;</span><span class="p">:</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_step</span><span class="p">}]</span>
<span class="n">config_ck</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">save_checkpoint_epochs</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span>
                             <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">keep_checkpoint_max</span><span class="p">,</span>
                             <span class="n">append_info</span><span class="o">=</span><span class="n">ckpt_append_info</span><span class="p">)</span>
<span class="n">ckpt_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;resnet&quot;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="s2">&quot;./ckpt&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_ck</span><span class="p">)</span>
<span class="n">cb</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ckpt_cb</span><span class="p">]</span>
</pre></div>
</div>
<p>代码中get_lr引用自<a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/src/lr_generator.py">lr_generator.py</a>，init_group_params和init_loss_scale都引用自<a class="reference external" href="https://gitee.com/mindspore/models/blob/r1.8/official/cv/resnet/golden_stick/quantization/slb/train.py">train.py</a>。</p>
</div>
<div class="section" id="训练模型保存模型文件">
<h3>训练模型，保存模型文件<a class="headerlink" href="#训练模型保存模型文件" title="Permalink to this headline">¶</a></h3>
<p>定义好模型后，开始进行训练。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_sink_mode</span> <span class="o">=</span> <span class="n">target</span> <span class="o">!=</span> <span class="s2">&quot;CPU&quot;</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">cb</span><span class="p">,</span>
            <span class="n">sink_size</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">(),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="n">dataset_sink_mode</span><span class="p">)</span>
</pre></div>
</div>
<p>运行部分结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1562, loss is 1.4536957
Train epoch time: 101539.306 ms, per step time: 65.006 ms
epoch: 2 step: 1562, loss is 1.3616204
Train epoch time: 94238.882 ms, per step time: 60.332 ms
epoch: 3 step: 1562, loss is 1.2128768
Train epoch time: 94237.197 ms, per step time: 60.331 ms
epoch: 4 step: 1562, loss is 0.99068344
Train epoch time: 94084.353 ms, per step time: 60.233 ms
epoch: 5 step: 1562, loss is 0.89842224
Train epoch time: 94498.564 ms, per step time: 60.498 ms
epoch: 6 step: 1562, loss is 0.8985137
Train epoch time: 94106.722 ms, per step time: 60.248 ms
</pre></div>
</div>
</div>
<div class="section" id="加载模型对比精度">
<h3>加载模型，对比精度<a class="headerlink" href="#加载模型对比精度" title="Permalink to this headline">¶</a></h3>
<p>按照<a class="reference external" href="https://gitee.com/mindspore/models/tree/r1.8/official/cv/resnet">resnet模型仓</a>步骤获得普通训练的模型精度：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;top_1_accuracy&#39;: 0.9544270833333334, &#39;top_5_accuracy&#39;: 0.9969951923076923
</pre></div>
</div>
<p>加载上一步得到的模型文件，导入量化后模型评估精度。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_file_path</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">quant_net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">do_train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                         <span class="n">eval_image_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">eval_image_size</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">quant_net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;top_1_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;top_5_accuracy&#39;</span><span class="p">})</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">ds_eval</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;top_1_accuracy&#39;: 0.9485176282051282, &#39;top_5_accuracy&#39;: 0.9965945512820513.
</pre></div>
</div>
<p>在Graph模式下，对ResNet18网络应用SLB量化，并使用CIFAR-10数据集评估，实验结果如下图所示。其中，W32表示全精度模型。W4表示weight权重量化为4bit，W2表示权重量化为2bit，W1表示权重量化为1bit。可以发现，在当前任务中，与全精度模型相比，4bit权重量化后的模型top1精度没有损失，1bit权重量化的top1精度损失在0.6%以内。SLB量化大幅降低了模型的参数量，使得在资源受限的端侧部署模型变得更加便利。此处模型并非最终部署模型，由于增加了伪量化节点和权值概率矩阵，ckpt大小相较原始模型有较大程度的增加，增幅受权重量化比特影响，量化后的比特数越大增幅越大。根据权值概率矩阵对预设的量化权值进行挑选，便得到最终的量化模型，即最终部署模型。</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>量化类型</th>
<th>top1精度</th>
<th>top5精度</th>
</tr>
</thead>
<tbody>
<tr>
<td>W32</td>
<td>0.9544</td>
<td>0.9970</td>
</tr>
<tr>
<td>W4</td>
<td>0.9534</td>
<td>0.9970</td>
</tr>
<tr>
<td>W2</td>
<td>0.9503</td>
<td>0.9967</td>
</tr>
<tr>
<td>W1</td>
<td>0.9485</td>
<td>0.9966</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="参考文献">
<h2>参考文献<a class="headerlink" href="#参考文献" title="Permalink to this headline">¶</a></h2>
<p>[1] Bengio, Yoshua, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. 2013.</p>
<p>[2] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. ICLR, 2019.</p>
<p>[3] Yang Z, Wang Y, Han K, et al. Searching for low-bit weights in quantized neural networks. NIPS, 2020.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../pruner/overview.html" class="btn btn-neutral float-right" title="剪枝算法概述" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="simqat.html" class="btn btn-neutral float-left" title="应用SimQAT算法" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>