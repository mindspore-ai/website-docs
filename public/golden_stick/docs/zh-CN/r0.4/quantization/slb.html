<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>应用SLB算法 &mdash; MindSpore master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/translations.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="应用RoundToNearest后量化算法" href="../ptq/round_to_nearest.html" />
    <link rel="prev" title="应用SimQAT算法" href="simqat.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">安装MindSpore Golden Stick</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">量化感知训练算法</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">量化算法概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="simqat.html">应用SimQAT算法</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">应用SLB算法</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#背景">背景</a></li>
<li class="toctree-l2"><a class="reference internal" href="#算法原理介绍">算法原理介绍</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#温度因子">温度因子</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#算法特点">算法特点</a></li>
<li class="toctree-l2"><a class="reference internal" href="#slb量化训练">SLB量化训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="#slb量化训练示例">SLB量化训练示例</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#加载数据集">加载数据集</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义原网络">定义原网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="#应用量化算法">应用量化算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义优化器损失函数和训练的callbacks">定义优化器、损失函数和训练的callbacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#训练模型保存模型文件">训练模型，保存模型文件</a></li>
<li class="toctree-l3"><a class="reference internal" href="#加载模型对比精度">加载模型，对比精度</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#算法效果汇总">算法效果汇总</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#训练效果">训练效果</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#参考文献">参考文献</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">训练后量化算法</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../ptq/round_to_nearest.html">应用RoundToNearest后量化算法</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">剪枝算法</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../pruner/overview.html">剪枝算法概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pruner/scop.html">应用SCOP算法</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">模型部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/overview.html">模型部署概述</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deployment/convert.html">使用MindSpore Golden Stick进行模型转换</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.html">mindspore_gs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.common.html">mindspore_gs.common</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.quantization.html">mindspore_gs.quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.ptq.html">mindspore_gs.ptq</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mindspore_gs.pruner.html">mindspore_gs.pruner</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>应用SLB算法</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quantization/slb.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="应用slb算法">
<h1>应用SLB算法<a class="headerlink" href="#应用slb算法" title="永久链接至标题"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/golden_stick/docs/source_zh_cn/quantization/slb.md"><img alt="查看源文件" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.svg" /></a></p>
<section id="背景">
<h2>背景<a class="headerlink" href="#背景" title="永久链接至标题"></a></h2>
<p>传统的量化方法在计算梯度时，通常使用STE(Straight Through Estimator) [1]或者自行设计的梯度计算方式[2]。量化函数的不可微往往会导致计算出来的梯度有误差，从而提供不准确的优化方向，导致最终推理精度比较差。因此，迫切需要一种能规避这种不准确梯度估计的量化神经网络学习方法。</p>
</section>
<section id="算法原理介绍">
<h2>算法原理介绍<a class="headerlink" href="#算法原理介绍" title="永久链接至标题"></a></h2>
<p>SLB(Searching for low-bit weights) [3]是华为诺亚自研的权重量化算法，提供了一种基于权值搜索的低比特量化算法，能避开不准确的梯度估计。针对低比特网络量化，由于量化网络权值的有效解数量比较少，因此，对网络的量化可以通过对权值搜索实现，即将量化过程转换成权值搜索的过程。对给定量化网络预设一组量化权值，然后定义一个概率矩阵来表示不同量化权值被保留的概率，在训练阶段通过优化概率矩阵实现网络权重的量化。</p>
<p>下面左边图是用传统量化算法做二值量化，训练时用不准确的梯度更新浮点权重，最后对浮点权重做二值化(用sigmoid函数)处理得到量化权重。右边图是用SLB量化算法做二值量化，利用连续松弛策略搜索离散权重，训练时优化离散权重的权值概率矩阵，最后根据概率挑选离散权重实现量化。左边图中红色点对应的单个值是由sigmoid函数得到，表示权重被量化为-1的概率。蓝色点对应的单个值是由sigmoid函数得到，表示权重被量化为+1的概率。传统量化算法中不准确的梯度更新会影响浮点权重的更新，从而导致这里的概率出现较大的偏差。右边图中红蓝相间的点对应的2个值是由softmax函数得到，表示权重被量化为-1或+1的概率。由于避开了不准确的梯度更新，这里的概率会更精准。</p>
<p><img alt="SLB算法对比" src="../_images/slb_1.png" /></p>
<section id="温度因子">
<h3>温度因子<a class="headerlink" href="#温度因子" title="永久链接至标题"></a></h3>
<p>在分类任务中，softmax分布通常用于计算输出被分为各个类的概率。因此，SLB也使用softmax分布来计算权重被量化为各个量化权值的概率，并最终根据最大概率挑选对应权值作为量化结果。为了提高量化结果的置信度，SLB引入了温度因子，通过逐步调整温度因子，能使softmax分布逐渐变得陡峭，慢慢趋近于one-hot分布，从而最大化量化结果的置信度，缩减量化误差。</p>
<p>下面左边公式是标准的softmax函数，右边是SLB算法中引入了温度因子后的softmax函数。</p>
<p><img alt="softmax函数" src="../_images/slb_2.png" /></p>
<p>下图展示了逐步调整温度因子时，softmax分布的变化过程，最右侧是one-hot分布。</p>
<p><img alt="softmax分布变化" src="../_images/slb_3.png" /></p>
</section>
</section>
<section id="算法特点">
<h2>算法特点<a class="headerlink" href="#算法特点" title="永久链接至标题"></a></h2>
<ul class="simple">
<li><p>提出了一种新的权值搜索方法，用于训练量化深度神经网络，能规避不准确梯度估计。</p></li>
<li><p>利用连续松弛策略搜索离散权重，训练时优化离散权重的概率分布，最后根据概率挑选离散权重实现量化。</p></li>
<li><p>为了进一步消除搜索后的推理精度差距，保证训练和测试的一致性，提出了逐步调整温度因子的策略。</p></li>
<li><p>与传统的量化算法相比，规避了不准确的梯度更新过程，能获得更高的推理精度，在极低比特量化中更有优势。</p></li>
</ul>
</section>
<section id="slb量化训练">
<h2>SLB量化训练<a class="headerlink" href="#slb量化训练" title="永久链接至标题"></a></h2>
<p>SLB量化算法的训练规格如下表所示。</p>
<p>表1：SLB量化训练规格</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>规格</p></th>
<th class="head"><p>规格说明</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>硬件支持</p></td>
<td><p>GPU</p></td>
</tr>
<tr class="row-odd"><td><p>网络支持</p></td>
<td><p>ResNet18，具体请参见<a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.3/official/cv/ResNet#%E5%BA%94%E7%94%A8mindspore-golden-stick%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95">https://gitee.com/mindspore/models/tree/r2.3/official/cv/ResNet#应用mindspore-golden-stick模型压缩算法</a>。</p></td>
</tr>
<tr class="row-even"><td><p>方案支持</p></td>
<td><p>支持1、2、4比特的权重量化方案，支持8比特的激活量化方案。</p></td>
</tr>
<tr class="row-odd"><td><p>数据类型支持</p></td>
<td><p>GPU平台支持FP32。</p></td>
</tr>
<tr class="row-even"><td><p>运行模式支持</p></td>
<td><p>Graph模式和PyNative模式。</p></td>
</tr>
</tbody>
</table>
</section>
<section id="slb量化训练示例">
<h2>SLB量化训练示例<a class="headerlink" href="#slb量化训练示例" title="永久链接至标题"></a></h2>
<p>SLB量化训练与一般训练步骤一致，在定义量化网络和生成量化模型阶段需要进行额外的操作，完整流程如下：</p>
<ol class="arabic simple">
<li><p>加载数据集，处理数据。</p></li>
<li><p>定义网络。</p></li>
<li><p>定义SLB量化算法，应用算法生成量化模型。</p></li>
<li><p>定义优化器、损失函数和callbacks。</p></li>
<li><p>训练网络，保存模型文件。</p></li>
<li><p>加载模型文件，对比量化后精度。</p></li>
</ol>
<p>接下来以ResNet18网络为例，分别叙述这些步骤。</p>
<blockquote>
<div><p>完整代码见<a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.3/official/cv/ResNet/README_CN.md#%E5%BA%94%E7%94%A8mindspore-golden-stick%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E7%AE%97%E6%B3%95">resnet模型仓</a>，其中<a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.3/official/cv/ResNet/golden_stick/quantization/slb/train.py">train.py</a>为完整的训练代码，<a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.3/official/cv/ResNet/golden_stick/quantization/slb/eval.py">eval.py</a>为精度验证代码。</p>
</div></blockquote>
<section id="加载数据集">
<h3>加载数据集<a class="headerlink" href="#加载数据集" title="永久链接至标题"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">do_train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                         <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">train_image_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">train_image_size</span><span class="p">,</span>
                         <span class="n">eval_image_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">eval_image_size</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device_target</span><span class="p">,</span>
                         <span class="n">distribute</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">run_distribute</span><span class="p">)</span>
</pre></div>
</div>
<p>代码中create_dataset引用自<a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.3/official/cv/ResNet/src/dataset.py">dataset.py</a>，config.data_path和config.batch_size分别在<a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.3/official/cv/ResNet/golden_stick/quantization/slb/resnet18_cifar10_config.yaml">配置文件</a>中配置，下同。</p>
</section>
<section id="定义原网络">
<h3>定义原网络<a class="headerlink" href="#定义原网络" title="永久链接至标题"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">src.resnet</span> <span class="kn">import</span> <span class="n">resnet18</span> <span class="k">as</span> <span class="n">resnet</span>

<span class="o">...</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">resnet</span><span class="p">(</span><span class="n">class_num</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">class_num</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
<p>原始网络结构如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ResNet&lt;
  (conv1): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
  (bn1): BatchNorm2d&lt;num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
  (pad): Pad&lt;&gt;
  (maxpool): MaxPool2d&lt;kernel_size=3, stride=2, pad_mode=VALID&gt;
  (layer1): SequentialCell&lt;
    (0): ResidualBlockBase&lt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.0.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.0.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.0.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.0.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.0.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.0.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.0.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.0.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      &gt;
    (1): ResidualBlockBase&lt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.1.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.1.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.1.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.1.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1.1.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1.1.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1.1.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1.1.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      &gt;
    &gt;
  (layer2): SequentialCell&lt;...&gt;
  (layer3): SequentialCell&lt;...&gt;
  (layer4): SequentialCell&lt;...&gt;
  (flatten): Flatten&lt;&gt;
  (end_point): Dense&lt;input_channels=512, output_channels=10, has_bias=True&gt;
  &gt;
</pre></div>
</div>
<p>ResNet18网络定义见<a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.3/official/cv/ResNet/src/resnet.py">resnet.py</a>。</p>
</section>
<section id="应用量化算法">
<h3>应用量化算法<a class="headerlink" href="#应用量化算法" title="永久链接至标题"></a></h3>
<p>量化网络是指在原网络定义的基础上，修改需要量化的网络层后生成的带有伪量化节点的网络，通过构造MindSpore Golden Stick下的<code class="docutils literal notranslate"><span class="pre">SlbQuantAwareTraining</span></code>类，并将其应用到原网络上将原网络转换为量化网络。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_gs</span> <span class="kn">import</span> <span class="n">SlbQuantAwareTraining</span> <span class="k">as</span> <span class="n">SlbQAT</span>
<span class="kn">from</span> <span class="nn">mindspore</span> <span class="kn">import</span> <span class="n">QuantDtype</span>

<span class="o">...</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">SlbQAT</span><span class="p">()</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_weight_quant_dtype</span><span class="p">(</span><span class="n">QuantDtype</span><span class="o">.</span><span class="n">INT1</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_act_quant_dtype</span><span class="p">(</span><span class="n">QuantDtype</span><span class="o">.</span><span class="n">INT8</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_enable_act_quant</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_enable_bn_calibration</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_epoch_size</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_has_trained_epoch</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_t_start_val</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_t_start_time</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_t_end_time</span><span class="p">(</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">set_t_factor</span><span class="p">(</span><span class="mf">1.2</span><span class="p">)</span>
<span class="n">quant_net</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">algo</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">quant_net</span><span class="p">)</span>
</pre></div>
</div>
<p>打印量化器，会得到如下的信息，其中包含各个属性的配置信息，可以用来检查算法是否配置成功。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>SlbQuantAwareTraining&lt;weight_quant_dtype=INT1, act_quant_dtype=INT8, enable_act_quant=True, enable_bn_calibration=True, epoch_size=100, has_trained_epoch=0, t_start_val=1.0, t_start_time=0.2, t_end_time=0.6, t_factor=1.2&gt;
</pre></div>
</div>
<p>打印量化后的网络，会得到如下的网络结构，其中QuantizeWrapperCell为SLB量化对原有Conv2d的封装类，包括了原有的算子和权重的伪量化节点，用户可以参考<a class="reference external" href="https://www.mindspore.cn/golden_stick/docs/zh-CN/r0.4/quantization/mindspore_gs.quantization.SlbQuantAwareTraining.html#mindspore_gs.quantization.SlbQuantAwareTraining">API</a> 修改算法配置，并通过检查QuantizeWrapperCell的属性确认算法是否配置成功。</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ResNetOpt&lt;
  (_handler): ResNet&lt;...&gt;
  (conv1): Conv2d&lt;input_channels=3, output_channels=64, kernel_size=(7, 7), stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
  (bn1): BatchNorm2d&lt;num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
  (pad): Pad&lt;&gt;
  (maxpool): MaxPool2d&lt;kernel_size=3, stride=2, pad_mode=VALID&gt;
  (layer1): SequentialCellOpt&lt;
    (_handler): SequentialCell&lt;...&gt;
    (cell_list_0): ResidualBlockBaseOpt&lt;
      (_handler): ResidualBlockBase&lt;...&gt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.0.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.0.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.0.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.0.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.0.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.0.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.0.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.0.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      (Conv2dSlbQuant): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        (_input_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;
        (_output_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;
        &gt;
      (Conv2dSlbQuant_1): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        (_input_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;
        (_output_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;
        &gt;
      &gt;
    (cell_list_1): ResidualBlockBaseOpt_1&lt;
    (_handler): ResidualBlockBase&lt;...&gt;
      (conv1): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn1d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.1.bn1d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.1.bn1d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.1.bn1d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.1.bn1d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (conv2): Conv2d&lt;input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False, weight_init=..., bias_init=zeros, format=NCHW&gt;
      (bn2d): BatchNorm2d&lt;num_features=64, eps=0.0001, momentum=0.09999999999999998, gamma=Parameter (name=layer1._handler.1.bn2d.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=layer1._handler.1.bn2d.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=layer1._handler.1.bn2d.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=layer1._handler.1.bn2d.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)&gt;
      (relu): ReLU&lt;&gt;
      (Conv2dSlbQuant): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        (_input_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;
        (_output_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;
        &gt;
      (Conv2dSlbQuant_1): QuantizeWrapperCell&lt;
        (_handler): Conv2dSlbQuant&lt;
          in_channels=64, out_channels=64, kernel_size=(3, 3), weight_bit_num=1, stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=False
          (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
          &gt;
        (_input_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;
        (_output_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;
        &gt;
      &gt;
    &gt;
  (layer2): SequentialCellOpt_1&lt;...&gt;
  (layer3): SequentialCellOpt_3&lt;...&gt;
  (layer4): SequentialCellOpt_5&lt;...&gt;
  (flatten): Flatten&lt;&gt;
  (end_point): Dense&lt;input_channels=512, output_channels=10, has_bias=True&gt;
  (Conv2dSlbQuant): QuantizeWrapperCell&lt;
    (_handler): Conv2dSlbQuant&lt;
      in_channels=3, out_channels=64, kernel_size=(7, 7), weight_bit_num=1, stride=(2, 2), pad_mode=pad, padding=3, dilation=(1, 1), group=1, has_bias=False
      (fake_quant_weight): SlbFakeQuantizerPerLayer&lt;bit_num=1&gt;
      &gt;
    (_input_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;
    (_output_quantizer): SlbActQuantizer&lt;bit_num=8, symmetric=False, narrow_range=False, ema=False(0.999), per_channel=False, quant_delay=900&gt;
    &gt;
  &gt;
</pre></div>
</div>
<p>与原网络相比，量化后的网络里面的conv被替换成了Conv2dSlbQuant。</p>
</section>
<section id="定义优化器损失函数和训练的callbacks">
<h3>定义优化器、损失函数和训练的callbacks<a class="headerlink" href="#定义优化器损失函数和训练的callbacks" title="永久链接至标题"></a></h3>
<p>对于SLB量化算法，除了要定义训练中常用的callbacks，还需要通过调用<code class="docutils literal notranslate"><span class="pre">SlbQuantAwareTraining</span></code>类的<code class="docutils literal notranslate"><span class="pre">callbacks</span></code>接口来定义SLB量化算法特有的一些callbacks，其中包括用于调节温度因子的callback。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore</span> <span class="k">as</span> <span class="nn">ms</span>
<span class="kn">import</span> <span class="nn">mindspore.train.callback</span> <span class="k">as</span> <span class="nn">callback</span>
<span class="kn">from</span> <span class="nn">mindspore.amp</span> <span class="kn">import</span> <span class="n">FixedLossScaleManager</span>
<span class="kn">from</span> <span class="nn">mindspore.train</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="n">CheckpointConfig</span><span class="p">,</span> <span class="n">LossMonitor</span><span class="p">,</span> <span class="n">TimeMonitor</span><span class="p">,</span> <span class="n">Model</span>

<span class="n">step_size</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">()</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">get_lr</span><span class="p">(</span><span class="n">lr_init</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_init</span><span class="p">,</span>
            <span class="n">lr_end</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_end</span><span class="p">,</span>
            <span class="n">lr_max</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_max</span><span class="p">,</span>
            <span class="n">warmup_epochs</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">warmup_epochs</span><span class="p">,</span>
            <span class="n">total_epochs</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span><span class="p">,</span>
            <span class="n">steps_per_epoch</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span>
            <span class="n">lr_decay_mode</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">lr_decay_mode</span><span class="p">)</span>
<span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">pre_trained</span><span class="p">:</span>
    <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">[</span><span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">:]</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
<span class="c1"># define optimizer</span>
<span class="n">group_params</span> <span class="o">=</span> <span class="n">init_group_params</span><span class="p">(</span><span class="n">quant_net</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span><span class="n">group_params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
                  <span class="n">loss_scale</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">init_loss_scale</span><span class="p">()</span>
<span class="n">loss_scale</span> <span class="o">=</span> <span class="n">FixedLossScaleManager</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">drop_overflow_update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;acc&quot;</span><span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">quant_net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span> <span class="n">loss_scale_manager</span><span class="o">=</span><span class="n">loss_scale</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
                 <span class="n">amp_level</span><span class="o">=</span><span class="s2">&quot;O0&quot;</span><span class="p">,</span> <span class="n">boost_level</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">boost_mode</span><span class="p">,</span> <span class="n">keep_batchnorm_fp32</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">eval_network</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">boost_config_dict</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;grad_freeze&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;total_steps&quot;</span><span class="p">:</span> <span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">}})</span>

<span class="c1"># define callbacks</span>
<span class="n">time_cb</span> <span class="o">=</span> <span class="n">TimeMonitor</span><span class="p">(</span><span class="n">data_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">)</span>
<span class="n">loss_cb</span> <span class="o">=</span> <span class="n">LossCallBack</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">)</span>

<span class="n">cb</span> <span class="o">=</span> <span class="p">[</span><span class="n">time_cb</span><span class="p">,</span> <span class="n">loss_cb</span><span class="p">]</span>
<span class="n">algo_cb_list</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">callbacks</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>
<span class="n">cb</span> <span class="o">+=</span> <span class="n">algo_cb_list</span>

<span class="n">ckpt_append_info</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;epoch_num&quot;</span><span class="p">:</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">,</span> <span class="s2">&quot;step_num&quot;</span><span class="p">:</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_step</span><span class="p">}]</span>
<span class="n">config_ck</span> <span class="o">=</span> <span class="n">CheckpointConfig</span><span class="p">(</span><span class="n">save_checkpoint_steps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">save_checkpoint_epochs</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">,</span>
                             <span class="n">keep_checkpoint_max</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">keep_checkpoint_max</span><span class="p">,</span>
                             <span class="n">append_info</span><span class="o">=</span><span class="n">ckpt_append_info</span><span class="p">)</span>
<span class="n">ckpt_cb</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;resnet&quot;</span><span class="p">,</span> <span class="n">directory</span><span class="o">=</span><span class="s2">&quot;./ckpt&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_ck</span><span class="p">)</span>
<span class="n">cb</span> <span class="o">+=</span> <span class="p">[</span><span class="n">ckpt_cb</span><span class="p">]</span>
</pre></div>
</div>
<p>代码中get_lr引用自<a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.3/official/cv/ResNet/src/lr_generator.py">lr_generator.py</a>，init_group_params和init_loss_scale都引用自<a class="reference external" href="https://gitee.com/mindspore/models/blob/r2.3/official/cv/ResNet/golden_stick/quantization/slb/train.py">train.py</a>。</p>
</section>
<section id="训练模型保存模型文件">
<h3>训练模型，保存模型文件<a class="headerlink" href="#训练模型保存模型文件" title="永久链接至标题"></a></h3>
<p>定义好模型后，开始进行训练。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_sink_mode</span> <span class="o">=</span> <span class="n">target</span> <span class="o">!=</span> <span class="s2">&quot;CPU&quot;</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">epoch_size</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">has_trained_epoch</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">cb</span><span class="p">,</span>
            <span class="n">sink_size</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">get_dataset_size</span><span class="p">(),</span> <span class="n">dataset_sink_mode</span><span class="o">=</span><span class="n">dataset_sink_mode</span><span class="p">)</span>
</pre></div>
</div>
<p>运行部分结果如下：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>epoch: 1 step: 1562, loss is 1.4536957
Train epoch time: 101539.306 ms, per step time: 65.006 ms
epoch: 2 step: 1562, loss is 1.3616204
Train epoch time: 94238.882 ms, per step time: 60.332 ms
epoch: 3 step: 1562, loss is 1.2128768
Train epoch time: 94237.197 ms, per step time: 60.331 ms
epoch: 4 step: 1562, loss is 0.99068344
Train epoch time: 94084.353 ms, per step time: 60.233 ms
epoch: 5 step: 1562, loss is 0.89842224
Train epoch time: 94498.564 ms, per step time: 60.498 ms
epoch: 6 step: 1562, loss is 0.8985137
Train epoch time: 94106.722 ms, per step time: 60.248 ms
</pre></div>
</div>
</section>
<section id="加载模型对比精度">
<h3>加载模型，对比精度<a class="headerlink" href="#加载模型对比精度" title="永久链接至标题"></a></h3>
<p>按照<a class="reference external" href="https://gitee.com/mindspore/models/tree/r2.3/official/cv/ResNet">resnet模型仓</a>步骤获得普通训练的模型精度：</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;top_1_accuracy&#39;: 0.9544270833333334, &#39;top_5_accuracy&#39;: 0.9969951923076923
</pre></div>
</div>
<p>加载上一步得到的模型文件，导入量化后模型评估精度。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param_dict</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">checkpoint_file_path</span><span class="p">)</span>
<span class="n">ms</span><span class="o">.</span><span class="n">load_param_into_net</span><span class="p">(</span><span class="n">quant_net</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">)</span>
<span class="n">ds_eval</span> <span class="o">=</span> <span class="n">create_dataset</span><span class="p">(</span><span class="n">dataset_path</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">data_path</span><span class="p">,</span> <span class="n">do_train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                         <span class="n">eval_image_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">eval_image_size</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">device_target</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">quant_net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;top_1_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;top_5_accuracy&#39;</span><span class="p">})</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">ds_eval</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&#39;top_1_accuracy&#39;: 0.9466145833333334, &#39;top_5_accuracy&#39;: 0.9964050320512820.
</pre></div>
</div>
</section>
</section>
<section id="算法效果汇总">
<h2>算法效果汇总<a class="headerlink" href="#算法效果汇总" title="永久链接至标题"></a></h2>
<blockquote>
<div><p>-表示尚未测试，NS表示尚未支持</p>
</div></blockquote>
<section id="训练效果">
<h3>训练效果<a class="headerlink" href="#训练效果" title="永久链接至标题"></a></h3>
<p>使用图模式进行训练，使用的代码为：<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/v1.9.0">MindSpore</a>，<a class="reference external" href="https://gitee.com/mindspore/golden-stick/tree/v0.2.0/">MindSpore Golden Stick</a>，<a class="reference external" href="https://gitee.com/mindspore/models/commit/f20d3d46ea48a465b26462ef5c62a7d381a34828">MindSpore Models</a>。</p>
<p>W4表示weight权重量化为4bit，W2表示权重量化为2bit，W1表示权重量化为1bit，A8表示激活量化为8bit。</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>算法</p></th>
<th class="head"><p>网络</p></th>
<th class="head"><p>数据集</p></th>
<th class="head"><p>CUDA11 Top1Acc</p></th>
<th class="head"><p>CUDA11 Top5Acc</p></th>
<th class="head"><p>Ascend910 Top1Acc</p></th>
<th class="head"><p>Ascend910 Top5Acc</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>baseline</p></td>
<td><p>resnet18</p></td>
<td><p>CIFAR10</p></td>
<td><p>94.25%</p></td>
<td><p>99.93%</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>SLB W4</p></td>
<td><p>resnet18</p></td>
<td><p>CIFAR10</p></td>
<td><p>95.18%</p></td>
<td><p>99.67%</p></td>
<td><p>NS</p></td>
<td><p>NS</p></td>
</tr>
<tr class="row-even"><td><p>SLB W2</p></td>
<td><p>resnet18</p></td>
<td><p>CIFAR10</p></td>
<td><p>95.12%</p></td>
<td><p>99.68%</p></td>
<td><p>NS</p></td>
<td><p>NS</p></td>
</tr>
<tr class="row-odd"><td><p>SLB W1</p></td>
<td><p>resnet18</p></td>
<td><p>CIFAR10</p></td>
<td><p>95.23%</p></td>
<td><p>99.87%</p></td>
<td><p>NS</p></td>
<td><p>NS</p></td>
</tr>
<tr class="row-even"><td><p>baseline</p></td>
<td><p>resnet18</p></td>
<td><p>Imagenet2012</p></td>
<td><p>70.14%</p></td>
<td><p>89.71%</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>SLB W4</p></td>
<td><p>resnet18</p></td>
<td><p>Imagenet2012</p></td>
<td><p>68.65%</p></td>
<td><p>88.57%</p></td>
<td><p>NS</p></td>
<td><p>NS</p></td>
</tr>
<tr class="row-even"><td><p>SLB W2</p></td>
<td><p>resnet18</p></td>
<td><p>Imagenet2012</p></td>
<td><p>68.42%</p></td>
<td><p>88.40%</p></td>
<td><p>NS</p></td>
<td><p>NS</p></td>
</tr>
<tr class="row-odd"><td><p>SLB W1</p></td>
<td><p>resnet18</p></td>
<td><p>Imagenet2012</p></td>
<td><p>66.75%</p></td>
<td><p>87.08%</p></td>
<td><p>NS</p></td>
<td><p>NS</p></td>
</tr>
</tbody>
</table>
<p>可以发现，与全精度模型相比，4bit权重量化后的模型top1精度没有损失，1bit权重量化的top1精度损失在0.6%以内。在做了权重量化后，再做8bit激活量化，top1精度损失在0.4%以内。SLB量化大幅降低了模型的参数量和计算量，使得在资源受限的环境部署AI能力变得更加便利。需要注意的是，此处量化网络并非最终部署网络，由于增加了伪量化节点和权值概率矩阵，ckpt大小相较原始网络有较大程度的增加，增幅受权重量化比特影响，量化的比特数越大增幅越大。</p>
</section>
</section>
<section id="参考文献">
<h2>参考文献<a class="headerlink" href="#参考文献" title="永久链接至标题"></a></h2>
<p>[1] Bengio, Yoshua, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. 2013.</p>
<p>[2] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. ICLR, 2019.</p>
<p>[3] Yang Z, Wang Y, Han K, et al. Searching for low-bit weights in quantized neural networks. NIPS, 2020.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="simqat.html" class="btn btn-neutral float-left" title="应用SimQAT算法" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="../ptq/round_to_nearest.html" class="btn btn-neutral float-right" title="应用RoundToNearest后量化算法" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>