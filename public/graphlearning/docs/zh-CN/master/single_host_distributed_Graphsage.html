

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>单机多卡分布式训练 &mdash; MindSpore master 文档</title>
  

  
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/bootstrap.min.css" type="text/css" />
   
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/translations.js"></script>
        
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
    <link rel="next" title="mindspore_gl" href="mindspore_gl.html" />
    <link rel="prev" title="时空图训练网络" href="spatio_temporal_graph_training_STGCN.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">安装部署</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindspore_graphlearning_install.html">安装 Graph Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">使用指南</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="full_training_of_GCN.html">整图训练网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="batched_graph_training_GIN.html">批次图训练网络</a></li>
<li class="toctree-l1"><a class="reference internal" href="spatio_temporal_graph_training_STGCN.html">时空图训练网络</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">单机多卡分布式训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#概述">概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="#graphsage原理">GraphSAGE原理</a></li>
<li class="toctree-l2"><a class="reference internal" href="#设置运行脚本">设置运行脚本</a></li>
<li class="toctree-l2"><a class="reference internal" href="#定义网络结构">定义网络结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="#定义loss函数">定义loss函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="#构造数据集">构造数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="#网络训练和验证">网络训练和验证</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#设置环境变量">设置环境变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="#定义训练网络">定义训练网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="#网络训练及验证">网络训练及验证</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#执行并查看结果">执行并查看结果</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#运行过程">运行过程</a></li>
<li class="toctree-l3"><a class="reference internal" href="#执行结果">执行结果</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API参考</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindspore_gl.html">mindspore_gl</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_gl.dataloader.html">mindspore_gl.dataloader</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_gl.dataset.html">mindspore_gl.dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_gl.graph.html">mindspore_gl.graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_gl.nn.html">mindspore_gl.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_gl.sampling.html">mindspore_gl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="mindspore_gl.utils.html">mindspore_gl.utils</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">参考文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>单机多卡分布式训练</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/single_host_distributed_Graphsage.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section class="tex2jax_ignore mathjax_ignore" id="单机多卡分布式训练">
<h1>单机多卡分布式训练<a class="headerlink" href="#单机多卡分布式训练" title="永久链接至标题"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/docs/graphlearning/docs/source_zh_cn/single_host_distributed_Graphsage.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source.png"></a>
  </p>
<section id="概述">
<h2>概述<a class="headerlink" href="#概述" title="永久链接至标题"></a></h2>
<p>在本例中将展示如何利用Graphsage在大尺寸图上进行单机多卡训练。</p>
<p>GraphSAGE是一个通用的归纳框架，它利用节点特征信息（例如，文本属性）为以前看不见的数据有效地生成节点嵌入。GraphSAGE不是为每个节点训练单个嵌入，而是学习一个函数，该函数通过从节点的本地邻居中采样和聚合特征来生成嵌入。</p>
<p>在Reddit数据集中，作者对50个大型社区进行了抽样调查，并构建了一个帖子到帖子的图，如果同一用户对这两个帖子都发表了评论，则连接帖子。每个帖子的标签为所属的社区。该数据集总共包含232965个帖子，平均度为492。</p>
<p>由于Reddit数据集较大，为了减少GraphSAGE训练时间，本例中在单机上执行分布式模型训练，以加快模型训练。</p>
<blockquote>
<div><p>下载完整的样例<a class="reference external" href="https://gitee.com/mindspore/graphlearning/tree/master/model_zoo/graphsage">GraphSAGE</a>代码。</p>
</div></blockquote>
</section>
<section id="graphsage原理">
<h2>GraphSAGE原理<a class="headerlink" href="#graphsage原理" title="永久链接至标题"></a></h2>
<p>论文链接：<a class="reference external" href="https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf">Inductive representation learning on large graphs</a></p>
</section>
<section id="设置运行脚本">
<h2>设置运行脚本<a class="headerlink" href="#设置运行脚本" title="永久链接至标题"></a></h2>
<p>在不同设备上，分布式训练的方式也不相同。</p>
<p>在GPU硬件平台上，MindSpore分布式并行训练中的通信使用的是英伟达集合通信库NVIDIA Collective Communication Library(简称为NCCL)。</p>
<p>更多关于在<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/master/parallel/train_gpu.html">GPU</a>上进行分布式训练的细节。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPU</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>,1,2,3,4,5,6,7
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_NUM</span><span class="o">=</span><span class="m">8</span>
rm<span class="w"> </span>-rf<span class="w"> </span>device
mkdir<span class="w"> </span>device
cp<span class="w"> </span>-r<span class="w"> </span>src<span class="w"> </span>./device
cp<span class="w"> </span>distributed_trainval_reddit.py<span class="w"> </span>./device
<span class="nb">cd</span><span class="w"> </span>./device
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>
mpirun<span class="w"> </span>--allow-run-as-root<span class="w"> </span>-n<span class="w"> </span><span class="si">${</span><span class="nv">CUDA_NUM</span><span class="si">}</span><span class="w"> </span>python3<span class="w"> </span>./distributed_trainval_reddit.py<span class="w"> </span>--data-path<span class="w"> </span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span><span class="w"> </span>--epochs<span class="w"> </span><span class="m">5</span><span class="w"> </span>&gt;<span class="w"> </span>train.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>MindSpore分布式并行训练的通信使用了华为集合通信库Huawei Collective Communication Library（以下简称HCCL），可以在Ascend AI处理器配套的软件包中找到。同时mindspore.communication.management中封装了HCCL提供的集合通信接口，方便用户配置分布式信息。</p>
<p>更多关于在<a class="reference external" href="https://www.mindspore.cn/tutorials/experts/zh-CN/master/parallel/train_ascend.html">Ascend</a>上进行分布式训练的细节。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Ascend</span>
<span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="nv">$3</span>
<span class="w">  </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_TABLE_FILE</span><span class="o">=</span><span class="si">${</span><span class="nv">RANK_TABLE_FILE</span><span class="si">}</span>
<span class="w">  </span><span class="k">for</span><span class="o">((</span><span class="nv">i</span><span class="o">=</span><span class="m">0</span><span class="p">;</span>i&lt;<span class="m">8</span><span class="p">;</span>i++<span class="o">))</span><span class="p">;</span>
<span class="w">  </span><span class="k">do</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">RANK_ID</span><span class="o">=</span>$<span class="o">[</span>i+RANK_START<span class="o">]</span>
<span class="w">    </span><span class="nb">export</span><span class="w"> </span><span class="nv">DEVICE_ID</span><span class="o">=</span>$<span class="o">[</span>i+RANK_START<span class="o">]</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="si">${</span><span class="nv">DEVICE_ID</span><span class="si">}</span>
<span class="w">    </span>rm<span class="w"> </span>-rf<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/device_<span class="nv">$RANK_ID</span>
<span class="w">    </span>mkdir<span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/device_<span class="nv">$RANK_ID</span>
<span class="w">    </span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">execute_path</span><span class="si">}</span>/device_<span class="nv">$RANK_ID</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">exit</span>
<span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;start training&quot;</span>
<span class="w">    </span>python3<span class="w"> </span><span class="si">${</span><span class="nv">self_path</span><span class="si">}</span>/distributed_trainval_reddit.py<span class="w"> </span>--data-path<span class="w"> </span><span class="si">${</span><span class="nv">DATA_PATH</span><span class="si">}</span><span class="w"> </span>--epochs<span class="w"> </span><span class="m">2</span><span class="w"> </span>&gt;<span class="w"> </span>train<span class="nv">$RANK_ID</span>.log<span class="w"> </span><span class="m">2</span>&gt;<span class="p">&amp;</span><span class="m">1</span><span class="w"> </span><span class="p">&amp;</span>
<span class="w">  </span><span class="k">done</span>
</pre></div>
</div>
</section>
<section id="定义网络结构">
<h2>定义网络结构<a class="headerlink" href="#定义网络结构" title="永久链接至标题"></a></h2>
<p>mindspore_gl.nn提供了SAGEConv的API可以直接调用。使用SAGEConv实现一个两层的GraphSAGE网络代码如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SAGENet</span><span class="p">(</span><span class="n">Cell</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;graphsage net&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_feat_size</span><span class="p">,</span> <span class="n">hidden_feat_size</span><span class="p">,</span> <span class="n">appr_feat_size</span><span class="p">,</span> <span class="n">out_feat_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">SAGEConv</span><span class="p">(</span><span class="n">in_feat_size</span><span class="p">,</span> <span class="n">hidden_feat_size</span><span class="p">,</span> <span class="n">aggregator_type</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">SAGEConv</span><span class="p">(</span><span class="n">hidden_feat_size</span><span class="p">,</span> <span class="n">appr_feat_size</span><span class="p">,</span> <span class="n">aggregator_type</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_out</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">appr_feat_size</span><span class="p">,</span> <span class="n">out_feat_size</span><span class="p">,</span> <span class="n">has_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                     <span class="n">weight_init</span><span class="o">=</span><span class="n">XavierUniform</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">ms</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">construct</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_feat</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">,</span> <span class="n">n_edges</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;graphsage net forward&quot;&quot;&quot;</span>
        <span class="n">node_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">node_feat</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">edges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">edges</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_nodes</span><span class="p">,</span> <span class="n">n_edges</span><span class="p">)</span>
        <span class="n">node_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">node_feat</span><span class="p">)</span>
        <span class="n">node_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">node_feat</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">node_feat</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">edges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">edges</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_nodes</span><span class="p">,</span> <span class="n">n_edges</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_out</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ret</span>
</pre></div>
</div>
<p>SAGEConv执行的更多细节可以看mindspore_gl.nn.SAGEConv的<a class="reference external" href="https://gitee.com/mindspore/graphlearning/blob/master/mindspore_gl/nn/conv/sageconv.py">API</a>代码。</p>
</section>
<section id="定义loss函数">
<h2>定义loss函数<a class="headerlink" href="#定义loss函数" title="永久链接至标题"></a></h2>
<p>由于本次任务为分类任务，可以采用交叉熵来作为损失函数，实现方法与<a class="reference external" href="https://www.mindspore.cn/graphlearning/docs/zh-CN/master/full_training_of_GCN.html#%E5%AE%9A%E4%B9%89loss%E5%87%BD%E6%95%B0">GCN</a>类似。</p>
</section>
<section id="构造数据集">
<h2>构造数据集<a class="headerlink" href="#构造数据集" title="永久链接至标题"></a></h2>
<p>下面以<a class="reference external" href="https://data.dgl.ai/dataset/reddit.zip">Reddit</a>数据集为例。输入数据路径，构造数据类。</p>
<p>get_group_size用于获取分布式训练的进程总数，get_rank用于获取当前进程的ID。数据加载器的构建方法可以参考<a class="reference external" href="https://www.mindspore.cn/graphlearning/docs/zh-CN/master/batched_graph_training_GIN.html#%E6%9E%84%E9%80%A0%E6%95%B0%E6%8D%AE%E9%9B%86">GIN</a>。</p>
<p>与GIN不同的时，在本例中采样器调用的是mindspore_gl.dataloader.DistributeRandomBatchSampler。DistributeRandomBatchSampler可以根据进程ID拆分数据集索引，确保每个进程获取的数据集批次的不同部分。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_gl.dataset</span> <span class="kn">import</span> <span class="n">Reddit</span>
<span class="kn">from</span> <span class="nn">mindspore.communication</span> <span class="kn">import</span> <span class="n">get_rank</span><span class="p">,</span> <span class="n">get_group_size</span>

<span class="n">rank_id</span> <span class="o">=</span> <span class="n">get_rank</span><span class="p">()</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="n">get_group_size</span><span class="p">()</span>
<span class="n">graph_dataset</span> <span class="o">=</span> <span class="n">Reddit</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_path</span><span class="p">)</span>
<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">DistributeRandomBatchSampler</span><span class="p">(</span><span class="n">rank_id</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">data_source</span><span class="o">=</span><span class="n">graph_dataset</span><span class="o">.</span><span class="n">train_nodes</span><span class="p">,</span>
                                             <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">test_sampler</span> <span class="o">=</span> <span class="n">RandomBatchSampler</span><span class="p">(</span><span class="n">data_source</span><span class="o">=</span><span class="n">graph_dataset</span><span class="o">.</span><span class="n">test_nodes</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">GraphSAGEDataset</span><span class="p">(</span><span class="n">graph_dataset</span><span class="p">,</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">train_sampler</span><span class="p">)),</span> <span class="n">single_size</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">GraphSAGEDataset</span><span class="p">(</span><span class="n">graph_dataset</span><span class="p">,</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">test_sampler</span><span class="p">)),</span> <span class="n">single_size</span><span class="p">)</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;seeds_idx&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;nid_feat&#39;</span><span class="p">,</span> <span class="s1">&#39;edges&#39;</span><span class="p">],</span>
                                       <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span> <span class="n">python_multiprocessing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">GeneratorDataset</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;seeds_idx&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;nid_feat&#39;</span><span class="p">,</span> <span class="s1">&#39;edges&#39;</span><span class="p">],</span>
                                      <span class="n">sampler</span><span class="o">=</span><span class="n">test_sampler</span><span class="p">,</span> <span class="n">python_multiprocessing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>mindspore_gl.sampling.sage_sampler_on_homo提供了k-hop的采样方法。在<code class="docutils literal notranslate"><span class="pre">self.neighbor_nums</span></code>为list的形式，设定了每次从中心节点往外的采样点个数。
由于每个点的度数不一样，经过k-hop采样后的数组的尺寸也不一样。通过接口mindspore_gl.graph.PadArray2d将采样得到的结果离散化成5个固定的值。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mindspore_gl.dataloader.dataset</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">mindspore_gl.sampling.neighbor</span> <span class="kn">import</span> <span class="n">sage_sampler_on_homo</span>

<span class="k">class</span> <span class="nc">GraphSAGEDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Do sampling from neighbour nodes&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_dataset</span><span class="p">,</span> <span class="n">neighbor_nums</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">single_size</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">graph_dataset</span> <span class="o">=</span> <span class="n">graph_dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">graph_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neighbor_nums</span> <span class="o">=</span> <span class="n">neighbor_nums</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">graph_dataset</span><span class="o">.</span><span class="n">node_feat</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">graph_dataset</span><span class="o">.</span><span class="n">node_label</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sampled_nodes_num</span> <span class="o">=</span> <span class="n">neighbor_nums</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">neighbor_nums</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">single_size</span> <span class="o">=</span> <span class="n">single_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">length</span> <span class="o">=</span> <span class="n">length</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_nodes</span><span class="p">):</span>
        <span class="n">batch_nodes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_nodes</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">sage_sampler_on_homo</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">graph</span><span class="p">,</span> <span class="n">batch_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">neighbor_nums</span><span class="p">)</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">array_kernel</span><span class="o">.</span><span class="n">int_1d_array_slicing</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_nodes</span><span class="p">)</span>
        <span class="n">layered_edges_0</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;layered_edges_0&#39;</span><span class="p">]</span>
        <span class="n">layered_edges_1</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;layered_edges_1&#39;</span><span class="p">]</span>
        <span class="n">sample_edges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">layered_edges_0</span><span class="p">,</span> <span class="n">layered_edges_1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sample_edges</span> <span class="o">=</span> <span class="n">sample_edges</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">:]</span>
        <span class="n">num_sample_edges</span> <span class="o">=</span> <span class="n">sample_edges</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">num_sample_nodes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;all_nodes&#39;</span><span class="p">])</span>
        <span class="n">max_sampled_nodes_num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sampled_nodes_num</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">single_size</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">num_sample_nodes</span> <span class="o">&lt;</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.2</span><span class="o">*</span><span class="n">max_sampled_nodes_num</span><span class="p">):</span>
                <span class="n">pad_node_num</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.2</span><span class="o">*</span><span class="n">max_sampled_nodes_num</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">num_sample_nodes</span> <span class="o">&lt;</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.4</span><span class="o">*</span><span class="n">max_sampled_nodes_num</span><span class="p">):</span>
                <span class="n">pad_node_num</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.4</span> <span class="o">*</span> <span class="n">max_sampled_nodes_num</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">num_sample_nodes</span> <span class="o">&lt;</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.6</span><span class="o">*</span><span class="n">max_sampled_nodes_num</span><span class="p">):</span>
                <span class="n">pad_node_num</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.6</span> <span class="o">*</span> <span class="n">max_sampled_nodes_num</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">num_sample_nodes</span> <span class="o">&lt;</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.8</span><span class="o">*</span><span class="n">max_sampled_nodes_num</span><span class="p">):</span>
                <span class="n">pad_node_num</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">max_sampled_nodes_num</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pad_node_num</span> <span class="o">=</span> <span class="n">max_sampled_nodes_num</span>

            <span class="k">if</span> <span class="n">num_sample_edges</span> <span class="o">&lt;</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.2</span><span class="o">*</span><span class="n">max_sampled_nodes_num</span><span class="p">):</span>
                <span class="n">pad_edge_num</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.2</span><span class="o">*</span><span class="n">max_sampled_nodes_num</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">num_sample_edges</span> <span class="o">&lt;</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.4</span><span class="o">*</span><span class="n">max_sampled_nodes_num</span><span class="p">):</span>
                <span class="n">pad_edge_num</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.4</span> <span class="o">*</span> <span class="n">max_sampled_nodes_num</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">num_sample_edges</span> <span class="o">&lt;</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.6</span><span class="o">*</span><span class="n">max_sampled_nodes_num</span><span class="p">):</span>
                <span class="n">pad_edge_num</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.6</span> <span class="o">*</span> <span class="n">max_sampled_nodes_num</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">num_sample_edges</span> <span class="o">&lt;</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.8</span><span class="o">*</span><span class="n">max_sampled_nodes_num</span><span class="p">):</span>
                <span class="n">pad_edge_num</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="mf">0.8</span> <span class="o">*</span> <span class="n">max_sampled_nodes_num</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pad_edge_num</span> <span class="o">=</span> <span class="n">max_sampled_nodes_num</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">pad_node_num</span> <span class="o">=</span> <span class="n">max_sampled_nodes_num</span>
            <span class="n">pad_edge_num</span> <span class="o">=</span> <span class="n">max_sampled_nodes_num</span>

        <span class="n">layered_edges_pad_op</span> <span class="o">=</span> <span class="n">PadArray2d</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">PadMode</span><span class="o">.</span><span class="n">CONST</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad_edge_num</span><span class="p">],</span>
                                          <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="n">PadDirection</span><span class="o">.</span><span class="n">ROW</span><span class="p">,</span>
                                          <span class="n">fill_value</span><span class="o">=</span><span class="n">pad_node_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                                          <span class="p">)</span>
        <span class="n">nid_feat_pad_op</span> <span class="o">=</span> <span class="n">PadArray2d</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">PadMode</span><span class="o">.</span><span class="n">CONST</span><span class="p">,</span>
                                     <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">pad_node_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_dataset</span><span class="o">.</span><span class="n">node_feat_size</span><span class="p">],</span>
                                     <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">graph_dataset</span><span class="o">.</span><span class="n">node_feat</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                                     <span class="n">direction</span><span class="o">=</span><span class="n">PadDirection</span><span class="o">.</span><span class="n">COL</span><span class="p">,</span>
                                     <span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                     <span class="n">reset_with_fill_value</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                     <span class="n">use_shared_numpy</span><span class="o">=</span><span class="kc">True</span>
                                     <span class="p">)</span>
        <span class="n">sample_edges</span> <span class="o">=</span> <span class="n">sample_edges</span><span class="p">[:,</span> <span class="p">:</span><span class="n">pad_edge_num</span><span class="p">]</span>
        <span class="n">pad_sample_edges</span> <span class="o">=</span> <span class="n">layered_edges_pad_op</span><span class="p">(</span><span class="n">sample_edges</span><span class="p">)</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="n">nid_feat_pad_op</span><span class="o">.</span><span class="n">lazy</span><span class="p">([</span><span class="n">num_sample_nodes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_dataset</span><span class="o">.</span><span class="n">node_feat_size</span><span class="p">])</span>
        <span class="n">array_kernel</span><span class="o">.</span><span class="n">float_2d_gather_with_dst</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">graph_dataset</span><span class="o">.</span><span class="n">node_feat</span><span class="p">,</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;all_nodes&#39;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;seeds_idx&#39;</span><span class="p">],</span> <span class="n">label</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">pad_sample_edges</span>
</pre></div>
</div>
</section>
<section id="网络训练和验证">
<h2>网络训练和验证<a class="headerlink" href="#网络训练和验证" title="永久链接至标题"></a></h2>
<section id="设置环境变量">
<h3>设置环境变量<a class="headerlink" href="#设置环境变量" title="永久链接至标题"></a></h3>
<p>分布式训练时，采用数据并行方式导入数据。在每个训练步骤结束时，各个进程会统一模型参数，在Ascend上，必须确保每个进程中的数据shape相同。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">device_target</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DEVICE_TARGET&#39;</span><span class="p">))</span>
<span class="k">if</span> <span class="n">device_target</span> <span class="o">==</span> <span class="s1">&#39;Ascend&#39;</span><span class="p">:</span>
    <span class="n">device_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;DEVICE_ID&#39;</span><span class="p">))</span>
    <span class="n">ms</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">)</span>
    <span class="n">single_size</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">init</span><span class="p">()</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">init</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
    <span class="n">single_size</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>图算编译优化设置可以参考<a class="reference external" href="https://www.mindspore.cn/graphlearning/docs/zh-CN/master/full_training_of_GCN.html#%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">GCN</a>。</p>
</section>
<section id="定义训练网络">
<h3>定义训练网络<a class="headerlink" href="#定义训练网络" title="永久链接至标题"></a></h3>
<p>实例化模型主体以及LossNet和优化器。
实现方法与GCN类似，可以参考<a class="reference external" href="https://www.mindspore.cn/graphlearning/docs/zh-CN/master/full_training_of_GCN.html#%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C">GCN</a>。</p>
</section>
<section id="网络训练及验证">
<h3>网络训练及验证<a class="headerlink" href="#网络训练及验证" title="永久链接至标题"></a></h3>
<p>训练与验证方法可以参考<a class="reference external" href="https://www.mindspore.cn/graphlearning/docs/zh-CN/master/full_training_of_GCN.html#%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83%E5%8F%8A%E9%AA%8C%E8%AF%81">GCN</a>。</p>
</section>
</section>
<section id="执行并查看结果">
<h2>执行并查看结果<a class="headerlink" href="#执行并查看结果" title="永久链接至标题"></a></h2>
<section id="运行过程">
<h3>运行过程<a class="headerlink" href="#运行过程" title="永久链接至标题"></a></h3>
<p>运行程序后，翻译代码并开始训练。</p>
</section>
<section id="执行结果">
<h3>执行结果<a class="headerlink" href="#执行结果" title="永久链接至标题"></a></h3>
<p>执行脚本<a class="reference external" href="https://gitee.com/mindspore/graphlearning/blob/master/model_zoo/graphsage/distributed_run.sh">distributed_run.sh</a>启动训练。</p>
<ul>
<li><p>GPU</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>model_zoo/graphsage
bash<span class="w"> </span>distributed_run.sh<span class="w"> </span>GPU<span class="w"> </span>DATA_PATH
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">{DATA_PATH}</span></code>为数据集存放路径。</p>
</li>
<li><p>Ascend</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>model_zoo/graphsage
bash<span class="w"> </span>bash<span class="w"> </span>distributed_run.sh<span class="w"> </span>Ascend<span class="w"> </span>DATA_PATH<span class="w"> </span>RANK_START<span class="w"> </span>RANK_SIZE<span class="w"> </span>RANK_TABLE_FILE
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">{DATA_PATH}</span></code>为数据集存放路径。<code class="docutils literal notranslate"><span class="pre">{ANK_START}</span></code>为使用的Ascend卡的第一个ID。<code class="docutils literal notranslate"><span class="pre">{RANK_SIZE}</span></code>为使用的卡的张数。<code class="docutils literal notranslate"><span class="pre">{RANK_TABLE_FILE}</span></code>为’rank_table_*pcs.json’文件的根路径.</p>
</li>
</ul>
<p>可以看到训练的结果如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>...
Iteration/Epoch:<span class="w"> </span><span class="m">30</span>:4<span class="w"> </span>train<span class="w"> </span>loss:<span class="w"> </span><span class="m">0</span>.41629112
Iteration/Epoch:<span class="w"> </span><span class="m">30</span>:4<span class="w"> </span>train<span class="w"> </span>loss:<span class="w"> </span><span class="m">0</span>.5337528
Iteration/Epoch:<span class="w"> </span><span class="m">30</span>:4<span class="w"> </span>train<span class="w"> </span>loss:<span class="w"> </span><span class="m">0</span>.42849028
Iteration/Epoch:<span class="w"> </span><span class="m">30</span>:4<span class="w"> </span>train<span class="w"> </span>loss:<span class="w"> </span><span class="m">0</span>.5358513
rank_id:3<span class="w"> </span>Epoch/Time:<span class="w"> </span><span class="m">4</span>:76.17579555511475
rank_id:1<span class="w"> </span>Epoch/Time:<span class="w"> </span><span class="m">4</span>:37.79207944869995
rank_id:2<span class="w"> </span>Epoch/Time:<span class="w"> </span><span class="m">4</span>:76.04292225837708
rank_id:0<span class="w"> </span>Epoch/Time:<span class="w"> </span><span class="m">4</span>:75.64319372177124
rank_id:2<span class="w"> </span><span class="nb">test</span><span class="w"> </span>accuracy<span class="w"> </span>:<span class="w"> </span><span class="m">0</span>.9276439525462963
rank_id:0<span class="w"> </span><span class="nb">test</span><span class="w"> </span>accuracy<span class="w"> </span>:<span class="w"> </span><span class="m">0</span>.9305013020833334
rank_id:3<span class="w"> </span><span class="nb">test</span><span class="w"> </span>accuracy<span class="w"> </span>:<span class="w"> </span><span class="m">0</span>.9290907118055556
rank_id:1<span class="w"> </span><span class="nb">test</span><span class="w"> </span>accuracy<span class="w"> </span>:<span class="w"> </span><span class="m">0</span>.9279513888888888
</pre></div>
</div>
<p>在Reddit数据上的验证精度为0.92。</p>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="spatio_temporal_graph_training_STGCN.html" class="btn btn-neutral float-left" title="时空图训练网络" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="mindspore_gl.html" class="btn btn-neutral float-right" title="mindspore_gl" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2022, MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>