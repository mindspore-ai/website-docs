<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MindSpore Lite API &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="_static/collapsible-lists/css/tree_view.css" type="text/css" /><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script><script src="_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script><script src="_static/collapsible-lists/js/apply-collapsible-lists.js"></script><script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="mindspore" href="api_cpp/mindspore.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_api.html">mindspore::api</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_api_utils.html">mindspore::api::utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_converter.html">mindspore::converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset.html">mindspore::dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset_config.html">mindspore::dataset::config</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset_text.html">mindspore::dataset::text</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset_transforms.html">mindspore::dataset::transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset_vision.html">mindspore::dataset::vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_kernel.html">mindspore::kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_ops.html">mindspore::ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_registry.html">mindspore::registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/lite_cpp_example.html">Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">JAVA API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_java/class_list.html">Class List</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/model_parallel_runner.html">ModelParallelRunner</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/mscontext.html">MSContext</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/mstensor.html">MSTensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/runner_config.html">RunnerConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/graph.html">Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/lite_java_example.html">Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindspore_lite.html">mindspore_lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">C API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_c/context_c.html">context_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/data_type_c.html">data_type_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/format_c.html">format_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/model_c.html">model_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/tensor_c.html">tensor_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/types_c.html">types_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/lite_c_example.html">Example</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>MindSpore Lite API</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mindspore-lite-api">
<h1>MindSpore Lite API<a class="headerlink" href="#mindspore-lite-api" title="Permalink to this headline"></a></h1>
<section id="summary-of-mindspore-lite-api-support">
<h2>Summary of MindSpore Lite API support<a class="headerlink" href="#summary-of-mindspore-lite-api-support" title="Permalink to this headline"></a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 7%" />
<col style="width: 17%" />
<col style="width: 28%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Class</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>C++ API</p></th>
<th class="head"><p>Python API</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Context</p></td>
<td><p>Set the number of threads at runtime</p></td>
<td><p>void SetThreadNum(int32_t thread_num)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_num</a></p></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>Get the current thread number setting</p></td>
<td><p>int32_t GetThreadNum() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_num</a></p></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>Set the parallel number of operators at runtime</p></td>
<td><p>void SetInterOpParallelNum(int32_t parallel_num)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.inter_op_parallel_num</a></p></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>Get the current operators parallel number setting</p></td>
<td><p>int32_t GetInterOpParallelNum() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.inter_op_parallel_num</a></p></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>Set the thread affinity to CPU cores</p></td>
<td><p>void SetThreadAffinity(int mode)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_affinity_mode</a></p></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>Get the thread affinity of CPU cores</p></td>
<td><p>int GetThreadAffinityMode() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_affinity_mode</a></p></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>Set the thread lists to CPU cores</p></td>
<td><p>void SetThreadAffinity(const std::vector&lt;int&gt; &amp;core_list)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_affinity_core_list</a></p></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>Get the thread lists of CPU cores</p></td>
<td><p>std::vector&lt;int32_t&gt; GetThreadAffinityCoreList() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_affinity_core_list</a></p></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>Set the status whether to perform model inference or training in parallel</p></td>
<td><p>void SetEnableParallel(bool is_parallel)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>Get the status whether to perform model inference or training in parallel</p></td>
<td><p>bool GetEnableParallel() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>Set built-in delegate mode to access third-party AI framework</p></td>
<td><p>void SetBuiltInDelegate(DelegateMode mode)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>Get the built-in delegate mode of the third-party AI framework</p></td>
<td><p>DelegateMode GetBuiltInDelegate() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>Set Delegate to access third-party AI framework</p></td>
<td><p>set_delegate(const std::shared_ptr&lt;AbstractDelegate&gt; &amp;delegate)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>Get the delegate of the third-party AI framework</p></td>
<td><p>std::shared_ptr&lt;AbstractDelegate&gt; get_delegate() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>Set quant model to run as float model in multi device</p></td>
<td><p>void SetMultiModalHW(bool float_mode)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>Get the mode of the model run</p></td>
<td><p>bool GetMultiModalHW() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>Get a mutable reference of DeviceInfoContext vector in this context</p></td>
<td><p>std::vector&lt;std::shared_ptr&lt;DeviceInfoContext&gt;&gt; &amp;MutableDeviceInfo()</p></td>
<td><p>Wrapped in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.target</a></p></td>
</tr>
<tr class="row-odd"><td><p>DeviceInfoContext</p></td>
<td><p>Get the type of this DeviceInfoContext</p></td>
<td><p>enum DeviceType GetDeviceType() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>DeviceInfoContext</p></td>
<td><p>converts DeviceInfoContext to a shared pointer of type T</p></td>
<td><p>std::shared_ptr&lt;T&gt; Cast()</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>DeviceInfoContext</p></td>
<td><p>set provider’s name</p></td>
<td><p>void SetProvider(const std::string &amp;provider)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>DeviceInfoContext</p></td>
<td><p>obtain provider’s name</p></td>
<td><p>std::string GetProvider() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>DeviceInfoContext</p></td>
<td><p>set provider’s device type</p></td>
<td><p>void SetProviderDevice(const std::string &amp;device)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>DeviceInfoContext</p></td>
<td><p>obtain provider’s device type</p></td>
<td><p>std::string GetProviderDevice() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>DeviceInfoContext</p></td>
<td><p>set memory allocator</p></td>
<td><p>void SetAllocator(const std::shared_ptr&lt;Allocator&gt; &amp;allocator)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>DeviceInfoContext</p></td>
<td><p>obtain memory allocator</p></td>
<td><p>std::shared_ptr&lt;Allocator&gt; GetAllocator() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>CPUDeviceInfo</p></td>
<td><p>Get the type of this DeviceInfoContext</p></td>
<td><p>enum DeviceType GetDeviceType() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">context.cpu</a></p></td>
</tr>
<tr class="row-even"><td><p>CPUDeviceInfo</p></td>
<td><p>Set enables to perform the float16 inference</p></td>
<td><p>void SetEnableFP16(bool is_fp16)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.precision_mode</a></p></td>
</tr>
<tr class="row-odd"><td><p>CPUDeviceInfo</p></td>
<td><p>Get enables to perform the float16 inference</p></td>
<td><p>bool GetEnableFP16() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.precision_mode</a></p></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>Get the type of this DeviceInfoContext</p></td>
<td><p>enum DeviceType GetDeviceType() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu</a></p></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>Set device id</p></td>
<td><p>void SetDeviceID(uint32_t device_id)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.device_id</a></p></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>Get the device id</p></td>
<td><p>uint32_t GetDeviceID() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.device_id</a></p></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>Get the distribution rank id</p></td>
<td><p>int GetRankID() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.rank_id</a></p></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>Get the distribution group size</p></td>
<td><p>int GetGroupSize() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.group_size</a></p></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>Set the precision mode</p></td>
<td><p>void SetPrecisionMode(const std::string &amp;precision_mode)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>Get the precision mode</p></td>
<td><p>std::string GetPrecisionMode() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>Set enables to perform the float16 inference</p></td>
<td><p>void SetEnableFP16(bool is_fp16)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.precision_mode</a></p></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>Get enables to perform the float16 inference</p></td>
<td><p>bool GetEnableFP16() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.precision_mode</a></p></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>Set enables to sharing mem with OpenGL</p></td>
<td><p>void SetEnableGLTexture(bool is_enable_gl_texture)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>Get enables to sharing mem with OpenGL</p></td>
<td><p>bool GetEnableGLTexture() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>Set current OpenGL context</p></td>
<td><p>void SetGLContext(void *gl_context)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>Get current OpenGL context</p></td>
<td><p>void *GetGLContext() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>Set current OpenGL display</p></td>
<td><p>void SetGLDisplay(void *gl_display)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>Get current OpenGL display</p></td>
<td><p>void *GetGLDisplay() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get the type of this DeviceInfoContext</p></td>
<td><p>enum DeviceType GetDeviceType() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.ascend</a></p></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set device id</p></td>
<td><p>void SetDeviceID(uint32_t device_id)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.ascend.device_id</a></p></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get the device id</p></td>
<td><p>uint32_t GetDeviceID() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.ascend.device_id</a></p></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set AIPP configuration file path</p></td>
<td><p>void SetInsertOpConfigPath(const std::string &amp;cfg_path)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get AIPP configuration file path</p></td>
<td><p>std::string GetInsertOpConfigPath() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set format of model inputs</p></td>
<td><p>void SetInputFormat(const std::string &amp;format)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get format of model inputs</p></td>
<td><p>std::string GetInputFormat() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set shape of model inputs</p></td>
<td><p>void SetInputShape(const std::string &amp;shape)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get shape of model inputs</p></td>
<td><p>std::string GetInputShape() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set shape of model inputs</p></td>
<td><p>void SetInputShapeMap(const std::map&lt;int, std::vector &lt;int&gt;&gt; &amp;shape)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get shape of model inputs</p></td>
<td><p>std::map&lt;int, std::vector &lt;int&gt;&gt; GetInputShapeMap() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set dynamic batch sizes of model inputs. Ranges from 2 to 100</p></td>
<td><p>void SetDynamicBatchSize(const std::vector&lt;size_t&gt; &amp;dynamic_batch_size)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get dynamic batch sizes of model inputs</p></td>
<td><p>std::string GetDynamicBatchSize() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set the dynamic image size of model inputs</p></td>
<td><p>void SetDynamicImageSize(const std::string &amp;dynamic_image_size)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get dynamic image size of model inputs</p></td>
<td><p>std::string GetDynamicImageSize() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set type of model outputs</p></td>
<td><p>void SetOutputType(enum DataType output_type)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get type of model outputs</p></td>
<td><p>enum DataType GetOutputType() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set precision mode of model</p></td>
<td><p>void SetPrecisionMode(const std::string &amp;precision_mode)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.ascend.precision_mode</a></p></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get precision mode of model</p></td>
<td><p>std::string GetPrecisionMode() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.ascend.precision_mode</a></p></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set op select implementation mode</p></td>
<td><p>void SetOpSelectImplMode(const std::string &amp;op_select_impl_mode)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get op select implementation mode</p></td>
<td><p>std::string GetOpSelectImplMode() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set fusion switch config file path. Controls which fusion passes to be turned off</p></td>
<td><p>void SetFusionSwitchConfigPath(const std::string &amp;cfg_path)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get fusion switch config file path</p></td>
<td><p>std::string GetFusionSwitchConfigPath() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>Set buffer optimize mode</p></td>
<td><p>void SetBufferOptimizeMode(const std::string &amp;buffer_optimize_mode)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>Get buffer optimize mode</p></td>
<td><p>std::string GetBufferOptimizeMode() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>KirinNPUDeviceInfo</p></td>
<td><p>Get the type of this DeviceInfoContext</p></td>
<td><p>enum DeviceType GetDeviceType() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>KirinNPUDeviceInfo</p></td>
<td><p>Set enables to perform the float16 inference</p></td>
<td><p>void SetEnableFP16(bool is_fp16)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>KirinNPUDeviceInfo</p></td>
<td><p>Get enables to perform the float16 inference</p></td>
<td><p>bool GetEnableFP16() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>KirinNPUDeviceInfo</p></td>
<td><p>Set the NPU frequency</p></td>
<td><p>void SetFrequency(int frequency)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>KirinNPUDeviceInfo</p></td>
<td><p>Get the NPU frequency</p></td>
<td><p>int GetFrequency() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Build a model from model buffer so that it can run on a device</p></td>
<td><p>Status Build(const void *model_data, size_t data_size, ModelType model_type, const std::shared_ptr &lt;Context&gt; &amp;model_context = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Load and build a model from model buffer so that it can run on a device</p></td>
<td><p>Status Build(const std::string &amp;model_path, ModelType model_type, const std::shared_ptr &lt;Context&gt; &amp;model_context = nullptr)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.build_from_file">Model.build_from_file</a></p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Build a model from model buffer so that it can run on a device</p></td>
<td><p>Status Build(const void *model_data, size_t data_size, ModelType model_type, const std::shared_ptr &lt;Context&gt; &amp;model_context, const Key &amp;dec_key, const std::string &amp;dec_mode, const std::string &amp;cropto_lib_path)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Load and build a model from model buffer so that it can run on a device</p></td>
<td><p>Status Build(const std::string &amp;model_path, ModelType model_type, const std::shared_ptr &lt;Context&gt; &amp;model_context, const Key &amp;dec_key, const std::string &amp;dec_mode, const std::string &amp;cropto_lib_path)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Build a train model from GraphCell so that it can run on a device</p></td>
<td><p>Status Build(GraphCell graph, const std::shared_ptr &lt;Context&gt; &amp;model_context = nullptr, const std::shared_ptr &lt;TrainCfg&gt; &amp;train_cfg = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Build a train model from GraphCell so that it can run on a device</p></td>
<td><p>Status Build(GraphCell graph, Node *optimizer, std::vector&lt;Expr *&gt; inputs, const std::shared_ptr &lt;Context&gt; &amp;model_context, const std::shared_ptr &lt;TrainCfg&gt; &amp;train_cfg)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Build a Transfer Learning model where the backbone weights are fixed and the head weights are trainable</p></td>
<td><p>Status BuildTransferLearning(GraphCell backbone, GraphCell head, const std::shared_ptr &lt;Context&gt; &amp;context, const std::shared_ptr &lt;TrainCfg&gt; &amp;train_cfg = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Resize the shapes of inputs</p></td>
<td><p>Status Resize(const std::vector &lt;MSTensor&gt; &amp;inputs, const std::vector &lt;std::vector&lt;int64_t&gt;&gt; &amp;dims)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.resize">Model.resize</a></p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Change the size and or content of weight tensors</p></td>
<td><p>Status UpdateWeights(const std::vector &lt;MSTensor&gt; &amp;new_weights)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Inference model API</p></td>
<td><p>Status Predict(const std::vector &lt;MSTensor&gt; &amp;inputs, std::vector &lt;MSTensor&gt; *outputs, const MSKernelCallBack &amp;before = nullptr, const MSKernelCallBack &amp;after = nullptr)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.predict)(notsupportcallback">Model.predict</a></p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Inference model API only with callback</p></td>
<td><p>Status Predict(const MSKernelCallBack &amp;before = nullptr, const MSKernelCallBack &amp;after = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Training API, Run model by step</p></td>
<td><p>Status RunStep(const MSKernelCallBack &amp;before = nullptr, const MSKernelCallBack &amp;after = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Inference model with preprocess in model</p></td>
<td><p>Status PredictWithPreprocess(const std::vector &lt;std::vector&lt;MSTensor&gt;&gt; &amp;inputs, std::vector &lt;MSTensor&gt; *outputs, const MSKernelCallBack &amp;before = nullptr, const MSKernelCallBack &amp;after = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Apply data preprocess if it exits in model</p></td>
<td><p>Status Preprocess(const std::vector &lt;std::vector&lt;MSTensor&gt;&gt; &amp;inputs, std::vector &lt;MSTensor&gt; *outputs)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Check if data preprocess exists in model</p></td>
<td><p>bool HasPreprocess()</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Load config file</p></td>
<td><p>Status LoadConfig(const std::string &amp;config_path)</p></td>
<td><p>Wrapped in the parameter <cite>config_path</cite> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.build_from_file">Model.build_from_file</a></p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Update config</p></td>
<td><p>Status UpdateConfig(const std::string &amp;section, const std::pair&lt;std::string, std::string&gt; &amp;config)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Obtains all input tensors of the model</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetInputs()</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.get_inputs">Model.get_inputs</a></p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Obtains the input tensor of the model by name</p></td>
<td><p>MSTensor GetInputByTensorName(const std::string &amp;tensor_name)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Obtain all gradient tensors of the model</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetGradients() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Update gradient tensors of the model</p></td>
<td><p>Status ApplyGradients(const std::vector &lt;MSTensor&gt; &amp;gradients)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Obtain all weights tensors of the model</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetFeatureMaps() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Obtain all trainable parameters of the model optimizers</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetTrainableParams() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Update weights tensors of the model</p></td>
<td><p>Status UpdateFeatureMaps(const std::vector &lt;MSTensor&gt; &amp;new_weights)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Obtain optimizer params tensors of the model</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetOptimizerParams() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Update the optimizer parameters</p></td>
<td><p>Status SetOptimizerParams(const std::vector &lt;MSTensor&gt; &amp;params)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Setup training with virtual batches</p></td>
<td><p>Status SetupVirtualBatch(int virtual_batch_multiplier, float lr = -1.0f, float momentum = -1.0f)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Set the Learning Rate of the training</p></td>
<td><p>Status SetLearningRate(float learning_rate)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Get the Learning Rate of the optimizer</p></td>
<td><p>float GetLearningRate()</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Initialize object with metrics</p></td>
<td><p>Status InitMetrics(std::vector&lt;Metrics *&gt; metrics)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Accessor to TrainLoop metric objects</p></td>
<td><p>std::vector&lt;Metrics *&gt; GetMetrics()</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Obtains all output tensors of the model</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetOutputs()</p></td>
<td><p>Wrapped in the return value of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.predict)(notsupportcallback">Model.predict</a></p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Obtains names of all output tensors of the model</p></td>
<td><p>std::vector &lt;std::string&gt; GetOutputTensorNames()</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Obtains the output tensor of the model by name</p></td>
<td><p>MSTensor GetOutputByTensorName(const std::string &amp;tensor_name)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Get output MSTensors of model by node name</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetOutputsByNodeName(const std::string &amp;node_name)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Bind GLTexture2D object to cl Memory</p></td>
<td><p>Status BindGLTexture2DMemory(const std::map&lt;std::string, unsigned int&gt; &amp;inputGLTexture, std::map&lt;std::string, unsigned int&gt; *outputGLTexture)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Set the model running mode</p></td>
<td><p>Status SetTrainMode(bool train)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Get the model running mode</p></td>
<td><p>bool GetTrainMode() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Performs the training Loop in Train Mode</p></td>
<td><p>Status Train(int epochs, std::shared_ptr &lt;dataset::Dataset&gt;ds, std::vector&lt;TrainCallBack *&gt; cbs)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>Performs the training loop over all data in Eval Mode</p></td>
<td><p>Status Evaluate(std::shared_ptr &lt;dataset::Dataset&gt; ds, std::vector&lt;TrainCallBack *&gt; cbs)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>Check if the device supports the model</p></td>
<td><p>static bool CheckModelSupport(enum DeviceType device_type, ModelType model_type)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>RunnerConfig</p></td>
<td><p>Set the number of workers at runtime</p></td>
<td><p>void SetWorkersNum(int32_t workers_num)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.workers_num</a></p></td>
</tr>
<tr class="row-odd"><td><p>RunnerConfig</p></td>
<td><p>Get the current operators parallel workers number setting</p></td>
<td><p>int32_t GetWorkersNum() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.workers_num</a></p></td>
</tr>
<tr class="row-even"><td><p>RunnerConfig</p></td>
<td><p>Set the context at runtime</p></td>
<td><p>void SetContext(const std::shared_ptr &lt;Context&gt; &amp;context)</p></td>
<td><p>Wrapped in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel</a></p></td>
</tr>
<tr class="row-odd"><td><p>RunnerConfig</p></td>
<td><p>Get the current context setting</p></td>
<td><p>std::shared_ptr &lt;Context&gt; GetContext() const</p></td>
<td><p>Wrapped in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel</a></p></td>
</tr>
<tr class="row-even"><td><p>RunnerConfig</p></td>
<td><p>Set the config before runtime</p></td>
<td><p>void SetConfigInfo(const std::string &amp;section, const std::map&lt;std::string, std::string&gt; &amp;config)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.config_info</a></p></td>
</tr>
<tr class="row-odd"><td><p>RunnerConfig</p></td>
<td><p>Get the current config setting</p></td>
<td><p>std::map&lt;std::string, std::map&lt;std::string, std::string&gt;&gt; GetConfigInfo() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.config_info</a></p></td>
</tr>
<tr class="row-even"><td><p>RunnerConfig</p></td>
<td><p>Set the config path before runtime</p></td>
<td><p>void SetConfigPath(const std::string &amp;config_path)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.config_path</a></p></td>
</tr>
<tr class="row-odd"><td><p>RunnerConfig</p></td>
<td><p>Get the current config path</p></td>
<td><p>std::string GetConfigPath() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.config_path</a></p></td>
</tr>
<tr class="row-even"><td><p>ModelParallelRunner</p></td>
<td><p>build a model parallel runner from model path so that it can run on a device</p></td>
<td><p>Status Init(const std::string &amp;model_path, const std::shared_ptr &lt;RunnerConfig&gt; &amp;runner_config = nullptr)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.build_from_file">Model.parallel_runner.build_from_file</a></p></td>
</tr>
<tr class="row-odd"><td><p>ModelParallelRunner</p></td>
<td><p>build a model parallel runner from model buffer so that it can run on a device</p></td>
<td><p>Status Init(const void *model_data, const size_t data_size, const std::shared_ptr &lt;RunnerConfig&gt; &amp;runner_config = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>ModelParallelRunner</p></td>
<td><p>Obtains all input tensors information of the model</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetInputs()</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.get_inputs">Model.parallel_runner.get_inputs</a></p></td>
</tr>
<tr class="row-odd"><td><p>ModelParallelRunner</p></td>
<td><p>Obtains all output tensors information of the model</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetOutputs()</p></td>
<td><p>Wrapped in the return value of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.predict">Model.parallel_runner.predict</a></p></td>
</tr>
<tr class="row-even"><td><p>ModelParallelRunner</p></td>
<td><p>Inference ModelParallelRunner</p></td>
<td><p>Status Predict(const std::vector &lt;MSTensor&gt; &amp;inputs, std::vector &lt;MSTensor&gt; *outputs,const MSKernelCallBack &amp;before = nullptr, const MSKernelCallBack &amp;after = nullptr)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.predict">Model.parallel_runner.predict</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Creates a MSTensor object, whose data need to be copied before accessed by Model</p></td>
<td><p>static inline MSTensor *CreateTensor(const std::string &amp;name, DataType type, const std::vector&lt;int64_t&gt; &amp;shape, const void *data, size_t data_len) noexcept</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor">Tensor</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Creates a MSTensor object, whose data can be directly accessed by Model</p></td>
<td><p>static inline MSTensor *CreateRefTensor(const std::string &amp;name, DataType type, const std::vector&lt;int64_t&gt; &amp;shape, const void *data, size_t data_len, bool own_data = true) noexcept</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Creates a MSTensor object, whose device data can be directly accessed by Model</p></td>
<td><p>static inline MSTensor CreateDeviceTensor(const std::string &amp;name, DataType type, const std::vector&lt;int64_t&gt; &amp;shape, void *data, size_t data_len) noexcept</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Creates a MSTensor object from local file</p></td>
<td><p>static inline MSTensor *CreateTensorFromFile(const std::string &amp;file, DataType type = DataType::kNumberTypeUInt8, const std::vector&lt;int64_t&gt; &amp;shape = {}) noexcept</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Create a string type MSTensor object whose data can be accessed by Model only after being copied</p></td>
<td><p>static inline MSTensor *StringsToTensor(const std::string &amp;name, const std::vectorstd::string &amp;str)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Parse the string type MSTensor object into strings</p></td>
<td><p>static inline std::vectorstd::string TensorToStrings(const MSTensor &amp;tensor)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Destroy an object created by <cite>Clone</cite> , <cite>StringsToTensor</cite> , <cite>CreateRefTensor</cite> or <cite>CreateTensor</cite></p></td>
<td><p>static void DestroyTensorPtr(MSTensor *tensor) noexcept</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Obtains the name of the MSTensor</p></td>
<td><p>std::string Name() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.name">Tensor.name</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Obtains the data type of the MSTensor</p></td>
<td><p>enum DataType DataType() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.dtype">Tensor.dtype</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Obtains the shape of the MSTensor</p></td>
<td><p>const std::vector&lt;int64_t&gt; &amp;Shape() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.shape">Tensor.shape</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Obtains the number of elements of the MSTensor</p></td>
<td><p>int64_t ElementNum() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.element_num">Tensor.element_num</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Obtains a shared pointer to the copy of data of the MSTensor</p></td>
<td><p>std::shared_ptr &lt;const void&gt; Data() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Obtains the pointer to the data of the MSTensor</p></td>
<td><p>void *MutableData()</p></td>
<td><p>Wrapped in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.get_data_to_numpy">Tensor.get_data_to_numpy</a> and  <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.set_data_from_numpy">Tensor.set_data_from_numpy</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Obtains the length of the data of the MSTensor, in bytes</p></td>
<td><p>size_t DataSize() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.data_size">Tensor.data_size</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Get whether the MSTensor data is const data</p></td>
<td><p>bool IsConst() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Gets the boolean value that indicates whether the memory of MSTensor is on device</p></td>
<td><p>bool IsDevice() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Gets a deep copy of the MSTensor</p></td>
<td><p>MSTensor *Clone() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Gets the boolean value that indicates whether the MSTensor is valid</p></td>
<td><p>bool operator==(std::nullptr_t) const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Gets the boolean value that indicates whether the MSTensor is valid</p></td>
<td><p>bool operator!=(std::nullptr_t) const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Get the boolean value that indicates whether the MSTensor equals tensor</p></td>
<td><p>bool operator==(const MSTensor &amp;tensor) const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Get the boolean value that indicates whether the MSTensor not equals tensor</p></td>
<td><p>bool operator!=(const MSTensor &amp;tensor) const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Set the shape of for the MSTensor</p></td>
<td><p>void SetShape(const std::vector&lt;int64_t&gt; &amp;shape)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.shape">Tensor.shape</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Set the data type for the MSTensor</p></td>
<td><p>void SetDataType(enum DataType data_type)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.dtype">Tensor.dtype</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Set the name for the MSTensor</p></td>
<td><p>void SetTensorName(const std::string &amp;name)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.name">Tensor.name</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Set the Allocator for the MSTensor</p></td>
<td><p>void SetAllocator(std::shared_ptr &lt;Allocator&gt; allocator)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Obtain the Allocator of the MSTensor</p></td>
<td><p>std::shared_ptr &lt;Allocator&gt; allocator() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Set the format for the MSTensor</p></td>
<td><p>void SetFormat(mindspore::Format format)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.format">Tensor.format</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Obtain the format of the MSTensor</p></td>
<td><p>mindspore::Format format() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.format">Tensor.format</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Set the data for the MSTensor</p></td>
<td><p>void SetData(void *data, bool own_data = true)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Set the device data address for the MSTensor</p></td>
<td><p>void SetDeviceData(void *data)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Get the device data address of the MSTensor set by SetDeviceData</p></td>
<td><p>void *GetDeviceData()</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>Get the quantization parameters of the MSTensor</p></td>
<td><p>std::vector &lt;QuantParam&gt; QuantParams() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>Set the quantization parameters for the MSTensor</p></td>
<td><p>void SetQuantParams(std::vector &lt;QuantParam&gt; quant_params)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>ModelGroup</p></td>
<td><p>Construct a ModelGroup object and indicate shared workspace memory or shared weight memory, with default shared workspace memory</p></td>
<td><p>ModelGroup(ModelGroupFlag flags = ModelGroupFlag::kShareWorkspace)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelGroup.html#mindspore_lite.ModelGroup">ModelGroup</a></p></td>
</tr>
<tr class="row-odd"><td><p>ModelGroup</p></td>
<td><p>When sharing weight memory, add model objects that require shared weight memory</p></td>
<td><p>Status AddModel(const std::vector&lt;Model&gt; &amp;model_list)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelGroup.html#mindspore_lite.ModelGroup.add_model">ModelGroup.add_model</a></p></td>
</tr>
<tr class="row-even"><td><p>ModelGroup</p></td>
<td><p>When sharing workspace memory, add the path of the model that requires shared workspace memory</p></td>
<td><p>Status AddModel(const std::vector&lt;std::string&gt; &amp;model_path_list)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelGroup.html#mindspore_lite.ModelGroup.add_model">ModelGroup.add_model</a></p></td>
</tr>
<tr class="row-odd"><td><p>ModelGroup</p></td>
<td><p>When sharing workspace memory, add a model buffer that requires shared workspace memory</p></td>
<td><p>Status AddModel(const std::vector&lt;std::pair&lt;const void *, size_t&gt;&gt; &amp;model_buff_list)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>ModelGroup</p></td>
<td><p>When sharing workspace memory, calculate the maximum workspace memory size</p></td>
<td><p>Status CalMaxSizeOfWorkspace(ModelType model_type, const std::shared_ptr&lt;Context&gt; &amp;ms_context)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelGroup.html#mindspore_lite.ModelGroup.cal_max_size_of_workspace">ModelGroup.cal_max_size_of_workspace</a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="mindspore-lite-api-1">
<h1>MindSpore Lite API<a class="headerlink" href="#mindspore-lite-api-1" title="Permalink to this headline"></a></h1>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_api.html">mindspore::api</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_api_utils.html">mindspore::api::utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_converter.html">mindspore::converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset.html">mindspore::dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset_config.html">mindspore::dataset::config</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset_text.html">mindspore::dataset::text</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset_transforms.html">mindspore::dataset::transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset_vision.html">mindspore::dataset::vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_kernel.html">mindspore::kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_ops.html">mindspore::ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_registry.html">mindspore::registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/lite_cpp_example.html">Example</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">JAVA API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_java/class_list.html">Class List</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/model_parallel_runner.html">ModelParallelRunner</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/mscontext.html">MSContext</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/mstensor.html">MSTensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/runner_config.html">RunnerConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/graph.html">Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/lite_java_example.html">Example</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindspore_lite.html">mindspore_lite</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">C API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_c/context_c.html">context_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/data_type_c.html">data_type_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/format_c.html">format_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/model_c.html">model_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/tensor_c.html">tensor_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/types_c.html">types_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/lite_c_example.html">Example</a></li>
</ul>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="api_cpp/mindspore.html" class="btn btn-neutral float-right" title="mindspore" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>