<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore_lite.converter &mdash; MindSpore master documentation</title>
      <link rel="stylesheet" href="../../_static/collapsible-lists/css/tree_view.css" type="text/css" /><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_NN.html">mindspore::NN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_api.html">mindspore::api</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_api_utils.html">mindspore::api::utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_converter.html">mindspore::converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_dataset.html">mindspore::dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_dataset_config.html">mindspore::dataset::config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_dataset_text.html">mindspore::dataset::text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_dataset_transforms.html">mindspore::dataset::transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_dataset_vision.html">mindspore::dataset::vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_kernel.html">mindspore::kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_ops.html">mindspore::ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_registry.html">mindspore::registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_cpp/lite_cpp_example.html">Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">JAVA API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api_java/class_list.html">Class List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_java/model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_java/model_parallel_runner.html">ModelParallelRunner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_java/mscontext.html">MSContext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_java/mstensor.html">MSTensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_java/runner_config.html">RunnerConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_java/graph.html">Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_java/lite_java_example.html">Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../mindspore_lite.html">mindspore_lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">C API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api_c/context_c.html">context_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_c/data_type_c.html">data_type_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_c/format_c.html">format_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_c/model_c.html">model_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_c/tensor_c.html">tensor_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_c/types_c.html">types_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_c/lite_c_example.html">Example</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">Module code</a> &raquo;</li>
      <li>mindspore_lite.converter</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mindspore_lite.converter</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2022-2023 Huawei Technologies Co., Ltd</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1"># http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ============================================================================</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Converter API.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>

<span class="kn">from</span> <span class="nn">mindspore_lite._checkparam</span> <span class="kn">import</span> <span class="n">check_isinstance</span><span class="p">,</span> <span class="n">check_input_shape</span><span class="p">,</span> <span class="n">check_config_info</span>
<span class="kn">from</span> <span class="nn">mindspore_lite.lib</span> <span class="kn">import</span> <span class="n">_c_lite_wrapper</span>
<span class="kn">from</span> <span class="nn">mindspore_lite.tensor</span> <span class="kn">import</span> <span class="n">DataType</span><span class="p">,</span> <span class="n">Format</span><span class="p">,</span> <span class="n">data_type_py_cxx_map</span><span class="p">,</span> <span class="n">data_type_cxx_py_map</span><span class="p">,</span> <span class="n">format_py_cxx_map</span><span class="p">,</span> \
    <span class="n">format_cxx_py_map</span>
<span class="kn">from</span> <span class="nn">mindspore_lite.model</span> <span class="kn">import</span> <span class="n">ModelType</span><span class="p">,</span> <span class="n">model_type_py_cxx_map</span><span class="p">,</span> <span class="n">model_type_cxx_py_map</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;FmkType&#39;</span><span class="p">,</span> <span class="s1">&#39;Converter&#39;</span><span class="p">]</span>


<div class="viewcode-block" id="FmkType"><a class="viewcode-back" href="../../mindspore_lite/mindspore_lite.FmkType.html#mindspore_lite.FmkType">[docs]</a><span class="k">class</span> <span class="nc">FmkType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    When Converter, the `FmkType` is used to define Input model framework type.</span>

<span class="sd">    Currently, the following model framework types are supported:</span>

<span class="sd">    ===========================  ============================================================================</span>
<span class="sd">    Definition                    Description</span>
<span class="sd">    ===========================  ============================================================================</span>
<span class="sd">    `FmkType.TF`                 TensorFlow model&#39;s framework type, and the model uses .pb as suffix.</span>
<span class="sd">    `FmkType.CAFFE`              Caffe model&#39;s framework type, and the model uses .prototxt as suffix.</span>
<span class="sd">    `FmkType.ONNX`               ONNX model&#39;s framework type, and the model uses .onnx as suffix.</span>
<span class="sd">    `FmkType.MINDIR`             MindSpore model&#39;s framework type, and the model uses .mindir as suffix.</span>
<span class="sd">    `FmkType.TFLITE`             TensorFlow Lite model&#39;s framework type, and the model uses .tflite as suffix.</span>
<span class="sd">    `FmkType.PYTORCH`            PyTorch model&#39;s framework type, and the model uses .pt or .pth as suffix.</span>
<span class="sd">    ===========================  ============================================================================</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Method 1: Import mindspore_lite package</span>
<span class="sd">        &gt;&gt;&gt; import mindspore_lite as mslite</span>
<span class="sd">        &gt;&gt;&gt; print(mslite.FmkType.TF)</span>
<span class="sd">        FmkType.TF</span>
<span class="sd">        &gt;&gt;&gt; # Method 2: from mindspore_lite package import FmkType</span>
<span class="sd">        &gt;&gt;&gt; from mindspore_lite import FmkType</span>
<span class="sd">        &gt;&gt;&gt; print(FmkType.TF)</span>
<span class="sd">        FmkType.TF</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">TF</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">CAFFE</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">ONNX</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">MINDIR</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">TFLITE</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">PYTORCH</span> <span class="o">=</span> <span class="mi">5</span></div>


<div class="viewcode-block" id="Converter"><a class="viewcode-back" href="../../mindspore_lite/mindspore_lite.Converter.html#mindspore_lite.Converter">[docs]</a><span class="k">class</span> <span class="nc">Converter</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Constructs a `Converter` class.</span>

<span class="sd">    Used in the following scenarios:</span>

<span class="sd">    1. Convert the third-party model into MindSpore model or MindSpore Lite model.</span>

<span class="sd">    2. Convert MindSpore model into MindSpore model or MindSpore Lite model.</span>

<span class="sd">    Convert to MindSpore model is recommended. Currently, Convert to MindSpore Lite model is supported,</span>
<span class="sd">    but it will be deprecated in the future. If you want to convert to MindSpore Lite model, please use</span>
<span class="sd">    `converter_tool &lt;https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/converter_tool.html&gt;`_  instead of</span>
<span class="sd">    The Python interface. The Model api and ModelParallelRunner api only support MindSpore model.</span>

<span class="sd">    Note:</span>
<span class="sd">        Please construct the `Converter` class first, and then generate the model by executing the Converter.convert()</span>
<span class="sd">        method.</span>

<span class="sd">        The encryption and decryption function is only valid when it is set to `MSLITE_ENABLE_MODEL_ENCRYPTION=on` at</span>
<span class="sd">        compile time, and only supports Linux x86 platforms. `decrypt_key` and `encrypt_key` are string expressed in</span>
<span class="sd">        hexadecimal. For example, if encrypt_key is set as &quot;30313233343637383939414243444546&quot;, the corresponding</span>
<span class="sd">        hexadecimal expression is &#39;(b)0123456789ABCDEF&#39; . Linux platform users can use the&#39; xxd &#39;tool to convert the</span>
<span class="sd">        key expressed in bytes into hexadecimal expressions. It should be noted that the encryption and decryption</span>
<span class="sd">        algorithm has been updated in version 1.7, resulting in the new Python interface does not support the conversion</span>
<span class="sd">        of MindSpore Lite&#39;s encryption exported models in version 1.6 and earlier.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # testcase based on cloud inference package.</span>
<span class="sd">        &gt;&gt;&gt; import mindspore_lite as mslite</span>
<span class="sd">        &gt;&gt;&gt; converter = mslite.Converter()</span>
<span class="sd">        &gt;&gt;&gt; # The ms model may be generated only after converter.convert() is executed after the class is constructed.</span>
<span class="sd">        &gt;&gt;&gt; converter.weight_fp16 = True</span>
<span class="sd">        &gt;&gt;&gt; converter.input_shape = {&quot;inTensor1&quot;: [1, 3, 32, 32]}</span>
<span class="sd">        &gt;&gt;&gt; converter.input_format = mslite.Format.NHWC</span>
<span class="sd">        &gt;&gt;&gt; converter.input_data_type = mslite.DataType.FLOAT32</span>
<span class="sd">        &gt;&gt;&gt; converter.output_data_type = mslite.DataType.FLOAT32</span>
<span class="sd">        &gt;&gt;&gt; converter.save_type = mslite.ModelType.MINDIR</span>
<span class="sd">        &gt;&gt;&gt; converter.decrypt_key = &quot;30313233343637383939414243444546&quot;</span>
<span class="sd">        &gt;&gt;&gt; converter.decrypt_mode = &quot;AES-GCM&quot;</span>
<span class="sd">        &gt;&gt;&gt; converter.enable_encryption = True</span>
<span class="sd">        &gt;&gt;&gt; converter.encrypt_key = &quot;30313233343637383939414243444546&quot;</span>
<span class="sd">        &gt;&gt;&gt; converter.infer = True</span>
<span class="sd">        &gt;&gt;&gt; converter.optimize = &quot;general&quot;</span>
<span class="sd">        &gt;&gt;&gt; converter.device = &quot;Ascend&quot;</span>
<span class="sd">        &gt;&gt;&gt; section = &quot;common_quant_param&quot;</span>
<span class="sd">        &gt;&gt;&gt; config_info_in = {&quot;quant_type&quot;: &quot;WEIGHT_QUANT&quot;}</span>
<span class="sd">        &gt;&gt;&gt; converter.set_config_info(section, config_info_in)</span>
<span class="sd">        &gt;&gt;&gt; print(converter.get_config_info())</span>
<span class="sd">        {&#39;common_quant_param&#39;: {&#39;quant_type&#39;: &#39;WEIGHT_QUANT&#39;}}</span>
<span class="sd">        &gt;&gt;&gt; print(converter)</span>
<span class="sd">        config_info: {&#39;common_quant_param&#39;: {&#39;quant_type&#39;: &#39;WEIGHT_QUANT&#39;}},</span>
<span class="sd">        weight_fp16: True,</span>
<span class="sd">        input_shape: {&#39;inTensor1&#39;: [1, 3, 32, 32]},</span>
<span class="sd">        input_format: Format.NHWC,</span>
<span class="sd">        input_data_type: DataType.FLOAT32,</span>
<span class="sd">        output_data_type: DataType.FLOAT32,</span>
<span class="sd">        save_type: ModelType.MINDIR,</span>
<span class="sd">        decrypt_key: 30313233343637383939414243444546,</span>
<span class="sd">        decrypt_mode: AES-GCM,</span>
<span class="sd">        enable_encryption: True,</span>
<span class="sd">        encrypt_key: 30313233343637383939414243444546,</span>
<span class="sd">        infer: True,</span>
<span class="sd">        optimize: general,</span>
<span class="sd">        device: Ascend.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span> <span class="o">=</span> <span class="n">_c_lite_wrapper</span><span class="o">.</span><span class="n">ConverterBind</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimize_user_defined</span> <span class="o">=</span> <span class="s2">&quot;general&quot;</span>

    <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;config_info: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">get_config_info</span><span class="p">()</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;weight_fp16: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_fp16</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;input_shape: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input_shape</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;input_format: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input_format</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;input_data_type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input_data_type</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;output_data_type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">output_data_type</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;save_type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">save_type</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;decrypt_key: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">decrypt_key</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;decrypt_mode: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">decrypt_mode</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;enable_encryption: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">enable_encryption</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;encrypt_key: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">encrypt_key</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;infer: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">infer</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;optimize: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimize</span><span class="si">}</span><span class="s2">,</span><span class="se">\n</span><span class="s2">&quot;</span> \
              <span class="sa">f</span><span class="s2">&quot;device: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">decrypt_key</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the key used to decrypt the encrypted MindIR file.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str, the key used to decrypt the encrypted MindIR file, expressed in hexadecimal characters. Only valid when</span>
<span class="sd">            fmk_type is FmkType.MINDIR.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_decrypt_key</span><span class="p">()</span>

    <span class="nd">@decrypt_key</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">decrypt_key</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decrypt_key</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the key used to decrypt the encrypted MindIR file</span>

<span class="sd">        Args:</span>
<span class="sd">            decrypt_key (str): Set the key used to decrypt the encrypted MindIR file, expressed in hexadecimal</span>
<span class="sd">                characters. Only valid when fmk_type is FmkType.MINDIR.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `decrypt_key` is not a str.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;decrypt_key&quot;</span><span class="p">,</span> <span class="n">decrypt_key</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_decrypt_key</span><span class="p">(</span><span class="n">decrypt_key</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">decrypt_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get decryption mode for the encrypted MindIR file.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str, decryption mode for the encrypted MindIR file. Only valid when dec_key is set. Options are &quot;AES-GCM&quot; |</span>
<span class="sd">            &quot;AES-CBC&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_decrypt_mode</span><span class="p">()</span>

    <span class="nd">@decrypt_mode</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">decrypt_mode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decrypt_mode</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set decryption mode for the encrypted MindIR file.</span>

<span class="sd">        Args:</span>
<span class="sd">            decrypt_mode (str): Set decryption mode for the encrypted MindIR file. Only valid when dec_key is set.</span>
<span class="sd">                Options are &quot;AES-GCM&quot; | &quot;AES-CBC&quot;.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `decrypt_mode` is not a str.</span>
<span class="sd">            ValueError: `decrypt_mode` is neither &quot;AES-GCM&quot; nor &quot;AES-CBC&quot; when it is a str.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;decrypt_mode&quot;</span><span class="p">,</span> <span class="n">decrypt_mode</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">decrypt_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;AES-GCM&quot;</span><span class="p">,</span> <span class="s2">&quot;AES-CBC&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;decrypt_mode must be in [AES-GCM, AES-CBC], but got </span><span class="si">{</span><span class="n">decrypt_mode</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_decrypt_mode</span><span class="p">(</span><span class="n">decrypt_mode</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get target device when converter model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str, target device when converter model. Only valid for Ascend. The use case is when on the Ascend device,</span>
<span class="sd">            if you need to the converted model to have the ability to use Ascend backend to perform inference,</span>
<span class="sd">            you can set the parameter. If it is not set, the converted model will use CPU backend to perform</span>
<span class="sd">            inference by default. Option is &quot;Ascend&quot;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>

    <span class="nd">@device</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set target device when converter model.</span>

<span class="sd">        Args:</span>
<span class="sd">            device (str): Set target device when converter model. Only valid for Ascend. The use case is when on the</span>
<span class="sd">                Ascend device, if you need to the converted model to have the ability to use Ascend backend to perform</span>
<span class="sd">                inference, you can set the parameter. If it is not set, the converted model will use CPU backend to</span>
<span class="sd">                perform inference by default. Option is &quot;Ascend&quot;.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `device` is not a str.</span>
<span class="sd">            ValueError: `device` is not &quot;Ascend&quot; when it is a str.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;device&quot;</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Ascend&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;device must be in [Ascend], but got </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">enable_encryption</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the status whether to encrypt the model when exporting.</span>

<span class="sd">        Returns:</span>
<span class="sd">            bool, whether to encrypt the model when exporting. Export encryption can protect the integrity of the model,</span>
<span class="sd">            but it will increase the initialization time at runtime.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_enable_encryption</span><span class="p">()</span>

    <span class="nd">@enable_encryption</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">enable_encryption</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enable_encryption</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set whether to encrypt the model when exporting.</span>

<span class="sd">        Args:</span>
<span class="sd">            enable_encryption (bool): Whether to encrypt the model when exporting. Export encryption can protect the</span>
<span class="sd">                integrity of the model, but it will increase the initialization time at runtime.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `enable_encryption` is not a bool.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;enable_encryption&quot;</span><span class="p">,</span> <span class="n">enable_encryption</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_enable_encryption</span><span class="p">(</span><span class="n">enable_encryption</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">encrypt_key</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the key used to encrypt the model when exporting.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str, the key used to encrypt the model when exporting, expressed in hexadecimal characters. Only support to</span>
<span class="sd">            use it when `decrypt_mode` is &quot;AES-GCM&quot;, the key length is 16.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_encrypt_key</span><span class="p">()</span>

    <span class="nd">@encrypt_key</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">encrypt_key</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encrypt_key</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the key used to encrypt the model when exporting, expressed in hexadecimal characters.</span>

<span class="sd">        Args:</span>
<span class="sd">            encrypt_key (str): Set the key used to encrypt the model when exporting, expressed in hexadecimal</span>
<span class="sd">                characters. Only support when `decrypt_mode` is &quot;AES-GCM&quot;, the key length is 16.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `encrypt_key` is not a str.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;encrypt_key&quot;</span><span class="p">,</span> <span class="n">encrypt_key</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_encrypt_key</span><span class="p">(</span><span class="n">encrypt_key</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">infer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the status whether to perform pre-inference at the completion of the conversion.</span>

<span class="sd">        Returns:</span>
<span class="sd">            bool, whether to perform pre-inference at the completion of the conversion.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_infer</span><span class="p">()</span>

    <span class="nd">@infer</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">infer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">infer</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set whether to perform pre-inference at the completion of the conversion.</span>

<span class="sd">        Args:</span>
<span class="sd">            infer (bool): whether to perform pre-inference at the completion of the conversion.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `infer` is not a bool.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;infer&quot;</span><span class="p">,</span> <span class="n">infer</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_infer</span><span class="p">(</span><span class="n">infer</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">input_data_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the data type of the quantization model input Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            DataType, the data type of the quantization model input Tensor. It is only valid when the quantization</span>
<span class="sd">            parameters ( `scale` and `zero point` ) of the model input Tensor are available. The following 4</span>
<span class="sd">            DataTypes are supported: DataType.FLOAT32 | DataType.INT8 | DataType.UINT8 | DataType.UNKNOWN.</span>
<span class="sd">            For details, see</span>
<span class="sd">            `DataType &lt;https://mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.DataType.html&gt;`_ .</span>

<span class="sd">            - DataType.FLOAT32: 32-bit floating-point number.</span>
<span class="sd">            - DataType.INT8:    8-bit integer.</span>
<span class="sd">            - DataType.UINT8:   unsigned 8-bit integer.</span>
<span class="sd">            - DataType.UNKNOWN: Set the Same DataType as the model input Tensor.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">data_type_cxx_py_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_input_data_type</span><span class="p">())</span>

    <span class="nd">@input_data_type</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">input_data_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_data_type</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the data type of the quantization model input Tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_data_type (DataType): Set the data type of the quantization model input Tensor. It is only valid when</span>
<span class="sd">                the quantization parameters ( `scale` and `zero point` ) of the model input Tensor are available. The</span>
<span class="sd">                following 4 DataTypes are supported: DataType.FLOAT32 | DataType.INT8 | DataType.UINT8 |</span>
<span class="sd">                DataType.UNKNOWN. For details, see</span>
<span class="sd">                `DataType &lt;https://mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.DataType.html&gt;`_ .</span>

<span class="sd">                - DataType.FLOAT32: 32-bit floating-point number.</span>
<span class="sd">                - DataType.INT8:    8-bit integer.</span>
<span class="sd">                - DataType.UINT8:   unsigned 8-bit integer.</span>
<span class="sd">                - DataType.UNKNOWN: Set the Same DataType as the model input Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `input_data_type` is not a DataType.</span>
<span class="sd">            ValueError: `input_data_type` is not in [DataType.FLOAT32, DataType.INT8, DataType.UINT8, DataType.UNKNOWN]</span>
<span class="sd">                when `input_data_type` is a DataType.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;input_data_type&quot;</span><span class="p">,</span> <span class="n">input_data_type</span><span class="p">,</span> <span class="n">DataType</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_data_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">DataType</span><span class="o">.</span><span class="n">FLOAT32</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">INT8</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">UINT8</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input_data_type must be in [DataType.FLOAT32, DataType.INT8, DataType.UINT8, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;DataType.UNKNOWN].&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_input_data_type</span><span class="p">(</span><span class="n">data_type_py_cxx_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">input_data_type</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">input_format</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the input format of model.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Format, the input format of model. Only Valid for 4-dimensional input. The following 2 input formats are</span>
<span class="sd">            supported: Format.NCHW | Format.NHWC. For details,</span>
<span class="sd">            see `Format &lt;https://mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.Format.html&gt;`_ .</span>

<span class="sd">            - Format.NCHW: Store Tensor data in the order of batch N, channel C, height H and width W.</span>
<span class="sd">            - Format.NHWC: Store Tensor data in the order of batch N, height H, width W and channel C.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">format_cxx_py_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_input_format</span><span class="p">())</span>

    <span class="nd">@input_format</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">input_format</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_format</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the input format of model.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_format (Format): Set the input format of model. Only Valid for 4-dimensional input.The</span>
<span class="sd">                following 2 input formats are supported: Format.NCHW | Format.NHWC. For details,</span>
<span class="sd">                see `Format &lt;https://mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.Format.html&gt;`_ .</span>

<span class="sd">                - Format.NCHW: Store Tensor data in the order of batch N, channel C, height H and width W.</span>
<span class="sd">                - Format.NHWC: Store Tensor data in the order of batch N, height H, width W and channel C.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `input_format` is not a Format.</span>
<span class="sd">            ValueError: `input_format` is neither Format.NCHW nor Format.NHWC when it is a Format.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;input_format&quot;</span><span class="p">,</span> <span class="n">input_format</span><span class="p">,</span> <span class="n">Format</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_format</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Format</span><span class="o">.</span><span class="n">NCHW</span><span class="p">,</span> <span class="n">Format</span><span class="o">.</span><span class="n">NHWC</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;input_format must be in [Format.NCHW, Format.NHWC].&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_input_format</span><span class="p">(</span><span class="n">format_py_cxx_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">input_format</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the dimension of the model input.</span>

<span class="sd">        Returns:</span>
<span class="sd">            dict{str, list[int]}, the dimension of the model input. The order of input dimensions is consistent with the</span>
<span class="sd">            original model. In the following scenarios, users may need to set the parameter.</span>
<span class="sd">            For example, {&quot;inTensor1&quot;: [1, 32, 32, 32], &quot;inTensor2&quot;: [1, 1, 32, 32]}.</span>

<span class="sd">            - Usage 1:The input of the model to be converted is dynamic shape, but prepare to use fixed shape for</span>
<span class="sd">              inference, then set the parameter to fixed shape. After setting, when inferring on the converted</span>
<span class="sd">              model, the default input shape is the same as the parameter setting, no need to resize.</span>
<span class="sd">            - Usage 2: No matter whether the original input of the model to be converted is dynamic shape or not,</span>
<span class="sd">              but prepare to use fixed shape for inference, and the performance of the model is expected to be</span>
<span class="sd">              optimized as much as possible, then set the parameter to fixed shape. After setting, the model</span>
<span class="sd">              structure will be further optimized, but the converted model may lose the characteristics of dynamic</span>
<span class="sd">              shape(some operators strongly related to shape will be merged).</span>
<span class="sd">            - Usage 3: When using the converter function to generate code for Micro inference execution, it is</span>
<span class="sd">              recommended to set the parameter to reduce the probability of errors during deployment. When the model</span>
<span class="sd">              contains a Shape ops or the input of the model to be converted is a dynamic shape, you must set the</span>
<span class="sd">              parameter to fixed shape to support the relevant shape optimization and code generation.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_input_shape</span><span class="p">()</span>

    <span class="nd">@input_shape</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">input_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the dimension of the model input.</span>

<span class="sd">        Args:</span>
<span class="sd">            input_shape (dict{str, list[int]}): Set the dimension of the model input. The order of input dimensions is</span>
<span class="sd">                consistent with the original model. In the following scenarios, users may need to set the parameter.</span>
<span class="sd">                For example, {&quot;inTensor1&quot;: [1, 32, 32, 32], &quot;inTensor2&quot;: [1, 1, 32, 32]}.</span>

<span class="sd">                - Usage 1:The input of the model to be converted is dynamic shape, but prepare to use fixed shape for</span>
<span class="sd">                  inference, then set the parameter to fixed shape. After setting, when inferring on the converted</span>
<span class="sd">                  model, the default input shape is the same as the parameter setting, no need to resize.</span>
<span class="sd">                - Usage 2: No matter whether the original input of the model to be converted is dynamic shape or not,</span>
<span class="sd">                  but prepare to use fixed shape for inference, and the performance of the model is expected to be</span>
<span class="sd">                  optimized as much as possible, then set the parameter to fixed shape. After setting, the model</span>
<span class="sd">                  structure will be further optimized, but the converted model may lose the characteristics of dynamic</span>
<span class="sd">                  shape(some operators strongly related to shape will be merged).</span>
<span class="sd">                - Usage 3: When using the converter function to generate code for Micro inference execution, it is</span>
<span class="sd">                  recommended to set the parameter to reduce the probability of errors during deployment. When the model</span>
<span class="sd">                  contains a Shape ops or the input of the model to be converted is a dynamic shape, you must set the</span>
<span class="sd">                  parameter to fixed shape to support the relevant shape optimization and code generation.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `input_shape` is not a dict.</span>
<span class="sd">            TypeError: `input_shape` is a dict, but the keys are not str.</span>
<span class="sd">            TypeError: `input_shape` is a dict, the keys are str, but the values are not list.</span>
<span class="sd">            TypeError: `input_shape` is a dict, the keys are str, the values are list, but the value&#39;s elements are not</span>
<span class="sd">                int.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_input_shape</span><span class="p">(</span><span class="s2">&quot;input_shape&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_input_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the status whether avoid fusion optimization.</span>

<span class="sd">        optimize is used to set the mode of optimization during the offline conversion. If this parameter is set to</span>
<span class="sd">        &quot;none&quot;, no relevant graph optimization operations will be performed during the offline conversion phase of</span>
<span class="sd">        the model, and the relevant graph optimization operations will be performed during the execution of the</span>
<span class="sd">        inference phase. The advantage of this parameter is that the converted model can be deployed directly to any</span>
<span class="sd">        CPU/GPU/Ascend hardware backend since it is not optimized in a specific way, while the disadvantage is that</span>
<span class="sd">        the initialization time of the model increases during inference execution. If this parameter is set to</span>
<span class="sd">        &quot;general&quot;, general optimization will be performed, such as constant folding and operator fusion (the</span>
<span class="sd">        converted model only supports CPU/GPU hardware backend, not Ascend backend). If this parameter is set to</span>
<span class="sd">        &quot;ascend_oriented&quot;, the optimization for Ascend hardware will be performed (the converted model only supports</span>
<span class="sd">        Ascend hardware backend).</span>

<span class="sd">        For the MindSpore model, since it is already a `mindir` model, two approaches are suggested:</span>

<span class="sd">        1. Inference is performed directly without offline conversion.</span>

<span class="sd">        2. Setting `optimize` to &quot;general&quot; in CPU/GPU hardware backend and setting `optimize` to &quot;ascend_oriented&quot; in</span>
<span class="sd">        Ascend hardware when using offline conversion. The relevant optimization is done in the offline phase to reduce</span>
<span class="sd">        the initialization time of inference execution.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str, whether avoid fusion optimization. Options are &quot;none&quot; | &quot;general&quot; | &quot;ascend_oriented&quot;. &quot;none&quot; means</span>
<span class="sd">            fusion optimization is not allowed. &quot;general&quot; and &quot;ascend_oriented&quot; means fusion optimization is allowed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimize_user_defined</span>

    <span class="nd">@optimize</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimize</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set whether avoid fusion optimization.</span>

<span class="sd">        Args:</span>
<span class="sd">            optimize(str): Whether avoid fusion optimization. Options are &quot;none&quot; | &quot;general&quot; | &quot;ascend_oriented&quot;.</span>
<span class="sd">                &quot;none&quot; means fusion optimization is not allowed. &quot;general&quot; and &quot;ascend_oriented&quot; means fusion</span>
<span class="sd">                optimization is allowed.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `optimize` is not a str.</span>
<span class="sd">            ValueError: `optimize` is not in [&quot;none&quot;, &quot;general&quot;, &quot;ascend_oriented&quot;] when it is a str.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;optimize&quot;</span><span class="p">,</span> <span class="n">optimize</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">optimize</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_no_fusion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimize_user_defined</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
        <span class="k">elif</span> <span class="n">optimize</span> <span class="o">==</span> <span class="s2">&quot;general&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_no_fusion</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimize_user_defined</span> <span class="o">=</span> <span class="s2">&quot;general&quot;</span>
        <span class="k">elif</span> <span class="n">optimize</span> <span class="o">==</span> <span class="s2">&quot;ascend_oriented&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_no_fusion</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;Ascend&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimize_user_defined</span> <span class="o">=</span> <span class="s2">&quot;ascend_oriented&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;optimize must be &#39;none&#39;, &#39;general&#39; or &#39;ascend_oriented&#39;.&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_data_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the data type of the quantization model output Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">            DataType, the data type of the quantization model output Tensor. It is only valid when the quantization</span>
<span class="sd">            parameters ( `scale` and `zero point` ) of the model output Tensor are available. The following 4</span>
<span class="sd">            DataTypes are supported: DataType.FLOAT32 | DataType.INT8 | DataType.UINT8 | DataType.UNKNOWN.</span>
<span class="sd">            For details, see</span>
<span class="sd">            `DataType &lt;https://mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.DataType.html&gt;`_ .</span>

<span class="sd">            - DataType.FLOAT32: 32-bit floating-point number.</span>
<span class="sd">            - DataType.INT8:    8-bit integer.</span>
<span class="sd">            - DataType.UINT8:   unsigned 8-bit integer.</span>
<span class="sd">            - DataType.UNKNOWN: Set the Same DataType as the model output Tensor.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">data_type_cxx_py_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_output_data_type</span><span class="p">())</span>

    <span class="nd">@output_data_type</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">output_data_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_data_type</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the data type of the quantization model output Tensor.</span>

<span class="sd">        Args:</span>
<span class="sd">            output_data_type (DataType): Set the data type of the quantization model output Tensor. It is only valid</span>
<span class="sd">                when the quantization parameters ( `scale` and `zero point` ) of the model output Tensor are available.</span>
<span class="sd">                The following 4 DataTypes are supported: DataType.FLOAT32 | DataType.INT8 | DataType.UINT8 |</span>
<span class="sd">                DataType.UNKNOWN. For details, see</span>
<span class="sd">                `DataType &lt;https://mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.DataType.html&gt;`_ .</span>

<span class="sd">                - DataType.FLOAT32: 32-bit floating-point number.</span>
<span class="sd">                - DataType.INT8:    8-bit integer.</span>
<span class="sd">                - DataType.UINT8:   unsigned 8-bit integer.</span>
<span class="sd">                - DataType.UNKNOWN: Set the Same DataType as the model output Tensor.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `output_data_type` is not a DataType.</span>
<span class="sd">            ValueError: `output_data_type` is not in [DataType.FLOAT32, DataType.INT8, DataType.UINT8, DataType.UNKNOWN]</span>
<span class="sd">                when `output_data_type` is a DataType.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;output_data_type&quot;</span><span class="p">,</span> <span class="n">output_data_type</span><span class="p">,</span> <span class="n">DataType</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">output_data_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">DataType</span><span class="o">.</span><span class="n">FLOAT32</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">INT8</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">UINT8</span><span class="p">,</span> <span class="n">DataType</span><span class="o">.</span><span class="n">UNKNOWN</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output_data_type must be in [DataType.FLOAT32, DataType.INT8, DataType.UINT8, &quot;</span>
                             <span class="sa">f</span><span class="s2">&quot;DataType.UNKNOWN].&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_output_data_type</span><span class="p">(</span><span class="n">data_type_py_cxx_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">output_data_type</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">save_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the model type needs to be export.</span>

<span class="sd">        Returns:</span>
<span class="sd">            ModelType, the model type needs to be export. Options are ModelType.MINDIR |  ModelType.MINDIR_LITE.</span>
<span class="sd">            Convert to MindSpore model is recommended. Currently, Convert to MindSpore Lite model is supported,</span>
<span class="sd">            but it will be deprecated in the future. For details, see</span>
<span class="sd">            `ModelType &lt;https://mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.ModelType.html&gt;`_ .</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">model_type_cxx_py_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_save_type</span><span class="p">())</span>

    <span class="nd">@save_type</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">save_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_type</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set the model type needs to be export.</span>

<span class="sd">        Args:</span>
<span class="sd">            save_type (ModelType): Set the model type needs to be export. Options are ModelType.MINDIR |</span>
<span class="sd">                ModelType.MINDIR_LITE. Convert to MindSpore model is recommended. Currently, Convert to MindSpore Lite</span>
<span class="sd">                model is supported, but it will be deprecated in the future. For details, see</span>
<span class="sd">                `ModelType &lt;https://mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.ModelType.html&gt;`_ .</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `save_type` is not a ModelType.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;save_type&quot;</span><span class="p">,</span> <span class="n">save_type</span><span class="p">,</span> <span class="n">ModelType</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_save_type</span><span class="p">(</span><span class="n">model_type_py_cxx_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">save_type</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">weight_fp16</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the status whether the model will be saved as the Float16 data type.</span>

<span class="sd">        Returns:</span>
<span class="sd">            bool, whether the model will be saved as the Float16 data type. If it is True, the const Tensor of the</span>
<span class="sd">            Float32 in the model will be saved as the Float16 data type during Converter, and the generated model</span>
<span class="sd">            size will be compressed. Then, according to `Context.CPU` &#39;s `precision_mode` parameter determines the</span>
<span class="sd">            inputs&#39; data type to perform inference. The priority of `weight_fp16` is very low. For example, if</span>
<span class="sd">            quantization is enabled, for the weight of the quantized, `weight_fp16` will not take effect again.</span>
<span class="sd">            `weight_fp16` only effective for the const Tensor in Float32 data type.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_weight_fp16</span><span class="p">()</span>

    <span class="nd">@weight_fp16</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">weight_fp16</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight_fp16</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set whether the model will be saved as the Float16 data type.</span>

<span class="sd">        Args:</span>
<span class="sd">            weight_fp16 (bool): If it is True, the const Tensor of the Float32 in the model will be saved as the Float16</span>
<span class="sd">                data type during Converter, and the generated model size will be compressed. Then, according to</span>
<span class="sd">                `Context.CPU` &#39;s `precision_mode` parameter determines the inputs&#39; data type to perform inference. The</span>
<span class="sd">                priority of `weight_fp16` is very low. For example, if quantization is enabled, for the weight of the</span>
<span class="sd">                quantized, `weight_fp16` will not take effect again. `weight_fp16` only effective for the const Tensor</span>
<span class="sd">                in Float32 data type.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `weight_fp16` is not a bool.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;weight_fp16&quot;</span><span class="p">,</span> <span class="n">weight_fp16</span><span class="p">,</span> <span class="nb">bool</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_weight_fp16</span><span class="p">(</span><span class="n">weight_fp16</span><span class="p">)</span>

<div class="viewcode-block" id="Converter.convert"><a class="viewcode-back" href="../../mindspore_lite/mindspore_lite.Converter.html#mindspore_lite.Converter.convert">[docs]</a>    <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fmk_type</span><span class="p">,</span> <span class="n">model_file</span><span class="p">,</span> <span class="n">output_file</span><span class="p">,</span> <span class="n">weight_file</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">config_file</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform conversion, and convert the third-party model to the MindSpore model or MindSpore Lite model.</span>

<span class="sd">        Args:</span>
<span class="sd">            fmk_type (FmkType): Input model framework type. Options are FmkType.TF | FmkType.CAFFE |</span>
<span class="sd">                FmkType.ONNX | FmkType.MINDIR | FmkType.TFLITE | FmkType.PYTORCH. For details, see</span>
<span class="sd">                `FmkType &lt;https://mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.FmkType.html&gt;`_ .</span>
<span class="sd">            model_file (str): Set the path of the input model when convert. For example, &quot;/home/user/model.prototxt&quot;.</span>
<span class="sd">                Options are TF: &quot;model.pb&quot; | CAFFE: &quot;model.prototxt&quot; | ONNX: &quot;model.onnx&quot; | MINDIR: &quot;model.mindir&quot; |</span>
<span class="sd">                TFLITE: &quot;model.tflite&quot; | PYTORCH: &quot;model.pt or model.pth&quot;.</span>
<span class="sd">            output_file (str): Set the path of the output model. The suffix .ms or .mindir can be automatically</span>
<span class="sd">                generated. If set `save_type` to ModelType.MINDIR, then MindSpore&#39;s model will be generated, which uses</span>
<span class="sd">                .mindir as suffix. If set `save_type` to ModelType.MINDIR_LITE, then MindSpore Lite&#39;s model will be</span>
<span class="sd">                generated, which uses .ms as suffix. For example, the input model is &quot;/home/user/model.prototxt&quot;, set</span>
<span class="sd">                `save_type` to ModelType.MINDIR, it will generate the model named model.prototxt.mindir in /home/user/.</span>
<span class="sd">            weight_file (str, optional): Set the path of input model weight file. Required only when fmk_type is</span>
<span class="sd">                FmkType.CAFFE. The Caffe model is generally divided into two files: &#39;model.prototxt&#39; is model structure,</span>
<span class="sd">                corresponding to `model_file` parameter; &#39;model.Caffemodel&#39; is model weight value file, corresponding to</span>
<span class="sd">                `weight_file` parameter. For example, &quot;/home/user/model.caffemodel&quot;. Default: &quot;&quot;, indicating no weight</span>
<span class="sd">                file.</span>
<span class="sd">            config_file (str, optional): Set the path of the configuration file of Converter can be used to</span>
<span class="sd">                post-training, offline split op to parallel, disable op fusion ability and set plugin so path.</span>
<span class="sd">                `config_file` uses the `key = value` method to define the related parameters.</span>
<span class="sd">                For the configuration parameters related to post training quantization, please refer to</span>
<span class="sd">                `quantization &lt;https://www.mindspore.cn/lite/docs/en/r2.0/use/post_training_quantization.html&gt;`_ .</span>
<span class="sd">                For the configuration parameters related to extension, please refer to</span>
<span class="sd">                `extension  &lt;https://www.mindspore.cn/lite/docs/en/r2.0/use/nnie.html#extension-configuration&gt;`_ .</span>
<span class="sd">                For example, &quot;/home/user/model.cfg&quot;. Default: &quot;&quot;, indicating that no configuration file.</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `fmk_type` is not a FmkType.</span>
<span class="sd">            TypeError: `model_file` is not a str.</span>
<span class="sd">            TypeError: `output_file` is not a str.</span>
<span class="sd">            TypeError: `weight_file` is not a str.</span>
<span class="sd">            TypeError: `config_file` is not a str.</span>
<span class="sd">            RuntimeError: `model_file` does not exist.</span>
<span class="sd">            RuntimeError: `weight_file` is not &quot;&quot;, but `weight_file` does not exist.</span>
<span class="sd">            RuntimeError: `config_file` is not &quot;&quot;, but `config_file` does not exist.</span>
<span class="sd">            RuntimeError: convert model failed.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore_lite as mslite</span>
<span class="sd">            &gt;&gt;&gt; converter = mslite.Converter()</span>
<span class="sd">            &gt;&gt;&gt; converter.save_type = mslite.ModelType.MINDIR</span>
<span class="sd">            &gt;&gt;&gt; converter.convert(mslite.FmkType.TFLITE, &quot;./mobilenetv2/mobilenet_v2_1.0_224.tflite&quot;,</span>
<span class="sd">            ...                     &quot;mobilenet_v2_1.0_224.tflite&quot;)</span>
<span class="sd">            CONVERT RESULT SUCCESS:0</span>
<span class="sd">            &gt;&gt;&gt; # mobilenet_v2_1.0_224.tflite.mindir model will be generated.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;fmk_type&quot;</span><span class="p">,</span> <span class="n">fmk_type</span><span class="p">,</span> <span class="n">FmkType</span><span class="p">)</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;model_file&quot;</span><span class="p">,</span> <span class="n">model_file</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;output_file&quot;</span><span class="p">,</span> <span class="n">output_file</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;weight_file&quot;</span><span class="p">,</span> <span class="n">weight_file</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;config_file&quot;</span><span class="p">,</span> <span class="n">config_file</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">model_file</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Perform convert method failed, model_file does not exist!&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight_file</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">weight_file</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Perform convert method failed, weight_file does not exist!&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config_file</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">config_file</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Perform convert method failed, config_file does not exist!&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_config_file</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>

        <span class="n">fmk_type_py_cxx_map</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">FmkType</span><span class="o">.</span><span class="n">TF</span><span class="p">:</span> <span class="n">_c_lite_wrapper</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">kFmkTypeTf</span><span class="p">,</span>
            <span class="n">FmkType</span><span class="o">.</span><span class="n">CAFFE</span><span class="p">:</span> <span class="n">_c_lite_wrapper</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">kFmkTypeCaffe</span><span class="p">,</span>
            <span class="n">FmkType</span><span class="o">.</span><span class="n">ONNX</span><span class="p">:</span> <span class="n">_c_lite_wrapper</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">kFmkTypeOnnx</span><span class="p">,</span>
            <span class="n">FmkType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">:</span> <span class="n">_c_lite_wrapper</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">kFmkTypeMs</span><span class="p">,</span>
            <span class="n">FmkType</span><span class="o">.</span><span class="n">TFLITE</span><span class="p">:</span> <span class="n">_c_lite_wrapper</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">kFmkTypeTflite</span><span class="p">,</span>
            <span class="n">FmkType</span><span class="o">.</span><span class="n">PYTORCH</span><span class="p">:</span> <span class="n">_c_lite_wrapper</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">kFmkTypePytorch</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">fmk_type_py_cxx_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">fmk_type</span><span class="p">),</span> <span class="n">model_file</span><span class="p">,</span> <span class="n">output_file</span><span class="p">,</span> <span class="n">weight_file</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">ret</span><span class="o">.</span><span class="n">IsOk</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Converter model failed! model_file is </span><span class="si">{</span><span class="n">model_file</span><span class="si">}</span><span class="s2">, error is </span><span class="si">{</span><span class="n">ret</span><span class="o">.</span><span class="n">ToString</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Converter.get_config_info"><a class="viewcode-back" href="../../mindspore_lite/mindspore_lite.Converter.html#mindspore_lite.Converter.get_config_info">[docs]</a>    <span class="k">def</span> <span class="nf">get_config_info</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get config info of converter.It is used together with `set_config_info` method for online converter.</span>
<span class="sd">        Please use `set_config_info` method before `get_config_info` .</span>

<span class="sd">        Returns:</span>
<span class="sd">            :obj:`dict{str: dict{str: str}}`, the config info which has been set in converter.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore_lite as mslite</span>
<span class="sd">            &gt;&gt;&gt; converter = mslite.Converter()</span>
<span class="sd">            &gt;&gt;&gt; section = &quot;common_quant_param&quot;</span>
<span class="sd">            &gt;&gt;&gt; config_info_in = {&quot;quant_type&quot;: &quot;WEIGHT_QUANT&quot;}</span>
<span class="sd">            &gt;&gt;&gt; converter.set_config_info(section, config_info_in)</span>
<span class="sd">            &gt;&gt;&gt; config_info_out = converter.get_config_info()</span>
<span class="sd">            &gt;&gt;&gt; print(config_info_out)</span>
<span class="sd">            {&#39;common_quant_param&#39;: {&#39;quant_type&#39;: &#39;WEIGHT_QUANT&#39;}}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">get_config_info</span><span class="p">()</span></div>

<div class="viewcode-block" id="Converter.set_config_info"><a class="viewcode-back" href="../../mindspore_lite/mindspore_lite.Converter.html#mindspore_lite.Converter.set_config_info">[docs]</a>    <span class="k">def</span> <span class="nf">set_config_info</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">section</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">config_info</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set config info for Converter.It is used together with `get_config_info` method for online converter.</span>

<span class="sd">        Args:</span>
<span class="sd">            section (str, optional): The category of the configuration parameter.</span>
<span class="sd">                Set the individual parameters of the configfile together with `config_info` .</span>
<span class="sd">                For example, for `section` = &quot;common_quant_param&quot;, `config_info` = {&quot;quant_type&quot;: &quot;WEIGHT_QUANT&quot;}.</span>
<span class="sd">                Default: &quot;&quot;.</span>

<span class="sd">                For the configuration parameters related to post training quantization, please refer to</span>
<span class="sd">                `quantization &lt;https://www.mindspore.cn/lite/docs/en/r2.0/use/post_training_quantization.html&gt;`_ .</span>

<span class="sd">                For the configuration parameters related to extension, please refer to</span>
<span class="sd">                `extension  &lt;https://www.mindspore.cn/lite/docs/en/r2.0/use/nnie.html#extension-configuration&gt;`_ .</span>

<span class="sd">                - &quot;common_quant_param&quot;: Common quantization parameter.</span>
<span class="sd">                - &quot;mixed_bit_weight_quant_param&quot;: Mixed bit weight quantization parameter.</span>
<span class="sd">                - &quot;full_quant_param&quot;: Full quantization parameter.</span>
<span class="sd">                - &quot;data_preprocess_param&quot;: Data preprocess quantization parameter.</span>
<span class="sd">                - &quot;registry&quot;: Extension configuration parameter.</span>

<span class="sd">            config_info (:obj:`dict{str: str}`, optional): List of configuration parameters.</span>
<span class="sd">                Set the individual parameters of the configfile together with `section` .</span>
<span class="sd">                For example, for `section` = &quot;common_quant_param&quot;, `config_info` = {&quot;quant_type&quot;: &quot;WEIGHT_QUANT&quot;}.</span>
<span class="sd">                Default: None, None is equivalent to {}.</span>

<span class="sd">                For the configuration parameters related to post training quantization, please refer to</span>
<span class="sd">                `quantization &lt;https://www.mindspore.cn/lite/docs/en/r2.0/use/post_training_quantization.html&gt;`_ .</span>

<span class="sd">                For the configuration parameters related to extension, please refer to</span>
<span class="sd">                `extension  &lt;https://www.mindspore.cn/lite/docs/en/r2.0/use/nnie.html#extension-configuration&gt;`_ .</span>

<span class="sd">        Raises:</span>
<span class="sd">            TypeError: `section` is not a str.</span>
<span class="sd">            TypeError: `config_info` is not a dict .</span>
<span class="sd">            TypeError: `config_info` is a dict, but the keys are not str.</span>
<span class="sd">            TypeError: `config_info` is a dict, the keys are str, but the values are not str.</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; import mindspore_lite as mslite</span>
<span class="sd">            &gt;&gt;&gt; converter = mslite.Converter()</span>
<span class="sd">            &gt;&gt;&gt; section = &quot;common_quant_param&quot;</span>
<span class="sd">            &gt;&gt;&gt; config_info = {&quot;quant_type&quot;: &quot;WEIGHT_QUANT&quot;}</span>
<span class="sd">            &gt;&gt;&gt; converter.set_config_info(section, config_info)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_isinstance</span><span class="p">(</span><span class="s2">&quot;section&quot;</span><span class="p">,</span> <span class="n">section</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="n">check_config_info</span><span class="p">(</span><span class="s2">&quot;config_info&quot;</span><span class="p">,</span> <span class="n">config_info</span><span class="p">,</span> <span class="n">enable_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">section</span> <span class="o">!=</span> <span class="s2">&quot;&quot;</span> <span class="ow">and</span> <span class="n">config_info</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_converter</span><span class="o">.</span><span class="n">set_config_info</span><span class="p">(</span><span class="n">section</span><span class="p">,</span> <span class="n">config_info</span><span class="p">)</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>