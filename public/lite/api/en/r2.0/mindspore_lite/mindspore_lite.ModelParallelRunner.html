

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>mindspore_lite.ModelParallelRunner &mdash; MindSpore master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
        
        
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore_lite.DataType" href="mindspore_lite.DataType.html" />
    <link rel="prev" title="mindspore_lite.Model" href="mindspore_lite.Model.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_NN.html">mindspore::NN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_api.html">mindspore::api</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_api_utils.html">mindspore::api::utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_converter.html">mindspore::converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset.html">mindspore::dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_config.html">mindspore::dataset::config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_text.html">mindspore::dataset::text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_transforms.html">mindspore::dataset::transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_vision.html">mindspore::dataset::vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_kernel.html">mindspore::kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_ops.html">mindspore::ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_registry.html">mindspore::registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/lite_cpp_example.html">Example</a></li>
</ul>
<p class="caption"><span class="caption-text">JAVA API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_java/class_list.html">Class List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/model_parallel_runner.html">ModelParallelRunner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/mscontext.html">MSContext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/mstensor.html">MSTensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/runner_config.html">RunnerConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/graph.html">Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/lite_java_example.html">Example</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore_lite.html">mindspore_lite</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore_lite.html#context">Context</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore_lite.html#converter">Converter</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore_lite.html#model">Model</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mindspore_lite.ModelType.html">mindspore_lite.ModelType</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore_lite.Model.html">mindspore_lite.Model</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore_lite.ModelParallelRunner</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore_lite.html#tensor">Tensor</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">C API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_c/context_c.html">context_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/data_type_c.html">data_type_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/format_c.html">format_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/lite_c_example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/model_c.html">model_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/tensor_c.html">tensor_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/types_c.html">types_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/lite_c_example.html">Example</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../mindspore_lite.html">mindspore_lite</a> &raquo;</li>
        
      <li>mindspore_lite.ModelParallelRunner</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/mindspore_lite/mindspore_lite.ModelParallelRunner.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="mindspore-lite-modelparallelrunner">
<h1>mindspore_lite.ModelParallelRunner<a class="headerlink" href="#mindspore-lite-modelparallelrunner" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="mindspore_lite.ModelParallelRunner">
<em class="property">class </em><code class="sig-prename descclassname">mindspore_lite.</code><code class="sig-name descname">ModelParallelRunner</code><a class="reference internal" href="../_modules/mindspore_lite/model.html#ModelParallelRunner"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_lite.ModelParallelRunner" title="Permalink to this definition">¶</a></dt>
<dd><p>The <cite>ModelParallelRunner</cite> class defines a MindSpore Lite’s Runner, which support model parallelism. Compared with
<cite>model</cite> , <cite>model</cite> does not support parallelism, but <cite>ModelParallelRunner</cite> supports parallelism. A Runner contains
multiple workers, which are the units that actually perform parallel inferring. The primary use case is when
multiple clients send inference tasks to the server, the server perform parallel inference, shorten the inference
time, and then return the inference results to the clients.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use case: serving inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 1: Building MindSpore Lite serving package by export MSLITE_ENABLE_SERVER_INFERENCE=on.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 2: install wheel package of MindSpore Lite built by precondition 1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelParallelRunner</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model_parallel_runner</span><span class="p">)</span>
<span class="go">model_path: .</span>
</pre></div>
</div>
<dl class="method">
<dt id="mindspore_lite.ModelParallelRunner.build_from_file">
<code class="sig-name descname">build_from_file</code><span class="sig-paren">(</span><em class="sig-param">model_path</em>, <em class="sig-param">context=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_lite/model.html#ModelParallelRunner.build_from_file"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_lite.ModelParallelRunner.build_from_file" title="Permalink to this definition">¶</a></dt>
<dd><p>build a model parallel runner from model path so that it can run on a device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_path</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Define the model path.</p></li>
<li><p><strong>context</strong> (<a class="reference internal" href="mindspore_lite.Context.html#mindspore_lite.Context" title="mindspore_lite.Context"><em>Context</em></a><em>, </em><em>optional</em>) – Define the config used to transfer context and options during building model.
Default: None. None means the Context with cpu target. Context has the default parallel
attribute.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>model_path</cite> is not a str.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>context</cite> is neither a Context nor None.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – <cite>model_path</cite> does not exist.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – ModelParallelRunner’s init failed.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use case: serving inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 1: Building MindSpore Lite serving package by export MSLITE_ENABLE_SERVER_INFERENCE=on.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 2: install wheel package of MindSpore Lite built by precondition 1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">workers_num</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelParallelRunner</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;mobilenetv2.ms&quot;</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model_parallel_runner</span><span class="p">)</span>
<span class="go">model_path: mobilenetv2.ms.</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mindspore_lite.ModelParallelRunner.get_inputs">
<code class="sig-name descname">get_inputs</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_lite/model.html#ModelParallelRunner.get_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_lite.ModelParallelRunner.get_inputs" title="Permalink to this definition">¶</a></dt>
<dd><p>Obtains all input Tensors of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list[Tensor], the input Tensor list of the model.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use case: serving inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 1: Building MindSpore Lite serving package by export MSLITE_ENABLE_SERVER_INFERENCE=on.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 2: install wheel package of MindSpore Lite built by precondition 1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">workers_num</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelParallelRunner</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;mobilenetv2.ms&quot;</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">model_parallel_runner</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="mindspore_lite.ModelParallelRunner.predict">
<code class="sig-name descname">predict</code><span class="sig-paren">(</span><em class="sig-param">inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_lite/model.html#ModelParallelRunner.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#mindspore_lite.ModelParallelRunner.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Inference ModelParallelRunner.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore_lite.Tensor.html#mindspore_lite.Tensor" title="mindspore_lite.Tensor"><em>Tensor</em></a><em>]</em>) – A list that includes all input Tensors in order.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list[Tensor], outputs, the model outputs are filled in the container in sequence.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>inputs</cite> is not a list.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>inputs</cite> is a list, but the elements are not Tensor.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – predict model failed.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use case: serving inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Precondition 1: Download MindSpore Lite serving package or building MindSpore Lite serving package by</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                 export MSLITE_ENABLE_SERVER_INFERENCE=on.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Precondition 2: Install wheel package of MindSpore Lite built by precondition 1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The result can be find in the tutorial of runtime_parallel_python.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">time</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the number of threads of one worker.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># WORKERS_NUM * THREAD_NUM should not exceed the number of cores of the machine.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">THREAD_NUM</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># In parallel inference, the number of workers in one `ModelParallelRunner` in server.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If you prepare to compare the time difference between parallel inference and serial inference,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># you can set WORKERS_NUM = 1 as serial inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">WORKERS_NUM</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Simulate 5 clients, and each client sends 2 inference tasks to the server at the same time.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">PARALLEL_NUM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TASK_NUM</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">parallel_runner_predict</span><span class="p">(</span><span class="n">parallel_runner</span><span class="p">,</span> <span class="n">parallel_id</span><span class="p">):</span>
<span class="gp">... </span>    <span class="c1"># One Runner with 3 workers, set model input, execute inference and get output.</span>
<span class="gp">... </span>    <span class="n">task_index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="gp">... </span>    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="gp">... </span>        <span class="k">if</span> <span class="n">task_index</span> <span class="o">==</span> <span class="n">TASK_NUM</span><span class="p">:</span>
<span class="gp">... </span>            <span class="k">break</span>
<span class="gp">... </span>        <span class="n">task_index</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">... </span>        <span class="c1"># Set model input</span>
<span class="gp">... </span>        <span class="n">inputs</span> <span class="o">=</span> <span class="n">parallel_runner</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()</span>
<span class="gp">... </span>        <span class="n">in_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="s2">&quot;input.bin&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">once_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># Execute inference</span>
<span class="gp">... </span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">parallel_runner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">once_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="gp">... </span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;parallel id: &quot;</span><span class="p">,</span> <span class="n">parallel_id</span><span class="p">,</span> <span class="s2">&quot; | task index: &quot;</span><span class="p">,</span> <span class="n">task_index</span><span class="p">,</span> <span class="s2">&quot; | run once time: &quot;</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">once_end_time</span> <span class="o">-</span> <span class="n">once_start_time</span><span class="p">,</span> <span class="s2">&quot; s&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="c1"># Get output</span>
<span class="gp">... </span>        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
<span class="gp">... </span>            <span class="n">tensor_name</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>
<span class="gp">... </span>            <span class="n">data_size</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">data_size</span>
<span class="gp">... </span>            <span class="n">element_num</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">element_num</span>
<span class="gp">... </span>            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor name is:</span><span class="si">%s</span><span class="s2"> tensor size is:</span><span class="si">%s</span><span class="s2"> tensor elements num is:</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tensor_name</span><span class="p">,</span>
<span class="gp">... </span>                                                                                     <span class="n">data_size</span><span class="p">,</span>
<span class="gp">... </span>                                                                                     <span class="n">element_num</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">... </span>            <span class="n">data</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get_data_to_numpy</span><span class="p">()</span>
<span class="gp">... </span>            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="gp">... </span>            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output data is:&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="gp">... </span>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
<span class="gp">... </span>                <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="gp">... </span>            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Init RunnerConfig and context, and add CPU device info</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">enable_fp16</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_num</span> <span class="o">=</span> <span class="n">THREAD_NUM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">inter_op_parallel_num</span> <span class="o">=</span> <span class="n">THREAD_NUM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">workers_num</span> <span class="o">=</span> <span class="n">WORKERS_NUM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build ModelParallelRunner from file</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelParallelRunner</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;mobilenetv2.ms&quot;</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The server creates 5 threads to store the inference tasks of 5 clients.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">total_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">PARALLEL_NUM</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">parallel_runner_predict</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">model_parallel_runner</span><span class="p">,</span> <span class="n">i</span><span class="p">,)))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Start threads to perform parallel inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">th</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">th</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">th</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">th</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">total_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;total run time: &quot;</span><span class="p">,</span> <span class="n">total_end_time</span> <span class="o">-</span> <span class="n">total_start_time</span><span class="p">,</span> <span class="s2">&quot; s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="mindspore_lite.DataType.html" class="btn btn-neutral float-right" title="mindspore_lite.DataType" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="mindspore_lite.Model.html" class="btn btn-neutral float-left" title="mindspore_lite.Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
	<script async="async" src="https://cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>