<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore_lite.Context &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script><script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore_lite.FmkType" href="mindspore_lite.FmkType.html" />
    <link rel="prev" title="mindspore_lite" href="../mindspore_lite.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_api.html">mindspore::api</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_api_utils.html">mindspore::api::utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_converter.html">mindspore::converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset.html">mindspore::dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_config.html">mindspore::dataset::config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_text.html">mindspore::dataset::text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_transforms.html">mindspore::dataset::transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_vision.html">mindspore::dataset::vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_kernel.html">mindspore::kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_ops.html">mindspore::ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_registry.html">mindspore::registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/lite_cpp_example.html">Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">JAVA API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_java/class_list.html">Class List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/model_parallel_runner.html">ModelParallelRunner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/mscontext.html">MSContext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/mstensor.html">MSTensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/runner_config.html">RunnerConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/graph.html">Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/lite_java_example.html">Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore_lite.html">mindspore_lite</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore_lite.html#context">Context</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore_lite.Context</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore_lite.html#converter">Converter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore_lite.html#model">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore_lite.html#tensor">Tensor</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">C API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_c/context_c.html">context_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/data_type_c.html">data_type_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/format_c.html">format_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/model_c.html">model_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/tensor_c.html">tensor_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/types_c.html">types_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/lite_c_example.html">Example</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore_lite.html">mindspore_lite</a> &raquo;</li>
      <li>mindspore_lite.Context</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/mindspore_lite/mindspore_lite.Context.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mindspore-lite-context">
<h1>mindspore_lite.Context<a class="headerlink" href="#mindspore-lite-context" title="Permalink to this headline"></a></h1>
<a class="reference external image-reference" href="https://gitee.com/mindspore/mindspore/blob/r2.3/mindspore/lite/python/api/context.py"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore_lite.Context">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore_lite.</span></span><span class="sig-name descname"><span class="pre">Context</span></span><a class="reference internal" href="../_modules/mindspore_lite/context.html#Context"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_lite.Context" title="Permalink to this definition"></a></dt>
<dd><p>The <cite>Context</cite> class is used to transfer environment variables during execution.</p>
<p>The context should be configured before running the program.
If it is not configured, the <cite>target</cite> will be set to <code class="docutils literal notranslate"><span class="pre">cpu</span></code>, and automatically set <code class="docutils literal notranslate"><span class="pre">cpu</span></code> attributes by default.</p>
<p>Context.parallel defines the context and configuration of <cite>ModelParallelRunner</cite> class.</p>
<dl class="simple">
<dt>Context.parallel properties:</dt><dd><ul>
<li><p><strong>workers_num</strong> (int) - the num of workers. A <cite>ModelParallelRunner</cite> contains multiple workers, which
are the units that actually perform parallel inferring. Setting <cite>workers_num</cite> to 0 represents
<cite>workers_num</cite> will be automatically adjusted based on computer performance and core numbers.</p></li>
<li><p><strong>config_info</strong> (dict{str, dict{str, str}}) - Nested map for transferring user defined options during building
<cite>ModelParallelRunner</cite> online. More configurable options refer to <cite>config_path</cite> .
For example, <code class="docutils literal notranslate"><span class="pre">{&quot;model_file&quot;:</span> <span class="pre">{&quot;mindir_path&quot;:</span> <span class="pre">&quot;/home/user/model_graph.mindir&quot;}}</span></code>.
<cite>section</cite> is <code class="docutils literal notranslate"><span class="pre">&quot;model_file&quot;</span></code>, one of the keys is <code class="docutils literal notranslate"><span class="pre">&quot;mindir_path&quot;</span></code>,
the corresponding value in the map is <code class="docutils literal notranslate"><span class="pre">&quot;/home/user/model_graph.mindir&quot;</span></code>.</p></li>
<li><p><strong>config_path</strong> (str) - Set the config file path. The config file is used to transfer user-defined
options during building <cite>ModelParallelRunner</cite> . In the following scenarios, users may need to set the
parameter. For example, <code class="docutils literal notranslate"><span class="pre">&quot;/home/user/config.txt&quot;</span></code>.</p>
<ul>
<li><p>Usage 1: Set mixed precision inference. The content and description of the configuration file are as
follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">execution_plan</span><span class="p">]</span>
<span class="p">[</span><span class="n">op_name1</span><span class="p">]</span><span class="o">=</span><span class="n">data_Type</span><span class="p">:</span> <span class="n">float16</span> <span class="p">(</span><span class="n">The</span> <span class="n">operator</span> <span class="n">named</span> <span class="n">op_name1</span> <span class="n">sets</span> <span class="n">the</span> <span class="n">data</span> <span class="nb">type</span> <span class="k">as</span> <span class="n">float16</span><span class="p">)</span>
<span class="p">[</span><span class="n">op_name2</span><span class="p">]</span><span class="o">=</span><span class="n">data_Type</span><span class="p">:</span> <span class="n">float32</span> <span class="p">(</span><span class="n">The</span> <span class="n">operator</span> <span class="n">named</span> <span class="n">op_name2</span> <span class="n">sets</span> <span class="n">the</span> <span class="n">data</span> <span class="nb">type</span> <span class="k">as</span> <span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Usage 2: When GPU inference, set the configuration of TensorRT. The content and description of the
configuration file are as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ms_cache</span><span class="p">]</span>
<span class="n">serialize_Path</span><span class="o">=</span><span class="p">[</span><span class="n">serialization</span> <span class="n">model</span> <span class="n">path</span><span class="p">](</span><span class="n">storage</span> <span class="n">path</span> <span class="n">of</span> <span class="n">serialization</span> <span class="n">model</span><span class="p">)</span>
<span class="p">[</span><span class="n">gpu_context</span><span class="p">]</span>
<span class="n">input_shape</span><span class="o">=</span><span class="n">input_Name</span><span class="p">:</span> <span class="p">[</span><span class="n">input_dim</span><span class="p">]</span> <span class="p">(</span><span class="n">Model</span> <span class="nb">input</span> <span class="n">dimension</span><span class="p">,</span> <span class="k">for</span> <span class="n">dynamic</span> <span class="n">shape</span><span class="p">)</span>
<span class="n">dynamic_Dims</span><span class="o">=</span><span class="p">[</span><span class="n">min_dim</span><span class="o">~</span><span class="n">max_dim</span><span class="p">]</span> <span class="p">(</span><span class="n">dynamic</span> <span class="n">dimension</span> <span class="nb">range</span> <span class="n">of</span> <span class="n">model</span> <span class="nb">input</span><span class="p">,</span> <span class="k">for</span> <span class="n">dynamic</span> <span class="n">shape</span><span class="p">)</span>
<span class="n">opt_Dims</span><span class="o">=</span><span class="p">[</span><span class="n">opt_dim</span><span class="p">]</span> <span class="p">(</span><span class="n">the</span> <span class="n">optimal</span> <span class="nb">input</span> <span class="n">dimension</span> <span class="n">of</span> <span class="n">the</span> <span class="n">model</span><span class="p">,</span> <span class="k">for</span> <span class="n">dynamic</span> <span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Usage 3: For the large model, when using the model buffer to load and compile, you need to set the path
of the weight file separately through passing the path of the large model. And it is necessary to ensure
that the large model file and the folder where the weight file is located are in the same folder.
For example, when the directory is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>.
└── /home/user/
     ├── model_graph.mindir
     └── model_variables
          └── data_0
</pre></div>
</div>
<p>The content and description of the configuration file are as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">model_file</span><span class="p">]</span>
<span class="n">mindir_path</span><span class="o">=</span><span class="p">[</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">user</span><span class="o">/</span><span class="n">model_graph</span><span class="o">.</span><span class="n">mindir</span><span class="p">](</span><span class="n">storage</span> <span class="n">path</span> <span class="n">of</span> <span class="n">the</span> <span class="n">large</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># create default context, which target is cpu by default.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
<span class="go">target: [&#39;cpu&#39;].</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># testcase 2 about context&#39;s attribute parallel based on server inference package</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># (export MSLITE_ENABLE_SERVER_INFERENCE=on before compile lite or use cloud inference package)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">workers_num</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">config_info</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;model_file&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;mindir_path&quot;</span><span class="p">:</span> <span class="s2">&quot;/home/user/model_graph.mindir&quot;</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">config_path</span> <span class="o">=</span> <span class="s2">&quot;/home/user/config.txt&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="p">)</span>
<span class="go">workers num: 4,</span>
<span class="go">config info: model_file: mindir_path /home/user/model_graph.mindir,</span>
<span class="go">config path: /home/user/config.txt.</span>
</pre></div>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="mindspore_lite.Context.group_info_file">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">group_info_file</span></span><a class="headerlink" href="#mindspore_lite.Context.group_info_file" title="Permalink to this definition"></a></dt>
<dd><p>Get communication group info file for distributed inference.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="mindspore_lite.Context.target">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">target</span></span><a class="headerlink" href="#mindspore_lite.Context.target" title="Permalink to this definition"></a></dt>
<dd><p>Get the target device information of context.</p>
<p>Currently support target: <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code> , <code class="docutils literal notranslate"><span class="pre">&quot;gpu&quot;</span></code> , <code class="docutils literal notranslate"><span class="pre">&quot;ascend&quot;</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>After gpu is added to target, cpu will be added automatically as the backup target.
Because when ops are not supported on gpu, The system will try whether the cpu supports it.
At that time, need to switch to the context with cpu.</p>
<p>After Ascend is added, cpu will be added automatically as the backup target. when the inputs format of the
original model is inconsistent with that of the model generated by Converter, the model generated by
Converter on Ascend device will contain the ‘Transpose’ node, which needs to be executed on the cpu device
currently. So it needs to switch to the context with cpu target.</p>
</div>
<dl class="simple">
<dt>cpu properties:</dt><dd><ul class="simple">
<li><p><strong>inter_op_parallel_num</strong> (int) - Set the parallel number of operators at runtime.
<cite>inter_op_parallel_num</cite> cannot be greater than <cite>thread_num</cite> . Setting <cite>inter_op_parallel_num</cite>
to <code class="docutils literal notranslate"><span class="pre">0</span></code> represents <cite>inter_op_parallel_num</cite> will be automatically adjusted based on computer
performance and core num.</p></li>
<li><p><strong>precision_mode</strong> (str) - Set the mix precision mode. Options are <code class="docutils literal notranslate"><span class="pre">&quot;preferred_fp16&quot;</span></code> ,
<code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp32&quot;</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;preferred_fp16&quot;</span></code> : prefer to use fp16.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp32&quot;</span></code> : force use fp32.</p></li>
</ul>
</li>
<li><p><strong>thread_num</strong> (int) - Set the number of threads at runtime. <cite>thread_num</cite> cannot be less than
<cite>inter_op_parallel_num</cite> . Setting <cite>thread_num</cite> to 0 represents <cite>thread_num</cite> will be automatically
adjusted based on computer performance and core numbers.</p></li>
<li><p><strong>thread_affinity_mode</strong> (int) - Set the mode of the CPU core binding policy at runtime. The
following <cite>thread_affinity_mode</cite> are supported.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">0</span></code> : no binding core.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">1</span></code> : binding big cores first.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">2</span></code> : binding middle cores first.</p></li>
</ul>
</li>
<li><p><strong>thread_affinity_core_list</strong> (list[int]) - Set the list of CPU core binding policies at runtime.
For example, [0,1] represents the specified binding of CPU0 and CPU1.</p></li>
</ul>
</dd>
<dt>gpu properties:</dt><dd><ul class="simple">
<li><p><strong>device_id</strong> (int) - The device id.</p></li>
<li><p><strong>group_size</strong> (int) - the number of the clusters. Get only, not settable.</p></li>
<li><p><strong>precision_mode</strong> (str) - Set the mix precision mode. Options are <code class="docutils literal notranslate"><span class="pre">&quot;preferred_fp16&quot;</span></code> ,
<code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp32&quot;</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;preferred_fp16&quot;</span></code>: prefer to use fp16.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp32&quot;</span></code>: force use fp32.</p></li>
</ul>
</li>
<li><p><strong>rank_id</strong> (int) - the ID of the current device in the cluster, which starts from 0. Get only,
not settable.</p></li>
</ul>
</dd>
<dt>ascend properties:</dt><dd><ul class="simple">
<li><p><strong>device_id</strong> (int) - The device id.</p></li>
<li><p><strong>precision_mode</strong> (str) - Set the mix precision mode. Options are <code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp32&quot;</span></code> ,
<code class="docutils literal notranslate"><span class="pre">&quot;preferred_fp32&quot;</span></code> , <code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp16&quot;</span></code> , <code class="docutils literal notranslate"><span class="pre">&quot;enforce_origin&quot;</span></code> , <code class="docutils literal notranslate"><span class="pre">&quot;preferred_optimal&quot;</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp32&quot;</span></code>: ACL option is force_fp32, force use fp32.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;preferred_fp32&quot;</span></code>: ACL option is allow_fp32_to_fp16, prefer to use fp32.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp16&quot;</span></code>: ACL option is force_fp16, force use fp16.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;enforce_origin&quot;</span></code>: ACL option is must_keep_origin_dtype, force use original type.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;preferred_optimal&quot;</span></code>: ACL option is allow_mix_precision, prefer to use fp16+ mix precision mode.</p></li>
</ul>
</li>
<li><p><strong>provider</strong> (str) - The provider that supports the inference capability of the target device,
can be <code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;ge&quot;</span></code>. The default is <code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code>.</p></li>
<li><p><strong>rank_id</strong> (int) - The ID of the current device in the cluster, which starts from <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list[str], the target device information of context.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># create default context, which target is cpu by default.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># set context with cpu target.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="go">[&#39;cpu&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">precision_mode</span> <span class="o">=</span> <span class="s2">&quot;preferred_fp16&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_num</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">inter_op_parallel_num</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_affinity_mode</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_affinity_core_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">device_type: DeviceType.kCPU,</span>
<span class="go">precision_mode: preferred_fp16,</span>
<span class="go">thread_num: 2,</span>
<span class="go">inter_op_parallel_num: 2,</span>
<span class="go">thread_affinity_mode: 1,</span>
<span class="go">thread_affinity_core_list: [0, 1].</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># set context with gpu target.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gpu&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="go">[&#39;gpu&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">precision_mode</span> <span class="o">=</span> <span class="s2">&quot;preferred_fp16&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">rank_id</span><span class="p">)</span>
<span class="go">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">group_size</span><span class="p">)</span>
<span class="go">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">gpu</span><span class="p">)</span>
<span class="go">device_type: DeviceType.kGPU,</span>
<span class="go">precision_mode: preferred_fp16,</span>
<span class="go">device_id: 2,</span>
<span class="go">rank_id: 0,</span>
<span class="go">group_size: 1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># set context with ascend target.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ascend&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="go">[&#39;ascend&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">precision_mode</span> <span class="o">=</span> <span class="s2">&quot;enforce_fp32&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">provider</span> <span class="o">=</span> <span class="s2">&quot;ge&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">rank_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="p">)</span>
<span class="go">device_type: DeviceType.kAscend,</span>
<span class="go">precision_mode: enforce_fp32,</span>
<span class="go">device_id: 2,</span>
<span class="go">provider: ge,</span>
<span class="go">rank_id: 0.</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../mindspore_lite.html" class="btn btn-neutral float-left" title="mindspore_lite" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore_lite.FmkType.html" class="btn btn-neutral float-right" title="mindspore_lite.FmkType" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>