<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mindspore_lite.ModelParallelRunner &mdash; MindSpore master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script><script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="mindspore_lite.ModelType" href="mindspore_lite.ModelType.html" />
    <link rel="prev" title="mindspore_lite.ModelGroupFlag" href="mindspore_lite.ModelGroupFlag.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_api.html">mindspore::api</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_api_utils.html">mindspore::api::utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_converter.html">mindspore::converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset.html">mindspore::dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_config.html">mindspore::dataset::config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_text.html">mindspore::dataset::text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_transforms.html">mindspore::dataset::transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_dataset_vision.html">mindspore::dataset::vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_kernel.html">mindspore::kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_ops.html">mindspore::ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_registry.html">mindspore::registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_cpp/lite_cpp_example.html">Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">JAVA API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_java/class_list.html">Class List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/model_parallel_runner.html">ModelParallelRunner</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/mscontext.html">MSContext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/mstensor.html">MSTensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/runner_config.html">RunnerConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/graph.html">Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_java/lite_java_example.html">Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../mindspore_lite.html">mindspore_lite</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../mindspore_lite.html#context">Context</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore_lite.html#converter">Converter</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../mindspore_lite.html#model">Model</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="mindspore_lite.Model.html">mindspore_lite.Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore_lite.ModelGroup.html">mindspore_lite.ModelGroup</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore_lite.ModelGroupFlag.html">mindspore_lite.ModelGroupFlag</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">mindspore_lite.ModelParallelRunner</a></li>
<li class="toctree-l3"><a class="reference internal" href="mindspore_lite.ModelType.html">mindspore_lite.ModelType</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mindspore_lite.html#tensor">Tensor</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">C API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_c/context_c.html">context_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/data_type_c.html">data_type_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/format_c.html">format_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/model_c.html">model_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/tensor_c.html">tensor_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/types_c.html">types_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_c/lite_c_example.html">Example</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../mindspore_lite.html">mindspore_lite</a> &raquo;</li>
      <li>mindspore_lite.ModelParallelRunner</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/mindspore_lite/mindspore_lite.ModelParallelRunner.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mindspore-lite-modelparallelrunner">
<h1>mindspore_lite.ModelParallelRunner<a class="headerlink" href="#mindspore-lite-modelparallelrunner" title="Permalink to this headline"></a></h1>
<a class="reference external image-reference" href="https://gitee.com/mindspore/mindspore/blob/r2.3/mindspore/lite/python/api/model.py"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a>
<dl class="py class">
<dt class="sig sig-object py" id="mindspore_lite.ModelParallelRunner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mindspore_lite.</span></span><span class="sig-name descname"><span class="pre">ModelParallelRunner</span></span><a class="reference internal" href="../_modules/mindspore_lite/model.html#ModelParallelRunner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_lite.ModelParallelRunner" title="Permalink to this definition"></a></dt>
<dd><p>The <cite>ModelParallelRunner</cite> class defines a MindSpore Lite’s Runner, which support model parallelism. Compared with
<cite>model</cite> , <cite>model</cite> does not support parallelism, but <cite>ModelParallelRunner</cite> supports parallelism. A Runner contains
multiple workers, which are the units that actually perform parallel inferring. The primary use case is when
multiple clients send inference tasks to the server, the server perform parallel inference, shorten the inference
time, and then return the inference results to the clients.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use case: serving inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 1: Building MindSpore Lite serving package by export MSLITE_ENABLE_SERVER_INFERENCE=on.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 2: install wheel package of MindSpore Lite built by precondition 1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelParallelRunner</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model_parallel_runner</span><span class="p">)</span>
<span class="go">model_path: .</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="mindspore_lite.ModelParallelRunner.build_from_file">
<span class="sig-name descname"><span class="pre">build_from_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_lite/model.html#ModelParallelRunner.build_from_file"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_lite.ModelParallelRunner.build_from_file" title="Permalink to this definition"></a></dt>
<dd><p>build a model parallel runner from model path so that it can run on a device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_path</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a>) – Define the model path.</p></li>
<li><p><strong>context</strong> (<a class="reference internal" href="mindspore_lite.Context.html#mindspore_lite.Context" title="mindspore_lite.Context"><em>Context</em></a><em>, </em><em>optional</em>) – Define the config used to transfer context and options during building model.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>. <code class="docutils literal notranslate"><span class="pre">None</span></code> means the Context with cpu target. Context has the default parallel
attribute.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>model_path</cite> is not a str.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>context</cite> is neither a Context nor <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – <cite>model_path</cite> does not exist.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – ModelParallelRunner’s init failed.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use case: serving inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 1: Building MindSpore Lite serving package by export MSLITE_ENABLE_SERVER_INFERENCE=on.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 2: install wheel package of MindSpore Lite built by precondition 1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">workers_num</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelParallelRunner</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;mobilenetv2.mindir&quot;</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">model_parallel_runner</span><span class="p">)</span>
<span class="go">model_path: mobilenetv2.mindir.</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_lite.ModelParallelRunner.get_inputs">
<span class="sig-name descname"><span class="pre">get_inputs</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_lite/model.html#ModelParallelRunner.get_inputs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_lite.ModelParallelRunner.get_inputs" title="Permalink to this definition"></a></dt>
<dd><p>Obtains all input Tensors of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list[Tensor], the input Tensor list of the model.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use case: serving inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 1: Building MindSpore Lite serving package by export MSLITE_ENABLE_SERVER_INFERENCE=on.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># precondition 2: install wheel package of MindSpore Lite built by precondition 1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">workers_num</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelParallelRunner</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;mobilenetv2.mindir&quot;</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">inputs</span> <span class="o">=</span> <span class="n">model_parallel_runner</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mindspore_lite.ModelParallelRunner.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/mindspore_lite/model.html#ModelParallelRunner.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mindspore_lite.ModelParallelRunner.predict" title="Permalink to this definition"></a></dt>
<dd><p>Inference ModelParallelRunner.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore_lite.Tensor.html#mindspore_lite.Tensor" title="mindspore_lite.Tensor"><em>Tensor</em></a><em>]</em>) – A list that includes all input Tensors in order.</p></li>
<li><p><strong>outputs</strong> (<a class="reference external" href="https://docs.python.org/library/stdtypes.html#list" title="(in Python v3.8)"><em>list</em></a><em>[</em><a class="reference internal" href="mindspore_lite.Tensor.html#mindspore_lite.Tensor" title="mindspore_lite.Tensor"><em>Tensor</em></a><em>]</em><em>, </em><em>optional</em>) – A list that includes all output Tensors in order,
this tensor include output data buffer.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list[Tensor], outputs, the model outputs are filled in the container in sequence.</p>
</dd>
<dt class="field-odd">Raises</dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>inputs</cite> is not a list.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#TypeError" title="(in Python v3.8)"><strong>TypeError</strong></a> – <cite>inputs</cite> is a list, but the elements are not Tensor.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/library/exceptions.html#RuntimeError" title="(in Python v3.8)"><strong>RuntimeError</strong></a> – predict model failed.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use case: serving inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Precondition 1: Download MindSpore Lite serving package or building MindSpore Lite serving package by</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1">#                 export MSLITE_ENABLE_SERVER_INFERENCE=on.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Precondition 2: Install wheel package of MindSpore Lite built by precondition 1.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The result can be find in the tutorial of runtime_parallel_python.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">time</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the number of threads of one worker.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># WORKERS_NUM * THREAD_NUM should not exceed the number of cores of the machine.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">THREAD_NUM</span> <span class="o">=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># In parallel inference, the number of workers in one `ModelParallelRunner` in server.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># If you prepare to compare the time difference between parallel inference and serial inference,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># you can set WORKERS_NUM = 1 as serial inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">WORKERS_NUM</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Simulate 5 clients, and each client sends 2 inference tasks to the server at the same time.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">PARALLEL_NUM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">TASK_NUM</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">parallel_runner_predict</span><span class="p">(</span><span class="n">parallel_runner</span><span class="p">,</span> <span class="n">parallel_id</span><span class="p">):</span>
<span class="gp">... </span>    <span class="c1"># One Runner with 3 workers, set model input, execute inference and get output.</span>
<span class="gp">... </span>    <span class="n">task_index</span> <span class="o">=</span> <span class="mi">0</span>
<span class="gp">... </span>    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="gp">... </span>        <span class="k">if</span> <span class="n">task_index</span> <span class="o">==</span> <span class="n">TASK_NUM</span><span class="p">:</span>
<span class="gp">... </span>            <span class="k">break</span>
<span class="gp">... </span>        <span class="n">task_index</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">... </span>        <span class="c1"># Set model input</span>
<span class="gp">... </span>        <span class="n">inputs</span> <span class="o">=</span> <span class="n">parallel_runner</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()</span>
<span class="gp">... </span>        <span class="n">in_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="s2">&quot;input.bin&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">once_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># Execute inference</span>
<span class="gp">... </span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">parallel_runner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="gp">... </span>        <span class="n">once_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="gp">... </span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;parallel id: &quot;</span><span class="p">,</span> <span class="n">parallel_id</span><span class="p">,</span> <span class="s2">&quot; | task index: &quot;</span><span class="p">,</span> <span class="n">task_index</span><span class="p">,</span> <span class="s2">&quot; | run once time: &quot;</span><span class="p">,</span>
<span class="gp">... </span>              <span class="n">once_end_time</span> <span class="o">-</span> <span class="n">once_start_time</span><span class="p">,</span> <span class="s2">&quot; s&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="c1"># Get output</span>
<span class="gp">... </span>        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
<span class="gp">... </span>            <span class="n">tensor_name</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>
<span class="gp">... </span>            <span class="n">data_size</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">data_size</span>
<span class="gp">... </span>            <span class="n">element_num</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">element_num</span>
<span class="gp">... </span>            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor name is:</span><span class="si">%s</span><span class="s2"> tensor size is:</span><span class="si">%s</span><span class="s2"> tensor elements num is:</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tensor_name</span><span class="p">,</span>
<span class="gp">... </span>                                                                                     <span class="n">data_size</span><span class="p">,</span>
<span class="gp">... </span>                                                                                     <span class="n">element_num</span><span class="p">))</span>
<span class="gp">...</span>
<span class="gp">... </span>            <span class="n">data</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get_data_to_numpy</span><span class="p">()</span>
<span class="gp">... </span>            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="gp">... </span>            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output data is:&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="gp">... </span>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
<span class="gp">... </span>                <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
<span class="gp">... </span>            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Init RunnerConfig and context, and add CPU device info</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">enable_fp16</span> <span class="o">=</span> <span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_num</span> <span class="o">=</span> <span class="n">THREAD_NUM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">inter_op_parallel_num</span> <span class="o">=</span> <span class="n">THREAD_NUM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">workers_num</span> <span class="o">=</span> <span class="n">WORKERS_NUM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Build ModelParallelRunner from file</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelParallelRunner</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_parallel_runner</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;mobilenetv2.mindir&quot;</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># The server creates 5 threads to store the inference tasks of 5 clients.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">total_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">PARALLEL_NUM</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">parallel_runner_predict</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">model_parallel_runner</span><span class="p">,</span> <span class="n">i</span><span class="p">,)))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Start threads to perform parallel inference.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">th</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">th</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">th</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">th</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">total_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;total run time: &quot;</span><span class="p">,</span> <span class="n">total_end_time</span> <span class="o">-</span> <span class="n">total_start_time</span><span class="p">,</span> <span class="s2">&quot; s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mindspore_lite.ModelGroupFlag.html" class="btn btn-neutral float-left" title="mindspore_lite.ModelGroupFlag" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="mindspore_lite.ModelType.html" class="btn btn-neutral float-right" title="mindspore_lite.ModelType" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>