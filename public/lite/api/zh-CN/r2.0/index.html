<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MindSpore Lite API &mdash; MindSpore Lite master 文档</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="_static/collapsible-lists/css/tree_view.css" type="text/css" /><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script><script src="_static/jquery.js"></script>
        <script src="_static/js/theme.js"></script><script src="_static/underscore.js"></script><script src="_static/doctools.js"></script><script src="_static/translations.js"></script><script src="_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script><script src="_static/collapsible-lists/js/apply-collapsible-lists.js"></script><script crossorigin="anonymous" integrity="sha256-1fEPhSsRKlFKGfK3eO710tEweHh1fwokU5wFGDHO+vg=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
    <link rel="next" title="mindspore" href="api_cpp/mindspore.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_NN.html">mindspore::NN</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_api.html">mindspore::api</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_api_utils.html">mindspore::api::utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_converter.html">mindspore::converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset.html">mindspore::dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset_transforms.html">mindspore::dataset::transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_kernel.html">mindspore::kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_ops.html">mindspore::ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_registry.html">mindspore::registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/lite_cpp_example.html">样例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">JAVA API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_java/class_list.html">类列表</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/model_parallel_runner.html">ModelParallelRunner</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/mscontext.html">MSContext</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/mstensor.html">MSTensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/runner_config.html">RunnerConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/graph.html">Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/lite_java_example.html">样例</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindspore_lite.html">mindspore_lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">C API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_c/context_c.html">context_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/data_type_c.html">data_type_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/format_c.html">format_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/model_c.html">model_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/tensor_c.html">tensor_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/types_c.html">types_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/lite_c_example.html">样例</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>MindSpore Lite API</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mindspore-lite-api">
<h1>MindSpore Lite API<a class="headerlink" href="#mindspore-lite-api" title="永久链接至标题"></a></h1>
<section id="mindspore-lite-api-支持情况汇总">
<h2>MindSpore Lite API 支持情况汇总<a class="headerlink" href="#mindspore-lite-api-支持情况汇总" title="永久链接至标题"></a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 3%" />
<col style="width: 15%" />
<col style="width: 31%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>类名</p></th>
<th class="head"><p>接口说明</p></th>
<th class="head"><p>C++ 接口</p></th>
<th class="head"><p>Python 接口</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Context</p></td>
<td><p>设置运行时的线程数</p></td>
<td><p>void SetThreadNum(int32_t thread_num)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_num</a></p></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>获取当前线程数设置</p></td>
<td><p>int32_t GetThreadNum() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_num</a></p></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>设置运行时的算子并行推理数目</p></td>
<td><p>void SetInterOpParallelNum(int32_t parallel_num)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.inter_op_parallel_num</a></p></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>获取当前算子并行数设置</p></td>
<td><p>int32_t GetInterOpParallelNum() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.inter_op_parallel_num</a></p></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>设置运行时的CPU绑核策略</p></td>
<td><p>void SetThreadAffinity(int mode)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_affinity_mode</a></p></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>获取当前CPU绑核策略</p></td>
<td><p>int GetThreadAffinityMode() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_affinity_mode</a></p></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>设置运行时的CPU绑核列表</p></td>
<td><p>void SetThreadAffinity(const std::vector&lt;int&gt; &amp;core_list)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_affinity_core_list</a></p></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>获取当前CPU绑核列表</p></td>
<td><p>std::vector&lt;int32_t&gt; GetThreadAffinityCoreList() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.thread_affinity_core_list</a></p></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>设置运行时是否支持并行</p></td>
<td><p>void SetEnableParallel(bool is_parallel)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>获取当前是否支持并行</p></td>
<td><p>bool GetEnableParallel() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>设置内置Delegate模式，以使用第三方AI框架辅助推理</p></td>
<td><p>void SetBuiltInDelegate(DelegateMode mode)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>获取当前内置Delegate模式</p></td>
<td><p>DelegateMode GetBuiltInDelegate() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>设置Delegate，Delegate定义了用于支持第三方AI框架接入的代理</p></td>
<td><p>set_delegate(const std::shared_ptr&lt;AbstractDelegate&gt; &amp;delegate)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>获取当前Delegate</p></td>
<td><p>std::shared_ptr&lt;AbstractDelegate&gt; get_delegate() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>在多设备中，配置量化模型是否以浮点模式运行</p></td>
<td><p>void SetMultiModalHW(bool float_mode)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Context</p></td>
<td><p>获取当前配置中，量化模型的运行模式</p></td>
<td><p>bool GetMultiModalHW() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Context</p></td>
<td><p>修改该context下的DeviceInfoContext数组</p></td>
<td><p>std::vector&lt;std::shared_ptr&lt;DeviceInfoContext&gt;&gt; &amp;MutableDeviceInfo()</p></td>
<td><p>封装在 <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.target</a></p></td>
</tr>
<tr class="row-odd"><td><p>DeviceInfoContext</p></td>
<td><p>获取该DeviceInfoContext的类型</p></td>
<td><p>enum DeviceType GetDeviceType() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>DeviceInfoContext</p></td>
<td><p>将DeviceInfoContext转换为T类型的指针</p></td>
<td><p>std::shared_ptr&lt;T&gt; Cast()</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>DeviceInfoContext</p></td>
<td><p>设置设备生产商名</p></td>
<td><p>void SetProvider(const std::string &amp;provider)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>DeviceInfoContext</p></td>
<td><p>获取设备的生产商名</p></td>
<td><p>std::string GetProvider() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>DeviceInfoContext</p></td>
<td><p>设置生产商设备名</p></td>
<td><p>void SetProviderDevice(const std::string &amp;device)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>DeviceInfoContext</p></td>
<td><p>获取生产商设备名</p></td>
<td><p>std::string GetProviderDevice() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>DeviceInfoContext</p></td>
<td><p>设置内存管理器</p></td>
<td><p>void SetAllocator(const std::shared_ptr&lt;Allocator&gt; &amp;allocator)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>DeviceInfoContext</p></td>
<td><p>获取内存管理器</p></td>
<td><p>std::shared_ptr&lt;Allocator&gt; GetAllocator() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>CPUDeviceInfo</p></td>
<td><p>获取该DeviceInfoContext的类型</p></td>
<td><p>enum DeviceType GetDeviceType() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">context.cpu</a></p></td>
</tr>
<tr class="row-even"><td><p>CPUDeviceInfo</p></td>
<td><p>设置是否以FP16精度进行推理</p></td>
<td><p>void SetEnableFP16(bool is_fp16)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.precision_mode</a></p></td>
</tr>
<tr class="row-odd"><td><p>CPUDeviceInfo</p></td>
<td><p>获取当前是否以FP16精度进行推理</p></td>
<td><p>bool GetEnableFP16() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.cpu.precision_mode</a></p></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>获取该DeviceInfoContext的类型</p></td>
<td><p>enum DeviceType GetDeviceType() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu</a></p></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>设置设备ID</p></td>
<td><p>void SetDeviceID(uint32_t device_id)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.device_id</a></p></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>获取设备ID</p></td>
<td><p>uint32_t GetDeviceID() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.device_id</a></p></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>获取当前运行的RANK ID</p></td>
<td><p>int GetRankID() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.rank_id</a></p></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>获取当前运行的GROUP SIZE</p></td>
<td><p>int GetGroupSize() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.group_size</a></p></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>设置推理时算子精度</p></td>
<td><p>void SetPrecisionMode(const std::string &amp;precision_mode)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>获取推理时算子精度</p></td>
<td><p>std::string GetPrecisionMode() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>设置是否以FP16精度进行推理</p></td>
<td><p>void SetEnableFP16(bool is_fp16)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.precision_mode</a></p></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>获取是否以FP16精度进行推理</p></td>
<td><p>bool GetEnableFP16() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.gpu.precision_mode</a></p></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>设置是否绑定OpenGL纹理数据</p></td>
<td><p>void SetEnableGLTexture(bool is_enable_gl_texture)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>获取是否绑定OpenGL纹理数据</p></td>
<td><p>bool GetEnableGLTexture() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>设置指定OpenGL EGLContext</p></td>
<td><p>void SetGLContext(void *gl_context)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>获取当前OpenGL EGLContext</p></td>
<td><p>void *GetGLContext() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>GPUDeviceInfo</p></td>
<td><p>设置指定OpenGL EGLDisplay</p></td>
<td><p>void SetGLDisplay(void *gl_display)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>GPUDeviceInfo</p></td>
<td><p>获取当前OpenGL EGLDisplay</p></td>
<td><p>void *GetGLDisplay() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取该DeviceInfoContext的类型</p></td>
<td><p>enum DeviceType GetDeviceType() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.ascend</a></p></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置设备ID</p></td>
<td><p>void SetDeviceID(uint32_t device_id)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.ascend.device_id</a></p></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取设备ID</p></td>
<td><p>uint32_t GetDeviceID() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.ascend.device_id</a></p></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置AIPP配置文件路径</p></td>
<td><p>void SetInsertOpConfigPath(const std::string &amp;cfg_path)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取AIPP配置文件路径</p></td>
<td><p>std::string GetInsertOpConfigPath() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置模型输入format</p></td>
<td><p>void SetInputFormat(const std::string &amp;format)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取模型输入format</p></td>
<td><p>std::string GetInputFormat() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置模型输入shape</p></td>
<td><p>void SetInputShape(const std::string &amp;shape)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取模型输入shape</p></td>
<td><p>std::string GetInputShape() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置模型输入shape</p></td>
<td><p>void SetInputShapeMap(const std::map&lt;int, std::vector &lt;int&gt;&gt; &amp;shape)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取模型输入shape</p></td>
<td><p>std::map&lt;int, std::vector &lt;int&gt;&gt; GetInputShapeMap() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置模型动态batch的挡位，支持个数范围[2, 100]</p></td>
<td><p>void SetDynamicBatchSize(const std::vector&lt;size_t&gt; &amp;dynamic_batch_size)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取已配置模型的动态batch</p></td>
<td><p>std::string GetDynamicBatchSize() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置模型动态分辨率档位</p></td>
<td><p>void SetDynamicImageSize(const std::string &amp;dynamic_image_size)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取已配置模型的动态分辨率</p></td>
<td><p>std::string GetDynamicImageSize() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置模型输出type</p></td>
<td><p>void SetOutputType(enum DataType output_type)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取模型输出type</p></td>
<td><p>enum DataType GetOutputType() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置模型精度模式</p></td>
<td><p>void SetPrecisionMode(const std::string &amp;precision_mode)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.ascend.precision_mode</a></p></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取模型精度模式</p></td>
<td><p>std::string GetPrecisionMode() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.ascend.precision_mode</a></p></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置算子实现方式</p></td>
<td><p>void SetOpSelectImplMode(const std::string &amp;op_select_impl_mode)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取已配置的算子选择模式</p></td>
<td><p>std::string GetOpSelectImplMode() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置融合开关配置文件，可指定关闭特定融合规则</p></td>
<td><p>void SetFusionSwitchConfigPath(const std::string &amp;cfg_path)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取已配置的融合开关文件路径</p></td>
<td><p>std::string GetFusionSwitchConfigPath() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>AscendDeviceInfo</p></td>
<td><p>设置缓存优化模式</p></td>
<td><p>void SetBufferOptimizeMode(const std::string &amp;buffer_optimize_mode)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>AscendDeviceInfo</p></td>
<td><p>获取缓存优化模式</p></td>
<td><p>std::string GetBufferOptimizeMode() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>KirinNPUDeviceInfo</p></td>
<td><p>获取该DeviceInfoContext的类型</p></td>
<td><p>enum DeviceType GetDeviceType() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>KirinNPUDeviceInfo</p></td>
<td><p>设置是否以FP16精度进行推理</p></td>
<td><p>void SetEnableFP16(bool is_fp16)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>KirinNPUDeviceInfo</p></td>
<td><p>获取是否以FP16精度进行推理</p></td>
<td><p>bool GetEnableFP16() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>KirinNPUDeviceInfo</p></td>
<td><p>设置NPU频率</p></td>
<td><p>void SetFrequency(int frequency)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>KirinNPUDeviceInfo</p></td>
<td><p>获取NPU频率</p></td>
<td><p>int GetFrequency() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>从内存缓冲区加载模型，并将模型编译至可在Device上运行的状态</p></td>
<td><p>Status Build(const void *model_data, size_t data_size, ModelType model_type, const std::shared_ptr &lt;Context&gt; &amp;model_context = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>从内存缓冲区加载模型，并将模型编译至可在Device上运行的状态</p></td>
<td><p>Status Build(const std::string &amp;model_path, ModelType model_type, const std::shared_ptr &lt;Context&gt; &amp;model_context = nullptr)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.build_from_file">Model.build_from_file</a></p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>根据路径读取加载模型，并将模型编译至可在Device上运行的状态</p></td>
<td><p>Status Build(const void *model_data, size_t data_size, ModelType model_type, const std::shared_ptr &lt;Context&gt; &amp;model_context, const Key &amp;dec_key, const std::string &amp;dec_mode, const std::string &amp;cropto_lib_path)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>根据路径读取加载模型，并将模型编译至可在Device上运行的状态</p></td>
<td><p>Status Build(const std::string &amp;model_path, ModelType model_type, const std::shared_ptr &lt;Context&gt; &amp;model_context, const Key &amp;dec_key, const std::string &amp;dec_mode, const std::string &amp;cropto_lib_path)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>将GraphCell存储的模型编译至可在Device上运行的状态</p></td>
<td><p>Status Build(GraphCell graph, const std::shared_ptr &lt;Context&gt; &amp;model_context = nullptr, const std::shared_ptr &lt;TrainCfg&gt; &amp;train_cfg = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>将GraphCell存储的模型编译至可在Device上运行的状态</p></td>
<td><p>Status Build(GraphCell graph, Node *optimizer, std::vector&lt;Expr *&gt; inputs, const std::shared_ptr &lt;Context&gt; &amp;model_context, const std::shared_ptr &lt;TrainCfg&gt; &amp;train_cfg)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>构建一个迁移学习模型，其中主干权重是固定的，头部权重是可训练的</p></td>
<td><p>Status BuildTransferLearning(GraphCell backbone, GraphCell head, const std::shared_ptr &lt;Context&gt; &amp;context, const std::shared_ptr &lt;TrainCfg&gt; &amp;train_cfg = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>调整已编译模型的输入张量形状</p></td>
<td><p>Status Resize(const std::vector &lt;MSTensor&gt; &amp;inputs, const std::vector &lt;std::vector&lt;int64_t&gt;&gt; &amp;dims)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.resize">Model.resize</a></p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>更新模型的权重Tensor的大小和内容</p></td>
<td><p>Status UpdateWeights(const std::vector &lt;MSTensor&gt; &amp;new_weights)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>推理模型</p></td>
<td><p>Status Predict(const std::vector &lt;MSTensor&gt; &amp;inputs, std::vector &lt;MSTensor&gt; *outputs, const MSKernelCallBack &amp;before = nullptr, const MSKernelCallBack &amp;after = nullptr)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.predict)(notsupportcallback">Model.predict</a></p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>仅带callback的推理模型</p></td>
<td><p>Status Predict(const MSKernelCallBack &amp;before = nullptr, const MSKernelCallBack &amp;after = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>单步训练模型</p></td>
<td><p>Status RunStep(const MSKernelCallBack &amp;before = nullptr, const MSKernelCallBack &amp;after = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>进行推理模型，并在推理前进行数据预处理</p></td>
<td><p>Status PredictWithPreprocess(const std::vector &lt;std::vector&lt;MSTensor&gt;&gt; &amp;inputs, std::vector &lt;MSTensor&gt; *outputs, const MSKernelCallBack &amp;before = nullptr, const MSKernelCallBack &amp;after = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>若模型配置了数据预处理，对模型输入数据进行数据预处理</p></td>
<td><p>Status Preprocess(const std::vector &lt;std::vector&lt;MSTensor&gt;&gt; &amp;inputs, std::vector &lt;MSTensor&gt; *outputs)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>检查模型是否配置了数据预处理</p></td>
<td><p>bool HasPreprocess()</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>根据路径读取配置文件</p></td>
<td><p>Status LoadConfig(const std::string &amp;config_path)</p></td>
<td><p>封装在 <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.build_from_file">Model.build_from_file</a> 方法的 <cite>config_path</cite> 参数中</p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>刷新配置</p></td>
<td><p>Status UpdateConfig(const std::string &amp;section, const std::pair&lt;std::string, std::string&gt; &amp;config)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>获取模型所有输入张量</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetInputs()</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.get_inputs">Model.get_inputs</a></p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>获取模型指定名字的输入张量</p></td>
<td><p>MSTensor GetInputByTensorName(const std::string &amp;tensor_name)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>获取所有Tensor的梯度</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetGradients() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>应用所有Tensor的梯度</p></td>
<td><p>Status ApplyGradients(const std::vector &lt;MSTensor&gt; &amp;gradients)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>获取模型的所有权重Tensors</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetFeatureMaps() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>获取optimizer中所有参与权重更新的MSTensor</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetTrainableParams() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>更新模型的权重Tensor内容</p></td>
<td><p>Status UpdateFeatureMaps(const std::vector &lt;MSTensor&gt; &amp;new_weights)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>获取optimizer参数MSTensor</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetOptimizerParams() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>更新optimizer参数</p></td>
<td><p>Status SetOptimizerParams(const std::vector &lt;MSTensor&gt; &amp;params)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>设置虚拟batch用于训练</p></td>
<td><p>Status SetupVirtualBatch(int virtual_batch_multiplier, float lr = -1.0f, float momentum = -1.0f)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>设置训练学习率</p></td>
<td><p>Status SetLearningRate(float learning_rate)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>获取优化器学习率</p></td>
<td><p>float GetLearningRate()</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>训练指标参数初始化</p></td>
<td><p>Status InitMetrics(std::vector&lt;Metrics *&gt; metrics)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>获取训练指标参数</p></td>
<td><p>std::vector&lt;Metrics *&gt; GetMetrics()</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>获取模型所有输出张量</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetOutputs()</p></td>
<td><p>封装在 <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.predict)(notsupportcallback">Model.predict</a> 的返回值</p></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>获取模型所有输出张量的名字</p></td>
<td><p>std::vector &lt;std::string&gt; GetOutputTensorNames()</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>获取模型指定名字的输出张量</p></td>
<td><p>MSTensor GetOutputByTensorName(const std::string &amp;tensor_name)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>通过节点名获取模型的MSTensors输出张量</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetOutputsByNodeName(const std::string &amp;node_name)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>将OpenGL纹理数据与模型的输入和输出进行绑定</p></td>
<td><p>Status BindGLTexture2DMemory(const std::map&lt;std::string, unsigned int&gt; &amp;inputGLTexture, std::map&lt;std::string, unsigned int&gt; *outputGLTexture)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>session设置训练模式</p></td>
<td><p>Status SetTrainMode(bool train)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>获取session是否是训练模式</p></td>
<td><p>bool GetTrainMode() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>模型训练</p></td>
<td><p>Status Train(int epochs, std::shared_ptr &lt;dataset::Dataset&gt;ds, std::vector&lt;TrainCallBack *&gt; cbs)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Model</p></td>
<td><p>模型验证</p></td>
<td><p>Status Evaluate(std::shared_ptr &lt;dataset::Dataset&gt; ds, std::vector&lt;TrainCallBack *&gt; cbs)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Model</p></td>
<td><p>检查设备是否支持该模型</p></td>
<td><p>static bool CheckModelSupport(enum DeviceType device_type, ModelType model_type)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>RunnerConfig</p></td>
<td><p>设置RunnerConfig的worker的个数</p></td>
<td><p>void SetWorkersNum(int32_t workers_num)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.workers_num</a></p></td>
</tr>
<tr class="row-odd"><td><p>RunnerConfig</p></td>
<td><p>获取RunnerConfig的worker的个数</p></td>
<td><p>int32_t GetWorkersNum() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.workers_num</a></p></td>
</tr>
<tr class="row-even"><td><p>RunnerConfig</p></td>
<td><p>设置RunnerConfig的context参数</p></td>
<td><p>void SetContext(const std::shared_ptr &lt;Context&gt; &amp;context)</p></td>
<td><p>封装在 <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel</a></p></td>
</tr>
<tr class="row-odd"><td><p>RunnerConfig</p></td>
<td><p>获取RunnerConfig配置的上下文参数</p></td>
<td><p>std::shared_ptr &lt;Context&gt; GetContext() const</p></td>
<td><p>封装在 <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel</a></p></td>
</tr>
<tr class="row-even"><td><p>RunnerConfig</p></td>
<td><p>设置RunnerConfig的配置参数</p></td>
<td><p>void SetConfigInfo(const std::string &amp;section, const std::map&lt;std::string, std::string&gt; &amp;config)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.config_info</a></p></td>
</tr>
<tr class="row-odd"><td><p>RunnerConfig</p></td>
<td><p>获取RunnerConfig配置参数信息</p></td>
<td><p>std::map&lt;std::string, std::map&lt;std::string, std::string&gt;&gt; GetConfigInfo() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.config_info</a></p></td>
</tr>
<tr class="row-even"><td><p>RunnerConfig</p></td>
<td><p>设置RunnerConfig中的配置文件路径</p></td>
<td><p>void SetConfigPath(const std::string &amp;config_path)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.config_path</a></p></td>
</tr>
<tr class="row-odd"><td><p>RunnerConfig</p></td>
<td><p>获取RunnerConfig中的配置文件的路径</p></td>
<td><p>std::string GetConfigPath() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel.config_path</a></p></td>
</tr>
<tr class="row-even"><td><p>ModelParallelRunner</p></td>
<td><p>根据路径读取加载模型，生成一个或者多个模型，并将所有模型编译至可在Device上运行的状态</p></td>
<td><p>Status Init(const std::string &amp;model_path, const std::shared_ptr &lt;RunnerConfig&gt; &amp;runner_config = nullptr)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.build_from_file">Model.parallel_runner.build_from_file</a></p></td>
</tr>
<tr class="row-odd"><td><p>ModelParallelRunner</p></td>
<td><p>根据模文件数据，生成一个或者多个模型，并将所有模型编译至可在Device上运行的状态</p></td>
<td><p>Status Init(const void *model_data, const size_t data_size, const std::shared_ptr &lt;RunnerConfig&gt; &amp;runner_config = nullptr)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>ModelParallelRunner</p></td>
<td><p>获取模型所有输入张量</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetInputs()</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.get_inputs">Model.parallel_runner.get_inputs</a></p></td>
</tr>
<tr class="row-odd"><td><p>ModelParallelRunner</p></td>
<td><p>获取模型所有输出张量</p></td>
<td><p>std::vector &lt;MSTensor&gt; GetOutputs()</p></td>
<td><p>封装在 <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.predict">Model.parallel_runner.predict</a> 的返回值</p></td>
</tr>
<tr class="row-even"><td><p>ModelParallelRunner</p></td>
<td><p>并发推理模型</p></td>
<td><p>Status Predict(const std::vector &lt;MSTensor&gt; &amp;inputs, std::vector &lt;MSTensor&gt; *outputs,const MSKernelCallBack &amp;before = nullptr, const MSKernelCallBack &amp;after = nullptr)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.predict">Model.parallel_runner.predict</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>创建一个MSTensor对象，其数据需复制后才能由Model访问</p></td>
<td><p>static inline MSTensor *CreateTensor(const std::string &amp;name, DataType type, const std::vector&lt;int64_t&gt; &amp;shape, const void *data, size_t data_len) noexcept</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor">Tensor</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>创建一个MSTensor对象，其数据可以直接由Model访问</p></td>
<td><p>static inline MSTensor *CreateRefTensor(const std::string &amp;name, DataType type, const std::vector&lt;int64_t&gt; &amp;shape, const void *data, size_t data_len, bool own_data = true) noexcept</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>创建一个MSTensor对象，其device数据可以直接由Model访问</p></td>
<td><p>static inline MSTensor CreateDeviceTensor(const std::string &amp;name, DataType type, const std::vector&lt;int64_t&gt; &amp;shape, void *data, size_t data_len) noexcept</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>创建一个MSTensor对象，其数据由文件路径file所指定</p></td>
<td><p>static inline MSTensor *CreateTensorFromFile(const std::string &amp;file, DataType type = DataType::kNumberTypeUInt8, const std::vector&lt;int64_t&gt; &amp;shape = {}) noexcept</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>创建一个字符串类型的MSTensor对象，其数据需复制后才能由Model访问</p></td>
<td><p>static inline MSTensor *StringsToTensor(const std::string &amp;name, const std::vectorstd::string &amp;str)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>将字符串类型的MSTensor对象解析为字符串</p></td>
<td><p>static inline std::vectorstd::string TensorToStrings(const MSTensor &amp;tensor)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>销毁一个由 <cite>Clone</cite> 、 <cite>StringsToTensor</cite> 、 <cite>CreateRefTensor</cite> 或 <cite>CreateTensor</cite> 所创建的对象</p></td>
<td><p>static void DestroyTensorPtr(MSTensor *tensor) noexcept</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>获取MSTensor的名字</p></td>
<td><p>std::string Name() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.name">Tensor.name</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>获取MSTensor的数据类型</p></td>
<td><p>enum DataType DataType() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.dtype">Tensor.dtype</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>获取MSTensor的Shape</p></td>
<td><p>const std::vector&lt;int64_t&gt; &amp;Shape() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.shape">Tensor.shape</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>获取MSTensor的元素个数</p></td>
<td><p>int64_t ElementNum() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.element_num">Tensor.element_num</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>获取指向MSTensor中的数据拷贝的智能指针</p></td>
<td><p>std::shared_ptr &lt;const void&gt; Data() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>获取MSTensor中的数据的指针</p></td>
<td><p>void *MutableData()</p></td>
<td><p>封装在 <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.get_data_to_numpy">Tensor.get_data_to_numpy</a> 和 <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.set_data_from_numpy">Tensor.set_data_from_numpy</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>获取MSTensor中的数据的以字节为单位的内存长度</p></td>
<td><p>size_t DataSize() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.data_size">Tensor.data_size</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>判断MSTensor中的数据是否是常量数据</p></td>
<td><p>bool IsConst() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>判断MSTensor中是否在设备上</p></td>
<td><p>bool IsDevice() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>获取指向深拷贝副本的指针</p></td>
<td><p>MSTensor *Clone() const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>判断MSTensor是否合法</p></td>
<td><p>bool operator==(std::nullptr_t) const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>判断MSTensor是否非法</p></td>
<td><p>bool operator!=(std::nullptr_t) const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>判断MSTensor是否与另一个MSTensor相等</p></td>
<td><p>bool operator==(const MSTensor &amp;tensor) const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>判断MSTensor是否与另一个MSTensor不相等</p></td>
<td><p>bool operator!=(const MSTensor &amp;tensor) const</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>设置MSTensor的Shape</p></td>
<td><p>void SetShape(const std::vector&lt;int64_t&gt; &amp;shape)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.shape">Tensor.shape</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>设置MSTensor的DataType</p></td>
<td><p>void SetDataType(enum DataType data_type)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.dtype">Tensor.dtype</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>设置MSTensor的名字</p></td>
<td><p>void SetTensorName(const std::string &amp;name)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.name">Tensor.name</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>设置MSTensor数据所属的内存池</p></td>
<td><p>void SetAllocator(std::shared_ptr &lt;Allocator&gt; allocator)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>获取MSTensor数据所属的内存池</p></td>
<td><p>std::shared_ptr &lt;Allocator&gt; allocator() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>设置MSTensor数据的format</p></td>
<td><p>void SetFormat(mindspore::Format format)</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.format">Tensor.format</a></p></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>获取MSTensor数据的format</p></td>
<td><p>mindspore::Format format() const</p></td>
<td><p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor.format">Tensor.format</a></p></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>设置指向MSTensor数据的指针</p></td>
<td><p>void SetData(void *data, bool own_data = true)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>设置MSTensor数据的设备地址</p></td>
<td><p>void SetDeviceData(void *data)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>获取由SetDeviceData接口设置的MSTensor数据的设备地址</p></td>
<td><p>void *GetDeviceData()</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>MSTensor</p></td>
<td><p>获取MSTensor的量化参数</p></td>
<td><p>std::vector &lt;QuantParam&gt; QuantParams() const</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>MSTensor</p></td>
<td><p>设置MSTensor的量化参数</p></td>
<td><p>void SetQuantParams(std::vector &lt;QuantParam&gt; quant_params)</p></td>
<td></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="mindspore-lite-api-1">
<h1>MindSpore Lite API<a class="headerlink" href="#mindspore-lite-api-1" title="永久链接至标题"></a></h1>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore.html">mindspore</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_NN.html">mindspore::NN</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_api.html">mindspore::api</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_api_utils.html">mindspore::api::utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_converter.html">mindspore::converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset.html">mindspore::dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_dataset_transforms.html">mindspore::dataset::transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_kernel.html">mindspore::kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_ops.html">mindspore::ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_registry.html">mindspore::registry</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_cpp/lite_cpp_example.html">样例</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">JAVA API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_java/class_list.html">类列表</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/model.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/model_parallel_runner.html">ModelParallelRunner</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/mscontext.html">MSContext</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/mstensor.html">MSTensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/runner_config.html">RunnerConfig</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/graph.html">Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_java/lite_java_example.html">样例</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mindspore_lite.html">mindspore_lite</a></li>
</ul>
</div>
<div class="toctree-wrapper compound">
<p class="caption" role="heading"><span class="caption-text">C API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_c/context_c.html">context_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/data_type_c.html">data_type_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/format_c.html">format_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/model_c.html">model_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/tensor_c.html">tensor_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/types_c.html">types_c</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_c/lite_c_example.html">样例</a></li>
</ul>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="api_cpp/mindspore.html" class="btn btn-neutral float-right" title="mindspore" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 MindSpore.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用了 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>