<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quick Start to Cloud-side Inference &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script src="../_static/js/theme.js"></script>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Device-side Inference Sample" href="../device_infer_example.html" />
    <link rel="prev" title="Quick Start to Device-side Inference" href="one_hour_introduction.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick Start to Cloud-side Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#preparation">Preparation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#environment-variables">Environment Variables</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mindspore-lite-environment-variables">MindSpore Lite Environment Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ascend-hardware-backend-environment-variables">Ascend Hardware Backend Environment Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nvidia-gpu-hardware-backend-environment-variables">Nvidia GPU Hardware Backend Environment Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="#setting-host-side-logging-level">Setting Host-side Logging Level</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#integration-inference">Integration Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#configuring-cmake">Configuring CMake</a></li>
<li class="toctree-l3"><a class="reference internal" href="#writing-code">Writing Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiling">Compiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-inference-program">Running the Inference Program</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Quick Start to Cloud-side Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quick_start/one_hour_introduction_cloud.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quick-start-to-cloud-side-inference">
<h1>Quick Start to Cloud-side Inference<a class="headerlink" href="#quick-start-to-cloud-side-inference" title="Permalink to this headline">ÔÉÅ</a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_en/quick_start/one_hour_introduction_cloud.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>This article introduces you to the basic functions and usage of MindSpore Lite by using MindSpore Lite to perform cloud-side inference as an example.</p>
<p>MindSpore Lite cloud-side inference is supported to run in Linux environment deployment only. Ascend 310/310P/910, Nvidia GPU and CPU hardware backends are supported.</p>
<p>Before starting using MindSpore Lite in this chapter, users should have a Linux (e.g. Ubuntu/CentOS/EulerOS) environment ready to operate the verification.</p>
<p>To experience the MindSpore Lite device-side inference process, please refer to the document <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/quick_start/one_hour_introduction.html">Quick Start to Device-Side Inference</a>.</p>
<p>We will demonstrate how to use MindSpore Lite distributions for integrated development and write your own inference programs, taking MindSpore Lite C++ interface for integration as an example. For detailed usage of MindSpore Lite C++ interface, users can refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/runtime_cpp.html">Cloud-Side inference with C++ Interface</a>.</p>
<p>In addition, users can use Python interface and Java interface of MindSpore Lite for integration. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/runtime_python.html">Cloud-side inference by using Python interface</a> and <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/runtime_java.html">Cloud-side inference by using Java interface</a>.</p>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline">ÔÉÅ</a></h2>
<ol class="arabic">
<li><p>Environment requirements</p>
<ul class="simple">
<li><p>System environment: Linux x86_64, Ubuntu 18.04.02LTS recommended</p></li>
</ul>
</li>
<li><p>Download distributions</p>
<p>Users can download the MindSpore Lite cloud-side inference package <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{</span> <span class="pre">version}-linux-{arch}.tar.gz</span></code> on the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/downloads.html">download page</a> of MindSpore official website, <code class="docutils literal notranslate"><span class="pre">{arch}</span></code> for <code class="docutils literal notranslate"><span class="pre">x64</span></code> or <code class="docutils literal notranslate"><span class="pre">aarch64</span></code>. <code class="docutils literal notranslate"><span class="pre">x64</span></code> version supports Ascend, Nvidia GPU, CPU three hardware backends, <code class="docutils literal notranslate"><span class="pre">aarch64</span></code> only supports Ascend and CPU hardware backends.</p>
<p>The following is the contents of the <code class="docutils literal notranslate"><span class="pre">x64</span></code> tar package.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
‚îú‚îÄ‚îÄ runtime
‚îÇ   ‚îú‚îÄ‚îÄ include                          # API header files for MindSpore Lite integrated development
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ lib
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libascend_ge_plugin.so       # Ascend Hardware Backend Remote Mode Plugin
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libascend_kernel_plugin.so   # Ascend Hardware Backend Plugin
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libdvpp_utils.so             # Ascend Hardware Backend DVPP Plugin
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libminddata-lite.a           # Image processing static library
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libminddata-lite.so          # Image processing dynamic library
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libmindspore_core.so         # Dynamic library for MindSpore Lite inference framework
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libmindspore_glog.so.0       # MindSpore Lite Logging Dynamic Library
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libmindspore-lite-jni.so     # JNI dynamic library for MindSpore Lite inference framework
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libmindspore-lite.so         # Dynamic library for MindSpore Lite inference framework
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libmsplugin-ge-litert.so     # CPU Hardware Backend Plugin
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libruntime_convert_plugin.so # Online Converter Plugin
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libtensorrt_plugin.so        # Nvidia GPU Hardware Backend Plugin
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ libtransformer-shared.so     # Transformer Dynamic Library
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ mindspore-lite-java.jar      # MindSpore Lite inference framework jar package
‚îÇ   ‚îî‚îÄ‚îÄ third_party
‚îî‚îÄ‚îÄ tools
    ‚îú‚îÄ‚îÄ benchmark       # Benchmark Test Tools Catalogue
    ‚îî‚îÄ‚îÄ converter       # Model Converter Catalogue
</pre></div>
</div>
</li>
<li><p>Obtain model</p>
<p>MindSpore Lite cloud-side inference currently only supports MindIR model format of MindSpore. You can export MindIR model by MindSpore or get MindIR model by <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/converter_tool.html">model converter</a> to convert models in Tensorflow, Onnx, Caffe.</p>
<p>The model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.mindir">mobilenetv2.mindir</a> can be downloaded as a sample model.</p>
</li>
<li><p>Obtain sample</p>
<p>The sample code of this section is put in the directory <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/master/mindspore/lite/examples/cloud_infer/quick_start_cpp">mindspore/lite/examples/cloud_infer/quick_start_cpp</a>.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>quick_start_cpp
‚îú‚îÄ‚îÄ CMakeLists.txt
‚îú‚îÄ‚îÄ main.cc
‚îú‚îÄ‚îÄ build                           # Temporary build directory
‚îî‚îÄ‚îÄ model
    ‚îî‚îÄ‚îÄ mobilenetv2.mindir          # Model files
</pre></div>
</div>
</li>
</ol>
</section>
<section id="environment-variables">
<h2>Environment Variables<a class="headerlink" href="#environment-variables" title="Permalink to this headline">ÔÉÅ</a></h2>
<p><strong>To ensure that the script will work properly, environment variables need to be set before building and executing the inference.</strong></p>
<section id="mindspore-lite-environment-variables">
<h3>MindSpore Lite Environment Variables<a class="headerlink" href="#mindspore-lite-environment-variables" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>After unzipping the MindSpore Lite cloud-side inference package, set the <code class="docutils literal notranslate"><span class="pre">LITE_HOME</span></code> environment variable to the path of the unzipping, e.g.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LITE_HOME</span><span class="o">=</span><span class="nv">$some_path</span>/mindpsore-lite-2.0.0-linux-x64
</pre></div>
</div>
<p>Set the environment variable <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LITE_HOME</span>/runtime/lib:<span class="nv">$LITE_HOME</span>/runtime/third_party/dnnl:<span class="nv">$LITE_HOME</span>/tools/converter/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
<p>If you need to use the <code class="docutils literal notranslate"><span class="pre">convert_lite</span></code> or <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tools, you need to set the environment variable <code class="docutils literal notranslate"><span class="pre">PATH</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$LITE_HOME</span>/tools/converter/converter:<span class="nv">$LITE_HOME</span>/tools/benchmark:<span class="nv">$PATH</span>
</pre></div>
</div>
</section>
<section id="ascend-hardware-backend-environment-variables">
<h3>Ascend Hardware Backend Environment Variables<a class="headerlink" href="#ascend-hardware-backend-environment-variables" title="Permalink to this headline">ÔÉÅ</a></h3>
<ol class="arabic">
<li><p>Verify the run package installation path</p>
<p>If you use the root user to complete the run package installation, the default path is ‚Äò/usr/local/Ascend‚Äô, and the default installation path for non-root users is ‚Äò/home/HwHiAiUser/Ascend‚Äô.</p>
<p>Taking the path of the root user as an example, set the environment variables as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_HOME</span><span class="o">=</span>/usr/local/Ascend<span class="w">  </span><span class="c1"># the root directory of run package</span>
</pre></div>
</div>
</li>
<li><p>Distinguish run package versions</p>
<p>The run package is divided into 2 versions, distinguished by whether the ‚Äòascend-toolkit‚Äô folder is set in the installation directory.</p>
<p>If the ‚Äòascend-toolkit‚Äô folder exists, set the environment variables as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_HOME</span><span class="o">=</span>/usr/local/Ascend
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/compiler/bin:<span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/compiler/ccec_compiler/bin/:<span class="si">${</span><span class="nv">PATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/driver/lib64:<span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/lib64:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/opp
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_AICPU_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/compiler/python/site-packages:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TOOLCHAIN_HOME</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/ascend-toolkit/latest/toolkit
</pre></div>
</div>
<p>If not exist, set the environment variables as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_HOME</span><span class="o">=</span>/usr/local/Ascend
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/compiler/bin:<span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/compiler/ccec_compiler/bin:<span class="si">${</span><span class="nv">PATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/driver/lib64:<span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/lib64:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/opp
<span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_AICPU_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/compiler/python/site-packages:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">TOOLCHAIN_HOME</span><span class="o">=</span><span class="si">${</span><span class="nv">ASCEND_HOME</span><span class="si">}</span>/latest/toolkit
</pre></div>
</div>
</li>
</ol>
</section>
<section id="nvidia-gpu-hardware-backend-environment-variables">
<h3>Nvidia GPU Hardware Backend Environment Variables<a class="headerlink" href="#nvidia-gpu-hardware-backend-environment-variables" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>When the hardware backend is an Nvidia GPU, inference relies on cuda and TensorRT, and users need to install cuda and TensorRT first.</p>
<p>The following is an example of cuda11.1 and TensorRT8.5.1.7. Users need to set the environment variables according to the actual installation path.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_HOME</span><span class="o">=</span>/usr/local/cuda-11.1
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$CUDA_HOME</span>/bin:<span class="nv">$PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$CUDA_HOME</span>/lib64:<span class="nv">$LD_LIBRARY_PATH</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">TENSORRT_PATH</span><span class="o">=</span>/usr/local/TensorRT-8.5.1.7
<span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$TENSORRT_PATH</span>/bin:<span class="nv">$PATH</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$TENSORRT_PATH</span>/lib:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
</section>
<section id="setting-host-side-logging-level">
<h3>Setting Host-side Logging Level<a class="headerlink" href="#setting-host-side-logging-level" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The Host logging level defaults to <code class="docutils literal notranslate"><span class="pre">WARNING</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">GLOG_v</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="c1"># 0-DEBUG, 1-INFO, 2-WARNING, 3-ERROR, 4-CRITICAL, default level is WARNING.</span>
</pre></div>
</div>
</section>
</section>
<section id="integration-inference">
<h2>Integration Inference<a class="headerlink" href="#integration-inference" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>We will demonstrate how to use MindSpore Lite distributions for integrated development and write your own inference programs, using MindSpore Lite C++ interface for integration as an example.</p>
<p>Before integration, users can also directly use the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/benchmark_tool.html">benchmark tool (benchmark)</a> distributed with the distribution to perform inference tests.</p>
<section id="configuring-cmake">
<h3>Configuring CMake<a class="headerlink" href="#configuring-cmake" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Users need to integrate the <code class="docutils literal notranslate"><span class="pre">mindspore-lite</span></code> library file inside the distribution and perform model inference through the API interface declared in the MindSpore Lite header file.</p>
<p>The following is sample code when integrating the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> dynamic library via CMake. The environment variable <code class="docutils literal notranslate"><span class="pre">LITE_HOME</span></code> is read to get the unpacked header and library file directories of MindSpore Lite tar package.</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.14</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">QuickStartCpp</span><span class="p">)</span>

<span class="nb">if</span><span class="p">(</span><span class="s">CMAKE_CXX_COMPILER_ID</span><span class="w"> </span><span class="s">STREQUAL</span><span class="w"> </span><span class="s2">&quot;GNU&quot;</span><span class="w"> </span><span class="s">AND</span><span class="w"> </span><span class="s">CMAKE_CXX_COMPILER_VERSION</span><span class="w"> </span><span class="s">VERSION_LESS</span><span class="w"> </span><span class="s">7.3.0</span><span class="p">)</span>
<span class="w">    </span><span class="nb">message</span><span class="p">(</span><span class="s">FATAL_ERROR</span><span class="w"> </span><span class="s2">&quot;GCC version ${CMAKE_CXX_COMPILER_VERSION} must not be less than 7.3.0&quot;</span><span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span>

<span class="nb">if</span><span class="p">(</span><span class="s">DEFINED</span><span class="w"> </span><span class="s">ENV{LITE_HOME}</span><span class="p">)</span>
<span class="w">    </span><span class="nb">set</span><span class="p">(</span><span class="s">LITE_HOME</span><span class="w"> </span><span class="o">$ENV{</span><span class="nv">LITE_HOME</span><span class="o">}</span><span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span>

<span class="c"># Add directory to include search path</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">LITE_HOME</span><span class="o">}</span><span class="s">/runtime</span><span class="p">)</span>
<span class="c"># Add directory to linker search path</span>
<span class="nb">link_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">LITE_HOME</span><span class="o">}</span><span class="s">/runtime/lib</span><span class="p">)</span>
<span class="nb">link_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">LITE_HOME</span><span class="o">}</span><span class="s">/tools/converter/lib</span><span class="p">)</span>

<span class="nb">file</span><span class="p">(</span><span class="s">GLOB_RECURSE</span><span class="w"> </span><span class="s">QUICK_START_CXX</span><span class="w"> </span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="s">/*.cc</span><span class="p">)</span>
<span class="nb">add_executable</span><span class="p">(</span><span class="s">mindspore_quick_start_cpp</span><span class="w"> </span><span class="o">${</span><span class="nv">QUICK_START_CXX</span><span class="o">}</span><span class="p">)</span>

<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">mindspore_quick_start_cpp</span><span class="w"> </span><span class="s">mindspore-lite</span><span class="w"> </span><span class="s">pthread</span><span class="w"> </span><span class="s">dl</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="writing-code">
<h3>Writing Code<a class="headerlink" href="#writing-code" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The code in <code class="docutils literal notranslate"><span class="pre">main.cc</span></code> is shown below:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;algorithm&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;random&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;fstream&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cstring&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;memory&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/model.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/context.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/status.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/types.h&quot;</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">Distribution</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">GenerateRandomData</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Distribution</span><span class="w"> </span><span class="n">distribution</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">mt19937</span><span class="w"> </span><span class="n">random_engine</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">elements_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">);</span>
<span class="w">  </span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">generate_n</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">elements_num</span><span class="p">,</span>
<span class="w">                        </span><span class="p">[</span><span class="o">&amp;</span><span class="n">distribution</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">random_engine</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">distribution</span><span class="p">(</span><span class="n">random_engine</span><span class="p">));</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;MallocData for inTensor failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">GenerateRandomData</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">(),</span><span class="w"> </span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">uniform_real_distribution</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">0.1f</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">QuickStart</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">argc</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model file must be provided.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Read model file.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">model_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_path</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model path &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">model_path</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; is invalid.&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Create and init context, add CPU device info</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="w">  </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Input</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// Generate random data as input data.</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Generate Random Input Data failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Model Predict</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Print Output Tensor Data.</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">kNumPrintOfOutData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tensor name is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">Name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; tensor size is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span>
<span class="w">              </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; tensor elements num is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">());</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;output data is:&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">kNumPrintOfOutData</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">QuickStart</span><span class="p">(</span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="n">argv</span><span class="p">);</span><span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
<p>The code function is parsed as follows:</p>
<ol class="arabic">
<li><p>Initialize the Context configuration</p>
<p>Context holds the relevant configurations needed for model inference, including operator preferences, number of threads, automatic concurrency, and other configurations related to the inference processor.
For more details about Context, please refer to <a class="reference external" href="https://mindspore.cn/lite/api/en/master/generate/classmindspore_Context.html">API interface description</a> of Context.
When loading the model in MindSpore Lite, an object of class <code class="docutils literal notranslate"><span class="pre">Context</span></code> must be provided, so in this example, an object <code class="docutils literal notranslate"><span class="pre">context</span></code> of class <code class="docutils literal notranslate"><span class="pre">Context</span></code> is first requested.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
<p>Next, get the device management list of the <code class="docutils literal notranslate"><span class="pre">context</span></code> object through the <code class="docutils literal notranslate"><span class="pre">Context::MutableDeviceInfo</span></code> interface.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
</pre></div>
</div>
<p>In this example, since the CPU is used for inference, an object <code class="docutils literal notranslate"><span class="pre">device_info</span></code> of class <code class="docutils literal notranslate"><span class="pre">CPUDeviceInfo</span></code> needs to be requested.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
<p>Since the default CPU settings are used, there is no need to do any settings for the <code class="docutils literal notranslate"><span class="pre">device_info</span></code> object and it is directly added to the device management list of <code class="docutils literal notranslate"><span class="pre">context</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Load models</p>
<p>First create the object <code class="docutils literal notranslate"><span class="pre">model</span></code> of a <code class="docutils literal notranslate"><span class="pre">Model</span></code> class, and the <code class="docutils literal notranslate"><span class="pre">Model</span></code> class defines the model in MindSpore for computational graph management.
For a detailed description of the <code class="docutils literal notranslate"><span class="pre">Model</span></code> class, please refer to the <a class="reference external" href="https://mindspore.cn/lite/api/en/master/generate/classmindspore_Model.html">API documentation</a>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
</pre></div>
</div>
<p>Then call the <code class="docutils literal notranslate"><span class="pre">Build</span></code> interface to pass in the model and compile it to a running state on the device.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Pass in data</p>
<p>Before performing model inference, you need to set the input data for inference.
In this example, all the input tensor of the model is obtained through the <code class="docutils literal notranslate"><span class="pre">Model.GetInputs</span></code> interface. The format of the individual tensor is <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.
For a detailed description of the <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> tensor, please refer to the <a class="reference external" href="https://mindspore.cn/lite/api/en/master/generate/classmindspore_MSTensor.html">API description</a> of <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">MutableData</span></code> interface of the tensor can get the data memory pointer of the tensor, and the <code class="docutils literal notranslate"><span class="pre">DataSize</span></code> interface of the tensor can get the data byte length of the tensor. The data type of the tensor can be obtained through the <code class="docutils literal notranslate"><span class="pre">DataType</span></code> interface of the tensor, and users can do different processing according to the data format of their models.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
</pre></div>
</div>
<p>Next, the data on which we want to perform inference is passed inside the tensor via a data pointer.
In this case we pass in floating point data randomly generated from 0.1 to 1 and the data is evenly distributed.
In practical inference, after reading the actual data such as images or audio, the user needs to perform algorithm-specific pre-processing operations and pass the processed data into the model.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">Distribution</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">GenerateRandomData</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Distribution</span><span class="w"> </span><span class="n">distribution</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">mt19937</span><span class="w"> </span><span class="n">random_engine</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">elements_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">);</span>
<span class="w">  </span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">generate_n</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">elements_num</span><span class="p">,</span>
<span class="w">                        </span><span class="p">[</span><span class="o">&amp;</span><span class="n">distribution</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">random_engine</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">distribution</span><span class="p">(</span><span class="n">random_engine</span><span class="p">));</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;MallocData for inTensor failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">GenerateRandomData</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">(),</span><span class="w"> </span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">uniform_real_distribution</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">0.1f</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Input</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// Generate random data as input data.</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Generate Random Input Data failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Execute inference</p>
<p>First, an array <code class="docutils literal notranslate"><span class="pre">outputs</span></code> is requested to hold the output tensor of the model inference, and then the model inference interface <code class="docutils literal notranslate"><span class="pre">Predict</span></code> is called with the input tensor and output tensor as its parameters.
After a successful inference, the output tensor is stored in <code class="docutils literal notranslate"><span class="pre">outputs</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Obtain inference results</p>
<p>The data pointer to the output tensor is obtained via <code class="docutils literal notranslate"><span class="pre">Data</span></code>.
In this case, it is strongly converted to a floating point pointer, and the user can convert the corresponding type according to the data type of model, or get the data type through the <code class="docutils literal notranslate"><span class="pre">DataType</span></code> interface of the tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">());</span>
</pre></div>
</div>
<p>In this example, the inference output is printed directly.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">kNumPrintOfOutData</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>Release the model object</p>
<p>Model destructions will release model-related resources.</p>
</li>
</ol>
</section>
<section id="compiling">
<h3>Compiling<a class="headerlink" href="#compiling" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Set the environment variables as described in the Environment Variables section. Then compile the program as follows.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>../
make
</pre></div>
</div>
<p>After successful compilation, you can get the <code class="docutils literal notranslate"><span class="pre">quick_start_cpp</span></code> executable in the <code class="docutils literal notranslate"><span class="pre">build</span></code> directory.</p>
</section>
<section id="running-the-inference-program">
<h3>Running the Inference Program<a class="headerlink" href="#running-the-inference-program" title="Permalink to this headline">ÔÉÅ</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./mindspore_quick_start_cpp<span class="w"> </span>../model/mobilenetv2.mindir
</pre></div>
</div>
<p>After execution, the following results will be obtained, printing the name of the output Tensor, the size of the output Tensor, the number of the output Tensor and the first 50 data:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tensor name is:Default/head-MobileNetV2Head/Softmax-op204 tensor size is:4000 tensor elements num is:1000
output data is:5.07155e-05 0.00048712 0.000312549 0.00035624 0.0002022 8.58958e-05 0.000187147 0.000365937 0.000281044 0.000255672 0.00108948 0.00390996 0.00230398 0.00128984 0.00307477 0.00147607 0.00106759 0.000589853 0.000848115 0.00143693 0.000685777 0.00219331 0.00160639 0.00215123 0.000444315 0.000151986 0.000317552 0.00053971 0.00018703 0.000643944 0.000218269 0.000931556 0.000127084 0.000544278 0.000887942 0.000303909 0.000273875 0.00035335 0.00229062 0.000453207 0.0011987 0.000621194 0.000628335 0.000838564 0.000611029 0.000372603 0.00147742 0.000270685 8.29869e-05 0.000116974 0.000876237
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="one_hour_introduction.html" class="btn btn-neutral float-left" title="Quick Start to Device-side Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../device_infer_example.html" class="btn btn-neutral float-right" title="Device-side Inference Sample" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>