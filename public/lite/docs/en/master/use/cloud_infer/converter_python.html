<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Python Interface to Perform Model Conversions &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Ascend Conversion Tool Description" href="converter_tool_ascend.html" />
    <link rel="prev" title="Offline Conversion of Inference Models" href="converter_tool.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="converter.html">Model Converter</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="converter_tool.html">Offline Conversion of Inference Models</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Python Interface to Perform Model Conversions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linux-environment-usage-instructions">Linux Environment Usage Instructions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#directory-structure">Directory Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#description-of-attributes">Description of Attributes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#method-of-convert">Method of convert</a></li>
<li class="toctree-l4"><a class="reference internal" href="#usage-examples">Usage Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="#advanced-usage">Advanced Usage</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="converter_tool_ascend.html">Ascend Conversion Tool Description</a></li>
<li class="toctree-l2"><a class="reference internal" href="converter_tool_graph_kernel.html">Graph Kernel Fusion Configuration Instructions (Beta Feature)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="converter.html">Model Converter</a> &raquo;</li>
      <li>Using Python Interface to Perform Model Conversions</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/use/cloud_infer/converter_python.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="using-python-interface-to-perform-model-conversions">
<h1>Using Python Interface to Perform Model Conversions<a class="headerlink" href="#using-python-interface-to-perform-model-conversions" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_en/use/cloud_infer/converter_python.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite cloud-side inference supports model conversion via Python interface, supporting multiple types of model conversion, and the converted mindir models can be used for inference. The interface contains a variety of personalized parameters to provide a convenient conversion path for users. This tutorial describes how to use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Converter.html">Python interface</a> for model conversion.</p>
<p>The currently supported input model formats are MindSpore, TensorFlow Lite, Caffe, TensorFlow, and ONNX.</p>
<p>When the input model type is MindSpore, since it is already a <code class="docutils literal notranslate"><span class="pre">mindir</span></code> model, two approaches are recommended:</p>
<ol class="arabic simple">
<li><p>Inference is performed directly without offline conversion.</p></li>
<li><p>When using offline conversion, setting <code class="docutils literal notranslate"><span class="pre">optimize</span></code> to <code class="docutils literal notranslate"><span class="pre">general</span></code> in CPU/GPU hardware backend (for general optimization), setting <code class="docutils literal notranslate"><span class="pre">optimize</span></code> to <code class="docutils literal notranslate"><span class="pre">gpu_oriented</span></code> in GPU hardware (for GPU extra optimization based on general optimization), setting <code class="docutils literal notranslate"><span class="pre">optimize</span></code> to <code class="docutils literal notranslate"><span class="pre">ascend_oriented</span></code> in Ascend hardware. The relevant optimization is done in the offline phase to reduce the initialization time of inference execution.</p></li>
</ol>
</section>
<section id="linux-environment-usage-instructions">
<h2>Linux Environment Usage Instructions<a class="headerlink" href="#linux-environment-usage-instructions" title="Permalink to this headline"></a></h2>
<section id="environment-preparation">
<h3>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline"></a></h3>
<p>The following environment preparation is required for model conversion by using MindSpore Lite Python interface for cloud-side inference.</p>
<ul>
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/build.html">Compile</a> or <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/downloads.html">download</a> whl installation package for MindSpore Lite cloud-side inference with Converter component.</p>
<blockquote>
<div><p>Currently, the installation package corresponding to Python 3.7 is available for download. If you need other Python versions, please use the compile function to generate the installation package.</p>
</div></blockquote>
</li>
<li><p>Then use the <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> command to install. After installation, you can use the following command to check if the installation is successful. If no error is reported, the installation is successful.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import mindspore_lite&quot;</span>
</pre></div>
</div>
</li>
<li><p>After installation, you can use the following command to check if MindSpore Lite built-in AKG is successfully installed. If no error is reported, the installation is successful.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import mindspore_lite.akg&quot;</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="directory-structure">
<h3>Directory Structure<a class="headerlink" href="#directory-structure" title="Permalink to this headline"></a></h3>
<p>After successful installation, you can use the <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">show</span> <span class="pre">mindspore_lite</span></code> command to see where the Python module for MindSpore Lite cloud-side inference is installed.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore_lite
├── __pycache__
├── akg                                                         # AKG-related interfaces
├── include
├── lib
|   ├── libakg.so                                               # Dynamic link libraries used by AKG
│   ├── _c_lite_wrapper.cpython-37m-x86_64-linux-gnu.so         # MindSpore Lite cloud-side inference python module encapsulates the dynamic library of the C++ interface framework
│   ├── libmindspore_converter.so                               # Dynamic library for model conversion
│   ├── libmindspore_core.so                                    # MindSpore Core Dynamic Library
│   ├── libmindspore_glog.so.0                                  # Glog dynamic library
│   ├── libmindspore-lite.so                                    # MindSpore Lite Dynamic Library for Cloud-Side inference
│   ├── libmslite_converter_plugin.so                           # Model Conversion Plugin
│   ├── libascend_pass_plugin.so                                # Register for Ascend Backend Graph Optimization Plugin Dynamic Library
│   ├── libmslite_shared_lib.so                                 # Adaptation of the dynamic library in the backend of Ascend
│   ├── libascend_kernel_plugin.so                              # Ascend backend kernel plugin
│   ├── libtensorrt_plugin.so                                   # tensorrt backend kernel plugin
│   ├── libopencv_core.so.4.5                                   # Dynamic library for OpenCV
│   ├── libopencv_imgcodecs.so.4.5                              # Dynamic library for OpenCV
│   └── libopencv_imgproc.so.4.5                                # Dynamic library for OpenCV
├── __init__.py        # Initialization package
├── _checkparam.py     # Check parameter tool
├── context.py         # Code related to context interface
├── converter.py       # Code related to converter interface, conversion portal
├── model.py           # Code related to model, inference portal
├── tensor.py          # Code related to tensor interface
└── version.py         # MindSpore Lite cloud-side inference version number
</pre></div>
</div>
</section>
<section id="description-of-attributes">
<h3>Description of Attributes<a class="headerlink" href="#description-of-attributes" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite cloud-side inference model converter provides various attribute settings that users can choose to use according to their needs.</p>
<p>Detailed descriptions of the parameters and their correspondence to the parameters in <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/converter_tool.html">Offline Conversion of Inference Models</a> are provided below.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Converter attributes</p></th>
<th class="head"><p>Types of attributes</p></th>
<th class="head"><p>Parameters corresponding to the offline conversion of the model</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Value range</p></th>
<th class="head"><p>Remarks</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>decrypt_key</p></td>
<td><p>str</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--decryptKey=&lt;DECRYPTKEY&gt;</span></code></p></td>
<td><p>Set the key used to load the cipher text MindIR. The key is expressed in hexadecimal and is only valid when <code class="docutils literal notranslate"><span class="pre">fmk_type</span></code> is MINDIR.</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>decrypt_mode</p></td>
<td><p>str</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--decryptMode=&lt;DECRYPTMODE&gt;</span></code></p></td>
<td><p>Set the mode to load cipher MindIR, only valid when <code class="docutils literal notranslate"><span class="pre">decrypt_key</span></code> is specified.</p></td>
<td><p>“AES-GCM”, “AES-CBC”</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p>device</p></td>
<td><p>str</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--device=&lt;DEVICE&gt;</span></code></p></td>
<td><p>Set target device when converter model. The use case is when on the Ascend device, if you need to the converted model to have the ability to use Ascend backend to perform inference, you can set the attribute. If it is not set, the converted model will use CPU backend to perform inference by default.</p></td>
<td><p>“Ascend”</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>encrypt_key</p></td>
<td><p>str</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--encryptKey=&lt;ENCRYPTKEY&gt;</span></code></p></td>
<td><p>Set the key used to encrypt the file, in hexadecimal characters. Only supported when <code class="docutils literal notranslate"><span class="pre">decrypt_mode</span></code> is “AES-GCM” and the key length is 16.</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p>enable_encryption</p></td>
<td><p>bool</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--encryption=&lt;ENCRYPTION&gt;</span></code></p></td>
<td><p>Whether to encrypt the model when exporting. Exporting encryption protects model integrity, but increases runtime initialization time.</p></td>
<td><p>True, False</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>infer</p></td>
<td><p>bool</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--infer=&lt;INFER&gt;</span></code></p></td>
<td><p>Whether to perform pre-inference at the completion of the conversion.</p></td>
<td><p>True, False</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p>input_data_type</p></td>
<td><p>DataType</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--inputDataType=&lt;INPUTDATATYPE&gt;</span></code></p></td>
<td><p>Set the data type of the quantized model input Tensor. Only valid if the quantization parameters (<code class="docutils literal notranslate"><span class="pre">scale</span></code> and <code class="docutils literal notranslate"><span class="pre">zero</span> <span class="pre">point</span></code>) of the model input Tensor are available. The default is to keep the same data type as the original model input Tensor.</p></td>
<td><p>DataType.FLOAT32, DataType.INT8, DataType.UINT8, DataType.UNKNOWN</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>input_format</p></td>
<td><p>Format</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--inputDataFormat=&lt;INPUTDATAFORMAT&gt;</span></code></p></td>
<td><p>Set the input format of the exported model, valid only for 4-dimensional inputs.</p></td>
<td><p>Format.NCHW, Format.NHWC</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p>input_shape</p></td>
<td><p>dict{string:list[int]}</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--inputShape=&lt;INPUTSHAPE&gt;</span></code></p></td>
<td><p>Set the dimensions of the model input, and the order of the input dimensions is kept the same as the original model. For example: {“inTensor1”: [1, 32, 32, 32], “inTensor2”: [1, 1, 32, 32]}</p></td>
<td><p>-</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>optimize</p></td>
<td><p>str</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--optimize=&lt;OPTIMIZE&gt;</span></code></p></td>
<td><p>Set the mode of optimization during the offline conversion.</p></td>
<td><p>“none”, “general”, “gpu_oriented”, “ascend_oriented”</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p>output_data_type</p></td>
<td><p>DataType</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--outputDataType=&lt;OUTPUTDATATYPE&gt;</span></code></p></td>
<td><p>Set the data type of the quantized model output Tensor. Only valid if the quantization parameters (<code class="docutils literal notranslate"><span class="pre">scale</span></code> and <code class="docutils literal notranslate"><span class="pre">zero</span> <span class="pre">point</span></code>) of the model output Tensor are available. The default is to keep the same data type as the original model output Tensor.</p></td>
<td><p>DataType.FLOAT32, DataType.INT8, DataType.UINT8, DataType.UNKNOWN</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>save_type</p></td>
<td><p>ModelType</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--saveType=&lt;SAVETYPE&gt;</span></code></p></td>
<td><p>Required</p></td>
<td><p>Set the model type needs to be export.</p></td>
<td><p>ModelType.MINDIR</p></td>
</tr>
<tr class="row-even"><td><p>weight_fp16</p></td>
<td><p>bool</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">--fp16=&lt;FP16&gt;</span></code></p></td>
<td><p>Set whether the weights in Float32 data format need to be stored in Float16 data format during model serialization.</p></td>
<td><p>True, False</p></td>
<td><p>-</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>The encryption and decryption function only takes effect when <code class="docutils literal notranslate"><span class="pre">MSLITE_ENABLE_MODEL_ENCRYPTION=on</span></code> is set at <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/build.html">compile</a> time and only supports Linux x86 platforms. <code class="docutils literal notranslate"><span class="pre">decrypt_key</span></code> and <code class="docutils literal notranslate"><span class="pre">encrypt_key</span></code> are string expressed in hexadecimal. For example, if encrypt_key is set as “30313233343637383939414243444546”, the corresponding hexadecimal expression is ‘(b)0123456789ABCDEF’ . Linux platform users can use the’ xxd ‘tool to convert the key expressed in bytes into hexadecimal expressions.
<code class="docutils literal notranslate"><span class="pre">input_shape</span></code> is a attribute that the user may need to set in the following scenarios:</p>
<ul class="simple">
<li><p>Usage 1: The input of the model to be converted is dynamic shape, and the fixed-shape inference is prepared, then this attribute is set to fixed-shape. After setting, when inference about the model after the Converter, the default input shape is the same as this attribute setting, and no resize operation is needed.</p></li>
<li><p>Usage 2: Regardless of whether the original input of the model to be converted is a dynamic shape, use fixed-shape inference and want the performance of the model to be optimized as much as possible, then set this attribute to fixed-shape. After setting, the model structure will be further optimized, but the converted model may lose the characteristics of the dynamic shape (some operators strongly related to the shape will be fused).
<code class="docutils literal notranslate"><span class="pre">optimize</span></code> is an attribute, it used to set the mode of optimization during the offline conversion. If this attribute is set to “none”, no relevant graph optimization operations will be performed during the offline conversion phase of the model, and the relevant graph optimization operations will be performed during the execution of the inference phase. The advantage of this attribute is that the converted model can be deployed directly to any CPU/GPU/Ascend hardware backend since it is not optimized in a specific way, while the disadvantage is that the initialization time of the model increases during inference execution. If this attribute is set to “general”, general optimization will be performed, such as constant folding and operator fusion (the converted model only supports CPU/GPU hardware backend, not Ascend backend). If this parameter is set to “gpu_oriented”, the general optimization and extra optimization for GPU hardware will be performed (the converted model only supports GPU hardware backend). If this attribute is set to “ascend_oriented”, the optimization for Ascend hardware will be performed (the converted model only supports Ascend hardware backend).</p></li>
</ul>
</div></blockquote>
</section>
<section id="method-of-convert">
<h3>Method of convert<a class="headerlink" href="#method-of-convert" title="Permalink to this headline"></a></h3>
<p>Usage scenario: Convert a third-party model into a MindSpore model. You can call the convert method multiple times to convert multiple models.</p>
<p>Detailed descriptions of the parameters and their correspondence to the parameters in <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/converter_tool.html">Offline Conversion of Inference Models</a> are provided below.</p>
<p>|| Method of convert parameters | Tpyes of parameters  | Parameters corresponding to the offline conversion of the model  |  Required or not   |  Description of parameters  | Value range | Default values |
| ——– | —– | ——– | ——- | —– | — | —- |
| fmk_type | FmkType | <code class="docutils literal notranslate"><span class="pre">--fmk=&lt;FMK&gt;</span></code>  | Required | Input model frame type. | FmkType.TF, FmkType.CAFFE, FmkType.ONNX, FmkType.TFLITE | - |
| model_file | str | <code class="docutils literal notranslate"><span class="pre">--modelFile=&lt;MODELFILE&gt;</span></code> | Required | The path of the input model file for the conversion. | - | - |
| output_file | str | <code class="docutils literal notranslate"><span class="pre">--outputFile=&lt;OUTPUTFILE&gt;</span></code> | Required | The path to the output model at the time of conversion can be automatically generated with the <code class="docutils literal notranslate"><span class="pre">.mindir</span></code> suffix. | - | - |
| weight_file | str | <code class="docutils literal notranslate"><span class="pre">--weightFile=&lt;WEIGHTFILE&gt;</span></code> | Required when converting Caffe models | The path to the input model weights file. | - | “” |
| config_file | str | <code class="docutils literal notranslate"><span class="pre">--configFile=&lt;CONFIGFILE&gt;</span></code> | Not | Converter profile path to configure training post-quantization or offline splitting operators parallel or to disable the operator fusion function and set the plug-in to the so path, etc. | - | “” |</p>
<blockquote>
<div><p>For more information about the <code class="docutils literal notranslate"><span class="pre">fmk_type</span></code> parameter, see <a class="reference external" href="https://mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.FmkType.html">FmkType</a></p>
<p>Example of <code class="docutils literal notranslate"><span class="pre">model_file</span></code>: “/home/user/model.prototxt”. Examples of different types of model suffixes: TF: “model.pb” | CAFFE: “model.prototxt” | ONNX: “model.onnx” | TFLITE: “model.tflite”.</p>
<p>Caffe models are generally divided into two files: <code class="docutils literal notranslate"><span class="pre">*.prototxt</span></code> is model structure, corresponding to the <code class="docutils literal notranslate"><span class="pre">model_file</span></code> parameter, and <code class="docutils literal notranslate"><span class="pre">model.caffemodel</span></code> is model weights, corresponding to the <code class="docutils literal notranslate"><span class="pre">--weightFile</span></code> parameter.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">config_file</span></code> configuration file uses the <code class="docutils literal notranslate"><span class="pre">key</span> <span class="pre">=</span> <span class="pre">value</span></code> approach to define the relevant parameters.</p>
</div></blockquote>
</section>
<section id="usage-examples">
<h3>Usage Examples<a class="headerlink" href="#usage-examples" title="Permalink to this headline"></a></h3>
<p>The following selects common examples to illustrate the use of the conversion command.</p>
<ul>
<li><p>Take the Caffe model LeNet as an example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">()</span>
<span class="n">converter</span><span class="o">.</span><span class="n">save_type</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span>
<span class="n">converter</span><span class="o">.</span><span class="n">optimize</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
<span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">CAFFE</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;lenet.prototxt&quot;</span><span class="p">,</span><span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;lenet&quot;</span><span class="p">,</span> <span class="n">weight_file</span><span class="o">=</span><span class="s2">&quot;lenet.caffemodel&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In this example, because the Caffe model is used, two input files, model structure and model weights, are required. Together with the other two required parameters, fmk type and output path, it can be executed successfully.</p>
<p>The result is shown as:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERT RESULT SUCCESS:0
</pre></div>
</div>
<p>This indicates that the Caffe model has been successfully converted into a MindSpore Lite cloud-side inference model, obtaining the new file <code class="docutils literal notranslate"><span class="pre">lenet.mindir</span></code>.</p>
</li>
<li><p>Take MindSpore, TensorFlow Lite, TensorFlow and ONNX models as examples and execute the conversion command.</p>
<ul>
<li><p>MindSpore model <code class="docutils literal notranslate"><span class="pre">model.mindir</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">()</span>
<span class="n">converter</span><span class="o">.</span><span class="n">save_type</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span>
<span class="n">converter</span><span class="o">.</span><span class="n">optimize</span> <span class="o">=</span> <span class="s2">&quot;general&quot;</span>
<span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;model.mindir&quot;</span><span class="p">,</span><span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>TensorFlow Lite model <code class="docutils literal notranslate"><span class="pre">model.tflite</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">()</span>
<span class="n">converter</span><span class="o">.</span><span class="n">save_type</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span>
<span class="n">converter</span><span class="o">.</span><span class="n">optimize</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
<span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">TFLITE</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;model.tflite&quot;</span><span class="p">,</span><span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>TensorFlow model <code class="docutils literal notranslate"><span class="pre">model.pb</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">()</span>
<span class="n">converter</span><span class="o">.</span><span class="n">save_type</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span>
<span class="n">converter</span><span class="o">.</span><span class="n">optimize</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
<span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">TF</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;model.pb&quot;</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>ONNX model <code class="docutils literal notranslate"><span class="pre">model.onnx</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">()</span>
<span class="n">converter</span><span class="o">.</span><span class="n">save_type</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span>
<span class="n">converter</span><span class="o">.</span><span class="n">optimize</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
<span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">ONNX</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;model.onnx&quot;</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>In all of the above cases, the following conversion success message is displayed and the <code class="docutils literal notranslate"><span class="pre">model.mindir</span></code> target file is obtained at the same time.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERT RESULT SUCCESS:0
</pre></div>
</div>
</li>
</ul>
</section>
<section id="advanced-usage">
<h3>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline"></a></h3>
<section id="online-conversion">
<h4>Online conversion<a class="headerlink" href="#online-conversion" title="Permalink to this headline"></a></h4>
<p>get_config_info method and set_config_info method is used for online conversion. Please refer to the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Converter.html#mindspore_lite.Converter.set_config_info">set_config_info</a> for details.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="converter_tool.html" class="btn btn-neutral float-left" title="Offline Conversion of Inference Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="converter_tool_ascend.html" class="btn btn-neutral float-right" title="Ascend Conversion Tool Description" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>