

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Offline Conversion of Inference Models &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Using Python Interface to Perform Model Conversions" href="converter_python.html" />
    <link rel="prev" title="Model Converter" href="converter.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Compiling Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-Side Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Inference on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud-side Tools</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="converter.html">Model Converter</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Offline Conversion of Inference Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linux-environment-usage-instructions">Linux Environment Usage Instructions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#directory-structure">Directory Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#description-of-parameters">Description of Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="#usage-examples">Usage Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="converter_python.html">Using Python Interface to Perform Model Conversions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="converter.html">Model Converter</a> &raquo;</li>
        
      <li>Offline Conversion of Inference Models</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/use/cloud_infer/converter_tool.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="offline-conversion-of-inference-models">
<h1>Offline Conversion of Inference Models<a class="headerlink" href="#offline-conversion-of-inference-models" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_en/use/cloud_infer/converter_tool.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>MindSpore Lite cloud-side inference provides tools for offline conversion of models, and supports many types of model conversions, and the converted models can be used for inference. Command line parameters include a variety of personalized options to provide users with convenient conversion paths.</p>
<p>The currently supported input formats are MindSpore, TensorFlow Lite, Caffe, TensorFlow, and ONNX.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">mindir</span></code> model converted by the converter supports the converter companion and higher versions of the Runtime inference framework to perform inference.</p>
</div>
<div class="section" id="linux-environment-usage-instructions">
<h2>Linux Environment Usage Instructions<a class="headerlink" href="#linux-environment-usage-instructions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="environment-preparation">
<h3>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline">¶</a></h3>
<p>To use MindSpore Lite cloud-side inference model converter, the following environment preparation is required.</p>
<ul>
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/build.html">Compile</a> or <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/downloads.html">download</a> model converter.</p></li>
<li><p>Add the dynamic link libraries required by the converter to the environment variable LD_LIBRARY_PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} is the path of the compiled or downloaded package after unpacking.</p>
</li>
</ul>
</div>
<div class="section" id="directory-structure">
<h3>Directory Structure<a class="headerlink" href="#directory-structure" title="Permalink to this headline">¶</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
└── tools
    └── converter
        ├── include                            # Custom operators, model parsing, node parsing, conversion optimization registration headers
        ├── converter                          # Model converter
        │   └── converter_lite                 # Executable programs
        └── lib                                # Dynamic libraries that the converter depends on
            ├── libmindspore_glog.so.0         # Glog dynamic libraries
            ├── libascend_pass_plugin.so       # Register for Ascend Backend Graph Optimization Plugin Dynamic Library
            ├── libmslite_shared_lib.so        # Adaptation of the dynamic library in the backend of Ascend
            ├── libmindspore_converter.so      # Dynamic library for model conversion
            ├── libmslite_converter_plugin.so  # Model conversion plugin
            ├── libmindspore_core.so           # MindSpore Core dynamic libraries
            ├── libopencv_core.so.4.5          # Dynamic libraries for OpenCV
            ├── libopencv_imgcodecs.so.4.5     # Dynamic libraries for OpenCV
            └── libopencv_imgproc.so.4.5       # Dynamic libraries for OpenCV
        ├── third_party                        # Third party model proto definition
</pre></div>
</div>
</div>
<div class="section" id="description-of-parameters">
<h3>Description of Parameters<a class="headerlink" href="#description-of-parameters" title="Permalink to this headline">¶</a></h3>
<p>MindSpore Lite cloud-side inference model converter provides various parameter settings that users can choose to use according to their needs. In addition, users can enter <code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">/converter_lite</span> <span class="pre">--help</span></code> for live help.</p>
<p>Detailed parameter descriptions are provided below.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameters</th>
<th>Required or not</th>
<th>Description of parameters</th>
<th>Value range</th>
<th>Default values</th>
<th>Remarks</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--help</code></td>
<td>Not</td>
<td>Print all help information.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--fmk=&lt;FMK&gt;</code></td>
<td>Required</td>
<td>The original format of the input model.</td>
<td>MINDIR、CAFFE、TFLITE、TF、ONNX</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--modelFile=&lt;MODELFILE&gt;</code></td>
<td>Required</td>
<td>The path of the input model.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--outputFile=&lt;OUTPUTFILE&gt;</code></td>
<td>Required</td>
<td>The path to the output model, without the suffix, can be automatically generated with the <code>.mindir</code> suffix.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--weightFile=&lt;WEIGHTFILE&gt;</code></td>
<td>Required when converting Caffe models</td>
<td>The path to the input model weight file.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--configFile=&lt;CONFIGFILE&gt;</code></td>
<td>Not</td>
<td>1. can be used as a post-training quantization profile path; 2. can be used as an extended function profile path.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--inputShape=&lt;INPUTSHAPE&gt;</code></td>
<td>Not</td>
<td>Set the dimensions of the model inputs, and keep the order of the input dimensions the same as the original model. The model structure can be further optimized for some specific models, but the converted model will probably lose the dynamic shape properties. Multiple inputs are split by <code>;</code>, along with double quotes <code>""</code>.</td>
<td>e.g.  "inTensorName_1: 1,32,32,4;inTensorName_2:1,64,64,4;"</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--saveType=&lt;SAVETYPE&gt;</code></td>
<td>Not</td>
<td>Set the exported model as <code>mindir</code> model or <code>ms</code> model.</td>
<td>MINDIR, MINDIR_LITE</td>
<td>MINDIR</td>
<td>This version can only be reasoned with models turned out by setting to MINDIR</td>
</tr>
<tr>
<td><code>--optimize=&lt;OPTIMIZE&gt;</code></td>
<td>Not</td>
<td>Set the optimization accomplished in the process of converting model.</td>
<td>none, general, ascend_oriented</td>
<td>general</td>
<td>-</td>
</tr>
<tr>
<td><code>--decryptKey=&lt;DECRYPTKEY&gt;</code></td>
<td>Not</td>
<td>Set the key used to load the cipher text MindIR. The key is expressed in hexadecimal and is only valid when <code>fmk</code> is MINDIR.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--decryptMode=&lt;DECRYPTMODE&gt;</code></td>
<td>Not</td>
<td>Set the mode to load the cipher MindIR, valid only when decryptKey is specified.</td>
<td>AES-GCM, AES-CBC</td>
<td>AES-GCM</td>
<td>-</td>
</tr>
<tr>
<td><code>--encryptKey=&lt;ENCRYPTKEY&gt;</code></td>
<td>Not</td>
<td>Set the key to export the encryption <code>mindir</code> model. The key is expressed in hexadecimal. Only AES-GCM is supported, and the key length is only 16Byte.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--encryption=&lt;ENCRYPTION&gt;</code></td>
<td>Not</td>
<td>Set whether to encrypt when exporting <code>mindir</code> models. Export encryption protects model integrity, but increases runtime initialization time.</td>
<td>true, false</td>
<td>true</td>
<td>-</td>
</tr>
<tr>
<td><code>--infer=&lt;INFER&gt;</code></td>
<td>Not</td>
<td>Set whether to perform pre-inference when the conversion is completed.</td>
<td>true, false</td>
<td>false</td>
<td>-</td>
</tr>
<tr>
<td><code>--inputDataFormat=&lt;INPUTDATAFORMAT&gt;</code></td>
<td>Not</td>
<td>Set the input format of the exported model, valid only for 4-dimensional inputs.</td>
<td>NHWC, NCHW</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--fp16=&lt;FP16&gt;</code></td>
<td>Not</td>
<td>Set whether the weights in Float32 data format need to be stored in Float16 data format during model serialization.</td>
<td>on, off</td>
<td>off</td>
<td>Not supported at the moment</td>
</tr>
<tr>
<td><code>--inputDataType=&lt;INPUTDATATYPE&gt;</code></td>
<td>Not</td>
<td>Set the data type of the quantized model input tensor. Only if the quantization parameters (scale and zero point) of the model input tensor are available. The default is to keep the same data type as the original model input tensor.</td>
<td>FLOAT32, INT8, UINT8, DEFAULT</td>
<td>DEFAULT</td>
<td>Not supported at the moment</td>
</tr>
<tr>
<td><code>--outputDataType=&lt;OUTPUTDATATYPE&gt;</code></td>
<td>Not</td>
<td>Set the data type of the quantized model output tensor. Only if the quantization parameters (scale and zero point) of the model output tensor are available. The default is to keep the same data type as the original model output tensor.</td>
<td>FLOAT32, INT8, UINT8, DEFAULT</td>
<td>DEFAULT</td>
<td>Not supported at the moment</td>
</tr>
<tr>
<td><code>--device=&lt;DEVICE&gt;</code></td>
<td>Not</td>
<td>Set target device when converter model. The use case is when on the Ascend device, if you need to the converted model to have the ability to use Ascend backend to perform inference, you can set the parameter. If it is not set, the converted model will use CPU backend to perform inference by default.</td>
<td>Ascend, Ascend310, Ascend310P</td>
<td>-</td>
<td>Ascend310 and Ascend310P will be deprecated</td>
</tr>
</tbody>
</table>
<p>Notes:</p>
<ul>
<li><p>The parameter name and the parameter value are connected by an equal sign without any space between them.</p></li>
<li><p>Caffe models are generally divided into two files: <code class="docutils literal notranslate"><span class="pre">*.prototxt</span></code> model structure, corresponding to the <code class="docutils literal notranslate"><span class="pre">--modelFile</span></code> parameter, and <code class="docutils literal notranslate"><span class="pre">*.caffemodel</span></code> model weights, corresponding to the <code class="docutils literal notranslate"><span class="pre">--weightFile</span></code> parameter.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">configFile</span></code> configuration file uses the <code class="docutils literal notranslate"><span class="pre">key=value</span></code> approach to define the relevant parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--optimize</span></code> parameter is used to set the mode of optimization during the offline conversion. If this parameter is set to none, no relevant graph optimization operations will be performed during the offline conversion phase of the model, and the relevant graph optimization operations will be done during the execution of the inference phase. The advantage of this parameter is that the converted model can be deployed directly to any CPU/GPU/Ascend hardware backend since it is not optimized in a specific way, while the disadvantage is that the initialization time of the model increases during inference execution. If this parameter is set to general, general optimization will be performed, such as constant folding and operator fusion (the converted model only supports CPU/GPU hardware backend, not Ascend backend). If this parameter is set to ascend_oriented, the optimization for Ascend hardware will be performed (the converted model only supports Ascend hardware backend).</p></li>
<li><p>The encryption and decryption function only takes effect when <code class="docutils literal notranslate"><span class="pre">MSLITE_ENABLE_MODEL_ENCRYPTION=on</span></code> is set at <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/build.html">compile</a> time and only supports Linux x86 platforms. <code class="docutils literal notranslate"><span class="pre">decrypt_key</span></code> and <code class="docutils literal notranslate"><span class="pre">encrypt_key</span></code> are string expressed in hexadecimal. For example, if encrypt_key is set as “30313233343637383939414243444546”, the corresponding hexadecimal expression is ‘(b)0123456789ABCDEF’ . Linux platform users can use the’ xxd ‘tool to convert the key expressed in bytes into hexadecimal expressions.</p></li>
<li><p>For the MindSpore model, since it is already a <code class="docutils literal notranslate"><span class="pre">mindir</span></code> model, two approaches are suggested:</p>
<p>Inference is performed directly without offline conversion.</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">--optimize</span></code> to general in CPU/GPU hardware backend and setting <code class="docutils literal notranslate"><span class="pre">--optimize</span></code> to ascend_oriented in Ascend hardware when using offline conversion. The relevant optimization is done in the offline phase to reduce the initialization time of inference execution.</p>
</li>
</ul>
</div>
<div class="section" id="usage-examples">
<h3>Usage Examples<a class="headerlink" href="#usage-examples" title="Permalink to this headline">¶</a></h3>
<p>The following selects common examples to illustrate the use of the conversion command.</p>
<ul>
<li><p>Take the Caffe model LeNet as an example and execute the conversion command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>CAFFE<span class="w"> </span>--saveType<span class="o">=</span>MINDIR<span class="w"> </span>--optimize<span class="o">=</span>none<span class="w"> </span>--modelFile<span class="o">=</span>lenet.prototxt<span class="w"> </span>--weightFile<span class="o">=</span>lenet.caffemodel<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
<p>In this example, because the Caffe model is used, two input files, model structure and model weights, are required. Together with the other two required parameters, fmk type and output path, it can be executed successfully.</p>
<p>The result is shown as:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERT RESULT SUCCESS:0
</pre></div>
</div>
<p>This indicates that the Caffe model has been successfully converted into a MindSpore Lite cloud-side inference model, obtaining the new file <code class="docutils literal notranslate"><span class="pre">lenet.mindir</span></code>.</p>
</li>
<li><p>Take MindSpore, TensorFlow Lite, TensorFlow and ONNX models as examples and execute the conversion command.</p>
<ul class="simple">
<li><p>MindSpore model <code class="docutils literal notranslate"><span class="pre">model.mindir</span></code></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--saveType<span class="o">=</span>MINDIR<span class="w"> </span>--optimize<span class="o">=</span>general<span class="w"> </span>--modelFile<span class="o">=</span>model.mindir<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
<ul class="simple">
<li><p>TensorFlow Lite model <code class="docutils literal notranslate"><span class="pre">model.tflite</span></code></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--saveType<span class="o">=</span>MINDIR<span class="w"> </span>--optimize<span class="o">=</span>none<span class="w"> </span>--modelFile<span class="o">=</span>model.tflite<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
<ul class="simple">
<li><p>TensorFlow model <code class="docutils literal notranslate"><span class="pre">model.pb</span></code></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TF<span class="w"> </span>--saveType<span class="o">=</span>MINDIR<span class="w"> </span>--optimize<span class="o">=</span>none<span class="w"> </span>--modelFile<span class="o">=</span>model.pb<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
<ul class="simple">
<li><p>ONNX model <code class="docutils literal notranslate"><span class="pre">model.onnx</span></code></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--saveType<span class="o">=</span>MINDIR<span class="w"> </span>--optimize<span class="o">=</span>none<span class="w"> </span>--modelFile<span class="o">=</span>model.onnx<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
<p>In all of the above cases, the following conversion success message is displayed and the <code class="docutils literal notranslate"><span class="pre">model.mindir</span></code> target file is obtained at the same time.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
</li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="converter_python.html" class="btn btn-neutral float-right" title="Using Python Interface to Perform Model Conversions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="converter.html" class="btn btn-neutral float-left" title="Model Converter" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>