

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Ascend Configuration File Description &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script src="../../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Benchmark Tool" href="benchmark.html" />
    <link rel="prev" title="Using Python Interface to Perform Model Conversions" href="converter_python.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud-side Tools</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="converter.html">Model Converter</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="converter_tool.html">Offline Conversion of Inference Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="converter_python.html">Using Python Interface to Perform Model Conversions</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Ascend Configuration File Description</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuration-file">Configuration File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-shape-configuration">Dynamic Shape Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dynamic-batch-size">Dynamic Batch Size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dynamic-resolution">Dynamic Resolution</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#aoe-auto-tuning">AOE Auto-tuning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="converter.html">Model Converter</a> &raquo;</li>
        
      <li>Ascend Configuration File Description</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/use/cloud_infer/converter_tool_ascend.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="ascend-configuration-file-description">
<h1>Ascend Configuration File Description<a class="headerlink" href="#ascend-configuration-file-description" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_en/use/cloud_infer/converter_tool_ascend.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>This article describes the options for the configuration file when the cloud-side model conversion tool specifies the configFile parameter on the Ascend backend.</p>
</div>
<div class="section" id="configuration-file">
<h2>Configuration File<a class="headerlink" href="#configuration-file" title="Permalink to this headline">¶</a></h2>
<p>Table 1: Configure [ascend_context] parameter</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameters</th>
<th>Attributes</th>
<th>Functions Description</th>
<th>Types</th>
<th>Values Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>input_format</code></td>
<td>Optional</td>
<td>Specify the model input format.</td>
<td>String</td>
<td>Options: <code>"NCHW"</code>, <code>"NHWC"</code>, and <code>"ND"</code></td>
</tr>
<tr>
<td><code>input_shape</code></td>
<td>Optional</td>
<td>Specify the model input Shape. input_name must be the input name in the network model before conversion, in the order of the inputs, separated by <code>;</code>.</td>
<td>String</td>
<td>Such as <code>"input1:[1,64,64,3];input2:[1,256,256,3]"</code></td>
</tr>
<tr>
<td><code>dynamic_dims</code></td>
<td>Optional</td>
<td>Specify the dynamic BatchSize and dynamic resolution parameters.</td>
<td>String</td>
<td>见<a href="#dynamic-shape-configuration">Dynamic shape configuration</a></td>
</tr>
<tr>
<td><code>precision_mode</code></td>
<td>Optional</td>
<td>Configure the model accuracy mode.</td>
<td>String</td>
<td>Options: <code>"enforce_fp32"</code>, <code>"preferred_fp32"</code>, <code>"enforce_fp16"</code>, <code>"enforce_origin"</code> or <code>"preferred_optimal"</code>. Default: <code>"enforce_fp16"</code></td>
</tr>
<tr>
<td><code>op_select_impl_mode</code></td>
<td>Optional</td>
<td>Configure the operator selection mode.</td>
<td>String</td>
<td>Optioans: <code>"high_performance"</code>, and <code>"high_precision"</code>. Default: <code>"high_performance"</code>.</td>
</tr>
<tr>
<td><code>output_type</code></td>
<td>Optional</td>
<td>Specify the data type of network output</td>
<td>String</td>
<td>Options: <code>"FP16"</code>, <code>"FP32"</code>, <code>"UINT8"</code></td>
</tr>
<tr>
<td><code>fusion_switch_config_file_path</code></td>
<td>Optional</td>
<td>Configure the <a href="https://www.hiascend.com/document/detail/en/canncommercial/601/inferapplicationdev/atctool/atctool_0078.html">Fusion Switch Configuration File</a> file path and file name.</td>
<td>String</td>
<td>Specify the configuration file for the fusion switch</td>
</tr>
<tr>
<td><code>insert_op_config_file_path</code></td>
<td>Optional</td>
<td>Model insertion <a href="https://www.hiascend.com/document/detail/en/canncommercial/601/inferapplicationdev/atctool/atctool_0018.html">AIPP</a> operator</td>
<td>String</td>
<td>Path of <a href="https://www.hiascend.com/document/detail/en/canncommercial/601/inferapplicationdev/atctool/atctool_0021.html">AIPP</a> configuration file</td>
</tr>
<tr>
<td><code>aoe_mode</code></td>
<td>Optional</td>
<td><a href="https://www.hiascend.com/document/detail/en/canncommercial/601/devtools/auxiliarydevtool/aoe_16_001.html">AOE</a> auto-tuning mode</td>
<td>String</td>
<td>Options: "subgraph turing", "operator turing" or "subgraph turing, operator turing". Default: Not enabled</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="dynamic-shape-configuration">
<h2>Dynamic Shape Configuration<a class="headerlink" href="#dynamic-shape-configuration" title="Permalink to this headline">¶</a></h2>
<p>In some inference scenarios, such as detecting a target and then executing the target recognition network, the number of targets is not fixed resulting in a variable input BatchSize for the target recognition network. If each inference is computed at the maximum BatchSize or maximum resolution, it will result in wasted computational resources. Therefore, it needs to support dynamic BatchSize and dynamic resolution scenarios during inference. Lite inference on Ascend supports dynamic BatchSize and dynamic resolution scenarios. The dynamic_dims dynamic parameter in [ascend_context] is configured via congFile in the convert phase, and the model <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/runtime_cpp.html#dynamic-shape-input">Resize</a> is used during inference, to change the input shape.</p>
<div class="section" id="dynamic-batch-size">
<h3>Dynamic Batch Size<a class="headerlink" href="#dynamic-batch-size" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Parameter Name</p>
<p>dynamic_dims</p>
</li>
<li><p>Functions</p>
<p>Set the dynamic batch profile parameter for scenarios where the number of images processed at a time is not fixed during inference. This parameter needs to be used in conjunction with input_shape, and the position of -1 in input_shape is the dimension where the dynamic batch is located.</p>
</li>
<li><p>Value</p>
<p>Support up to 100 profiles configuration. Each profile is separated by English comma. The value limit of each profile: [1~2048]. For example, the parameters in the configuration file are configured as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ascend_context</span><span class="p">]</span>
<span class="n">input_shape</span><span class="o">=</span><span class="nb">input</span><span class="p">:[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">dynamic_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
<p>“-1” in input_shape means setting dynamic batch, and the profile can take the value of “1,2”, that is, support profile 0: [1,64,64,3], profile 1: [2,64,64,3].</p>
<p>If more than one input exists, the profiles corresponding to the different inputs needs to be the same and separated by <code class="docutils literal notranslate"><span class="pre">;</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ascend_context</span><span class="p">]</span>
<span class="n">input_shape</span><span class="o">=</span><span class="n">input1</span><span class="p">:[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">3</span><span class="p">];</span><span class="n">input2</span><span class="p">:[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">dynamic_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">];[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>converter</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--configFile<span class="o">=</span>./config.txt<span class="w"> </span>--optimize<span class="o">=</span>ascend_oriented<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>Note: When enabling dynamic BatchSize, you do not need to specify the inputShape parameter, and only need to configure the [ascend_context] dynamic batch size through configFile, that is, the configuration content in the previous section.</p>
</li>
<li><p>Inference</p>
<p>Enable dynamic BatchSize. When the model inference is performed, the input shape can only choose the set value of the profile at the time of the converter. If you want to switch to the input shape corresponding to another profile, use the model <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/runtime_cpp.html#dynamic-shape-input">Resize</a> function.</p>
</li>
<li><p>Precautions</p>
<ol class="simple">
<li><p>If the user performs inference operations and the number of images processed is not fixed at a time, this parameter can be configured to dynamically allocate the number of images processed at a time. For example, if a user needs to process 2, 4, or 8 images each time to perform inference, it can be configured as 2, 4, and 8. Once the profile is requested, memory will be requested based on the actual profile during model inference.<br/></p></li>
<li><p>If the profile value set by the user is too large or the profiles are too many, it may cause model compilation failure, in which case the user is advised to reduce the profiles or turn down the profile value.<br/></p></li>
<li><p>If the profile value set by the user is too large or the profiles are too many, when performing inference in the runtime environment, it is recommended that the swapoff -a command be executed to turn off the swap interval as memory, to avoid that the swap space is continued to be called as memory, resulting in an unusually slow running environment due to the lack of memory.<br/></p></li>
</ol>
</li>
</ul>
</div>
<div class="section" id="dynamic-resolution">
<h3>Dynamic Resolution<a class="headerlink" href="#dynamic-resolution" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Parameter Name</p>
<p>dynamic_dims</p>
</li>
<li><p>Function</p>
<p>Set the dynamic resolution parameter of the input image. For scenarios where the width and height of the image are not fixed each time during inference. This parameter needs to be used in conjunction with input_shape, and the position of -1 in input_shape is the dimension where the dynamic resolution is located.</p>
</li>
<li><p>Value</p>
<p>Support up to 100 profiles configuration. Each profile is separated by English comma, such as “[imagesize1_height,imagesize1_width],[imagesize2_height,imagesize2_width]”. For example, the parameters in the configuration file are configured as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ascend_context</span><span class="p">]</span>
<span class="n">input_format</span><span class="o">=</span><span class="n">NHWC</span>
<span class="n">input_shape</span><span class="o">=</span><span class="nb">input</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">dynamic_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">],[</span><span class="mi">19200</span><span class="p">,</span><span class="mi">960</span><span class="p">]</span>
</pre></div>
</div>
<p>“-1” in input_shape means setting the dynamic resolution, i.e., it supports profile 0: [1,64,64,3] and profile 1: [1,19200,960,3].</p>
</li>
<li><p>converter</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--configFile<span class="o">=</span>./config.txt<span class="w"> </span>--optimize<span class="o">=</span>ascend_oriented<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>Note: When enabling dynamic BatchSize, you do not need to specify the inputShape parameter, and only need to configure the [ascend_context] dynamic resolution through configFile, that is, the configuration content in the previous section.</p>
</li>
<li><p>Inference</p>
<p>By enabling dynamic resolution, when model inference is performed, the input shape can only select the set profile value at the time of the converter. If you want to switch to the input shape corresponding to another profile, use the model <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/runtime_cpp.html#dynamic-shape-input">Resize</a> function.</p>
</li>
<li><p>Precautions</p>
<ol class="simple">
<li><p>If the resolution value set by the user is too large or the profiles are too many, it may cause model compilation failure, in which case the user is advised to reduce the profiles or turn down the profile value.<br/></p></li>
<li><p>If the user sets a dynamic resolution, the size of the dataset images used for the actual inference needs to match the specific resolution used.<br/></p></li>
<li><p>If the resolution value set by the user is too large or the profiles are too many, when performing inference in the runtime environment, it is recommended that the swapoff -a command be executed to turn off the swap interval as memory, to avoid that the swap space is continued to be called as memory, resulting in an unusually slow running environment due to the lack of memory.<br/></p></li>
</ol>
</li>
</ul>
</div>
</div>
<div class="section" id="aoe-auto-tuning">
<h2>AOE Auto-tuning<a class="headerlink" href="#aoe-auto-tuning" title="Permalink to this headline">¶</a></h2>
<p>AOE is a computational graph performance auto-tuning tool built specifically for the Davinci platform. Lite enables AOE ability to integrate the AOE offline executable in the converter phase, to perform performance tuning of the graph, generate a knowledge base, and save the offline model. This function supports subgraph tuning and operator tuning. The function supports subgraph tuning and operator tuning. The specific use process is as follows:</p>
<ol>
<li><p>Configure environment variables</p>
<p><code class="docutils literal notranslate"><span class="pre">${LOCAL_ASCEND}</span></code> is the path where the Ascend package is installed</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LOCAL_ASCEND</span><span class="o">=</span>/usr/local/Ascend
<span class="nb">source</span><span class="w"> </span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/latest/bin/setenv.bash
</pre></div>
</div>
<p>Confirm that the AOE executable program can be found and run in the environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>aoe<span class="w"> </span>-h
</pre></div>
</div>
</li>
<li><p>Specify the knowledge base path</p>
<p>AOE tuning generates an operator knowledge base. The default path:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/Ascend/latest/data/aoe/custom/graph<span class="o">(</span>op<span class="o">)</span>/<span class="si">${</span><span class="nv">soc_version</span><span class="si">}</span>
</pre></div>
</div>
<p>(Optional) You can also customize the knowledge base path with the <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">TUNE_BANK_PATH</span></code> environment variable.</p>
</li>
<li><p>Clear the cache</p>
<p>In order for the model compilation to get the knowledge base generated by AOE, it is best to delete the compilation cache before AOE is enabled to avoid cache reuse. Taking Ascend 310P environment with user as root for example, delete <code class="docutils literal notranslate"><span class="pre">/root/atc_data/kernel_cache/Ascend310P3</span></code> and <code class="docutils literal notranslate"><span class="pre">/root/atc_data/fuzzy_kernel_cache/Ascend310P3</span></code> directories.</p>
</li>
<li><p>Specified options of the configuration file</p>
<p>Specify the AOE tuning mode in the <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code> configuration file of the conversion tool config. In the following example, the subgraph tuning will be executed first, and then the operator tuning.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ascend_context<span class="o">]</span>
<span class="nv">aoe_mode</span><span class="o">=</span><span class="s2">&quot;subgraph tuning, operator tuning&quot;</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>The performance improvements will vary from environment to environment, and the actual latency reduction percentage is not exactly the same as the results shown in the tuning logs.</p></li>
<li><p>AOE tuning generates <code class="docutils literal notranslate"><span class="pre">aoe_workspace</span></code> directory in the current directory where the task is executed, which is used to save the models before and after tuning for performance improvement comparison, as well as the process data and result files necessary for tuning. This directory will occupy additional disk space, e.g., 2~10GB for a 500MB raw model, depending on the model size, operator type structure, input shape size and other factors. Therefore, it is recommended to reserve enough disk space, otherwise it may lead to tuning failure.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">aoe_workspace</span></code> directory needs to be deleted manually to free up disk space.</p></li>
</ul>
</div></blockquote>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="benchmark.html" class="btn btn-neutral float-right" title="Benchmark Tool" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="converter_python.html" class="btn btn-neutral float-left" title="Using Python Interface to Perform Model Conversions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>