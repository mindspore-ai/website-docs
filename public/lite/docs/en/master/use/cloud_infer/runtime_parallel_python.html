

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Using Python Interface to Perform Concurrent Inference &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Model Converter" href="converter.html" />
    <link rel="prev" title="Using Java Interface to Perform Concurrent Inference" href="runtime_parallel_java.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Performing Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="runtime_parallel_cpp.html">Using C++ Interface to Perform Concurrent Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="runtime_parallel_java.html">Using Java Interface to Perform Concurrent Inference</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Python Interface to Perform Concurrent Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparation">Preparation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#creating-configuration-context">Creating configuration context</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#modelparallelrunner-creation-and-compilation">ModelParallelRunner creation and compilation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#setting-concurrent-inference-tasks">Setting Concurrent Inference Tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#performing-concurrent-inference">Performing Concurrent Inference</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime_parallel.html">Performing Concurrent Inference</a> &raquo;</li>
      <li>Using Python Interface to Perform Concurrent Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/use/cloud_infer/runtime_parallel_python.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section class="tex2jax_ignore mathjax_ignore" id="using-python-interface-to-perform-concurrent-inference">
<h1>Using Python Interface to Perform Concurrent Inference<a class="headerlink" href="#using-python-interface-to-perform-concurrent-inference" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_en/use/cloud_infer/runtime_parallel_python.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite provides a multi-model concurrent inference interface <a class="reference external" href="https://mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelParallelRunner.html">ModelParallelRunner</a>, multi-model concurrent inference now supports Ascend 310/310P/910, Nvidia GPU, CPU backends.</p>
<p>After exporting the <code class="docutils literal notranslate"><span class="pre">mindir</span></code> model by MindSpore or converting it by <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/converter_tool.html">model conversion tool</a> to obtain the <code class="docutils literal notranslate"> <span class="pre">mindir</span></code> model, the concurrent inference process of the model can be executed in Runtime. This tutorial describes how to use the <a class="reference external" href="https://mindspore.cn/lite/api/en/master/mindspore_lite.html">Python interface</a> to perform concurrent inference with multiple models.</p>
<p>Concurrent inference with MindSpore Lite consists of the following main steps:</p>
<ol class="arabic simple">
<li><p>Preparation: Install the MindSpore Lite cloud-side inference Python package.</p></li>
<li><p>Create configuration context: Create a <a class="reference external" href="https://mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel</a> attributes to configure multi-model concurrency.</p></li>
<li><p>ModelParallelRunner creation and compilation: Before multi-model concurrent inference, you need to call <a class="reference external" href="https://mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.build_from_file">build_from_file</a> interface of <a class="reference external" href="https://mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelParallelRunner.html">ModelParallelRunner</a> for ModelParallelRunner loading and compilation.</p></li>
<li><p>Setting Concurrent Inference Tasks: Create multiple threads and bind concurrent inference tasks.</p></li>
<li><p>Perform concurrent inference: Use Predict interface of ModelParallelRunner for multi-model concurrent inference.</p></li>
<li><p>Free memory: when there is no need to use the MindSpore Lite concurrent inference framework, you need to release the ModelParallelRunner you created and the associated Tensor.</p></li>
</ol>
<p><img alt="" src="../../_images/server_inference.png" /></p>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>The following code samples are from <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/master/mindspore/lite/examples/cloud_infer/quick_start_parallel_python">Using Python interface to perform cloud-side inference sample code</a>.</p></li>
<li><p>Export the MindIR model via MindSpore, or get the MindIR model by converting it with <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/converter_tool.html">model conversion tool</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/quick_start_parallel_python</span></code> directory. You can download the MobileNetV2 model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.mindir">mobilenetv2.mindir</a> and input data <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/input.bin">input.bin</a>.</p></li>
<li><p>Install the MindSpore Lite cloud-side inference Python package for Python version 3.7 via pip.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>https://ms-release.obs.cn-north-4.myhuaweicloud.com/<span class="si">${</span><span class="nv">MINDSPORE_LITE_VERSION</span><span class="si">}</span>/MindSpore/lite/release/centos_x86/cloud_fusion/mindspore_lite-<span class="si">${</span><span class="nv">MINDSPORE_LITE_VERSION</span><span class="si">}</span>-cp37-cp37m-linux_x86.whl<span class="w"> </span>--trusted-host<span class="w"> </span>ms-release.obs.cn-north-4.myhuaweicloud.com<span class="w"> </span>-i<span class="w"> </span>https://pypi.tuna.tsinghua.edu.cn/simple
</pre></div>
</div>
</li>
</ol>
<section id="creating-configuration-context">
<h3>Creating configuration context<a class="headerlink" href="#creating-configuration-context" title="Permalink to this headline"></a></h3>
<p>The <a class="reference external" href="https://mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context.parallel</a> attributes, context related to multi-model concurrent inference, will hold some basic configuration parameters required for concurrent inference to guide the number of concurrent models as well as model compilation and model execution.</p>
<p>The following sample code demonstrates how to set Context.parallel attributes and configure the number of workers for concurrent inference.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>

<span class="c1"># the number of threads of one worker.</span>
<span class="c1"># WORKERS_NUM * THREAD_NUM should not exceed the number of cores of the machine.</span>
<span class="n">THREAD_NUM</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># In parallel inference, the number of workers in one `ModelParallelRunner` in server.</span>
<span class="c1"># If you prepare to compare the time difference between parallel inference and serial inference,</span>
<span class="c1"># you can set WORKERS_NUM = 1 as serial inference.</span>
<span class="n">WORKERS_NUM</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Simulate 5 clients, and each client sends 2 inference tasks to the server at the same time.</span>
<span class="n">PARALLEL_NUM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">TASK_NUM</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">THREAD_NUM</span></code>: The number of threads in a single worker. <code class="docutils literal notranslate"><span class="pre">WORKERS_NUM</span> <span class="pre">*</span> <span class="pre">THREAD_NUM</span></code> should be less than the number of machine cores.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WORKERS_NUM</span></code>: On the server side, specify the number of workers in a <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code>, i.e., the units that perform concurrent inference. If you want to compare the difference in inference time between concurrent inference and non-concurrent inference, you can set <code class="docutils literal notranslate"><span class="pre">WORKERS_NUM</span></code> to 1 for comparison.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PARALLEL_NUM</span></code>: The number of concurrent, i.e., the number of clients that are sending inference task requests at the same time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TASK_NUM</span></code>: The number of tasks, i.e., the number of inference task requests sent by a single client.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Init RunnerConfig and context, and add CPU device info</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
<span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_num</span> <span class="o">=</span> <span class="n">THREAD_NUM</span>
<span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">inter_op_parallel_num</span> <span class="o">=</span> <span class="n">THREAD_NUM</span>
<span class="n">context</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">workers_num</span> <span class="o">=</span> <span class="n">WORKERS_NUM</span>
</pre></div>
</div>
<blockquote>
<div><p>The configuration method of the Context is detailed in <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/cloud_infer/runtime_python.html#creating-configuration-context">Context</a>.</p>
<p>Multi-model concurrent inference does not support FP32 type data inference. CPU pinning only supports unbinding or binding large cores, does not support the parameter setting of binding middle cores, and does not support the configuration of binding core list.</p>
</div></blockquote>
</section>
</section>
<section id="modelparallelrunner-creation-and-compilation">
<h2>ModelParallelRunner creation and compilation<a class="headerlink" href="#modelparallelrunner-creation-and-compilation" title="Permalink to this headline"></a></h2>
<p>When using MindSpore Lite to perform concurrent inference, ModelParallelRunner is the main entry point for concurrent inference, you need to call <a class="reference external" href="https://mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.build_from_file">build_from_file</a> interface of ModelParallelRunner for ModelParallelRunner loading and compilation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build ModelParallelRunner from file</span>
<span class="n">model_parallel_runner</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelParallelRunner</span><span class="p">()</span>
<span class="n">model_parallel_runner</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;./model/mobilenetv2.mindir&quot;</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>If it is not configured, it will be set cpu target, and automatically set cpu attributes by default.</p>
</div></blockquote>
<section id="setting-concurrent-inference-tasks">
<h3>Setting Concurrent Inference Tasks<a class="headerlink" href="#setting-concurrent-inference-tasks" title="Permalink to this headline"></a></h3>
<p>Create multiple threads and bind concurrent inference tasks. The inference tasks include padding data into <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">Tensor</span></code>, using <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.predict">predict</a> interface of <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code> for concurrent inference and getting inference results via the <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">Tensor</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parallel_runner_predict</span><span class="p">(</span><span class="n">parallel_runner</span><span class="p">,</span> <span class="n">parallel_id</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    One Runner with 3 workers, set model input, execute inference and get output.</span>

<span class="sd">    Args:</span>
<span class="sd">        parallel_runner (mindspore_lite.ModelParallelRunner): Actuator Supporting Parallel inference.</span>
<span class="sd">        parallel_id (int): Simulate which client&#39;s task to process</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">task_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">task_index</span> <span class="o">==</span> <span class="n">TASK_NUM</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">task_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Set model input</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">parallel_runner</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()</span>
        <span class="n">in_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="s2">&quot;./model/input.bin&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">)</span>
        <span class="n">once_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="c1"># Execute inference</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">parallel_runner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">once_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;parallel id: &quot;</span><span class="p">,</span> <span class="n">parallel_id</span><span class="p">,</span> <span class="s2">&quot; | task index: &quot;</span><span class="p">,</span> <span class="n">task_index</span><span class="p">,</span> <span class="s2">&quot; | run once time: &quot;</span><span class="p">,</span>
              <span class="n">once_end_time</span> <span class="o">-</span> <span class="n">once_start_time</span><span class="p">,</span> <span class="s2">&quot; s&quot;</span><span class="p">)</span>
        <span class="c1"># Get output</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">tensor_name</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>
            <span class="n">data_size</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">data_size</span>
            <span class="n">element_num</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">element_num</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor name is:</span><span class="si">%s</span><span class="s2"> tensor size is:</span><span class="si">%s</span><span class="s2"> tensor elements num is:</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tensor_name</span><span class="p">,</span>
                                                                                     <span class="n">data_size</span><span class="p">,</span>
                                                                                     <span class="n">element_num</span><span class="p">))</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get_data_to_numpy</span><span class="p">()</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output data is:&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>


<span class="c1"># The server creates 5 threads to store the inference tasks of 5 clients.</span>
<span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">total_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">PARALLEL_NUM</span><span class="p">):</span>
    <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">parallel_runner_predict</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">model_parallel_runner</span><span class="p">,</span> <span class="n">i</span><span class="p">,)))</span>
</pre></div>
</div>
</section>
<section id="performing-concurrent-inference">
<h3>Performing Concurrent Inference<a class="headerlink" href="#performing-concurrent-inference" title="Permalink to this headline"></a></h3>
<p>Start a multi-thread and execute the configured concurrent inference task. During the execution, the single inference time and inference result in concurrent inference are printed, and the total concurrent inference time is printed after the end of the thread.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start threads to perform parallel inference.</span>
<span class="k">for</span> <span class="n">th</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
    <span class="n">th</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="k">for</span> <span class="n">th</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
    <span class="n">th</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
<span class="n">total_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;total run time: &quot;</span><span class="p">,</span> <span class="n">total_end_time</span> <span class="o">-</span> <span class="n">total_start_time</span><span class="p">,</span> <span class="s2">&quot; s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_parallel_java.html" class="btn btn-neutral float-left" title="Using Java Interface to Perform Concurrent Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="converter.html" class="btn btn-neutral float-right" title="Model Converter" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>