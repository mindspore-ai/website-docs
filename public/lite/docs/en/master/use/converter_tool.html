

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Offline Converting Models for Inference &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Converting MindSpore Lite Models" href="converter_train.html" />
    <link rel="prev" title="Converting Models for Inference" href="converter.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Tools</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="converter.html">Converting Models for Inference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Offline Converting Models for Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linux-environment-instructions">Linux Environment Instructions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#directory-structure">Directory Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parameter-description">Parameter Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cpu-model-optimization">CPU Model Optimization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example">Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#windows-environment-instructions">Windows Environment Instructions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#environment-preparation-1">Environment Preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#directory-structure-1">Directory Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parameter-description-1">Parameter Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example-1">Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="converter_train.html">Converting MindSpore Lite Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_parallel.html">Performing Concurrent Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="converter.html">Converting Models for Inference</a> &raquo;</li>
        
      <li>Offline Converting Models for Inference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/use/converter_tool.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="offline-converting-models-for-inference">
<h1>Offline Converting Models for Inference<a class="headerlink" href="#offline-converting-models-for-inference" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_en/use/converter_tool.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>MindSpore Lite provides a tool for offline model conversion. It supports conversion of multiple types of models. The converted models can be used for inference. The command line parameters contain multiple personalized options, providing a convenient conversion method for users.</p>
<p>Currently, the following input formats are supported: MindSpore, TensorFlow Lite, Caffe, TensorFlow, ONNX, and PyTorch.</p>
<p>The ms model converted by the conversion tool supports the conversion tool and the higher version of the Runtime framework to perform inference.</p>
</div>
<div class="section" id="linux-environment-instructions">
<h2>Linux Environment Instructions<a class="headerlink" href="#linux-environment-instructions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="environment-preparation">
<h3>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline">¶</a></h3>
<p>To use the MindSpore Lite model conversion tool, you need to prepare the environment as follows:</p>
<ul>
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/build.html">Compile</a> or <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/downloads.html">download</a> model transfer tool.</p></li>
<li><p>Add the path of dynamic library required by the conversion tool to the environment variables LD_LIBRARY_PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} is the decompressed package path obtained by compiling or downloading.</p>
</li>
</ul>
</div>
<div class="section" id="directory-structure">
<h3>Directory Structure<a class="headerlink" href="#directory-structure" title="Permalink to this headline">¶</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
└── tools
    └── converter
        ├── include
        │   └── registry             # Header files of customized op, model parser, node parser and pass registration
        ├── converter                # Model conversion tool
        │   └── converter_lite       # Executable program
        └── lib                      # The dynamic link library that converter depends
            ├── libmindspore_glog.so.0         # Dynamic library of Glog
            ├── libmslite_converter_plugin.so  # Dynamic library of plugin registry
            ├── libopencv_core.so.4.5          # Dynamic library of OpenCV
            ├── libopencv_imgcodecs.so.4.5     # Dynamic library of OpenCV
            └── libopencv_imgproc.so.4.5       # Dynamic library of OpenCV
</pre></div>
</div>
</div>
<div class="section" id="parameter-description">
<h3>Parameter Description<a class="headerlink" href="#parameter-description" title="Permalink to this headline">¶</a></h3>
<p>MindSpore Lite model conversion tool provides multiple parameters.
You can enter <code class="docutils literal notranslate"><span class="pre">./converter_lite</span> <span class="pre">--help</span></code> to obtain the help information in real time.</p>
<p>The following describes the parameters in detail.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Mandatory or Not</th>
<th>Parameter Description</th>
<th>Value Range</th>
<th>Default Value</th>
<th>Remarks</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--help</code></td>
<td>No</td>
<td>Prints all the help information.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--fmk=&lt;FMK&gt;</code></td>
<td>Yes</td>
<td>Original format of the input model.</td>
<td>MINDIR, CAFFE, TFLITE, TF, ONNX or PYTORCH</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--modelFile=&lt;MODELFILE&gt;</code></td>
<td>Yes</td>
<td>Path of the input model.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--outputFile=&lt;OUTPUTFILE&gt;</code></td>
<td>Yes</td>
<td>Path of the output model. The suffix <code>.ms</code> can be automatically generated.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--weightFile=&lt;WEIGHTFILE&gt;</code></td>
<td>Yes (for Caffe models only)</td>
<td>Path of the weight file of the input model.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--configFile=&lt;CONFIGFILE&gt;</code></td>
<td>No</td>
<td>1) Configure quantization parameter; 2) Profile path for extension.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--fp16=&lt;FP16&gt;</code></td>
<td>No</td>
<td>Serialize const tensor in Float16 data type, only effective for const tensor in Float32 data type.</td>
<td>on or off</td>
<td>off</td>
<td>-</td>
</tr>
<tr>
<td><code>--inputShape=&lt;INPUTSHAPE&gt;</code></td>
<td>No</td>
<td>Set the dimension of the model input, the order of input dimensions is consistent with the original model. For some models, the model structure can be further optimized, but the transformed model may lose the characteristics of dynamic shape. Multiple inputs are separated by <code>;</code>, and surround with <code>""</code></td>
<td>e.g.  "inTensorName_1: 1,32,32,4;inTensorName_2:1,64,64,4;"</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--saveType=&lt;SAVETYPE&gt;</code></td>
<td>No</td>
<td>Set the exported model as <code>mindir</code> model or <code>ms</code> model.</td>
<td>MINDIR, MINDIR_LITE</td>
<td>MINDIR_LITE</td>
<td>This device-side version can only be reasoned with models turned out by setting to MINDIR_LITE</td>
</tr>
<tr>
<td><code>--optimize=&lt;OPTIMIZE&gt;</code></td>
<td>No</td>
<td>Set the optimization accomplished in the process of converting model.</td>
<td>none, general, ascend_oriented</td>
<td>general</td>
<td>-</td>
</tr>
<tr>
<td><code>--inputDataFormat=&lt;INPUTDATAFORMAT&gt;</code></td>
<td>No</td>
<td>Set the input format of exported model. Only valid for 4-dimensional inputs.</td>
<td>NHWC, NCHW</td>
<td>NHWC</td>
<td>-</td>
</tr>
<tr>
<td><code>--decryptKey=&lt;DECRYPTKEY&gt;</code></td>
<td>No</td>
<td>The key used to decrypt the MindIR file, expressed in hexadecimal characters. Only valid when fmkIn is 'MINDIR'.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--decryptMode=&lt;DECRYPTMODE&gt;</code></td>
<td>No</td>
<td>Decryption mode for the MindIR file. Only valid when dec_key is set.</td>
<td>AES-GCM, AES-CBC</td>
<td>AES-GCM</td>
<td>-</td>
</tr>
<tr>
<td><code>--inputDataType=&lt;INPUTDATATYPE&gt;</code></td>
<td>No</td>
<td>Set data type of input tensor of quantized model. Only valid for input tensor which has quantization parameters(scale and zero point). Keep same with the data type of input tensor of origin model by default.</td>
<td>FLOAT32, INT8, UINT8, DEFAULT</td>
<td>DEFAULT</td>
<td>-</td>
</tr>
<tr>
<td><code>--outputDataType=&lt;OUTPUTDATATYPE&gt;</code></td>
<td>No</td>
<td>Set data type of output tensor of quantized model. Only valid for output tensor which has quantization parameters(scale and zero point). Keep same with the data type of output tensor of origin model by default.</td>
<td>FLOAT32, INT8, UINT8, DEFAULT</td>
<td>DEFAULT</td>
<td>-</td>
</tr>
<tr>
<td><code>--encryptKey=&lt;ENCRYPTKEY&gt;</code></td>
<td>No</td>
<td>Set the key for exporting encrypted <code>ms</code> models. The key is expressed in hexadecimal. Only AES-GCM is supported, and the key length is only 16Byte.</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>--encryption=&lt;ENCRYPTION&gt;</code></td>
<td>No</td>
<td>Set whether to encrypt when exporting the <code>ms</code> model. Exporting encryption can protect the integrity of the model, but it will increase the runtime initialization time.</td>
<td>true、false</td>
<td>true</td>
<td>-</td>
</tr>
<tr>
<td><code>--infer=&lt;INFER&gt;</code></td>
<td>No</td>
<td>Set whether to pre-inference when conversion is complete.</td>
<td>true、false</td>
<td>false</td>
<td>-</td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>The parameter name and parameter value are separated by an equal sign (=) and no space is allowed between them.</p></li>
<li><p>Because the compilation option that supports the conversion of PyTorch models is turned off by default, the downloaded installation package does not support the conversion of PyTorch models. You need to open the specified compilation option to compile locally. The following preconditions must be met for converting PyTorch models:<code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MSLITE_ENABLE_CONVERT_PYTORCH_MODEL</span> <span class="pre">=</span> <span class="pre">on</span></code> before compiling. Add the environment variable of libtorch before conversion: <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LD_LIBRARY_PATH=&quot;/home/user/libtorch/lib:${LD_LIBRARY_PATH}&quot;</span> <span class="pre">&amp;&amp;</span> <span class="pre">export</span> <span class="pre">LIB_TORCH_PATH=&quot;/home/user/libtorch&quot;</span></code>。 Users can download <a class="reference external" href="https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.12.1%2Bcpu.zip">CPU version libtorch</a>Then unzip it to the directory <code class="docutils literal notranslate"><span class="pre">/home/user/libtorch</span></code>.</p></li>
<li><p>The Caffe model is divided into two files: model structure <code class="docutils literal notranslate"><span class="pre">*.prototxt</span></code>, corresponding to the <code class="docutils literal notranslate"><span class="pre">--modelFile</span></code> parameter; model weight <code class="docutils literal notranslate"><span class="pre">*.caffemodel</span></code>, corresponding to the <code class="docutils literal notranslate"><span class="pre">--weightFile</span></code> parameter.</p></li>
<li><p>The priority of <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> option is very low. For example, if quantization is enabled, <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> will no longer take effect on const tensors that have been quantized. All in all, this option only takes effect on const tensors of Float32 when serializing model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inputDataFormat</span></code>: generally, in the scenario of integrating third-party hardware of NCHW specification(<a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/nnie.html#usage-description-of-the-integrated-nnie">Usage Description of the Integrated NNIE</a>), designated as NCHW will have a significant performance improvement over NHWC. In other scenarios, users can also set as needed.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">configFile</span></code> configuration files uses the <code class="docutils literal notranslate"><span class="pre">key=value</span></code> mode to define related parameters. For the configuration parameters related to quantization, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/post_training_quantization.html">post training quantization</a>. For the configuration parameters related to extension, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/nnie.html#extension-configuration">Extension Configuration</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--optimize</span></code> parameter is used to set the mode of optimization during the offline conversion. If this parameter is set to none, no relevant graph optimization operations will be performed during the offline conversion phase of the model, and the relevant graph optimization operations will be done during the execution of the inference phase. The advantage of this parameter is that the converted model can be deployed directly to any CPU/GPU/Ascend hardware backend since it is not optimized in a specific way, while the disadvantage is that the initialization time of the model increases during inference execution. If this parameter is set to general, general optimization will be performed, such as constant folding and operator fusion (the converted model only supports CPU/GPU hardware backend, not Ascend backend). If this parameter is set to ascend_oriented, the optimization for Ascend hardware will be performed (the converted model only supports Ascend hardware backend).</p></li>
<li><p>The encryption and decryption function only takes effect when <code class="docutils literal notranslate"><span class="pre">MSLITE_ENABLE_MODEL_ENCRYPTION=on</span></code> is set at <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/build.html">compile</a> time and only supports Linux x86 platforms, and the key is a string represented by hexadecimal. For example, if the key is defined as <code class="docutils literal notranslate"><span class="pre">b'0123456789ABCDEF'</span></code>, the corresponding hexadecimal representation is <code class="docutils literal notranslate"><span class="pre">30313233343536373839414243444546</span></code>. Users on the Linux platform can use the <code class="docutils literal notranslate"><span class="pre">xxd</span></code> tool to convert the key represented by the bytes to a hexadecimal representation.
It should be noted that the encryption and decryption algorithm has been updated in version 1.7. As a result, the new version of the converter tool does not support the conversion of the encrypted model exported by MindSpore in version 1.6 and earlier.</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="cpu-model-optimization">
<h3>CPU Model Optimization<a class="headerlink" href="#cpu-model-optimization" title="Permalink to this headline">¶</a></h3>
<p>If the converted ms model is running on android cpu backend, and hope the model compile with lower latency.  Try to turn on this optimization，Add the configuration item <code class="docutils literal notranslate"><span class="pre">[cpu_option_cfg_param]</span></code> in the <code class="docutils literal notranslate"><span class="pre">configFile</span></code> to get a lower compile latency model。At present, the optimization is only available when the model include Matmul operator and its data type is <code class="docutils literal notranslate"><span class="pre">Float32</span></code> or dynamic quantization is enabled.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Attribute</th>
<th>Function Description</th>
<th>Value Range</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>architecture</code></td>
<td>Mandatory</td>
<td>target cpu architecture, only support ARM64</td>
<td>ARM64</td>
</tr>
<tr>
<td><code>instruction</code></td>
<td>Mandatory</td>
<td>target instruction set, only support SMID_DOT</td>
<td>SIMD_DOT</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h3>
<p>The following describes how to use the conversion command by using several common examples.</p>
<ul>
<li><p>Take the Caffe model LeNet as an example. Run the following conversion command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>CAFFE<span class="w"> </span>--modelFile<span class="o">=</span>lenet.prototxt<span class="w"> </span>--weightFile<span class="o">=</span>lenet.caffemodel<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
<p>In this example, the Caffe model is used. Therefore, the model structure and model weight files are required. Two more parameters <code class="docutils literal notranslate"><span class="pre">fmk</span></code> and <code class="docutils literal notranslate"><span class="pre">outputFile</span></code> are also required.</p>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
<p>This indicates that the Caffe model is successfully converted into the MindSpore Lite model and the new file <code class="docutils literal notranslate"><span class="pre">lenet.ms</span></code> is generated.</p>
</li>
<li><p>The following uses the MindSpore, TensorFlow Lite, TensorFlow and ONNX models as examples to describe how to run the conversion command.</p>
<ul>
<li><p>MindSpore model <code class="docutils literal notranslate"><span class="pre">model.mindir</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>model.mindir<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> model exported by MindSpore v1.1.1 or earlier is recommended to be converted to the <code class="docutils literal notranslate"><span class="pre">ms</span></code> model using the converter tool of the corresponding version. MindSpore v1.1.1 and later versions, the converter tool will be forward compatible.</p>
</div></blockquote>
<ul>
<li><p>TensorFlow Lite model <code class="docutils literal notranslate"><span class="pre">model.tflite</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>model.tflite<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
</li>
<li><p>TensorFlow model <code class="docutils literal notranslate"><span class="pre">model.pb</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TF<span class="w"> </span>--modelFile<span class="o">=</span>model.pb<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
</li>
<li><p>ONNX model <code class="docutils literal notranslate"><span class="pre">model.onnx</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span>model.onnx<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
</li>
<li><p>PyTorch model <code class="docutils literal notranslate"><span class="pre">model.pt</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;/home/user/libtorch/lib:</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LIB_TORCH_PATH</span><span class="o">=</span><span class="s2">&quot;/home/user/libtorch&quot;</span>
./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>PYTORCH<span class="w"> </span>--modelFile<span class="o">=</span>model.pt<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
</li>
<li><p>PyTorch model <code class="docutils literal notranslate"><span class="pre">model.pth</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="s2">&quot;/home/user/libtorch/lib:</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LIB_TORCH_PATH</span><span class="o">=</span><span class="s2">&quot;/home/user/libtorch&quot;</span>
./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>PYTORCH<span class="w"> </span>--modelFile<span class="o">=</span>model.pth<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>The following preconditions must be met for converting PyTorch models: <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">MSLITE_ENABLE_CONVERT_PYTORCH_MODEL</span> <span class="pre">=</span> <span class="pre">on</span></code> before compiling. Add the environment variable of libtorch before conversion: <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LD_LIBRARY_PATH=&quot;/home/user/libtorch/lib:${LD_LIBRARY_PATH}&quot;</span> <span class="pre">&amp;&amp;</span> <span class="pre">export</span> <span class="pre">LIB_TORCH_PATH=&quot;/home/user/libtorch&quot;</span></code>。 Users can download <a class="reference external" href="https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.12.1%2Bcpu.zip">CPU version libtorch</a>Then unzip it to the directory <code class="docutils literal notranslate"><span class="pre">/home/user/libtorch</span></code>.</p>
</div></blockquote>
<p>In the preceding scenarios, the following information is displayed, indicating that the conversion is successful. In addition, the target file <code class="docutils literal notranslate"><span class="pre">model.ms</span></code> is obtained.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERT RESULT SUCCESS:0
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="section" id="windows-environment-instructions">
<h2>Windows Environment Instructions<a class="headerlink" href="#windows-environment-instructions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="environment-preparation-1">
<h3>Environment Preparation<a class="headerlink" href="#environment-preparation-1" title="Permalink to this headline">¶</a></h3>
<p>To use the MindSpore Lite model conversion tool, the following environment preparations are required.</p>
<ul>
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/build.html">Compile</a> or <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/downloads.html">download</a> model transfer tool.</p></li>
<li><p>Add the path of dynamic library required by the conversion tool to the environment variables PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">set</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>%PACKAGE_ROOT_PATH%<span class="se">\t</span>ools<span class="se">\c</span>onverter<span class="se">\l</span>ib<span class="p">;</span>%PATH%
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="directory-structure-1">
<h3>Directory Structure<a class="headerlink" href="#directory-structure-1" title="Permalink to this headline">¶</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-win-x64
└── tools
    └── converter # Model conversion tool
        ├── converter
        │   └── converter_lite.exe    # Executable program
        └── lib
            ├── libgcc_s_seh-1.dll    # Dynamic library of MinGW
            ├── libmindspore_glog.dll            # Dynamic library of Glog
            ├── libmslite_converter_plugin.dll   # Dynamic library of plugin registry
            ├── libmslite_converter_plugin.dll.a # Link file of Dynamic library of plugin registry
            ├── libssp-0.dll          # Dynamic library of MinGW
            ├── libstdc++-6.dll       # Dynamic library of MinGW
            └── libwinpthread-1.dll   # Dynamic library of MinGW
</pre></div>
</div>
</div>
<div class="section" id="parameter-description-1">
<h3>Parameter Description<a class="headerlink" href="#parameter-description-1" title="Permalink to this headline">¶</a></h3>
<p>Refer to the Linux environment model conversion tool <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/converter_tool.html#parameter-description">parameter description</a>.</p>
</div>
<div class="section" id="example-1">
<h3>Example<a class="headerlink" href="#example-1" title="Permalink to this headline">¶</a></h3>
<p>Set the log printing level to INFO.</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">set</span> <span class="nv">GLOG_v</span><span class="p">=</span>1
</pre></div>
</div>
<blockquote>
<div><p>Log level: 0 is DEBUG, 1 is INFO, 2 is WARNING, 3 is ERROR.</p>
</div></blockquote>
<p>Several common examples are selected below to illustrate the use of conversion commands.</p>
<ul>
<li><p>Take the Caffe model LeNet as an example to execute the conversion command.</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">call</span> converter_lite --fmk=CAFFE --modelFile=lenet.prototxt --weightFile=lenet.caffemodel --outputFile=lenet
</pre></div>
</div>
<p>In this example, because the Caffe model is used, two input files of model structure and model weight are required. Then with the fmk type and output path two parameters which are required, you can successfully execute.</p>
<p>The result is shown as:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
<p>This means that the Caffe model has been successfully converted to the MindSpore Lite model and the new file <code class="docutils literal notranslate"><span class="pre">lenet.ms</span></code> has been obtained.</p>
</li>
<li><p>Take MindSpore, TensorFlow Lite, ONNX model format and perceptual quantization model as examples to execute conversion commands.</p>
<ul>
<li><p>MindSpore model <code class="docutils literal notranslate"><span class="pre">model.mindir</span></code></p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">call</span> converter_lite --fmk=MINDIR --modelFile=model.mindir --outputFile=model
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> model exported by MindSpore v1.1.1 or earlier is recommended to be converted to the <code class="docutils literal notranslate"><span class="pre">ms</span></code> model using the converter tool of the corresponding version. MindSpore v1.1.1 and later versions, the converter tool will be forward compatible.</p>
</div></blockquote>
<ul>
<li><p>TensorFlow Lite model<code class="docutils literal notranslate"><span class="pre">model.tflite</span></code></p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">call</span> converter_lite --fmk=TFLITE --modelFile=model.tflite --outputFile=model
</pre></div>
</div>
</li>
<li><p>TensorFlow model <code class="docutils literal notranslate"><span class="pre">model.pb</span></code></p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">call</span> converter_lite --fmk=TF --modelFile=model.pb --outputFile=model
</pre></div>
</div>
</li>
<li><p>ONNX model<code class="docutils literal notranslate"><span class="pre">model.onnx</span></code></p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">call</span> converter_lite --fmk=ONNX --modelFile=model.onnx --outputFile=model
</pre></div>
</div>
</li>
</ul>
<p>In the above cases, the following conversion success prompt is displayed, and the <code class="docutils literal notranslate"><span class="pre">model.ms</span></code> target file is obtained at the same time.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
</li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="converter_train.html" class="btn btn-neutral float-right" title="Converting MindSpore Lite Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="converter.html" class="btn btn-neutral float-left" title="Converting Models for Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>