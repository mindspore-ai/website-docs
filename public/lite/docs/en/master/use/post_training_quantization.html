

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Post Training Quantization &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Data Preprocessing" href="data_preprocessing.html" />
    <link rel="prev" title="Android Application Development Based on Java Interface" href="../quick_start/image_segmentation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/build.html">Compiling Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-Side Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Inference on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Post Training Quantization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuration-parameter">Configuration Parameter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common-quantization-parameter">Common Quantization Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mixed-bit-weight-quantization-parameter">Mixed Bit Weight Quantization Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#full-quantization-parameters">Full Quantization Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-preprocessing">Data Preprocessing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#weight-quantization">Weight Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mixed-bit-weight-quantization">Mixed Bit Weight Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fixed-bit-weight-quantization">Fixed Bit Weight Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#partial-model-accuracy-result">Partial Model Accuracy Result</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#full-quantization">Full Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cpu">CPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nvdia">NVDIA</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dsp">DSP</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ascend">Ascend</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dynamic-quantization">Dynamic Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#partial-model-performance-results">Partial Model Performance Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#quantization-debug">Quantization Debug</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#skip-quantization-node">Skip Quantization Node</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recommendations">Recommendations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_parallel.html">Performing Concurrent Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Post Training Quantization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/use/post_training_quantization.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="post-training-quantization">
<h1>Post Training Quantization<a class="headerlink" href="#post-training-quantization" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_en/use/post_training_quantization.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Converting a trained <code class="docutils literal notranslate"><span class="pre">float32</span></code> model into an <code class="docutils literal notranslate"><span class="pre">int8</span></code> model through quantization after training can reduce the model size and improve the inference performance. In MindSpore Lite, this function is integrated into the model conversion tool <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>. You can add command line parameters to convert a model into a quantization model.</p>
<p>MindSpore Lite quantization after training is classified into two types:</p>
<ol class="simple">
<li><p>Weight quantization: quantizes a weight of a model and compresses only the model size. <code class="docutils literal notranslate"><span class="pre">float32</span></code> inference is still performed during inference.</p></li>
<li><p>Full quantization: quantizes the weight and activation value of a model. The <code class="docutils literal notranslate"><span class="pre">int</span></code> operation is performed during inference to improve the model inference speed and reduce power consumption.</p></li>
</ol>
</div>
<div class="section" id="configuration-parameter">
<h2>Configuration Parameter<a class="headerlink" href="#configuration-parameter" title="Permalink to this headline">¶</a></h2>
<p>Post training quantization can be enabled by configuring <code class="docutils literal notranslate"><span class="pre">configFile</span></code> through <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/converter_tool.html">Conversion Tool</a>. The configuration file adopts the style of <code class="docutils literal notranslate"><span class="pre">INI</span></code>, For quantization, configurable parameters include <code class="docutils literal notranslate"><span class="pre">common</span> <span class="pre">quantization</span> <span class="pre">parameter</span> <span class="pre">[common_quant_param]</span></code>, <code class="docutils literal notranslate"><span class="pre">mixed</span> <span class="pre">bit</span> <span class="pre">weight</span> <span class="pre">quantization</span> <span class="pre">parameter</span> <span class="pre">[mixed_bit_weight_quant_param]</span></code>,<code class="docutils literal notranslate"><span class="pre">full</span> <span class="pre">quantization</span> <span class="pre">parameter</span> <span class="pre">[full_quant_param]</span></code>, and <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">preprocess</span> <span class="pre">parameter</span> <span class="pre">[data_preprocess_param]</span></code>.</p>
<div class="section" id="common-quantization-parameter">
<h3>Common Quantization Parameter<a class="headerlink" href="#common-quantization-parameter" title="Permalink to this headline">¶</a></h3>
<p>common quantization parameters are the basic settings for post training quantization, mainly including <code class="docutils literal notranslate"><span class="pre">quant_type</span></code>, <code class="docutils literal notranslate"><span class="pre">bit_num</span></code>, <code class="docutils literal notranslate"><span class="pre">min_quant_weight_size</span></code>, and <code class="docutils literal notranslate"><span class="pre">min_quant_weight_channel</span></code>. The detailed description of the parameters is as follows:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Attribute</th>
<th>Function Description</th>
<th>Parameter Type</th>
<th>Default Value</th>
<th>Value Range</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>quant_type</code></td>
<td>Mandatory</td>
<td>The quantization type. When set to WEIGHT_QUANT, weight quantization is enabled; when set to FULL_QUANT, full quantization is enabled; when set to DYNAMIC_QUANT, dynamic quantization is enabled.</td>
<td>String</td>
<td>-</td>
<td>WEIGHT_QUANT,<br/> FULL_QUANT,<br/>DYNAMIC_QUANT</td>
</tr>
<tr>
<td><code>bit_num</code></td>
<td>Optional</td>
<td>The number of quantized bits. Currently, weight quantization supports 0-16bit quantization. When it is set to 1-16bit, it is fixed-bit quantization. When it is set to 0bit, mixed-bit quantization is enabled. Full quantization and Dynamic quantization supports 8bit quantization.</td>
<td>Integer</td>
<td>8</td>
<td>WEIGHT_QUANT:[0，16]<br/>FULL_QUANT: 8<br/>DYNAMIC_QUANT:8</td>
</tr>
<tr>
<td><code>min_quant_weight_size</code></td>
<td>Optional</td>
<td>Set the threshold of the weight size for quantization. If the number of weights is greater than this value, the weight will be quantized.</td>
<td>Integer</td>
<td>0</td>
<td>[0, 65535]</td>
</tr>
<tr>
<td><code>min_quant_weight_channel</code></td>
<td>Optional</td>
<td>Set the threshold of the number of weight channels for quantization. If the number of weight channels is greater than this value, the weight will be quantized.</td>
<td>Integer</td>
<td>16</td>
<td>[0, 65535]</td>
</tr>
<tr>
<td><code>skip_quant_node</code></td>
<td>Optional</td>
<td>Set the name of the operator that does not need to be quantified, and use <code>,</code> to split between multiple operators.</td>
<td>String</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>debug_info_save_path</code></td>
<td>Optional</td>
<td>Set the folder path where the quantized debug information file is saved.</td>
<td>String</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td><code>enable_encode</code></td>
<td>Optional</td>
<td>The enable switch of compression code for weight quantization.</td>
<td>Boolean</td>
<td>True</td>
<td>True, False</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">min_quant_weight_size</span></code> and <code class="docutils literal notranslate"><span class="pre">min_quant_weight_channel</span></code> are only valid for weight quantization.</p>
<p>Recommendation: When the accuracy of full quantization is not satisfied, you can set <code class="docutils literal notranslate"><span class="pre">debug_info_save_path</span></code> to turn on the Debug mode to get the relevant statistical report, and set <code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code> for operators that are not suitable for quantization to not quantize them.</p>
</div></blockquote>
<p>The common quantization parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>
<span class="c1"># Set the name of the operator that skips the quantization, and use `,` to split between multiple operators.</span>
<span class="na">skip_quant_node</span><span class="o">=</span><span class="s">node_name1,node_name2,node_name3</span>
<span class="c1"># Set the folder path where the quantization debug information file is saved.</span>
<span class="na">debug_info_save_path</span><span class="o">=</span><span class="s">/home/workspace/mindspore/debug_info_save_path</span>
<span class="c1"># Enable tensor compression for weight quantization. If parameter bit_num not equal to 8 or 16, it can not be set to false.</span>
<span class="na">enable_encode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">true</span>
</pre></div>
</div>
</div>
<div class="section" id="mixed-bit-weight-quantization-parameter">
<h3>Mixed Bit Weight Quantization Parameter<a class="headerlink" href="#mixed-bit-weight-quantization-parameter" title="Permalink to this headline">¶</a></h3>
<p>The mixed bit weight quantization parameters include <code class="docutils literal notranslate"><span class="pre">init_scale</span></code>. When enable the mixed bit weight quantization, the optimal number of bits will be automatically searched for different layers. The detailed description of the parameters is as follows:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Attribute</th>
<th>Function Description</th>
<th>Parameter Type</th>
<th>Default Value</th>
<th>Value Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>init_scale</td>
<td>Optional</td>
<td>Initialize the scale. The larger the value, the greater the compression rate, but it will also cause varying degrees of accuracy loss.</td>
<td>Float</td>
<td>0.02</td>
<td>(0 , 1)</td>
</tr>
<tr>
<td>auto_tune</td>
<td>Optional</td>
<td>Automatically search for the init_scale parameter. After setting, it will automatically search for a set of <code>init_scale</code> values whose cosine similarity of the model output Tensor is around 0.995.</td>
<td>Boolean</td>
<td>False</td>
<td>True，False</td>
</tr>
</tbody>
</table>
<p>The mixed bit quantization parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[mixed_bit_weight_quant_param]</span>
<span class="na">init_scale</span><span class="o">=</span><span class="s">0.02</span>
<span class="na">auto_tune</span><span class="o">=</span><span class="s">false</span>
</pre></div>
</div>
</div>
<div class="section" id="full-quantization-parameters">
<h3>Full Quantization Parameters<a class="headerlink" href="#full-quantization-parameters" title="Permalink to this headline">¶</a></h3>
<p>The full quantization parameters mainly include <code class="docutils literal notranslate"><span class="pre">activation_quant_method</span></code>, <code class="docutils literal notranslate"><span class="pre">bias_correction</span></code> and <code class="docutils literal notranslate"><span class="pre">target_device</span></code>. The detailed description of the parameters is as follows:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Attribute</th>
<th>Function Description</th>
<th>Parameter Type</th>
<th>Default Value</th>
<th>Value Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>activation_quant_method</td>
<td>Optional</td>
<td>Activation quantization algorithm</td>
<td>String</td>
<td>MAX_MIN</td>
<td>KL, MAX_MIN, or RemovalOutlier.<br/>KL: quantizes and calibrates the data range based on <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">KL divergence</a>.<br/>MAX_MIN: data quantization parameter computed based on the maximum and minimum values.<br/>RemovalOutlier: removes the maximum and minimum values of data based on a certain proportion and then calculates the quantization parameters.<br/>If the calibration dataset is consistent with the input data during actual inference, MAX_MIN is recommended. If the noise of the calibration dataset is large, KL or RemovalOutlier is recommended.</td>
</tr>
<tr>
<td>bias_correction</td>
<td>Optional</td>
<td>Indicate whether to correct the quantization error.</td>
<td>Boolean</td>
<td>True</td>
<td>True or False. After this parameter is enabled, the accuracy of the converted model can be improved. You are advised to set this parameter to true.</td>
</tr>
<tr>
<td>per_channel</td>
<td>Optional</td>
<td>Select PerChannel or PerLayer quantization type.</td>
<td>Boolean</td>
<td>True</td>
<td>True or False. Set to false to enable Perlayer quantization.</td>
</tr>
<tr>
<td>target_device</td>
<td>Optional</td>
<td>Full quantization supports multiple hardware backends. After setting the specific hardware, the converted quantization model can execute the proprietry hardware quantization operator library. If not setting, universal quantization lib will be called.</td>
<td>String</td>
<td>-</td>
<td>NVGPU: The quantized model can perform quantitative inference on the NVIDIA GPU. <br/>DSP: The quantized model can perform quantitative inference on the DSP devices.<br/>ASCEND: The quantized model can perform quantitative inference on the ASCEND devices.</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="data-preprocessing">
<h3>Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Permalink to this headline">¶</a></h3>
<p>Full quantization needs to provide 100-500 calibration data sets for pre-inference, which is used to calculate the quantization parameters of full quantization activation values. If there are multiple input Tensors, the calibration dataset for each input Tensor needs to be saved in a separate folder.</p>
<p>For the BIN calibration dataset, the <code class="docutils literal notranslate"><span class="pre">.bin</span></code> file stores the input data buffer, and the format of the input data needs to be consistent with the format of the input data during inference. For 4D data, the default is <code class="docutils literal notranslate"><span class="pre">NHWC</span></code>. If the command parameter <code class="docutils literal notranslate"><span class="pre">inputDataFormat</span></code> of the converter tool is configured, the format of the input Buffer needs to be consistent.</p>
<p>For the image calibration dataset, post training quantization provides data preprocessing functions such as channel conversion, normalization, resize, and center crop.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Attribute</th>
<th>Function Description</th>
<th>Parameter Type</th>
<th>Default Value</th>
<th>Value Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>calibrate_path</td>
<td>Mandatory</td>
<td>The directory where the calibration dataset is stored; if the model has multiple inputs, please fill in the directory where the corresponding data is located one by one, and separate the directory paths with <code>,</code></td>
<td>String</td>
<td>-</td>
<td>input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</td>
</tr>
<tr>
<td>calibrate_size</td>
<td>Mandatory</td>
<td>Calibration data size</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>input_type</td>
<td>Mandatory</td>
<td>Correction data file format type</td>
<td>String</td>
<td>-</td>
<td>IMAGE、BIN <br>IMAGE：image file data <br>BIN：binary <code>.bin</code> file data</td>
</tr>
<tr>
<td>image_to_format</td>
<td>Optional</td>
<td>Image format conversion</td>
<td>String</td>
<td>-</td>
<td>RGB、GRAY、BGR</td>
</tr>
<tr>
<td>normalize_mean</td>
<td>Optional</td>
<td>Normalized mean<br/>dst = (src - mean) / std</td>
<td>Vector</td>
<td>-</td>
<td>Channel 3: [mean_1, mean_2, mean_3] <br/>Channel 1: [mean_1]</td>
</tr>
<tr>
<td>normalize_std</td>
<td>Optional</td>
<td>Normalized standard deviation<br/>dst = (src - mean) / std</td>
<td>Vector</td>
<td>-</td>
<td>Channel 3: [std_1, std_2, std_3] <br/>Channel 1: [std_1]</td>
</tr>
<tr>
<td>resize_width</td>
<td>Optional</td>
<td>Resize width</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>resize_height</td>
<td>Optional</td>
<td>Resize height</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>resize_method</td>
<td>Optional</td>
<td>Resize algorithm</td>
<td>String</td>
<td>-</td>
<td>LINEAR, NEAREST, CUBIC<br/>LINEAR：Bilinear interpolation<br/>NEARST：Nearest neighbor interpolation<br/>CUBIC：Bicubic interpolation</td>
</tr>
<tr>
<td>center_crop_width</td>
<td>Optional</td>
<td>Center crop width</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>center_crop_height</td>
<td>Optional</td>
<td>Center crop height</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
</tbody>
</table>
<p>The data preprocessing parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[data_preprocess_param]</span>
<span class="c1"># Calibration dataset path, the format is input_name_1:input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Full quantification must provide correction dataset</span>
<span class="na">calibrate_path</span><span class="o">=</span><span class="s">input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Calibration data size</span>
<span class="na">calibrate_size</span><span class="o">=</span><span class="s">100</span>
<span class="c1"># Input type supports IMAGE or BIN</span>
<span class="c1"># When set to IMAGE, the image data will be read</span>
<span class="c1"># When set to BIN, the `.bin` binary file will be read</span>
<span class="na">input_type</span><span class="o">=</span><span class="s">IMAGE</span>
<span class="c1"># The output format of the preprocessed image</span>
<span class="c1"># Supports RGB or GRAY or BGR</span>
<span class="na">image_to_format</span><span class="o">=</span><span class="s">RGB</span>
<span class="c1"># Image normalization</span>
<span class="c1"># dst = (src - mean) / std</span>
<span class="na">normalize_mean</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="na">normalize_std</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="c1"># Image resize</span>
<span class="na">resize_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">resize_height</span><span class="o">=</span><span class="s">224</span>
<span class="c1"># Resize method supports LINEAR or NEAREST or CUBIC</span>
<span class="na">resize_method</span><span class="o">=</span><span class="s">LINEAR</span>
<span class="c1"># Image center crop</span>
<span class="na">center_crop_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">center_crop_height</span><span class="o">=</span><span class="s">224</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="weight-quantization">
<h2>Weight Quantization<a class="headerlink" href="#weight-quantization" title="Permalink to this headline">¶</a></h2>
<p>Weight quantization supports mixed bit quantization, as well as fixed bit quantization between 1 and 16. The lower the number of bits, the greater the model compression rate, but the accuracy loss is usually larger. The following describes how to use weight quantization and its effects.</p>
<div class="section" id="mixed-bit-weight-quantization">
<h3>Mixed Bit Weight Quantization<a class="headerlink" href="#mixed-bit-weight-quantization" title="Permalink to this headline">¶</a></h3>
<p>Currently, weight quantization supports mixed bit quantization. According to the distribution of model parameters and the initial value of <code class="docutils literal notranslate"><span class="pre">init_scale</span></code> set by the user, the number of bits that is most suitable for the current layer will be automatically searched out. When the <code class="docutils literal notranslate"><span class="pre">bit_num</span></code> of the configuration parameter is set to 0, mixed bit quantization will be enabled.</p>
<p>The general form of the mixed bit weight requantization command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--quantType<span class="o">=</span>WeightQuant<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/mixed_bit_weight_quant.cfg
</pre></div>
</div>
<p>The mixed bit weight quantification configuration file is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">5000</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">5</span>

<span class="k">[mixed_bit_weight_quant_param]</span>
<span class="c1"># Initialization scale in (0,1).</span>
<span class="c1"># A larger value can get a larger compression ratio, but it may also cause a larger error.</span>
<span class="na">init_scale</span><span class="o">=</span><span class="s">0.02</span>
</pre></div>
</div>
<p>Users can adjust the weighted parameters according to the model and their own needs.</p>
<blockquote>
<div><p>The init_scale default value is 0.02, and the compression rate is equivalent to the compression effect of 6-7 fixed bits quantization.</p>
<p>Mixed bits need to search for the best bit, and the waiting time may be longer. If you need to view the log, you can set export GLOG_v=1 before the execution to print the relevant Info level log.</p>
</div></blockquote>
</div>
<div class="section" id="fixed-bit-weight-quantization">
<h3>Fixed Bit Weight Quantization<a class="headerlink" href="#fixed-bit-weight-quantization" title="Permalink to this headline">¶</a></h3>
<p>Fixed-bit weighting supports fixed-bit quantization between 1 and 16, and users can adjust the weighting parameters according to the requirement.</p>
<p>The general form of the fixed bit weight quantization conversion command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/fixed_bit_weight_quant.cfg
</pre></div>
</div>
<p>The fixed bit weight quantization configuration file is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>
</pre></div>
</div>
</div>
<div class="section" id="partial-model-accuracy-result">
<h3>Partial Model Accuracy Result<a class="headerlink" href="#partial-model-accuracy-result" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model</th>
<th>Test Dataset</th>
<th>FP32 Model Accuracy</th>
<th>Weight Quantization Accuracy (8 bits)</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>77.60%</td>
<td>77.53%</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>70.96%</td>
<td>70.56%</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>71.56%</td>
<td>71.53%</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>All the preceding results are obtained in the x86 environment.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="full-quantization">
<h2>Full Quantization<a class="headerlink" href="#full-quantization" title="Permalink to this headline">¶</a></h2>
<p>In CV scenarios where the model running speed needs to be improved and the model running power consumption needs to be reduced, the full quantization after training can be used. The following describes how to use full quantization and its effects.</p>
<p>To calculate a quantization parameter of an activation value, you need to provide a calibration dataset. It is recommended that the calibration dataset be obtained from the actual inference scenario and can represent the actual input of a model. The number of data records is about 100 - 500, <strong>and the calibration dataset needs to be processed into the Format of <code class="docutils literal notranslate"><span class="pre">NHWC</span></code></strong>.</p>
<p>For image data, currently supports channel pack, normalization, resize, center crop processing. The user can set the corresponding <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/post_training_quantization.html#data-preprocessing">parameter</a> according to the preprocessing operation requirements.</p>
<p>Full quantization config’s info must include <code class="docutils literal notranslate"><span class="pre">[common_quant_param]</span></code>, <code class="docutils literal notranslate"><span class="pre">[data_preprocess_param]</span></code>, <code class="docutils literal notranslate"><span class="pre">[full_quant_param]</span></code>。</p>
<p>The general form of the full quantization conversion command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/full_quant.cfg
</pre></div>
</div>
<div class="section" id="cpu">
<h3>CPU<a class="headerlink" href="#cpu" title="Permalink to this headline">¶</a></h3>
<p>The full quantization parameter (PerChannel quantization type) configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
<span class="c1"># If set to true, it will enable PerChannel quantization, or set to false to enable PerLayer quantization.</span>
<span class="na">per_channel</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<p>The full quantization parameter (PerLayer quantization type) configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
<span class="c1"># If set to true, it will enable PerChannel quantization, or set to false to enable PerLayer quantization.</span>
<span class="na">per_channel</span><span class="o">=</span><span class="s">false</span>
</pre></div>
</div>
<p>The full quantization profile is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">FULL_QUANT</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>

<span class="k">[data_preprocess_param]</span>
<span class="c1"># Calibration dataset path, the format is input_name_1:input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Full quantification must provide correction dataset</span>
<span class="na">calibrate_path</span><span class="o">=</span><span class="s">input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Calibration data size</span>
<span class="na">calibrate_size</span><span class="o">=</span><span class="s">100</span>
<span class="c1"># Input type supports IMAGE or BIN</span>
<span class="c1"># When set to IMAGE, the image data will be read</span>
<span class="c1"># When set to BIN, the `.bin` binary file will be read</span>
<span class="na">input_type</span><span class="o">=</span><span class="s">IMAGE</span>
<span class="c1"># The output format of the preprocessed image</span>
<span class="c1"># Supports RGB or GRAY or BGR</span>
<span class="na">image_to_format</span><span class="o">=</span><span class="s">RGB</span>
<span class="c1"># Image normalization</span>
<span class="c1"># dst = (src - mean) / std</span>
<span class="na">normalize_mean</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="na">normalize_std</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="c1"># Image resize</span>
<span class="na">resize_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">resize_height</span><span class="o">=</span><span class="s">224</span>
<span class="c1"># Resize method supports LINEAR or NEAREST or CUBIC</span>
<span class="na">resize_method</span><span class="o">=</span><span class="s">LINEAR</span>
<span class="c1"># Image center crop</span>
<span class="na">center_crop_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">center_crop_height</span><span class="o">=</span><span class="s">224</span>

<span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<blockquote>
<div><p>Full quantification needs to perform inference, and the waiting time may be longer. If you need to view the log, you can set export GLOG_v=1 before the execution to print the relevant Info level log.</p>
</div></blockquote>
<p>Partial Model Accuracy Result</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model</th>
<th>Test Dataset</th>
<th>quant_method</th>
<th>FP32 Model Accuracy</th>
<th>Full Quantization Accuracy (8 bits)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>KL</td>
<td>77.60%</td>
<td>77.40%</td>
<td>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>KL</td>
<td>70.96%</td>
<td>70.31%</td>
<td>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>MAX_MIN</td>
<td>71.56%</td>
<td>71.16%</td>
<td>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>All the preceding results are obtained in the x86 environment.</p>
</div></blockquote>
</div>
<div class="section" id="nvdia">
<h3>NVDIA<a class="headerlink" href="#nvdia" title="Permalink to this headline">¶</a></h3>
<p>NVIDIA GPU full quantization parameter configuration only need add <code class="docutils literal notranslate"><span class="pre">target_device=NVGPU</span></code> to <code class="docutils literal notranslate"><span class="pre">[full_quant_param]</span></code> likes as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
<span class="c1"># Whether to support PerChannel quantization strategy. Recommended to set to true.</span>
<span class="na">per_channel</span><span class="o">=</span><span class="s">true</span>
<span class="c1"># Supports specific hardware backends</span>
<span class="na">target_device</span><span class="o">=</span><span class="s">NVGPU</span>
</pre></div>
</div>
</div>
<div class="section" id="dsp">
<h3>DSP<a class="headerlink" href="#dsp" title="Permalink to this headline">¶</a></h3>
<p>DSP full quantization parameter configuration only need add <code class="docutils literal notranslate"><span class="pre">target_device=DSP</span></code> to <code class="docutils literal notranslate"><span class="pre">[full_quant_param]</span></code>, and the command is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">false</span>
<span class="c1"># Supports specific hardware backends</span>
<span class="na">target_device</span><span class="o">=</span><span class="s">DSP</span>
</pre></div>
</div>
</div>
<div class="section" id="ascend">
<h3>Ascend<a class="headerlink" href="#ascend" title="Permalink to this headline">¶</a></h3>
<p>Ascend quantization need to set <code class="docutils literal notranslate"><span class="pre">optimize</span></code> to <code class="docutils literal notranslate"><span class="pre">ascend_oriented</span></code> for <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/converter_tool.html#parameter-description">converter tools</a> and we also need to set environment for Ascend.</p>
<p>Ascend quantization static shape parameter configuration</p>
<ul>
<li><p>In the static shape scenario, the general form of the conversion command for Ascend quantization as follow:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/full_quant.cfg<span class="w"> </span>--optimize<span class="o">=</span>ascend_oriented
</pre></div>
</div>
</li>
<li><p>Ascend static shape quantizion only need add <code class="docutils literal notranslate"><span class="pre">target_device=ASCEND</span></code> to <code class="docutils literal notranslate"><span class="pre">[full_quant_param]</span></code> likes as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
<span class="c1"># Supports specific hardware backends</span>
<span class="na">target_device</span><span class="o">=</span><span class="s">ASCEND</span>
</pre></div>
</div>
</li>
</ul>
<p>Ascend quantization also support dynamic shape. It is worth noting that the conversion command must set the same inputShape as the calibration dataset. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/converter_tool.html#parameter-description">Conversion Tool Parameter Description</a>.</p>
<ul>
<li><p>In the dynamic shape scenario, the general form of the conversion command for Ascend quantization as follow:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/full_quant.cfg<span class="w"> </span>--optimize<span class="o">=</span>ascend_oriented<span class="w"> </span>--inputShape<span class="o">=</span><span class="s2">&quot;inTensorName_1:1,32,32,4&quot;</span>
</pre></div>
</div>
</li>
<li><p>We must add configuration about <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code> for dynamic shape as follow:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
<span class="c1"># Supports specific hardware backends</span>
<span class="na">target_device</span><span class="o">=</span><span class="s">ASCEND</span>

<span class="k">[ascend_context]</span>
<span class="na">input_shape</span><span class="o">=</span><span class="s">input_1:[-1,32,32,4]</span>
<span class="na">dynamic_dims</span><span class="o">=</span><span class="s">[1~4],[8],[16]</span>

<span class="c1"># &quot;-1&quot; in input_shape indicates that the batch size is dynamic.</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="section" id="dynamic-quantization">
<h2>Dynamic Quantization<a class="headerlink" href="#dynamic-quantization" title="Permalink to this headline">¶</a></h2>
<p>In NLP scenarios where the model running speed needs to be improved and the model running power consumption needs to be reduced, the dynamic quantization after training can be used. The following describes how to use dynamic quantization and its effects.</p>
<p>In Dynamic quantization, the weights are quantized at the convert, and the activation are quantized at the runtime. Compared to static quantization, no calibration dataset is required.</p>
<p>The general form of the dynamic quantization conversion command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/dynamic_quant.cfg
</pre></div>
</div>
<p>The dynamic quantization profile is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">DYNAMIC_QUANT</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
</pre></div>
</div>
<blockquote>
<div><p>In order to ensure the quantization accuracy, the dynamic quantization does not support setting the FP16 mode .</p>
<p>The dynamic quantization will have a further acceleration effect on the ARM architecture that supports SDOT instructions.</p>
</div></blockquote>
<div class="section" id="partial-model-performance-results">
<h3>Partial Model Performance Results<a class="headerlink" href="#partial-model-performance-results" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>tinybert_encoder</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model Type</th>
<th>Runtime Mode</th>
<th>Model Size(M)</th>
<th>RAM(K)</th>
<th>Latency(ms)</th>
<th>Cos-Similarity</th>
<th>Compression Ratio</th>
<th>Memory compared to FP32</th>
<th>Latency compared to FP32</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>FP32</td>
<td>20</td>
<td>29,029</td>
<td>9.916</td>
<td>1</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>FP32</td>
<td>FP16</td>
<td>20</td>
<td>18,208</td>
<td>5.75</td>
<td>0.99999</td>
<td>1</td>
<td>-37.28%</td>
<td>-42.01%</td>
</tr>
<tr>
<td>FP16</td>
<td>FP16</td>
<td>12</td>
<td>18,105</td>
<td>5.924</td>
<td>0.99999</td>
<td>1.66667</td>
<td>-37.63%</td>
<td>-40.26%</td>
</tr>
<tr>
<td>Weight Quant(8 Bit)</td>
<td>FP16</td>
<td>5.3</td>
<td>19,324</td>
<td>5.764</td>
<td>0.99994</td>
<td>3.77358</td>
<td>-33.43%</td>
<td>-41.87%</td>
</tr>
<tr>
<td><strong>Dynamic Quant</strong></td>
<td><strong>INT8+FP32</strong></td>
<td><strong>5.2</strong></td>
<td><strong>15,709</strong></td>
<td><strong>4.517</strong></td>
<td><strong>0.99668</strong></td>
<td><strong>3.84615</strong></td>
<td><strong>-45.89%</strong></td>
<td><strong>-54.45%</strong></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>tinybert_decoder</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model Type</th>
<th>Runtime Mode</th>
<th>Model Size(M)</th>
<th>RAM(K)</th>
<th>Latency(ms)</th>
<th>Cos-Similarity</th>
<th>Compression Ratio</th>
<th>Memory compared to FP32</th>
<th>Latency compared to FP32</th>
</tr>
</thead>
<tbody>
<tr>
<td>FP32</td>
<td>FP32</td>
<td>43</td>
<td>51,355</td>
<td>4.161</td>
<td>1</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>FP32</td>
<td>FP16</td>
<td>43</td>
<td>29,462</td>
<td>2.184</td>
<td>0.99999</td>
<td>1</td>
<td>-42.63%</td>
<td>-47.51%</td>
</tr>
<tr>
<td>FP16</td>
<td>FP16</td>
<td>22</td>
<td>29,440</td>
<td>2.264</td>
<td>0.99999</td>
<td>1.95455</td>
<td>-42.67%</td>
<td>-45.59%</td>
</tr>
<tr>
<td>Weight Quant(8 Bit)</td>
<td>FP16</td>
<td>12</td>
<td>32,285</td>
<td>2.307</td>
<td>0.99998</td>
<td>3.58333</td>
<td>-37.13%</td>
<td>-44.56%</td>
</tr>
<tr>
<td><strong>Dynamic Quant</strong></td>
<td><strong>INT8+FP32</strong></td>
<td><strong>12</strong></td>
<td><strong>22,181</strong></td>
<td><strong>2.074</strong></td>
<td><strong>0.9993</strong></td>
<td><strong>3.58333</strong></td>
<td><strong>-56.81%</strong></td>
<td><strong>-50.16%</strong></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="quantization-debug">
<h2>Quantization Debug<a class="headerlink" href="#quantization-debug" title="Permalink to this headline">¶</a></h2>
<p>Turn on the quantization Debug function, you can get the data distribution statistics report, which is used to evaluate the quantization error and assist the decision-making model (operator) whether it is suitable for quantization. For full quantification, N data distribution statistics reports will be generated according to the number of correction datasets provided, that is, one report will be generated for each round; for weighting, only one data distribution statistics report will be generated.</p>
<p>When setting the <code class="docutils literal notranslate"><span class="pre">debug_info_save_path</span></code> parameter, the relevant debug report will be generated in the <code class="docutils literal notranslate"><span class="pre">/home/workspace/mindspore/debug_info_save_path</span></code> folder:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="na">debug_info_save_path</span><span class="o">=</span><span class="s">/home/workspace/mindspore/debug_info_save_path</span>
</pre></div>
</div>
<p>The quantized output summary report <code class="docutils literal notranslate"><span class="pre">output_summary.csv</span></code> contains the accuracy information of the output layer Tensor of the entire network. The related fields are as follows：</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Type</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>Round</td>
<td>The calibration training round</td>
</tr>
<tr>
<td>TensorName</td>
<td>The tensor name</td>
</tr>
<tr>
<td>CosineSimilarity</td>
<td>Cosine similarity compared with the original data</td>
</tr>
</tbody>
</table>
<p>The data distribution statistics report <code class="docutils literal notranslate"><span class="pre">report_*.csv</span></code>will count the original data distribution of each Tensor and the data distribution after dequantization of the quantized Tensor. The relevant fields of the data distribution statistics report are as follows:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Type</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>NodeName</td>
<td>The node name</td>
</tr>
<tr>
<td>NodeType</td>
<td>The node type</td>
</tr>
<tr>
<td>TensorName</td>
<td>The tensor name</td>
</tr>
<tr>
<td>InOutFlag</td>
<td>The input or output tensor</td>
</tr>
<tr>
<td>DataTypeFlag</td>
<td>The data type, use Origin for original model, use Dequant for quantization model</td>
</tr>
<tr>
<td>TensorTypeFlag</td>
<td>The data types such as input and output, use Activation, and constants, etc., use Weight.</td>
</tr>
<tr>
<td>Min</td>
<td>The minimum value</td>
</tr>
<tr>
<td>Q1</td>
<td>The 25% quantile</td>
</tr>
<tr>
<td>Median</td>
<td>The median</td>
</tr>
<tr>
<td>Q3</td>
<td>The 75% quantile</td>
</tr>
<tr>
<td>MAX</td>
<td>The maximum</td>
</tr>
<tr>
<td>Mean</td>
<td>The mean</td>
</tr>
<tr>
<td>Var</td>
<td>The var</td>
</tr>
<tr>
<td>Sparsity</td>
<td>The sparsity</td>
</tr>
<tr>
<td>Clip</td>
<td>The Clip</td>
</tr>
<tr>
<td>CosineSimilarity</td>
<td>Cosine similarity compared with the original data</td>
</tr>
</tbody>
</table>
<p>The quantization parameter report <code class="docutils literal notranslate"><span class="pre">quant_param.csv</span></code> contains the quantization parameter information of all quantized Tensors. The related fields of the quantization parameter are as follows:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Type</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr>
<td>NodeName</td>
<td>The node name</td>
</tr>
<tr>
<td>NodeType</td>
<td>The node type</td>
</tr>
<tr>
<td>TensorName</td>
<td>The tensor name</td>
</tr>
<tr>
<td>ElementsNum</td>
<td>The Tensor elements num</td>
</tr>
<tr>
<td>Dims</td>
<td>The tensor dims</td>
</tr>
<tr>
<td>Scale</td>
<td>The quantization parameter scale</td>
</tr>
<tr>
<td>ZeroPoint</td>
<td>The quantization parameter zeropoint</td>
</tr>
<tr>
<td>Bits</td>
<td>The number of quantization bits</td>
</tr>
<tr>
<td>CorrectionVar</td>
<td>Bias correction coefficient-variance</td>
</tr>
<tr>
<td>CorrectionMean</td>
<td>Bias correction coefficient-mean</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>Mixed bit quantization is non-standard quantization, the quantization parameter file may not exist.</p>
</div></blockquote>
<div class="section" id="skip-quantization-node">
<h3>Skip Quantization Node<a class="headerlink" href="#skip-quantization-node" title="Permalink to this headline">¶</a></h3>
<p>Quantization is to convert the Float32 operator to the Int8 operator. The current quantization strategy is to quantify all the nodes contained in a certain type of operator that can be supported, but there are some nodes that are more sensitive and will cause larger errors after quantization. At the same time, the inference speed of some layers after quantization is much lower than that of Float16. It supports non-quantization of the specified layer, which can effectively improve the accuracy and inference speed.</p>
<p>Below is an example of <code class="docutils literal notranslate"><span class="pre">conv2d_1</span></code> <code class="docutils literal notranslate"><span class="pre">add_8</span></code> <code class="docutils literal notranslate"><span class="pre">concat_1</span></code> without quantifying the three nodes:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">FULL_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support 8bit</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Set the name of the operator that skips the quantization, and use `,` to split between multiple operators.</span>
<span class="na">skip_quant_node</span><span class="o">=</span><span class="s">conv2d_1,add_8,concat_1</span>
</pre></div>
</div>
</div>
<div class="section" id="recommendations">
<h3>Recommendations<a class="headerlink" href="#recommendations" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>By filtering <code class="docutils literal notranslate"><span class="pre">InOutFlag</span> <span class="pre">==</span> <span class="pre">Output</span> <span class="pre">&amp;&amp;</span> <span class="pre">DataTypeFlag</span> <span class="pre">==</span> <span class="pre">Dequant</span></code>, the output layer of all quantization operators can be filtered out, and the accuracy loss of the operator can be judged by looking at the quantized output <code class="docutils literal notranslate"><span class="pre">CosineSimilarity</span></code>, the closer to 1 the smaller the loss.</p></li>
<li><p>For merging operators such as Add and Concat, if the distribution of <code class="docutils literal notranslate"><span class="pre">min</span></code> and <code class="docutils literal notranslate"><span class="pre">max</span></code> between different input Tensors is quite different, which is likely to cause large errors, you can set <code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code> to not quantize them.</p></li>
<li><p>For operators with a higher cutoff rate <code class="docutils literal notranslate"><span class="pre">Clip</span></code>, you can set <code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code> to not quantize it.</p></li>
</ol>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="data_preprocessing.html" class="btn btn-neutral float-right" title="Data Preprocessing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../quick_start/image_segmentation.html" class="btn btn-neutral float-left" title="Android Application Development Based on Java Interface" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>