<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Java Interface to Perform Inference &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Performing Inference or Training on MCU or Small Systems" href="micro.html" />
    <link rel="prev" title="Using C++ Interface to Perform Inference" href="runtime_cpp.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime.html">Executing Model Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="runtime_cpp.html">Using C++ Interface to Perform Inference</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Java Interface to Perform Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#referencing-the-mindspore-lite-java-library">Referencing the MindSpore Lite Java Library</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#linux-x86-project-referencing-the-jar-library">Linux X86 Project Referencing the JAR Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="#android-projects-referencing-the-aar-library">Android Projects Referencing the AAR Library</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#loading-a-model">Loading a Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-a-configuration-context">Creating a Configuration Context</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-cpu-backend">Configuring the CPU Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-gpu-backend">Configuring the GPU Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-npu-backend">Configuring the NPU Backend</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#loading-and-compiling-a-model">Loading and Compiling a Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inputting-data">Inputting Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#executing-inference">Executing Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#obtaining-the-output">Obtaining the Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="#releasing-the-memory">Releasing the Memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-usage">Advanced Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#resizing-the-input-dimension">Resizing the Input Dimension</a></li>
<li class="toctree-l4"><a class="reference internal" href="#viewing-logs">Viewing Logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-the-version-number">Obtaining the Version Number</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime.html">Executing Model Inference</a> &raquo;</li>
      <li>Using Java Interface to Perform Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/runtime_java.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-java-interface-to-perform-inference">
<h1>Using Java Interface to Perform Inference<a class="headerlink" href="#using-java-interface-to-perform-inference" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/master/docs/lite/docs/source_en/use/runtime_java.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/master/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>After the model is converted into a <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model by using the MindSpore Lite model conversion tool, the inference process can be performed in Runtime. For details, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/converter_tool.html">Converting Models for Inference</a>. This tutorial describes how to use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/index.html">Java API</a> to perform inference.</p>
<p>If MindSpore Lite is used in an Android project, you can use <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/index.html">C++ API</a> or <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/index.html">Java API</a> to run the inference framework. Compared with C++ APIs, Java APIs can be directly called in the Java class. Users do not need to implement the code at the JNI layer, which is more convenient. To run the MindSpore Lite inference framework, perform the following steps:</p>
<ol class="arabic simple">
<li><p>Load the model(optional): Read the <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model converted by the model conversion tool introduced in <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/use/converter_tool.html">Converting Models for Inference</a> from the file system.</p></li>
<li><p>Create a configuration context: Create a configuration context <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mscontext.html#mscontext">MSContext</a> to save some basic configuration parameters required by a model to guide graph build and execution, including <code class="docutils literal notranslate"><span class="pre">deviceType</span></code> (device type), <code class="docutils literal notranslate"><span class="pre">threadNum</span></code> (number of threads), <code class="docutils literal notranslate"><span class="pre">cpuBindMode</span></code> (CPU core binding mode), and <code class="docutils literal notranslate"><span class="pre">enable_float16</span></code> (whether to preferentially use the float16 operator).</p></li>
<li><p>Build a graph: Before building a graph, the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#build">build</a> API of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#model">Model</a> needs to be called to build the graph, including graph partition and operator selection and scheduling. This takes a long time. Therefore, it is recommended that with <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#model">Model</a> created each time, one graph be built. In this case, the inference will be performed for multiple times.</p></li>
<li><p>Input data: Before the graph is performed, data needs to be filled in to the <code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">Tensor</span></code>.</p></li>
<li><p>Perform inference: Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#model">Model</a> of the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#predict">predict</a> to perform model inference.</p></li>
<li><p>Obtain the output: After the graph execution is complete, you can obtain the inference result by <code class="docutils literal notranslate"><span class="pre">outputting</span> <span class="pre">the</span> <span class="pre">tensor</span></code>.</p></li>
<li><p>Release the memory: If the MindSpore Lite inference framework is not required, release the created <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#model">Model</a>.</p></li>
</ol>
<p><img alt="img" src="../_images/lite_runtime.png" /></p>
<blockquote>
<div><p>For details about the calling process of MindSpore Lite inference, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/master/quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a>.</p>
</div></blockquote>
</section>
<section id="referencing-the-mindspore-lite-java-library">
<h2>Referencing the MindSpore Lite Java Library<a class="headerlink" href="#referencing-the-mindspore-lite-java-library" title="Permalink to this headline"></a></h2>
<section id="linux-x86-project-referencing-the-jar-library">
<h3>Linux X86 Project Referencing the JAR Library<a class="headerlink" href="#linux-x86-project-referencing-the-jar-library" title="Permalink to this headline"></a></h3>
<p>When using <code class="docutils literal notranslate"><span class="pre">Maven</span></code> as the build tool, you can copy <code class="docutils literal notranslate"><span class="pre">mindspore-lite-java.jar</span></code> to the <code class="docutils literal notranslate"><span class="pre">lib</span></code> directory in the root directory and add the dependency of the JAR package to <code class="docutils literal notranslate"><span class="pre">pom.xml</span></code>.</p>
<div class="highlight-xml notranslate"><div class="highlight"><pre><span></span><span class="nt">&lt;dependencies&gt;</span>
<span class="w">    </span><span class="nt">&lt;dependency&gt;</span>
<span class="w">        </span><span class="nt">&lt;groupId&gt;</span>com.mindspore.lite<span class="nt">&lt;/groupId&gt;</span>
<span class="w">        </span><span class="nt">&lt;artifactId&gt;</span>mindspore-lite-java<span class="nt">&lt;/artifactId&gt;</span>
<span class="w">        </span><span class="nt">&lt;version&gt;</span>1.0<span class="nt">&lt;/version&gt;</span>
<span class="w">        </span><span class="nt">&lt;scope&gt;</span>system<span class="nt">&lt;/scope&gt;</span>
<span class="w">        </span><span class="nt">&lt;systemPath&gt;</span>${project.basedir}/lib/mindspore-lite-java.jar<span class="nt">&lt;/systemPath&gt;</span>
<span class="w">    </span><span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;/dependencies&gt;</span>
</pre></div>
</div>
</section>
<section id="android-projects-referencing-the-aar-library">
<h3>Android Projects Referencing the AAR Library<a class="headerlink" href="#android-projects-referencing-the-aar-library" title="Permalink to this headline"></a></h3>
<p>When <code class="docutils literal notranslate"><span class="pre">Gradle</span></code> is used as the build tool, move the <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}.aar</span></code> file to the <code class="docutils literal notranslate"><span class="pre">libs</span></code> directory of the target module, and then add the local reference directory to <code class="docutils literal notranslate"><span class="pre">repositories</span></code> of <code class="docutils literal notranslate"><span class="pre">build.gradle</span></code> of the target module, add the AAR dependency to <code class="docutils literal notranslate"><span class="pre">dependencies</span></code> as follows:</p>
<blockquote>
<div><p>Note that mindspore-lite-{version} is the AAR file name. Replace {version} with the corresponding version information.</p>
</div></blockquote>
<div class="highlight-groovy notranslate"><div class="highlight"><pre><span></span><span class="n">repositories</span><span class="w"> </span><span class="o">{</span>
<span class="w">    </span><span class="n">flatDir</span><span class="w"> </span><span class="o">{</span>
<span class="w">        </span><span class="n">dirs</span><span class="w"> </span><span class="s1">&#39;libs&#39;</span>
<span class="w">    </span><span class="o">}</span>
<span class="o">}</span>

<span class="n">dependencies</span><span class="w"> </span><span class="o">{</span>
<span class="w">    </span><span class="n">implementation</span><span class="w"> </span><span class="n">fileTree</span><span class="o">(</span><span class="nl">dir:</span><span class="w"> </span><span class="s2">&quot;libs&quot;</span><span class="o">,</span><span class="w"> </span><span class="nl">include:</span><span class="w"> </span><span class="o">[</span><span class="s1">&#39;*.aar&#39;</span><span class="o">])</span>
<span class="o">}</span>
</pre></div>
</div>
</section>
</section>
<section id="loading-a-model">
<h2>Loading a Model<a class="headerlink" href="#loading-a-model" title="Permalink to this headline"></a></h2>
<p>Before performing model inference, MindSpore Lite needs to load the <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model converted by the model conversion tool from the file system and parse the model.ss</p>
<p>The following sample code reads the model file from specified file path.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Load the .ms model.</span>
<span class="n">MappedByteBuffer</span><span class="w"> </span><span class="n">byteBuffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">null</span><span class="p">;</span>
<span class="k">try</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">fc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">RandomAccessFile</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;r&quot;</span><span class="p">).</span><span class="na">getChannel</span><span class="p">();</span>
<span class="w">    </span><span class="n">byteBuffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">fc</span><span class="p">.</span><span class="na">map</span><span class="p">(</span><span class="n">FileChannel</span><span class="p">.</span><span class="na">MapMode</span><span class="p">.</span><span class="na">READ_ONLY</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">fc</span><span class="p">.</span><span class="na">size</span><span class="p">()).</span><span class="na">load</span><span class="p">();</span>
<span class="p">}</span><span class="w"> </span><span class="k">catch</span><span class="w"> </span><span class="p">(</span><span class="n">IOException</span><span class="w"> </span><span class="n">e</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">e</span><span class="p">.</span><span class="na">printStackTrace</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="creating-a-configuration-context">
<h2>Creating a Configuration Context<a class="headerlink" href="#creating-a-configuration-context" title="Permalink to this headline"></a></h2>
<p>Create the configuration context <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mscontext.html#mscontext">MSContext</a> to save some basic configuration parameters required by the session to guide graph build and execution. Configure the number of threads, thread affinity and whether to enable heterogeneous parallel inference via the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mscontext.html#init">init</a> interface. MindSpore Lite has a built-in thread pool shared by processes. During inference, <code class="docutils literal notranslate"><span class="pre">threadNum</span></code> is used to specify the maximum number of threads in the thread pool. The default value is 2.</p>
<p>MindSpore Lite supports heterogeneous inference. The preferred backend for inference is specified by <code class="docutils literal notranslate"><span class="pre">deviceType</span></code> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mscontext.html#adddeviceinfo">AddDeviceInfo</a>. Currently, CPU, GPU and NPU are supported. During graph build, operator selection and scheduling are performed based on the preferred backend.If the backend supports Float16, you can use the Float16 operator first by setting <code class="docutils literal notranslate"><span class="pre">isEnableFloat16</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>. If it is an NPU backend, you can also set the NPU frequency value. The default frequency value is 3, and can be set to 1 (low power consumption), 2 (balanced), 3 (high performance), and 4 (extreme performance).</p>
<section id="configuring-the-cpu-backend">
<h3>Configuring the CPU Backend<a class="headerlink" href="#configuring-the-cpu-backend" title="Permalink to this headline"></a></h3>
<p>If the backend to be performed is a CPU, you need to configure <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mscontext.html#adddeviceinfo">addDeviceInfo</a> after <code class="docutils literal notranslate"><span class="pre">MSContext</span></code> is inited. In addition, the CPU supports the setting of the core binding mode and whether to preferentially use the float16 operator.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/runtime_java/app/src/main/java/com/mindspore/lite/demo/MainActivity.java#L59">MainActivity.java</a> demonstrates how to create a CPU backend, set the CPU core binding mode to large-core priority, and enable float16 inference:</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSContext</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MSContext</span><span class="p">();</span>
<span class="n">context</span><span class="p">.</span><span class="na">init</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">CpuBindMode</span><span class="p">.</span><span class="na">HIGHER_CPU</span><span class="p">);</span>
<span class="n">context</span><span class="p">.</span><span class="na">addDeviceInfo</span><span class="p">(</span><span class="n">DeviceType</span><span class="p">.</span><span class="na">DT_CPU</span><span class="p">,</span><span class="w"> </span><span class="kc">true</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>Float16 takes effect only when the CPU is of the ARM v8.2 architecture. Other models and x86 platforms that are not supported are automatically rolled back to float32.</p>
</div></blockquote>
</section>
<section id="configuring-the-gpu-backend">
<h3>Configuring the GPU Backend<a class="headerlink" href="#configuring-the-gpu-backend" title="Permalink to this headline"></a></h3>
<p>If the backend to be performed is heterogeneous inference based on CPU and GPU, you need to add successively <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/generate/classmindspore_GPUDeviceInfo.html">GPUDeviceInfo</a> and <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/generate/classmindspore_CPUDeviceInfo.html">CPUDeviceInfo</a> when call <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mscontext.html#adddeviceinfo">addDeviceInfo</a>, GPU inference will be used first after configuration. In addition, if enable_float16 is set to true, both the GPU and CPU preferentially use the float16 operator.</p>
<p>The following sample code demonstrates how to create the CPU and GPU heterogeneous inference backend and how to enable float16 inference for the GPU.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSContext</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MSContext</span><span class="p">();</span>
<span class="n">context</span><span class="p">.</span><span class="na">init</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">CpuBindMode</span><span class="p">.</span><span class="na">MID_CPU</span><span class="p">);</span>
<span class="n">context</span><span class="p">.</span><span class="na">addDeviceInfo</span><span class="p">(</span><span class="n">DeviceType</span><span class="p">.</span><span class="na">DT_GPU</span><span class="p">,</span><span class="w"> </span><span class="kc">true</span><span class="p">);</span>
<span class="n">context</span><span class="p">.</span><span class="na">addDeviceInfo</span><span class="p">(</span><span class="n">DeviceType</span><span class="p">.</span><span class="na">DT_CPU</span><span class="p">,</span><span class="w"> </span><span class="kc">true</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>Currently, the GPU can run only on Android mobile devices. Therefore, only the <code class="docutils literal notranslate"><span class="pre">AAR</span></code> library can be run.</p>
</div></blockquote>
</section>
<section id="configuring-the-npu-backend">
<h3>Configuring the NPU Backend<a class="headerlink" href="#configuring-the-npu-backend" title="Permalink to this headline"></a></h3>
<p>If the backend to be performed is heterogeneous inference based on CPU and GPU, you need to add successively <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/generate/classmindspore_KirinNPUDeviceInfo.html">KirinNPUDeviceInfo</a> and <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/generate/classmindspore_CPUDeviceInfo.html">CPUDeviceInfo</a> when call <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mscontext.html#adddeviceinfo">addDeviceInfo</a>, NPU inference will be used first after configuration. In addition, if enable_float16 is set to true, both the NPU and CPU preferentially use the float16 operator.</p>
<p>The following sample code demonstrates how to create the CPU and NPU heterogeneous inference backend and how to enable float16 inference for the NPU.KirinNPUDeviceInfo frequency can be set by <code class="docutils literal notranslate"><span class="pre">NPUFrequency</span></code>.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSContext</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MSContext</span><span class="p">();</span>
<span class="n">context</span><span class="p">.</span><span class="na">init</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">CpuBindMode</span><span class="p">.</span><span class="na">MID_CPU</span><span class="p">);</span>
<span class="n">context</span><span class="p">.</span><span class="na">addDeviceInfo</span><span class="p">(</span><span class="n">DeviceType</span><span class="p">.</span><span class="na">DT_NPU</span><span class="p">,</span><span class="w"> </span><span class="kc">true</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>
<span class="n">context</span><span class="p">.</span><span class="na">addDeviceInfo</span><span class="p">(</span><span class="n">DeviceType</span><span class="p">.</span><span class="na">DT_CPU</span><span class="p">,</span><span class="w"> </span><span class="kc">true</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>
<section id="loading-and-compiling-a-model">
<h2>Loading and Compiling a Model<a class="headerlink" href="#loading-and-compiling-a-model" title="Permalink to this headline"></a></h2>
<p>When using MindSpore Lite to perform inference, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#model">Model</a> is the main entrance of inference, and the model can be realized through Model Loading, model compilation and model execution. Using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mscontext.html#init">MSContext</a> created in the previous step, call the composite <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#build">build</a> interface to implement model loading and model compilation.</p>
<p>The following sample code demonstrates how to call <code class="docutils literal notranslate"><span class="pre">Build</span></code> to  loading and compilation a model.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">Model</span><span class="p">();</span>
<span class="kt">boolean</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">build</span><span class="p">(</span><span class="n">filePath</span><span class="p">,</span><span class="w"> </span><span class="n">ModelType</span><span class="p">.</span><span class="na">MT_MINDIR</span><span class="p">,</span><span class="w"> </span><span class="n">msContext</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="inputting-data">
<h2>Inputting Data<a class="headerlink" href="#inputting-data" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite Java APIs provide the <code class="docutils literal notranslate"><span class="pre">getInputsByTensorName</span></code> and <code class="docutils literal notranslate"><span class="pre">getInputs</span></code> methods to obtain the input tensor. Both the <code class="docutils literal notranslate"><span class="pre">byte[]</span></code> and <code class="docutils literal notranslate"><span class="pre">ByteBuffer</span></code> data types are supported. You can set the data of the input tensor by calling <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mstensor.html#setdata">setData</a>.</p>
<ol class="arabic">
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#getinputsbytensorname">getInputsByTensorName</a> method to obtain the tensor connected to the input node from the model input tensor based on the name of the model input tensor. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/runtime_java/app/src/main/java/com/mindspore/lite/demo/MainActivity.java#L151">MainActivity.java</a> demonstrates how to call the <code class="docutils literal notranslate"><span class="pre">getInputByTensorName</span></code> function to obtain the input tensor and fill in data.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSTensor</span><span class="w"> </span><span class="n">inputTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getInputByTensorName</span><span class="p">(</span><span class="s">&quot;2031_2030_1_construct_wrapper:x&quot;</span><span class="p">);</span>
<span class="c1">// Set Input Data.</span>
<span class="n">inputTensor</span><span class="p">.</span><span class="na">setData</span><span class="p">(</span><span class="n">inputData</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#getinputs">getInputs</a> method to directly obtain the vectors of all model input tensors. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/runtime_java/app/src/main/java/com/mindspore/lite/demo/MainActivity.java#L113">MainActivity.java</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">getInputs</span></code> to obtain the input tensors and fill in the data.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">List</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getInputs</span><span class="p">();</span>
<span class="n">MSTensor</span><span class="w"> </span><span class="n">inputTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="na">get</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// Set Input Data.</span>
<span class="n">inputTensor</span><span class="p">.</span><span class="na">setData</span><span class="p">(</span><span class="n">inputData</span><span class="p">);</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="executing-inference">
<h2>Executing Inference<a class="headerlink" href="#executing-inference" title="Permalink to this headline"></a></h2>
<p>After MindSpore Lite builds a model, it can call the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#predict">predict</a> function of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#model">Model</a> to perform model inference.</p>
<p>The following sample code demonstrates how to call <code class="docutils literal notranslate"><span class="pre">predict</span></code> to perform inference.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Run graph to infer results.</span>
<span class="kt">boolean</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">predict</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="obtaining-the-output">
<h2>Obtaining the Output<a class="headerlink" href="#obtaining-the-output" title="Permalink to this headline"></a></h2>
<p>After performing inference, MindSpore Lite can output a tensor to obtain the inference result. MindSpore Lite provides three methods to obtain the output <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mstensor.html">MSTensor</a> of a model and supports the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mstensor.html#getbytedata">getByteData</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mstensor.html#getfloatdata">getFloatData</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mstensor.html#getintdata">getIntData</a> and <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mstensor.html#getlongdata">getLongData</a> methods to obtain the output data.</p>
<ol class="arabic">
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#getoutputs">getOutputs</a> method to directly obtain the list of all model output <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mstensor.html#mstensor">MSTensor</a>. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/runtime_java/app/src/main/java/com/mindspore/lite/demo/MainActivity.java#L191">MainActivity.java</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">getOutputs</span></code> to obtain the output tensor.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">List</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outTensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getOutputs</span><span class="p">();</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#getoutputsbynodename">getOutputsByNodeName</a> method to obtain the vector of the tensor connected to the model output <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mstensor.html#mstensor">MSTensor</a> based on the name of the model output node. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/runtime_java/app/src/main/java/com/mindspore/lite/demo/MainActivity.java#L175">MainActivity.java</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">getOutputByTensorName</span></code> to obtain the output tensor.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSTensor</span><span class="w"> </span><span class="n">outTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getOutputsByNodeName</span><span class="p">(</span><span class="s">&quot;Default/head-MobileNetV2Head/Softmax-op204&quot;</span><span class="p">);</span>
<span class="c1">// Apply infer results.</span>
<span class="p">...</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#getoutputbytensorname">getOutputByTensorName</a> method to obtain the model output <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/mstensor.html#mstensor">MSTensor</a> based on the name of the model output tensor. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/runtime_java/app/src/main/java/com/mindspore/lite/demo/MainActivity.java#L182">MainActivity.java</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">getOutputByTensorName</span></code> to obtain the output tensor.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSTensor</span><span class="w"> </span><span class="n">outTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getOutputByTensorName</span><span class="p">(</span><span class="s">&quot;Default/head-MobileNetV2Head/Softmax-op204&quot;</span><span class="p">);</span>
<span class="c1">// Apply infer results.</span>
<span class="p">...</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="releasing-the-memory">
<h2>Releasing the Memory<a class="headerlink" href="#releasing-the-memory" title="Permalink to this headline"></a></h2>
<p>If the MindSpore Lite inference framework is not required, you need to release the created Model. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/runtime_java/app/src/main/java/com/mindspore/lite/demo/MainActivity.java#L204">MainActivity.java</a> demonstrates how to release the memory before the program ends.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">.</span><span class="na">free</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline"></a></h2>
<section id="resizing-the-input-dimension">
<h3>Resizing the Input Dimension<a class="headerlink" href="#resizing-the-input-dimension" title="Permalink to this headline"></a></h3>
<p>When using MindSpore Lite for inference, if you need to resize the input shape, you can call the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#resize">resize</a> API of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html">Model</a> to reset the shape of the input tensor after building a model.</p>
<blockquote>
<div><p>Some networks do not support variable dimensions. As a result, an error message is displayed and the model exits unexpectedly. For example, the model contains the MatMul operator, one input tensor of the MatMul operator is the weight, and the other input tensor is the input. If a variable dimension API is called, the input tensor does not match the shape of the weight tensor. As a result, the inference fails.</p>
</div></blockquote>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/runtime_java/app/src/main/java/com/mindspore/lite/demo/MainActivity.java#L164">MainActivity.java</a> demonstrates how to perform <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html#resize">resize</a> on the input tensor of MindSpore Lite:</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">List</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">session</span><span class="p">.</span><span class="na">getInputs</span><span class="p">();</span>
<span class="kt">int</span><span class="o">[][]</span><span class="w"> </span><span class="n">dims</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">300</span><span class="p">,</span><span class="w"> </span><span class="mi">300</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">}};</span>
<span class="n">bool</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">resize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">dims</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="viewing-logs">
<h3>Viewing Logs<a class="headerlink" href="#viewing-logs" title="Permalink to this headline"></a></h3>
<p>If an exception occurs during inference, you can view logs to locate the fault. For the Android platform, use the <code class="docutils literal notranslate"><span class="pre">Logcat</span></code> command line to view the MindSpore Lite inference log information and use <code class="docutils literal notranslate"><span class="pre">MS_LITE</span></code> to filter the log information.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>logcat<span class="w"> </span>-s<span class="w"> </span><span class="s2">&quot;MS_LITE&quot;</span>
</pre></div>
</div>
</section>
<section id="obtaining-the-version-number">
<h3>Obtaining the Version Number<a class="headerlink" href="#obtaining-the-version-number" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite provides the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/master/api_java/model.html">Version</a> method to obtain the version number, which is included in the <code class="docutils literal notranslate"><span class="pre">com.mindspore.lite.Version</span></code> header file. You can call this method to obtain the version number of MindSpore Lite.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/master/mindspore/lite/examples/runtime_java/app/src/main/java/com/mindspore/lite/demo/MainActivity.java#L215">MainActivity.java</a> demonstrates how to obtain the version number of MindSpore Lite:</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">com.mindspore.lite.config.Version</span><span class="p">;</span>
<span class="n">String</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Version</span><span class="p">.</span><span class="na">version</span><span class="p">();</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_cpp.html" class="btn btn-neutral float-left" title="Using C++ Interface to Perform Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="micro.html" class="btn btn-neutral float-right" title="Performing Inference or Training on MCU or Small Systems" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>