

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Troubleshooting &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/js/lite.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Log" href="log.html" />
    <link rel="prev" title="Scene Detection Model" href="scene_detection_lite.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="use/downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="quick_start/one_hour_introduction.html">Getting Started in One Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_start_server_inference_cpp.html">Experience C++ Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_start_server_inference_java.html">Experience Java Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/quick_start_c.html">Expriencing Simpcified Inference Demo with C-language</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="use/converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/micro.html">Performing Inference on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="use/converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Server Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="use/runtime_server_inference.html">Executing Server Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="use/register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="use/benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="use/obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_lite.html">Model List</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Troubleshooting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#failed-to-convert-a-model">Failed to Convert a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#post-training-quantization-conversion-failed">Post-training Quantization Conversion Failed</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#full-quantization-conversion-failed">Full Quantization Conversion Failed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-inference-failed">Model Inference Failed</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#failed-to-load-the-graph">Failed to Load the Graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cpu-inference-issues">CPU Inference Issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#failed-to-build-a-graph">Failed to Build a Graph</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#opencl-gpu-inference-issues">OpenCL GPU Inference Issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#failed-to-build-a-graph-1">Failed to Build a Graph</a></li>
<li class="toctree-l4"><a class="reference internal" href="#failed-to-execute-a-graph">Failed to Execute a Graph</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tensorrt-gpu-inference-issues">TensorRT GPU Inference Issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#failed-to-build-a-graph-2">Failed to Build a Graph</a></li>
<li class="toctree-l4"><a class="reference internal" href="#failed-to-execute-a-graph-1">Failed to Execute a Graph</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#npu-inference-issues">NPU Inference Issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#failed-to-build-a-graph-3">Failed to Build a Graph</a></li>
<li class="toctree-l4"><a class="reference internal" href="#failed-to-execute-a-graph-2">Failed to Execute a Graph</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-inference-accuracy-issues">Model Inference Accuracy Issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-inference-performance-issues">Model Inference Performance Issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="#issues-related-to-using-visual-studio">Issues Related to Using Visual Studio</a></li>
<li class="toctree-l2"><a class="reference internal" href="#issues-related-to-app-building-using-xcode">Issues Related to App Building Using Xcode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#other-issues">Other Issues</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Troubleshooting</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/troubleshooting_guide.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="troubleshooting">
<h1>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.10/docs/lite/docs/source_en/troubleshooting_guide.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.10/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>If you encounter an issue when using MindSpore Lite, you can view logs first. In most scenarios, you can locate the issue based on the error information reported in logs. You can set the environment variable <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r1.10/debug/custom_debug.html#log-related-environment-variables-and-configurations">GLOG_v</a> to adjust the log level to print more debug logs. The following describes how to locate and rectify common faults.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>The log line number may vary in different versions. In the following example, the line number in the error log information is represented by “**”.</p></li>
<li><p>Only common information is listed in the example logs. Other information related to specific scenarios is displayed as “****”.</p></li>
</ol>
</div></blockquote>
</section>
<section id="failed-to-convert-a-model">
<h2>Failed to Convert a Model<a class="headerlink" href="#failed-to-convert-a-model" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>The model path is incorrect or the file is damaged. The error log information is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="nl">WARNING</span><span class="p">:</span><span class="w"> </span><span class="n">Logging</span><span class="w"> </span><span class="n">before</span><span class="w"> </span><span class="n">InitGoogleLogging</span><span class="p">()</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">written</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">STDERR</span>
<span class="p">[</span><span class="n">WARNING</span><span class="p">]</span><span class="w"> </span><span class="n">LITE</span><span class="p">(</span><span class="mi">11979</span><span class="p">,</span><span class="mf">7f</span><span class="n">bdc90a8ec0</span><span class="p">,</span><span class="n">converter_lite</span><span class="p">)</span><span class="o">:</span><span class="mi">2021-12-13-16</span><span class="o">:</span><span class="mi">20</span><span class="o">:</span><span class="mf">49.506.071</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">common</span><span class="o">/</span><span class="n">protobuf_utils</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">94</span><span class="p">]</span><span class="w"> </span><span class="n">ReadProtoFromBinaryFile</span><span class="p">]</span><span class="w"> </span><span class="n">Parse</span><span class="w"> </span><span class="o">***</span><span class="p">.</span><span class="n">onnx</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
<span class="p">[</span><span class="n">ERROR</span><span class="p">]</span><span class="w"> </span><span class="n">LITE</span><span class="p">(</span><span class="mi">11979</span><span class="p">,</span><span class="mf">7f</span><span class="n">bdc90a8ec0</span><span class="p">,</span><span class="n">converter_lite</span><span class="p">)</span><span class="o">:</span><span class="mi">2021-12-13-16</span><span class="o">:</span><span class="mi">20</span><span class="o">:</span><span class="mf">49.506.122</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">build</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">parser</span><span class="o">/</span><span class="n">onnx</span><span class="o">/</span><span class="n">onnx_op_parser</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">3079</span><span class="p">]</span><span class="w"> </span><span class="n">InitOriginModel</span><span class="p">]</span><span class="w"> </span><span class="n">Read</span><span class="w"> </span><span class="n">onnx</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">failed</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">path</span><span class="o">:</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">ml_audio_kit_vocals_resunet</span><span class="p">.</span><span class="n">onnx</span>
<span class="p">[</span><span class="n">ERROR</span><span class="p">]</span><span class="w"> </span><span class="n">LITE</span><span class="p">(</span><span class="mi">11979</span><span class="p">,</span><span class="mf">7f</span><span class="n">bdc90a8ec0</span><span class="p">,</span><span class="n">converter_lite</span><span class="p">)</span><span class="o">:</span><span class="mi">2021-12-13-16</span><span class="o">:</span><span class="mi">20</span><span class="o">:</span><span class="mf">49.506.131</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">build</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">parser</span><span class="o">/</span><span class="n">onnx</span><span class="o">/</span><span class="n">onnx_op_parser</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">3026</span><span class="p">]</span><span class="w"> </span><span class="n">Parse</span><span class="p">]</span><span class="w"> </span><span class="n">init</span><span class="w"> </span><span class="n">origin</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
<span class="p">[</span><span class="n">ERROR</span><span class="p">]</span><span class="w"> </span><span class="n">LITE</span><span class="p">(</span><span class="mi">11979</span><span class="p">,</span><span class="mf">7f</span><span class="n">bdc90a8ec0</span><span class="p">,</span><span class="n">converter_lite</span><span class="p">)</span><span class="o">:</span><span class="mi">2021-12-13-16</span><span class="o">:</span><span class="mi">20</span><span class="o">:</span><span class="mf">49.506.137</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">converter</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">64</span><span class="p">]</span><span class="w"> </span><span class="n">BuildFuncGraph</span><span class="p">]</span><span class="w"> </span><span class="n">Get</span><span class="w"> </span><span class="n">funcGraph</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">fmk</span><span class="o">:</span><span class="w"> </span><span class="n">ONNX</span>
<span class="p">[</span><span class="n">ERROR</span><span class="p">]</span><span class="w"> </span><span class="n">LITE</span><span class="p">(</span><span class="mi">11979</span><span class="p">,</span><span class="mf">7f</span><span class="n">bdc90a8ec0</span><span class="p">,</span><span class="n">converter_lite</span><span class="p">)</span><span class="o">:</span><span class="mi">2021-12-13-16</span><span class="o">:</span><span class="mi">20</span><span class="o">:</span><span class="mf">49.506.143</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">converter</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">133</span><span class="p">]</span><span class="w"> </span><span class="n">Convert</span><span class="p">]</span><span class="w"> </span><span class="n">Parser</span><span class="o">/</span><span class="n">Import</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span>
<span class="p">[</span><span class="n">ERROR</span><span class="p">]</span><span class="w"> </span><span class="n">LITE</span><span class="p">(</span><span class="mi">11979</span><span class="p">,</span><span class="mf">7f</span><span class="n">bdc90a8ec0</span><span class="p">,</span><span class="n">converter_lite</span><span class="p">)</span><span class="o">:</span><span class="mi">2021-12-13-16</span><span class="o">:</span><span class="mi">20</span><span class="o">:</span><span class="mf">49.506.162</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">converter</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">209</span><span class="p">]</span><span class="w"> </span><span class="n">RunConverter</span><span class="p">]</span><span class="w"> </span><span class="n">CONVERT</span><span class="w"> </span><span class="n">RESULT</span><span class="w"> </span><span class="n">FAILED</span><span class="o">:</span><span class="mi">-1</span><span class="w"> </span><span class="n">Common</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">code</span><span class="p">.</span>
<span class="n">CONVERT</span><span class="w"> </span><span class="n">RESULT</span><span class="w"> </span><span class="n">FAILED</span><span class="o">:</span><span class="mi">-1</span><span class="w"> </span><span class="n">Common</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">code</span><span class="p">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: According to the error information, an error is reported during model import and the model import exits before the conversion process starts.</p></li>
<li><p>Solution: Check whether the model path in the command entered during model conversion is correct. If the path is correct, check whether the model is damaged. If the damaged file cannot be parsed, the system exits.</p></li>
</ul>
</li>
<li><p>Unsupported operators exist. The error log information is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">converter</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">BuildFuncGraph</span><span class="p">]</span><span class="w"> </span><span class="n">Get</span><span class="w"> </span><span class="n">funcGraph</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">fmk</span><span class="o">:</span><span class="w"> </span><span class="o">****</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">converter</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">Converter</span><span class="p">]</span><span class="w"> </span><span class="n">Parser</span><span class="o">/</span><span class="n">Import</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">converter_context</span><span class="p">.</span><span class="n">h</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">PrintOps</span><span class="p">]</span><span class="w"> </span><span class="o">===========================================</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">converter_context</span><span class="p">.</span><span class="n">h</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">PrintOps</span><span class="p">]</span><span class="w"> </span><span class="n">UNSUPPORTED</span><span class="w"> </span><span class="n">OP</span><span class="w"> </span><span class="n">LIST</span><span class="o">:</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">converter_context</span><span class="p">.</span><span class="n">h</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">PrintOps</span><span class="p">]</span><span class="w"> </span><span class="n">FMKTYPE</span><span class="o">:</span><span class="w"> </span><span class="o">****</span><span class="p">,</span><span class="w"> </span><span class="n">OP</span><span class="w"> </span><span class="n">TYPE</span><span class="o">:</span><span class="w"> </span><span class="o">****</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">converter_context</span><span class="p">.</span><span class="n">h</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">PrintOps</span><span class="p">]</span><span class="w"> </span><span class="o">===========================================</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">converter</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">RunConverter</span><span class="p">]</span><span class="w"> </span><span class="n">CONVERT</span><span class="w"> </span><span class="n">RESULT</span><span class="w"> </span><span class="n">FAILED</span><span class="o">:</span><span class="mi">-300</span><span class="w"> </span><span class="n">Failed</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="k">operator</span><span class="p">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: The model contains operators not supported by the MindSpore Lite converter. As a result, the conversion fails.</p></li>
<li><p>Solution: For unsupported operators, add parsers by inheriting the API <a class="reference external" href="https://mindspore.cn/lite/api/en/r1.10/generate/classmindspore_converter_NodeParser.html">NodeParser</a> and register the parsers by using <a class="reference external" href="https://mindspore.cn/lite/api/en/r1.10/generate/classmindspore_registry_NodeParserRegistry.html">NodeParserRegistry</a>. Alternatively, commit an <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">issue</a> to MindSpore Lite developers in the community.</p></li>
</ul>
</li>
<li><p>Unsupported operators exist. The error log information is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">parser</span><span class="o">/</span><span class="n">caffe</span><span class="o">/</span><span class="n">caffe_model_parser</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">ConvertLayers</span><span class="p">]</span><span class="w"> </span><span class="n">parse</span><span class="w"> </span><span class="n">node</span><span class="w"> </span><span class="o">****</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: The converter supports the operator conversion, but does not support a special attribute or parameter of the operator. As a result, the model conversion fails. (The following uses Caffe as an example. The log information of other frameworks is the same.)</p></li>
<li><p>Solution: Add the custom operator parsers by inheriting the API <a class="reference external" href="https://mindspore.cn/lite/api/en/r1.10/generate/classmindspore_converter_NodeParser.html">NodeParser</a> and register the parsers by using <a class="reference external" href="https://mindspore.cn/lite/api/en/r1.10/generate/classmindspore_registry_NodeParserRegistry.html">NodeParserRegistry</a>. Alternatively, commit an <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">issue</a> to MindSpore Lite developers in the community.</p></li>
</ul>
</li>
</ol>
</section>
<section id="post-training-quantization-conversion-failed">
<h2>Post-training Quantization Conversion Failed<a class="headerlink" href="#post-training-quantization-conversion-failed" title="Permalink to this headline"></a></h2>
<section id="full-quantization-conversion-failed">
<h3>Full Quantization Conversion Failed<a class="headerlink" href="#full-quantization-conversion-failed" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>For dynamic shape models, you need to set <code class="docutils literal notranslate"><span class="pre">--inputShape=&lt;INPUTSHAPE&gt;</span></code> listed in the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.10/use/converter_tool.html#parameter-description">Parameter Description</a>. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">converter_lite</span> <span class="o">--</span><span class="n">fmk</span><span class="o">=</span><span class="n">ModelType</span> <span class="o">--</span><span class="n">modelFile</span><span class="o">=</span><span class="n">ModelFilePath</span> <span class="o">--</span><span class="n">outputFile</span><span class="o">=</span><span class="n">ConvertedModelPath</span> <span class="o">--</span><span class="n">configFile</span><span class="o">=/</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">converter</span><span class="o">/</span><span class="n">quantizer</span><span class="o">/</span><span class="n">config</span><span class="o">/</span><span class="n">full_quant</span><span class="o">.</span><span class="n">cfg</span> <span class="o">--</span><span class="n">inputShape</span><span class="o">=</span><span class="n">intput_1</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">3</span><span class="p">;</span><span class="n">intput_2</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="mi">48</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>For a multi-batch model, the data preprocessing function cannot be directly used. You need to preprocess the calibration dataset in advance and then set the calibration dataset in the <code class="docutils literal notranslate"><span class="pre">BIN</span></code> format.</p></li>
</ol>
</section>
</section>
<section id="model-inference-failed">
<h2>Model Inference Failed<a class="headerlink" href="#model-inference-failed" title="Permalink to this headline"></a></h2>
<section id="failed-to-load-the-graph">
<h3>Failed to Load the Graph<a class="headerlink" href="#failed-to-load-the-graph" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>The model file is incorrect. The error log information is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_model</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">ConstructModel</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">invalid</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">fail</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">create</span><span class="w"> </span><span class="n">graph</span><span class="p">.</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_model</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">ImportFromBuffer</span><span class="p">]</span><span class="w"> </span><span class="n">construct</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: The cached content read from the MS model file is invalid. As a result, the graph fails to be loaded.</p></li>
<li><p>Solution: Ensure that the model used for inference is the MS model file converted by the converter. If the model file is transferred or downloaded, check the MD5 value to determine whether the MS model file is damaged.</p></li>
</ul>
</li>
<li><p>The model file is incompatible with the inference package version. The following error information is displayed in logs:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_model</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">ConstructModel</span><span class="p">]</span><span class="w"> </span><span class="n">Maybe</span><span class="w"> </span><span class="k">this</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">transferred</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="k">using</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">conversion</span><span class="w"> </span><span class="n">tool</span><span class="w"> </span><span class="n">before</span><span class="w"> </span><span class="mf">1.1.0</span><span class="p">.</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_model</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">ImportFromBuffer</span><span class="p">]</span><span class="w"> </span><span class="n">construct</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: The version of the converter used by the MS model file is too early. As a result, the graph fails to be loaded.</p></li>
<li><p>Solution: Use MindSpore Lite 1.1.0 or later to convert the MS model again.</p></li>
</ul>
</li>
</ol>
</section>
<section id="cpu-inference-issues">
<h3>CPU Inference Issues<a class="headerlink" href="#cpu-inference-issues" title="Permalink to this headline"></a></h3>
<section id="failed-to-build-a-graph">
<h4>Failed to Build a Graph<a class="headerlink" href="#failed-to-build-a-graph" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>The model file is incompatible with the inference package version. The following error information is displayed in logs:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">WARNING</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_model</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">ConstructModel</span><span class="p">]</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">MindSpore</span><span class="w"> </span><span class="n">Lite</span><span class="w"> </span><span class="mf">1.2.0</span><span class="p">,</span><span class="w"> </span><span class="n">inference</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">MindSpore</span><span class="w"> </span><span class="n">Lite</span><span class="w"> </span><span class="mf">1.5.0</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">equal</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">litert</span><span class="o">/</span><span class="n">infer_manager</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">KernelInferShape</span><span class="p">]</span><span class="w"> </span><span class="n">Get</span><span class="w"> </span><span class="n">infershape</span><span class="w"> </span><span class="n">func</span><span class="w"> </span><span class="n">failed</span><span class="o">!</span><span class="w"> </span><span class="n">type</span><span class="o">:</span><span class="w"> </span><span class="o">****</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">scheduler</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">ScheduleNodeToKernel</span><span class="p">]</span><span class="w"> </span><span class="n">FindBackendKernel</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="o">****</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">:</span><span class="w"> </span><span class="o">****</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">scheduler</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">ScheduleSubGraphToKernels</span><span class="p">]</span><span class="w"> </span><span class="n">schedule</span><span class="w"> </span><span class="n">node</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="o">****</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">:</span><span class="w"> </span><span class="o">****</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">scheduler</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">ScheduleMainSubGraphToKernels</span><span class="p">]</span><span class="w"> </span><span class="n">Schedule</span><span class="w"> </span><span class="n">subgraph</span><span class="w"> </span><span class="n">failed</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="o">:</span><span class="w"> </span><span class="mi">0</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">scheduler</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">ScheduleGraphToKernels</span><span class="p">]</span><span class="w"> </span><span class="n">ScheduleSubGraphToSubGraphKernel</span><span class="w"> </span><span class="n">failed</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">scheduler</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">Schedule</span><span class="p">]</span><span class="w"> </span><span class="n">Schedule</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">kernels</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_session</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">CompileGraph</span><span class="p">]</span><span class="w"> </span><span class="n">Schedule</span><span class="w"> </span><span class="n">kernels</span><span class="w"> </span><span class="n">failed</span><span class="o">:</span><span class="w"> </span><span class="mf">-1.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: The version of MindSpore Lite used for inference is later than that of the converter used for model conversion. As a result, compatibility issues occur. Some operators may be added or removed during the version upgrade, and the operator implementation is missing during inference.</p></li>
<li><p>Solution: Use MindSpore Lite of the same version as the converter during inference. Generally, MindSpore Lite inference is compatible with MS models of earlier versions, but compatibility issues may occur if the versions differ greatly. In addition, MindSpore Lite inference does not guarantee backward compatibility with MS models converted from later versions.</p></li>
</ul>
</li>
<li><p>The model input is a dynamic shape. The following error information is displayed in logs:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">common</span><span class="o">/</span><span class="n">tensor_util</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">CheckTensorsInvalid</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="n">contains</span><span class="w"> </span><span class="n">negative</span><span class="w"> </span><span class="n">dimension</span><span class="p">,</span><span class="w"> </span><span class="n">check</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">assign</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="n">Resize</span><span class="p">().</span>
<span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_session</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">RunGraph</span><span class="p">]</span><span class="w"> </span><span class="n">CheckInputs</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: The input shape of the MS model contains -1, that is, the model input is a dynamic shape. During direct inference, the shape is invalid. As a result, the inference fails.</p></li>
<li><p>Solution: MindSpore Lite requires that a proper shape be specified for a model that contains dynamic shape input during inference. When using the benchmark tool, set the <a class="reference external" href="https://mindspore.cn/lite/docs/en/r1.10/use/benchmark_tool.html#parameter-description">inputShapes</a> parameter. When using MindSpore Lite for integration and development, call the <a class="reference external" href="https://mindspore.cn/lite/api/en/r1.10/generate/classmindspore_ops_Resize.html">Resize</a> method to set the shape.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="opencl-gpu-inference-issues">
<h3>OpenCL GPU Inference Issues<a class="headerlink" href="#opencl-gpu-inference-issues" title="Permalink to this headline"></a></h3>
<section id="failed-to-build-a-graph-1">
<h4>Failed to Build a Graph<a class="headerlink" href="#failed-to-build-a-graph-1" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>The model file is incompatible with the inference package version. The following error information is displayed in logs:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_session</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">1539</span><span class="p">]</span><span class="w"> </span><span class="n">LoadModelByBuff</span><span class="p">]</span><span class="w"> </span><span class="n">Please</span><span class="w"> </span><span class="n">enable</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">convert</span><span class="p">.</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_session</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">1598</span><span class="p">]</span><span class="w"> </span><span class="n">LoadModelAndCompileByPath</span><span class="p">]</span><span class="w"> </span><span class="n">Read</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="n">failed</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">cxx_api</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">model_impl</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">93</span><span class="p">]</span><span class="w"> </span><span class="n">Build</span><span class="p">]</span><span class="w"> </span><span class="n">Init</span><span class="w"> </span><span class="n">session</span><span class="w"> </span><span class="n">failed</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">benchmark</span><span class="o">/</span><span class="n">benchmark_unified_api</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">845</span><span class="p">]</span><span class="w"> </span><span class="n">RunBenchmark</span><span class="p">]</span><span class="w"> </span><span class="n">ms_model_</span><span class="p">.</span><span class="n">Build</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">running</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">benchmark</span><span class="o">/</span><span class="n">run_benchmark</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">80</span><span class="p">]</span><span class="w"> </span><span class="n">RunBenchmark</span><span class="p">]</span><span class="w"> </span><span class="n">Run</span><span class="w"> </span><span class="n">Benchmark</span><span class="w"> </span><span class="n">Q888_CV_new_detect</span><span class="p">.</span><span class="n">pb</span><span class="p">.</span><span class="n">ms</span><span class="w"> </span><span class="n">Failed</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">-1</span>
<span class="n">ms_model_</span><span class="p">.</span><span class="n">Build</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">Run</span><span class="w"> </span><span class="n">Benchmark</span><span class="w"> </span><span class="n">Q888_CV_new_detect</span><span class="p">.</span><span class="n">pb</span><span class="p">.</span><span class="n">ms</span><span class="w"> </span><span class="n">Failed</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">-1</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: The converter is not used to convert the original model, or the version of MindSpore Lite used for inference is later than that of the converter used for model conversion. As a result, compatibility issues occur. Some operators may be added or removed during the version upgrade, and the operator implementation is missing during inference.</p></li>
<li><p>Solution: Use MindSpore Lite of the same version as the converter during inference. Generally, MindSpore Lite inference is compatible with MS models of earlier versions, but compatibility issues may occur if the versions differ greatly. In addition, MindSpore Lite inference does not guarantee backward compatibility with MS models converted from later versions.</p></li>
</ul>
</li>
</ol>
</section>
<section id="failed-to-execute-a-graph">
<h4>Failed to Execute a Graph<a class="headerlink" href="#failed-to-execute-a-graph" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>The model input is a dynamic shape. The following error information is displayed in logs:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">WARNING</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">litert</span><span class="o">/</span><span class="n">kernel</span><span class="o">/</span><span class="n">opencl</span><span class="o">/</span><span class="n">kernel</span><span class="o">/</span><span class="n">arithmetic_self</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">40</span><span class="p">]</span><span class="w"> </span><span class="n">CheckSpecs</span><span class="p">]</span><span class="w">  </span><span class="n">only</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">litert</span><span class="o">/</span><span class="n">kernel</span><span class="o">/</span><span class="n">opencl</span><span class="o">/</span><span class="n">opencl_kernel</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">222</span><span class="p">]</span><span class="w"> </span><span class="n">ReSize</span><span class="p">]</span><span class="w"> </span><span class="n">ReSize</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">check</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="n">specs</span><span class="o">!</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">inner_kernel</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">81</span><span class="p">]</span><span class="w"> </span><span class="n">Execute</span><span class="p">]</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="n">PreProcess</span><span class="w"> </span><span class="n">failed</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="n">Exp_1234</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">litert</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">opencl</span><span class="o">/</span><span class="n">opencl_executor</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">70</span><span class="p">]</span><span class="w"> </span><span class="n">RunOrTune</span><span class="p">]</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="n">failed</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="n">Exp_1234</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">litert</span><span class="o">/</span><span class="n">kernel</span><span class="o">/</span><span class="n">opencl</span><span class="o">/</span><span class="n">opencl_subgraph</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">574</span><span class="p">]</span><span class="w"> </span><span class="n">Execute</span><span class="p">]</span><span class="w"> </span><span class="n">Run</span><span class="w"> </span><span class="n">opencl</span><span class="w"> </span><span class="n">executor</span><span class="w"> </span><span class="n">failed</span><span class="o">:</span><span class="w"> </span><span class="mi">-1</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_mindrt</span><span class="p">.</span><span class="n">h</span><span class="o">:</span><span class="mi">58</span><span class="p">]</span><span class="w"> </span><span class="n">RunKernel</span><span class="p">]</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="n">failed</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="n">GpuSubGraph4_8</span>
<span class="n">WARNING</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">litert</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">opencl</span><span class="o">/</span><span class="n">opencl_allocator</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">475</span><span class="p">]</span><span class="w"> </span><span class="n">MapBuffer</span><span class="p">]</span><span class="w"> </span><span class="n">Host</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="n">map</span>
<span class="n">WARNING</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">litert</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">opencl</span><span class="o">/</span><span class="n">opencl_allocator</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">525</span><span class="p">]</span><span class="w"> </span><span class="n">UnmapBuffer</span><span class="p">]</span><span class="w"> </span><span class="n">Host</span><span class="w"> </span><span class="n">ptr</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">mapped</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: The input shape of the MS model contains -1, that is, the model input is a dynamic shape. During GPU inference, the operator specifications check related to the shape is skipped in the graph build phase. By default, the GPU supports this operator, and the operator specifications are checked again in the prediction phase. If the operator specifications are not supported, an error is reported and the execution exits.</p></li>
<li><p>Solution: Some operators are not supported. You can modify the operator types or parameter types in the model as prompted to avoid some errors. In most cases, you need to <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">commit an issue</a> in the MindSpore community to notify developers to fix and adapt the code.</p></li>
</ul>
</li>
<li><p>Map buffer errors</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">WARNING</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">litert</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">opencl</span><span class="o">/</span><span class="n">opencl_allocator</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">494</span><span class="p">]</span><span class="w"> </span><span class="n">MapBuffer</span><span class="p">]</span><span class="w"> </span><span class="n">Map</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="n">failed</span><span class="p">,</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">found</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="n">or</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">mapped</span><span class="p">,</span><span class="w"> </span><span class="n">dev_ptr</span><span class="o">=</span><span class="mh">0x7244929ff0</span><span class="p">,</span><span class="w"> </span><span class="n">host_ptr</span><span class="o">=</span><span class="mh">0x722fbacd80</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">litert</span><span class="o">/</span><span class="n">kernel</span><span class="o">/</span><span class="n">arm</span><span class="o">/</span><span class="n">base</span><span class="o">/</span><span class="n">strided_slice</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">179</span><span class="p">]</span><span class="w"> </span><span class="n">FastRun</span><span class="p">]</span><span class="w"> </span><span class="n">input_ptr_</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">null</span><span class="o">!</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">inner_kernel</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">88</span><span class="p">]</span><span class="w"> </span><span class="n">Execute</span><span class="p">]</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="n">failed</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="n">Slice_1147</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">sub_graph_kernel</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">223</span><span class="p">]</span><span class="w"> </span><span class="n">Execute</span><span class="p">]</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="n">failed</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="n">Slice_1147</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_mindrt</span><span class="p">.</span><span class="n">h</span><span class="o">:</span><span class="mi">56</span><span class="p">]</span><span class="w"> </span><span class="n">RunKernel</span><span class="p">]</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="n">failed</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="n">CpuFP32SubGraph0_1</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">mindrt_executor</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">193</span><span class="p">]</span><span class="w"> </span><span class="n">Run</span><span class="p">]</span><span class="w"> </span><span class="n">MindrtRun</span><span class="w"> </span><span class="n">failed</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_session</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">709</span><span class="p">]</span><span class="w"> </span><span class="n">RunGraph</span><span class="p">]</span><span class="w"> </span><span class="n">RunGraph</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">-1</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">cxx_api</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="n">model_impl</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">294</span><span class="p">]</span><span class="w"> </span><span class="n">Predict</span><span class="p">]</span><span class="w"> </span><span class="n">Run</span><span class="w"> </span><span class="n">graph</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">benchmark</span><span class="o">/</span><span class="n">benchmark_unified_api</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">721</span><span class="p">]</span><span class="w"> </span><span class="n">MarkAccuracy</span><span class="p">]</span><span class="w"> </span><span class="n">Inference</span><span class="w"> </span><span class="n">error</span>
<span class="n">Inference</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="n">Run</span><span class="w"> </span><span class="n">MarkAccuracy</span><span class="w"> </span><span class="n">error</span><span class="o">:</span><span class="w"> </span><span class="mi">-1</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: In the inference phase, the event check after the OpenCL operator is executed is ignored to improve the performance. However, the event check is inserted into the Enqueue class function in the OpenCL by default. If an error occurs during the execution of the OpenCL operator, an error is returned in the map phase.</p></li>
<li><p>Solution: The OpenCL operator has a bug. You are advised to <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">commit an issue</a> in the MindSpore community to notify developers to fix and adapt the code.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="tensorrt-gpu-inference-issues">
<h3>TensorRT GPU Inference Issues<a class="headerlink" href="#tensorrt-gpu-inference-issues" title="Permalink to this headline"></a></h3>
<section id="failed-to-build-a-graph-2">
<h4>Failed to Build a Graph<a class="headerlink" href="#failed-to-build-a-graph-2" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>If the model input is a dynamic shape or the model has a shape operator, the following error information about dimensions is displayed in the log:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">tensorrt</span><span class="o">/</span><span class="n">tensorrt_runtime</span><span class="p">.</span><span class="n">h</span><span class="o">:</span><span class="mi">31</span><span class="p">]</span><span class="w"> </span><span class="n">log</span><span class="p">]</span><span class="w"> </span><span class="n">Parameter</span><span class="w"> </span><span class="n">check</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="n">at</span><span class="o">:</span><span class="w"> </span><span class="n">optimizationProfile</span><span class="p">.</span><span class="n">cpp</span><span class="o">::</span><span class="n">setDimensions</span><span class="o">::</span><span class="mi">119</span><span class="p">,</span><span class="w"> </span><span class="n">condition</span><span class="o">:</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">all_of</span><span class="p">(</span><span class="n">dims</span><span class="p">.</span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="n">dims</span><span class="p">.</span><span class="n">d</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">dims</span><span class="p">.</span><span class="n">nbDims</span><span class="p">,</span><span class="w"> </span><span class="p">[](</span><span class="kt">int</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="p">})</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">tensorrt</span><span class="o">/</span><span class="n">tensorrt_subgraph</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">219</span><span class="p">]</span><span class="w"> </span><span class="n">ParseInputDimsProfile</span><span class="p">]</span><span class="w"> </span><span class="n">setDimensions</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">kMIN</span><span class="w"> </span><span class="n">failed</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">input</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">tensorrt</span><span class="o">/</span><span class="n">tensorrt_runtime</span><span class="p">.</span><span class="n">h</span><span class="o">:</span><span class="mi">31</span><span class="p">]</span><span class="w"> </span><span class="n">log</span><span class="p">]</span><span class="w"> </span><span class="n">xxx</span><span class="o">:</span><span class="w"> </span><span class="n">xxx</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">negative</span><span class="w"> </span><span class="n">dimension</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="mi">-1</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: TensorRT GPU graph construction does not support models with dynamic shapes. Specifically, the input shape of the model contains –1 or the model contains the shape operator.</p></li>
<li><p>Solution: When using the converter to convert the model to MS, set <code class="docutils literal notranslate"><span class="pre">--inputShape=&lt;INPUTSHAPE&gt;</span></code> in the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.10/use/converter_tool.html#parameter-description">conversion command</a> to specify the shape information of the input tensor. If you need to change the input shape during inference, you can set the <a class="reference external" href="https://mindspore.cn/lite/docs/en/r1.10/use/benchmark_tool.html#parameter-description">inputShapes</a> parameter when using the benchmark tool or call the <a class="reference external" href="https://mindspore.cn/lite/api/en/r1.10/generate/classmindspore_ops_Resize.html">Resize</a> method when using MindSpore Lite for integration and development. Note: The shape dimension of the <a class="reference external" href="https://mindspore.cn/lite/api/en/r1.10/generate/classmindspore_ops_Resize.html">Resize</a> input must be less than or equal to the dimension of the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.10/api_java/model.html#build">Build</a> model.</p></li>
</ul>
</li>
</ol>
</section>
<section id="failed-to-execute-a-graph-1">
<h4>Failed to Execute a Graph<a class="headerlink" href="#failed-to-execute-a-graph-1" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Offline broadcast operators do not support resizing. An error message is displayed, indicating that the input dimension of an operator does not match. For example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">tensorrt</span><span class="o">/</span><span class="n">tensorrt_runtime</span><span class="p">.</span><span class="n">h</span><span class="o">:</span><span class="mi">31</span><span class="p">]</span><span class="w"> </span><span class="n">log</span><span class="p">]</span><span class="w"> </span><span class="n">xxx</span><span class="o">:</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">compatible</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">xxx</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">tensorrt</span><span class="o">/</span><span class="n">tensorrt_runtime</span><span class="p">.</span><span class="n">h</span><span class="o">:</span><span class="mi">31</span><span class="p">]</span><span class="w"> </span><span class="n">log</span><span class="p">]</span><span class="w"> </span><span class="n">shapeMachine</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="p">(</span><span class="mi">252</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Shape</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="k">operator</span><span class="p">()</span><span class="o">:</span><span class="w"> </span><span class="n">broadcast</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">incompatible</span><span class="w"> </span><span class="n">Dimensions</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">tensorrt</span><span class="o">/</span><span class="n">tensorrt_runtime</span><span class="p">.</span><span class="n">h</span><span class="o">:</span><span class="mi">31</span><span class="p">]</span><span class="w"> </span><span class="n">log</span><span class="p">]</span><span class="w"> </span><span class="n">Instruction</span><span class="o">:</span><span class="w"> </span><span class="n">CHECK_BROADCAST</span><span class="w"> </span><span class="n">xx</span><span class="w"> </span><span class="n">xx</span>
<span class="n">ERROR</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">tensorrt</span><span class="o">/</span><span class="n">tensorrt_subgraph</span><span class="p">.</span><span class="n">cc</span><span class="o">:</span><span class="mi">500</span><span class="p">]</span><span class="w"> </span><span class="n">Execute</span><span class="p">]</span><span class="w"> </span><span class="n">TensorRT</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: When an operator is in the offline converter, the operator is automatically broadcast offline by specifying <code class="docutils literal notranslate"><span class="pre">--inputShape=&lt;INPUTSHAPE&gt;</span></code>. Take the ones like operator as an example. 1 is broadcast to the corresponding constant tensor based on the input shape information. In this case, when the input is resized to different dimensions, an error is reported for operators (such as concat and matmul) that are sensitive to the input tensor dimensions on the network.</p></li>
<li><p>Solution: Replace this type of operator with the input of a model, assign a value by copying the memory during inference, and specify the corresponding shape information during resizing.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="npu-inference-issues">
<h3>NPU Inference Issues<a class="headerlink" href="#npu-inference-issues" title="Permalink to this headline"></a></h3>
<section id="failed-to-build-a-graph-3">
<h4>Failed to Build a Graph<a class="headerlink" href="#failed-to-build-a-graph-3" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Failed to build an NPU graph. A tool is used to capture background logs and search for <strong>MS_LITE</strong> in the logs. The following error information is displayed:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_subgraph</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">BuildIRModel</span><span class="p">]</span><span class="w"> </span><span class="n">Build</span><span class="w"> </span><span class="n">IR</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
<span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_subgraph</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">Init</span><span class="p">]</span><span class="w"> </span><span class="n">Build</span><span class="w"> </span><span class="n">IR</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
<span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_graph</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">CreateNPUSubgraphKernel</span><span class="p">]</span><span class="w"> </span><span class="n">NPU</span><span class="w"> </span><span class="n">Subgraph</span><span class="w"> </span><span class="n">Init</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
<span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_delegate</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">Build</span><span class="p">]</span><span class="w"> </span><span class="n">Create</span><span class="w"> </span><span class="n">NPU</span><span class="w"> </span><span class="n">Graph</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: This error is caused by the NPU online graph construction failure.</p></li>
<li><p>Solution: The graph construction is completed by calling the <a class="reference external" href="https://developer.huawei.com/consumer/en/doc/development/HiAI-Library/ddk-download-0000001053590180">HiAI DDK</a> API. Therefore, the error is reported in the error log of HiAI. For some errors, you can modify the operator type or parameter type in the model as prompted. For most errors, you need to <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">commit an issue</a> in the MindSpore community to notify the developers to fix and adapt the code. The following provides common HiAI error messages so that you can clearly describe the issue when asking questions in the community and improve the issue locating efficiency.</p></li>
</ul>
<p>(1) Search for the keyword <strong>E AI_FMK</strong> in the log file. If the following error log is found before the “MS_LITE” error is reported:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="nl">AI_FMK</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="o">/</span><span class="n">conv_base_op_builder</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="n">CheckShapeMatch</span><span class="p">(</span><span class="o">**</span><span class="p">)</span><span class="o">::</span><span class="s">&quot;Input Channel ** does not match convolution weight channel ** * group **.&quot;</span>
<span class="nl">AI_FMK</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="o">/</span><span class="n">conv_base_op_builder</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="n">Init</span><span class="p">(</span><span class="o">**</span><span class="p">)</span><span class="o">::</span><span class="s">&quot;Shape of op **** does not match&quot;</span>
<span class="nl">AI_FMK</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="o">/</span><span class="n">op_builder</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="n">BuildOutputDesc</span><span class="p">(</span><span class="o">**</span><span class="p">)</span><span class="o">::</span><span class="s">&quot;&quot;</span><span class="n">Init</span><span class="s">&quot; failed. Node: ****.&quot;</span>
</pre></div>
</div>
<p>This indicates that the Transpose operator for tensor format conversion is inserted or omitted during graph construction. As a result, the input shape of the convolution does not match the weight format.</p>
<p>(2) Search for the keyword <strong>E AI_FMK</strong> in the log file. If the following error log is found before the “MS_LITE” error is reported:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="nl">AI_FMK</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="o">/</span><span class="n">model_compatibility_check</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="n">GetIRGraphCompatibilityCheckResult</span><span class="p">(</span><span class="o">**</span><span class="p">)</span><span class="o">::</span><span class="s">&quot;Node **** type **** don&#39;t support!&quot;</span>
<span class="nl">AI_FMK</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="o">/</span><span class="n">model_compatibility_check</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="n">CheckIRGraphCompatibility</span><span class="p">(</span><span class="o">**</span><span class="p">)</span><span class="o">::</span><span class="s">&quot;CompleteExecuteDeviceConfig CheckIRGraphCompatibility failed&quot;</span>
</pre></div>
</div>
<p>The error message indicates that the operator is not supported due to operator compatibility issues between HiAI ROM and HiAI DDK used by MindSpore Lite. You can update your phone system to update HiAI ROM, replace unsupported operators, or provide feedback in the open-source community.</p>
</li>
</ol>
</section>
<section id="failed-to-execute-a-graph-2">
<h4>Failed to Execute a Graph<a class="headerlink" href="#failed-to-execute-a-graph-2" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>NPU inference fails. A tool is used to capture background logs and search for <strong>MS_LITE</strong> in the logs. The following error information is displayed:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_executor</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">Run</span><span class="p">]</span><span class="w"> </span><span class="n">NPU</span><span class="w"> </span><span class="n">Process</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span><span class="w"> </span><span class="n">code</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="mi">1</span>
<span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_graph</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">Execute</span><span class="p">]</span><span class="w"> </span><span class="n">NPU</span><span class="w"> </span><span class="n">Subgraph</span><span class="w"> </span><span class="o">****</span><span class="w"> </span><span class="n">execute</span><span class="w"> </span><span class="n">failed</span><span class="p">.</span>
<span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">lite_mindrt</span><span class="p">.</span><span class="n">h</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">RunKernel</span><span class="p">]</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="n">failed</span><span class="p">,</span><span class="w"> </span><span class="n">name</span><span class="o">:</span><span class="w"> </span><span class="o">****</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: This error is reported because the NPU fails to perform inference.</p></li>
<li><p>Solution: The underlying inference of the NPU model is implemented by HiAI. Therefore, the error is first displayed in the HiAI error log. The following describes common HiAI error information to help you locate the fault.</p></li>
</ul>
<p>Search for the keyword <strong>E AI_FMK</strong> in the log file. If the following error log is found before the <strong>MS_LITE</strong> error is reported:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="nl">AI_FMK</span><span class="w">  </span><span class="p">:</span><span class="w"> </span><span class="o">/</span><span class="n">common_memory_allocator</span><span class="p">.</span><span class="n">cpp</span><span class="w"> </span><span class="n">Allocate</span><span class="p">(</span><span class="o">**</span><span class="p">)</span><span class="o">::</span><span class="s">&quot;Call rt api failed, ret: ****&quot;</span>
</pre></div>
</div>
<p>Search for <strong>DEVMM</strong>. If the following error information is displayed in the first 10 lines of the previous logs:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">vendor</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">hiaiserver</span><span class="o">:</span><span class="w"> </span><span class="p">[</span><span class="n">DEVMM</span><span class="p">][</span><span class="n">E</span><span class="p">]</span><span class="w"> </span><span class="n">DevmmJudgeAllocSize</span><span class="o">:**</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">alloc</span><span class="w"> </span><span class="n">memory</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="n">exceeds</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">specification</span><span class="w"> </span><span class="n">limit</span><span class="p">,</span><span class="w"> </span><span class="n">already</span><span class="w"> </span><span class="n">alloc</span><span class="w"> </span><span class="n">total</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mh">0x3ff95000</span>
</pre></div>
</div>
<p>This indicates that the memory application of the NPU exceeds the limit. Check whether the model file is large or whether a tensor with a large shape exists in the model. According to <a class="reference external" href="https://developer.huawei.com/consumer/en/doc/development/hiai-References/modelbuildoptions-0000001139374903">official HiAI requirements</a>, the size of a single NPU’s subgraph should not exceed 200 MB, and the memory requested for the array should not exceed the NPU’s video memory. For example, in the log in this example, the NPU can apply for a maximum of 1 GB video memory. If you still need to run the NPU, adjust the model structure, split the model, or adjust the shape of the tensor.</p>
</li>
</ol>
</section>
</section>
</section>
<section id="model-inference-accuracy-issues">
<h2>Model Inference Accuracy Issues<a class="headerlink" href="#model-inference-accuracy-issues" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>When MindSpore Lite is used for integration, the post-processing effect of the inference result is not ideal. How do I locate the issue of inference accuracy?</p>
<ul>
<li><p>Check whether the input data is correct. In MindSpore Lite 1.3.0 and earlier versions, the input data format of the MS model is NHWC. In MindSpore Lite 1.5.0 and later versions, the <a class="reference external" href="https://mindspore.cn/lite/docs/en/r1.10/use/converter_tool.html">inputDataFormat</a> parameter can be set to NHWC or NCHW. Ensure that the input data format is the same as that required by the MS model.</p></li>
<li><p>Use the <a class="reference external" href="https://mindspore.cn/lite/docs/en/r1.10/use/benchmark_tool.html">benchmark</a> tool provided by MindSpore Lite to test the accuracy. If the following logs are displayed, the accuracy may be incorrect. Otherwise, the MindSpore Lite inference accuracy is correct. In this case, you need to check whether the data pre- and post-processing processes are correct.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Mean</span><span class="w"> </span><span class="n">bias</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">nodes</span><span class="o">/</span><span class="n">tensors</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">too</span><span class="w"> </span><span class="n">big</span><span class="o">:</span><span class="w"> </span><span class="o">**</span>
<span class="n">Compare</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="mi">-1</span>
<span class="n">Run</span><span class="w"> </span><span class="n">MarkAccuracy</span><span class="w"> </span><span class="n">error</span><span class="o">:</span><span class="w"> </span><span class="mi">-1</span>
</pre></div>
</div>
</li>
<li><p>If the accuracy of the entire network inference performed by MindSpore Lite is incorrect, you can use the <a class="reference external" href="https://mindspore.cn/lite/docs/en/r1.10/use/benchmark_tool.html#dump">Dump function</a> of the benchmark tool to save the output of the operator layer and compare the output with the inference result of the original framework to further locate the operator with incorrect accuracy.</p></li>
<li><p>For operators with accuracy issues, you can download the <a class="reference external" href="https://gitee.com/mindspore/mindspore">MindSpore source code</a> to check the operator implementation and construct the corresponding single-operator network for debugging and fault locating. You can also <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">commit an issue</a> in the MindSpore community to MindSpore Lite developers for troubleshooting.</p></li>
</ul>
</li>
<li><p>What do I do if the FP32 inference result is correct but the FP16 inference result contains the NaN or Inf value?</p>
<ul class="simple">
<li><p>If the NaN or Inf value is displayed in the result, value overflow occurs during inference. You can view the model structure, filter out the operator layer where value overflow may occur, and use the <a class="reference external" href="https://mindspore.cn/lite/docs/en/r1.10/use/benchmark_tool.html#dump">Dump function</a> of the benchmark tool to save the output of the operator layer and confirm the operator where value overflow occurs.</p></li>
<li><p>MindSpore Lite 1.5.0 and later versions provide the mixed-precision inference capability. When FP16 is preferentially used for the entire network inference, an operator at a certain layer can be set for FP32 inference. For details, see <a class="reference external" href="https://mindspore.cn/lite/docs/en/r1.10/use/runtime_cpp.html#mixed-precision-inference">Mixed Precision Inference</a>. The overflow layer is set to FP32 to avoid the entire network inference accuracy issue during FP16 inference.</p></li>
</ul>
</li>
<li><p>What do I do if the NaN or Inf value exists in the FP32 and FP16 inference results of MindSpore Lite?</p>
<ul>
<li><p>Analysis: Operators that perform division operations exist on the entire network. During inference, if the division operation is performed and the divisor is 0, the NaN value may display. Take the following network structure as an example. If the network is used in the scenario where the input data is not normalized and the input data ranges from 0 to 255, the NaN value is displayed. The reason is that the input data is large and the output value of matmul is large when the input data is not normalized. As a result, the output value of the Tanh activation function is 1. As a result, the Div operator is divided by 0. However, if the network input data is normalized and the Tanh activation function is not 1, the network inference data does not have the NaN value.</p>
<p><img alt="image-20211214191139062" src="_images/troubleshooting_Fp32_NAN.png" /></p>
</li>
<li><p>Solution: If the input data is too large, you are advised to normalize the network input data during training. If the NaN value still exists after the input data is normalized, you need to use the <a class="reference external" href="https://mindspore.cn/lite/docs/en/r1.10/use/benchmark_tool.html#dump">Dump function</a> of the benchmark tool to save the output of the operator layer to confirm the operator where the value overflow occurs and analyze the issue.</p></li>
</ul>
</li>
</ol>
</section>
<section id="model-inference-performance-issues">
<h2>Model Inference Performance Issues<a class="headerlink" href="#model-inference-performance-issues" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>Why is the actual inference performance the same as that of the CPU after the device is specified as the NPU?</p>
<ul>
<li><p>If the device does not support the NPU but the NPU is specified in the context, the model automatically runs on a CPU instead of the NPU. In this case, the inference performance is the same as that of the CPU. You can use a tool (such as adb logcat) to capture background logs and search for the keyword <strong>MS_LITE</strong> in the logs to check whether the device supports NPUs. The common prompts and descriptions are as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_manager</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">IsSupportNPU</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">devices</span><span class="w"> </span><span class="n">NOT</span><span class="w"> </span><span class="n">SUPPORT</span><span class="w"> </span><span class="n">NPU</span><span class="p">.</span>
</pre></div>
</div>
</li>
<li><p>If the log contains only the preceding information, check whether your device is a Huawei device with a HiSilicon Kirin processor. If not, NPUs are not supported.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_manager</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">IsKirinChip</span><span class="p">]</span><span class="w"> </span><span class="n">Unsupported</span><span class="w"> </span><span class="n">KirinChip</span><span class="w"> </span><span class="o">***</span><span class="p">.</span>
<span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_manager</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">IsSupportNPU</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">devices</span><span class="w"> </span><span class="n">NOT</span><span class="w"> </span><span class="n">SUPPORT</span><span class="w"> </span><span class="n">NPU</span><span class="p">.</span>
</pre></div>
</div>
</li>
<li><p>If the log contains the preceding information, your device uses the Kirin chip, but the chip model does not support NPUs. Currently, the following Kirin chips support NPUs: Kirin 810, Kirin 820, Kirin 985, and later versions.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_manager</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">CheckDDKVerGreatEqual</span><span class="p">]</span><span class="w"> </span><span class="n">DDK</span><span class="w"> </span><span class="n">Version</span><span class="w"> </span><span class="mf">100.</span><span class="o">***</span><span class="p">.</span><span class="o">***</span><span class="p">.</span><span class="o">***</span><span class="w"> </span><span class="n">less</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="mf">100.320.011.019</span><span class="p">.</span>
<span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">npu_manager</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">IsSupportNPU</span><span class="p">]</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">devices</span><span class="w"> </span><span class="n">NOT</span><span class="w"> </span><span class="n">SUPPORT</span><span class="w"> </span><span class="n">NPU</span><span class="p">.</span>
</pre></div>
</div>
</li>
<li><p>If the log contains the preceding information, your device meets the hardware requirements, but the HiAI ROM version does not meet the requirements. As a result, the NPU operator cannot run. MindSpore Lite requires that the HiAI ROM version be later than 100.320.011.018.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">op</span><span class="o">/</span><span class="n">convolution_npu</span><span class="p">.</span><span class="n">cc</span><span class="o">:**</span><span class="p">]</span><span class="w"> </span><span class="n">GetNPUConvOp</span><span class="p">]</span><span class="w"> </span><span class="n">NPU</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">inference</span><span class="w"> </span><span class="n">shape</span><span class="p">.</span>
<span class="nl">MS_LITE</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">mindspore</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">delegate</span><span class="o">/</span><span class="n">npu</span><span class="o">/</span><span class="n">op</span><span class="o">/</span><span class="n">npu_op</span><span class="p">.</span><span class="n">h</span><span class="o">:**</span><span class="w"> </span><span class="n">GetNPUOp</span><span class="p">]</span><span class="w"> </span><span class="n">NPU</span><span class="w"> </span><span class="n">does</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">support</span><span class="w"> </span><span class="n">runtime</span><span class="w"> </span><span class="n">inference</span><span class="w"> </span><span class="n">shape</span><span class="p">.</span>
</pre></div>
</div>
</li>
<li><p>If either of the preceding two messages appears in the log for multiple times, check whether the model input is a dynamic shape and whether the input shape is specified before inference. If yes, the model cannot run on the NPU, and the program automatically switches to the CPU for execution.</p></li>
</ul>
</li>
<li><p>Why is the actual inference performance poorer than that of the CPU after the device is specified as the NPU?</p>
<ul class="simple">
<li><p>In most cases, the inference performance of the NPU is much better than that of the CPU. In a few cases, the inference performance of the NPU is poorer than that of the CPU.</p></li>
</ul>
<p>(1) Check whether there are a large number of Pad or StridedSlice operators in the model. The array format of the NPU is different from that of the CPU. The operation of these operators in the NPU involves array rearrangement. Therefore, the NPU has no advantage over the CPU and even is inferior to the CPU. If you need to run such an operator on the NPU, you are advised to remove or replace the operator.
(2) Use a tool (such as adb logcat) to capture background logs and search for the keyword <strong>BuildIRModel build successfully</strong>. It is found that related logs appear multiple times, indicating that the model is partitioned into multiple NPU-related subgraphs during online graph construction. Generally, subgraph partitioning is caused by the existence of Transpose and/or unsupported NPU operators in the graph. Currently, a maximum of 20 subgraphs can be partitioned. The more the subgraphs, the more time the NPU takes. You are advised to refer to the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.10/operator_list_lite.html">NPU operators</a> supported by MindSpore Lite and avoid unsupported operators during model building. Alternatively, <a class="reference external" href="https://gitee.com/mindspore/mindspore/issues">commit an issue</a> to MindSpore Lite developers.</p>
</li>
</ol>
</section>
<section id="issues-related-to-using-visual-studio">
<h2>Issues Related to Using Visual Studio<a class="headerlink" href="#issues-related-to-using-visual-studio" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>When the static library is used, an error message is displayed, indicating that the Creator function of the parameter cannot be found. The following error information is displayed in logs:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ERROR [mindspore\lite\src\ops\populate\populate_register.h:**] GetParameterCreator] Unsupported parameter type in Create : **
ERROR [mindspore\lite\src\scheduler.cc:**] InferNodeShape] parameter generator is nullptr.
ERROR [mindspore\lite\src\scheduler.cc:**] InferSubGraphShape] InferShape failed, name: **, type: **
ERROR [mindspore\lite\src\scheduler.cc:**] SchedulePreProcess] op infer shape failed.
ERROR [mindspore\lite\src\lite_session.cc:**] CompileGraph] Schedule kernels failed: -500
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: When the static library is linked, not all symbols in the static library are imported by default. The Creator function of parameter is registered with the singleton object through the global static object.</p></li>
<li><p>Solution: When linking the static library built by Visual Studio, choose “Properties &gt; Linker &gt; Command Line &gt; Additional Options” and add /WHOLEARCHIVE:libmindspore-lite.lib.</p></li>
</ul>
</li>
<li><p>The model verification fails. The error log information is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ERROR [mindspore\lite\src\lite_model.cc:**] ModelVerify] Model does not have inputs.
ERROR [mindspore\lite\src\lite_model.cc:**] ConstructModel] ModelVerify failed.
ERROR [mindspore\lite\src\lite_model.cc:**] ImportFromBuffer] construct model failed.
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: The read model file is incomplete.</p></li>
<li><p>Solution: When using the Visual Studio compiler, add std::ios::binary to the model stream to be read.</p></li>
</ul>
</li>
</ol>
</section>
<section id="issues-related-to-app-building-using-xcode">
<h2>Issues Related to App Building Using Xcode<a class="headerlink" href="#issues-related-to-app-building-using-xcode" title="Permalink to this headline"></a></h2>
<ol class="arabic">
<li><p>When Xcode uses a framework package to build an app, an error is reported indicating that a parameter is not supported when the GetParameterCreator function is executed. The log is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ERROR [mindspore/lite/src/ops/populate/populate_register.h:46] GetParameterCreator] Unsupported parameter type in Create : **
ERROR [mindspore/lite/src/scheduler.cc:208] InferNodeShape] parameter generator is nullptr.
ERROR [mindspore/lite/src/scheduler.cc:266] InferSubGraphShape] InferShape failed, name: **, type: **
ERROR [mindspore/lite/src/scheduler.cc:78] SchedulePreProcess] op infer shape failed.
ERROR [mindspore/lite/src/lite_session.cc:508] CompileGraph] Schedule kernels failed: -500
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: When the static library in the framework is linked, not all symbols in the static library are imported. The Creator function of parameter is registered with the singleton object through the global static object.</p></li>
<li><p>Solution: When using the framework package to build an app in Xcode, choose “Build Settings &gt; Linking &gt; Other Linker Flags” and add the “mindspore_lite.framework/mindspore_lite” path.</p></li>
</ul>
</li>
<li><p>When Xcode is used to build an app using the framework package, the following error message is displayed:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Undefined symbol:
mindspore::session::LiteSession::CreateSession(**);
</pre></div>
</div>
<ul class="simple">
<li><p>Analysis: The corresponding symbol is not found, and the framework package is not correctly imported to the Xcode project.</p></li>
<li><p>Solution: When using the framework package to build an app in Xcode, choose “Build Settings &gt; Search Paths &gt; Framework Search Paths” and add the “mindspore_lite.framework” path. In addition, choose “Build Settings &gt; Search Paths &gt; User Header Search Paths” and add the “mindspore_lite.framework/Headers” path.</p></li>
</ul>
</li>
</ol>
</section>
<section id="other-issues">
<h2>Other Issues<a class="headerlink" href="#other-issues" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>Why does a device not take effect after being specified as a GPU or NPU?
The device priority depends on the configuration sequence. Ensure that the GPU or NPU configuration in the context is prior to the CPU configuration.</p></li>
</ol>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="scene_detection_lite.html" class="btn btn-neutral float-left" title="Scene Detection Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="log.html" class="btn btn-neutral float-right" title="Log" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>