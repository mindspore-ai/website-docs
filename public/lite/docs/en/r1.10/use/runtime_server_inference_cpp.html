

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Using C++ Interface to Parallel Inference &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Using Java Interface to Parallel Inference" href="runtime_server_inference_java.html" />
    <link rel="prev" title="Executing Server Inference" href="runtime_server_inference.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Getting Started in One Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_cpp.html">Experience C++ Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_java.html">Experience Java Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_c.html">Expriencing Simpcified Inference Demo with C-language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption"><span class="caption-text">Inference on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Performing Inference on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Server Inference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="runtime_server_inference.html">Executing Server Inference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using C++ Interface to Parallel Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-configuration">Create configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#initialization">Initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#execute-concurrent-reasoning">Execute concurrent reasoning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#memory-release">Memory release</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="runtime_server_inference_java.html">Using Java Interface to Parallel Inference</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="runtime_server_inference.html">Executing Server Inference</a> &raquo;</li>
        
      <li>Using C++ Interface to Parallel Inference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/use/runtime_server_inference_cpp.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="using-c++-interface-to-parallel-inference">
<h1>Using C++ Interface to Parallel Inference<a class="headerlink" href="#using-c++-interface-to-parallel-inference" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.10/docs/lite/docs/source_en/use/runtime_server_inference_cpp.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.10/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>MindSpore Lite provides multi-model concurrent inference interface <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.10/generate/classmindspore_ModelParallelRunner.html">ModelParallelRunner</a>. Multi model concurrent inference now supports CPU and GPU backend.</p>
<blockquote>
<div><p>For a quick understanding of the complete calling process of MindSpore Lite executing concurrent reasoning, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.10/quick_start/quick_start_server_inference_cpp.html">Experience C++ Minimalist Concurrent Reasoning Demo</a>.</p>
</div></blockquote>
<p>After the model is converted into a <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model by using the MindSpore Lite model <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.10/use/converter_tool.html">conversion tool</a>, the inference process can be performed in Runtime. For details, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.10/use/converter_tool.html">Converting Models for Inference</a>. This tutorial describes how to use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.10/index.html">C++ API</a> to perform inference.</p>
<p>To use the MindSpore Lite parallel inference framework, perform the following steps:</p>
<ol class="simple">
<li><p>Create a configuration item: Create a multi-model concurrent inference configuration item <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.10/generate/classmindspore_RunnerConfig.html">RunnerConfig</a>, which is used to configure multiple model concurrency.</p></li>
<li><p>Initialization: initialization before multi-model concurrent inference.</p></li>
<li><p>Execute concurrent inference: Use the Predict interface of ModelParallelRunner to perform concurrent inference on multiple models.</p></li>
<li><p>Release memory: When you do not need to use the MindSpore Lite concurrent inference framework, you need to release the ModelParallelRunner and related Tensors you created.</p></li>
</ol>
</div>
<div class="section" id="create-configuration">
<h2>Create configuration<a class="headerlink" href="#create-configuration" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.10/generate/classmindspore_RunnerConfig.html">configuration item</a> will save some basic configuration parameters required for concurrent reasoning, which are used to guide the number of concurrent models, model compilation and model execution.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.10/mindspore/lite/examples/quick_start_server_inference_cpp/main.cc#L135">main.cc</a> demonstrates how to create a RunnerConfig and configure the number of workers for concurrent reasoning:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create and init context, add CPU device info</span>
<span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="c1">// Create model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">model_runner</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelParallelRunner</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_runner</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="n">runner_config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">RunnerConfig</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">runner_config</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;runner config is nullptr.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">runner_config</span><span class="o">-&gt;</span><span class="n">SetContext</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
<span class="n">runner_config</span><span class="o">-&gt;</span><span class="n">SetWorkersNum</span><span class="p">(</span><span class="n">kNumWorkers</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>For details on the configuration method of Context, see <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.10/generate/classmindspore_Context.html">Context</a>.</p>
<p>Multi-model concurrent inference currently only supports <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.10/generate/classmindspore_CPUDeviceInfo.html">CPUDeviceInfo</a> and <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.10/generate/classmindspore_GPUDeviceInfo.html">GPUDeviceInfo</a> two different hardware backends. When setting the GPU backend, you need to set the GPU backend first and then the CPU backend, otherwise it will report an error and exit.</p>
<p>Multi-model concurrent inference does not support FP32-type data inference. Binding cores only supports no core binding or binding large cores. It does not support the parameter settings of the bound cores, and does not support configuring the binding core list.</p>
</div></blockquote>
</div>
<div class="section" id="initialization">
<h2>Initialization<a class="headerlink" href="#initialization" title="Permalink to this headline">¶</a></h2>
<p>When using MindSpore Lite to execute concurrent inference, ModelParallelRunner is the main entry of concurrent reasoning. Through ModelParallelRunner, you can initialize and execute concurrent reasoning. Use the RunnerConfig created in the previous step and call the init interface of ModelParallelRunner to initialize ModelParallelRunner.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.10/mindspore/lite/examples/quick_start_server_inference_cpp/main.cc#L155">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">Predict</span></code> to execute reasoning:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Build model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_runner</span><span class="o">-&gt;</span><span class="n">Init</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">runner_config</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="k">delete</span><span class="w"> </span><span class="n">model_runner</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>For Initialization of ModelParallelRunner, you do not need to set the RunnerConfig configuration parameters, and the default parameters will be used for concurrent inference of multiple models.</p>
</div></blockquote>
</div>
<div class="section" id="execute-concurrent-reasoning">
<h2>Execute concurrent reasoning<a class="headerlink" href="#execute-concurrent-reasoning" title="Permalink to this headline">¶</a></h2>
<p>MindSpore Lite calls the Predict interface of ModelParallelRunner for model concurrent inference.</p>
<p>The following <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.10/mindspore/lite/examples/quick_start_server_inference_cpp/main.cc#L189">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">Predict</span></code> to execute inference.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Model Predict</span>
<span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_runner</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">delete</span><span class="w"> </span><span class="n">model_runner</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>It is recommended to use GetInputs and GetOutputs to obtain the inputs and outputs of the Predict interface. Users can set the memory address of the data and Shape-related information through SetData.</p>
</div></blockquote>
</div>
<div class="section" id="memory-release">
<h2>Memory release<a class="headerlink" href="#memory-release" title="Permalink to this headline">¶</a></h2>
<p>When you do not need to use the MindSpore Lite reasoning framework, you need to release the created ModelParallelRunner. The following <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.10/mindspore/lite/examples/quick_start_server_inference_cpp/main.cc#L220">main.cc</a> demonstrates how to free memory before the end of the program.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Delete model runner.</span>
<span class="k">delete</span><span class="w"> </span><span class="n">model_runner</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="runtime_server_inference_java.html" class="btn btn-neutral float-right" title="Using Java Interface to Parallel Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="runtime_server_inference.html" class="btn btn-neutral float-left" title="Executing Server Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>