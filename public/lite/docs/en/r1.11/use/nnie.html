<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Usage Description of the Integrated NNIE &mdash; MindSpore Lite master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="TensorRT Integration Information" href="tensorrt_info.html" />
    <link rel="prev" title="NPU Integration Information" href="npu_info.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="npu_info.html">NPU Integration Information</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Usage Description of the Integrated NNIE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#directory-structures">Directory Structures</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-converter-directory-structure-of-the-model-conversion-tool">The converter Directory Structure of the Model Conversion Tool</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-runtime-directory-structure-of-the-model-inference-tool">The runtime Directory Structure of the Model Inference Tool</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#using-tools">Using Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#converter">Converter</a></li>
<li class="toctree-l4"><a class="reference internal" href="#runtime">Runtime</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#integration">Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#svp-tool-chain-related-functions-and-precautions-advanced-options">SVP Tool Chain-related Functions and Precautions (Advanced Options)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nhwc-the-format-of-the-running-input-on-the-board">NHWC the Format of the Running Input on the Board</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-list-description">image_list Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-type-restrictions">image_type Restrictions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-list-and-roi-coordinate-file-quantity">image_list and roi_coordinate_file Quantity</a></li>
<li class="toctree-l4"><a class="reference internal" href="#suffix-cpu-of-the-node-name-in-the-prototxt-file">Suffix cpu of the Node Name in the prototxt File</a></li>
<li class="toctree-l4"><a class="reference internal" href="#custom-operator-in-the-prototxt-file">Custom Operator in the prototxt File</a></li>
<li class="toctree-l4"><a class="reference internal" href="#suffix-report-of-the-top-domain-in-the-prototxt-file">Suffix report of the Top Domain in the prototxt File</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inplace-mechanism">Inplace Mechanism</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-image-batch-running-and-multi-step-running">Multi-image Batch Running and Multi-step Running</a></li>
<li class="toctree-l4"><a class="reference internal" href="#node-name-change">Node Name Change</a></li>
<li class="toctree-l4"><a class="reference internal" href="#proposal-operator-usage-description">Proposal Operator Usage Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#segmentation-mechanism-and-restrictions">Segmentation Mechanism and Restrictions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="tensorrt_info.html">TensorRT Integration Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="ascend_info.html">Integrated Ascend</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_parallel.html">Performing Concurrent Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="asic.html">Application Specific Integrated Circuit Integration Instructions</a> &raquo;</li>
      <li>Usage Description of the Integrated NNIE</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/nnie.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="usage-description-of-the-integrated-nnie">
<h1>Usage Description of the Integrated NNIE<a class="headerlink" href="#usage-description-of-the-integrated-nnie" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.11/docs/lite/docs/source_en/use/nnie.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.11/resource/_static/logo_source_en.png" /></a></p>
<section id="directory-structures">
<h2>Directory Structures<a class="headerlink" href="#directory-structures" title="Permalink to this headline"></a></h2>
<section id="the-converter-directory-structure-of-the-model-conversion-tool">
<h3>The converter Directory Structure of the Model Conversion Tool<a class="headerlink" href="#the-converter-directory-structure-of-the-model-conversion-tool" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-runtime-linux-x64
└── tools
    └── converter
        └── providers
            └── Hi3516D                # Embedded board model number
                ├── libmslite_nnie_converter.so        # Dynamic library for converting the integrated NNIE
                ├── libmslite_nnie_data_process.so     # Dynamic library for processing NNIE input data
                ├── libnnie_mapper.so        # Dynamic library for building NNIE binary files
                └── third_party       # Third-party dynamic library on which the NNIE depends
                    ├── opencv-4.2.0
                    │   └── libopencv_xxx.so
                    └── protobuf-3.9.0
                        ├── libprotobuf.so
                        └── libprotoc.so
</pre></div>
</div>
<p>The preceding shows the integration directory structure of the NNIE. For details about other directory structures of the converter, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/converter_tool.html">Converting Models for Inference</a>.</p>
</section>
<section id="the-runtime-directory-structure-of-the-model-inference-tool">
<h3>The runtime Directory Structure of the Model Inference Tool<a class="headerlink" href="#the-runtime-directory-structure-of-the-model-inference-tool" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-aarch32
└── providers
    └── Hi3516D        # Embedded board model number
        └── libmslite_nnie.so  # Dynamic library of the integrated NNIE
        └── libmslite_proposal.so  # Sample dynamic library of the integrated proposal
</pre></div>
</div>
<p>The preceding shows the integration directory structure of the NNIE. For details about other directory structures of the inference tool runtime, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/build.html#directory-structure">Directory Structure</a>.</p>
</section>
</section>
<section id="using-tools">
<h2>Using Tools<a class="headerlink" href="#using-tools" title="Permalink to this headline"></a></h2>
<section id="converter">
<h3>Converter<a class="headerlink" href="#converter" title="Permalink to this headline"></a></h3>
<section id="overview">
<h4>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h4>
<p>MindSpore Lite provides a tool for offline model conversion. It can convert models of multiple types into board-dedicated models that support NNIE hardware acceleration inference and can run on the Hi3516 board.
The converted NNIE <code class="docutils literal notranslate"><span class="pre">ms</span></code> model can be used only on the associated embedded board. The runtime inference framework matching the associated embedded board can be used to perform inference. For more information about the conversion tool, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/converter_tool.html">Converting Models for Inference</a>.</p>
</section>
<section id="environment-preparation">
<h4>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline"></a></h4>
<p>To use the MindSpore Lite model conversion tool, you need to prepare the environment as follows:</p>
<ol class="arabic">
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/downloads.html">Download</a> NNIE-dedicated converter. Currently, only Linux is supported.</p></li>
<li><p>Decompress the downloaded package.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-zxvf<span class="w"> </span>mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-linux-x64.tar.gz
</pre></div>
</div>
<p>{version} indicates the version number of the release package.</p>
</li>
<li><p>Add the dynamic link library required by the conversion tool to the environment variable LD_LIBRARY_PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/runtime/lib:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/third_party/opencv-4.2.0:<span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/third_party/protobuf-3.9.0
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} indicates the path of the folder obtained after the decompression.</p>
</li>
</ol>
</section>
<section id="extension-configuration">
<h4>Extension Configuration<a class="headerlink" href="#extension-configuration" title="Permalink to this headline"></a></h4>
<p>To load the extension module when converting, users need to configure the path of extended dynamic library. The parameters related to the extension include <code class="docutils literal notranslate"><span class="pre">plugin_path</span></code>, <code class="docutils literal notranslate"><span class="pre">disable_fusion</span></code>. The detailed description of the parameters is as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Attribute</p></th>
<th class="head"><p>Function Description</p></th>
<th class="head"><p>Parameter Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Value Range</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>plugin_path</p></td>
<td><p>Optional</p></td>
<td><p>Third-party library path</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>If there are more than one, please use <code class="docutils literal notranslate"><span class="pre">;</span></code> to separate.</p></td>
</tr>
<tr class="row-odd"><td><p>disable_fusion</p></td>
<td><p>Optional</p></td>
<td><p>Indicate whether to close fusion</p></td>
<td><p>String</p></td>
<td><p>off</p></td>
<td><p>off or on.</p></td>
</tr>
<tr class="row-even"><td><p>fusion_blacklists</p></td>
<td><p>Optional</p></td>
<td><p>Specified fusion operator names to be closed</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>If there are more than one, please use <code class="docutils literal notranslate"><span class="pre">,</span></code> to separate</p></td>
</tr>
</tbody>
</table>
<p>We have generated the default configuration file which restores the relative path of the NNIE dynamic library for the users in the released package. The users need to decide whether the configuration file needs to be modified manually. The content is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[registry]</span>
<span class="na">plugin_path</span><span class="o">=</span><span class="s">../providers/Hi3516D/libmslite_nnie_converter.so</span>
</pre></div>
</div>
<p>If the user needs to turn off the specified operator fusions, the fusion configuration of the the specified operator names to be closed are as follows：</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[registry]</span>
<span class="c1"># When parameter `disable_fusion` is configured as `off`, the user can turn off the specified operator fusions by configuring parameter `fusion_blacklists`. While parameter `disable_fusion` is configured as `on`, the parameter  `fusion_blacklists` does not work.</span>
<span class="na">disable_fusion</span><span class="o">=</span><span class="s">off</span>
<span class="na">fusion_blacklists</span><span class="o">=</span><span class="s">ConvActivationFusion,MatMulActivationFusion</span>
</pre></div>
</div>
<p>The operator fusion names are as follows：</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>No</p></th>
<th class="head"><p>the operator fusion name</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>AddConcatActivationFusion</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>SqueezeFusion</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>TransposeFusion</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>ReshapeReshapeFusion</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>ConvBiasaddFusion</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>ConvBatchNormFusion</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>ConvScaleFusion</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>GroupNormFusion</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p>TfNormFusion</p></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><p>OnnxLayerNormFusion</p></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><p>OnnxLayerNormFusion2</p></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><p>BatchMatMulFusion</p></td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><p>BatchNormToScaleFusion</p></td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><p>SigmoidMulFusion</p></td>
</tr>
<tr class="row-even"><td><p>15</p></td>
<td><p>ActivationFusion</p></td>
</tr>
<tr class="row-odd"><td><p>16</p></td>
<td><p>ConvActivationFusion</p></td>
</tr>
<tr class="row-even"><td><p>17</p></td>
<td><p>ConvTupleGetItemFusion</p></td>
</tr>
<tr class="row-odd"><td><p>18</p></td>
<td><p>ConvTupleActivationFusion</p></td>
</tr>
<tr class="row-even"><td><p>19</p></td>
<td><p>TfliteLstmCellFusion</p></td>
</tr>
<tr class="row-odd"><td><p>20</p></td>
<td><p>TfLstmCellFusion</p></td>
</tr>
<tr class="row-even"><td><p>21</p></td>
<td><p>TfBidirectionGruFusion</p></td>
</tr>
<tr class="row-odd"><td><p>22</p></td>
<td><p>TfGeLUFusion</p></td>
</tr>
<tr class="row-even"><td><p>23</p></td>
<td><p>OnnxGeLUFusion</p></td>
</tr>
<tr class="row-odd"><td><p>24</p></td>
<td><p>TfliteRelPosMultiHeadAttentionFusion</p></td>
</tr>
<tr class="row-even"><td><p>25</p></td>
<td><p>GLUFusion</p></td>
</tr>
<tr class="row-odd"><td><p>26</p></td>
<td><p>ConstFoldPass</p></td>
</tr>
<tr class="row-even"><td><p>27</p></td>
<td><p>AffineFusion</p></td>
</tr>
<tr class="row-odd"><td><p>28</p></td>
<td><p>AffineActivationFusion</p></td>
</tr>
<tr class="row-even"><td><p>29</p></td>
<td><p>ConvConvFusion</p></td>
</tr>
<tr class="row-odd"><td><p>30</p></td>
<td><p>ConvPadFusion</p></td>
</tr>
<tr class="row-even"><td><p>31</p></td>
<td><p>MatMulAddFusion</p></td>
</tr>
<tr class="row-odd"><td><p>32</p></td>
<td><p>MatMulMulFusion</p></td>
</tr>
<tr class="row-even"><td><p>33</p></td>
<td><p>TransposeMatMulFusion</p></td>
</tr>
<tr class="row-odd"><td><p>34</p></td>
<td><p>MulAddFusion</p></td>
</tr>
<tr class="row-even"><td><p>35</p></td>
<td><p>ScaleActivationFusion</p></td>
</tr>
<tr class="row-odd"><td><p>36</p></td>
<td><p>ScaleScaleFusion</p></td>
</tr>
<tr class="row-even"><td><p>37</p></td>
<td><p>FullConnectedFusion</p></td>
</tr>
<tr class="row-odd"><td><p>38</p></td>
<td><p>FullconnectedAddFusion</p></td>
</tr>
<tr class="row-even"><td><p>39</p></td>
<td><p>TensorDotFusion</p></td>
</tr>
<tr class="row-odd"><td><p>40</p></td>
<td><p>MatMulActivationFusion</p></td>
</tr>
</tbody>
</table>
</section>
<section id="nnie-configuration">
<h4>NNIE Configuration<a class="headerlink" href="#nnie-configuration" title="Permalink to this headline"></a></h4>
<p>The NNIE model can use the NNIE hardware to accelerate the model running. To do so, the users also need to prepare NNIE’s own configuration file. Users can configure the configuration file required by MindSpore Lite by referring to the <code class="docutils literal notranslate"><span class="pre">description</span> <span class="pre">of</span> <span class="pre">configuration</span> <span class="pre">items</span> <span class="pre">for</span> <span class="pre">nnie_mapper</span></code> in the HiSVP Development Guide provided by HiSilicon. <code class="docutils literal notranslate"><span class="pre">nnie.cfg</span></code> indicates the configuration file.</p>
<p>The following is an example of the <code class="docutils literal notranslate"><span class="pre">nnie.cfg</span></code> file:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[net_type] 0
[image_list] ./input_nchw.txt
[image_type] 0
[norm_type] 0
[mean_file] null
</pre></div>
</div>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">input_nchw.txt</span></code> is the input data of the floating-point text format of the Caffe model to be converted. For details, see the description of <code class="docutils literal notranslate"><span class="pre">image_list</span></code> in the HiSVP Development Guide. In the configuration file, you can configure theitems other than caffemodel_file, prototxt_file, is_simulation and instructions_name.</p>
</div></blockquote>
</section>
<section id="executing-converter">
<h4>Executing Converter<a class="headerlink" href="#executing-converter" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Go to the converter directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/converter
</pre></div>
</div>
</li>
<li><p>(Optional) Set environment variables.</p>
<p>Skip this step if you have entered the converter directory by Step 1, and the default value takes effect. If you have not entered the converter directory, you need to declare the path of the .so files and benchmark binary programs on which the conversion tool depends in the environment variables, as shown in the following figure:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_MAPPER_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/libnnie_mapper.so
<span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_DATA_PROCESS_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/providers/Hi3516D/libmslite_nnie_data_process.so
<span class="nb">export</span><span class="w"> </span><span class="nv">BENCHMARK_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/benchmark
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} indicates the path of the decompressed package.</p>
</li>
<li><p>Copy the <code class="docutils literal notranslate"><span class="pre">nnie.cfg</span></code> file to the converter directory and set the following environment variable:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_CONFIG_PATH</span><span class="o">=</span>./nnie.cfg
</pre></div>
</div>
<p>Skip this step if the actual configuration file is <code class="docutils literal notranslate"><span class="pre">nnie.cfg</span></code> and is at the same level as the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> file.</p>
</li>
<li><p>Execute the Converter to generate an NNIE <code class="docutils literal notranslate"><span class="pre">ms</span></code> model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>CAFFE<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.prototxt<span class="w"> </span>--weightFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.caffemodel<span class="w"> </span>--configFile<span class="o">=</span>./converter.cfg<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>${model_name} indicates the model file name. The execution result is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERT RESULT SUCCESS:0
</pre></div>
</div>
<p>For details about the parameters of the converter_lite conversion tool, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/converter_tool.html#example">“Parameter Description” in Converting Models for Inference</a>.</p>
</li>
</ol>
</section>
</section>
<section id="runtime">
<h3>Runtime<a class="headerlink" href="#runtime" title="Permalink to this headline"></a></h3>
<section id="overview-1">
<h4>Overview<a class="headerlink" href="#overview-1" title="Permalink to this headline"></a></h4>
<p>After the converted model is obtained, you can perform inference on the associated embedded board by using the Runtime inference framework matching the board. MindSpore Lite provides a benchmark test tool, which can be used to perform quantitative analysis (performance) on the execution time consumed by forward inference of the MindSpore Lite model. In addition, you can perform comparative error analysis (accuracy) based on the output of a specified model.
For details about the inference tool, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/benchmark_tool.html">benchmark</a>.</p>
</section>
<section id="environment-preparation-1">
<h4>Environment Preparation<a class="headerlink" href="#environment-preparation-1" title="Permalink to this headline"></a></h4>
<p>You can perform equivalent operations based on the actual situation. See the following example:</p>
<ol class="arabic">
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/downloads.html">Download</a> NNIE-dedicated model inference tool. Currently, only Hi3516D is supported.</p></li>
<li><p>Decompress the downloaded package.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-zxvf<span class="w"> </span>mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-linux-aarch32.tar.gz
</pre></div>
</div>
<p>{version} indicates the version number of the release package.</p>
</li>
<li><p>Create a storage directory on the Hi3516D board.</p>
<p>Log in to the board and create a working directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>/user/mindspore<span class="w">          </span><span class="c1"># Stores benchmark execution files and models.</span>
mkdir<span class="w"> </span>/user/mindspore/lib<span class="w">      </span><span class="c1"># Stores dependent library files.</span>
</pre></div>
</div>
</li>
<li><p>Transfer files.</p>
<p>Transfer the benchmark tool, model, and .so library to the Hi3516D board. <code class="docutils literal notranslate"><span class="pre">libmslite_proposal.so</span></code> is an implementation sample .so file of the proposal operator provided by MindSpore Lite. If the user model contains a custom proposal operator, you need to generate the <code class="docutils literal notranslate"><span class="pre">libnnie_proposal.so</span></code> file by referring to the <a class="reference internal" href="#proposal-operator-usage-description"><span class="std std-doc">Proposal Operator Usage Description</span></a> to replace the .so file for correct inference.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>scp<span class="w"> </span>libmindspore-lite.so<span class="w"> </span>libmslite_nnie.so<span class="w"> </span>libmslite_proposal.so<span class="w"> </span>root@<span class="si">${</span><span class="nv">device_ip</span><span class="si">}</span>:/user/mindspore/lib
scp<span class="w"> </span>benchmark<span class="w"> </span><span class="si">${</span><span class="nv">model_path</span><span class="si">}</span><span class="w"> </span>root@<span class="si">${</span><span class="nv">device_ip</span><span class="si">}</span>:/user/mindspore
</pre></div>
</div>
<p>${model_path} indicates the path of the MS model file after conversion.</p>
</li>
<li><p>Set the dynamic library path.</p>
<p>NNIE model inference depends on the NNIE-related board dynamic libraries provided by HiSilicon, including libnnie.so, libmpi.so, libVoiceEngine.so, libupvqe.so and libdnvqe.so.</p>
<p>You need to save these .so files on the board and pass the path to the LD_LIBRARY_PATH environment variable.
In the example, the .so files are stored in /usr/lib. You need to configure the .so files according to the actual situation.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/user/mindspore/lib:/usr/lib:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
</pre></div>
</div>
</li>
<li><p>(Optional) Build image input.</p>
<p>If the calibration set sent to the mapper is an image when the model is exported by the Converter, the input data transferred to the benchmark must be of the int8 type. That is, the image must be converted into the int8 type before being transferred to the benchmark.
Python is used to provide a conversion example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="k">def</span> <span class="nf">usage</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;usage:</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;example: python generate_input_bin.py xxx.img BGR 224 224</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[1]: origin image path</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[2]: RGB_order[BGR, RGB], should be same as nnie mapper config file&#39;s [RGB_order], default is BGR</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[3]: input_h</span><span class="se">\n</span><span class="s2">&quot;</span>
          <span class="s2">&quot;argv[4]: input_w&quot;</span>
          <span class="p">)</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">argvs</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;-h&quot;</span><span class="p">:</span>
        <span class="n">usage</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXIT&quot;</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">()</span>
    <span class="n">img_path</span> <span class="o">=</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">rgb_order</span> <span class="o">=</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">input_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">argvs</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">input_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">argvs</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rgb_order</span> <span class="o">==</span> <span class="s2">&quot;RGB&quot;</span><span class="p">:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
    <span class="n">img_hwc</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="n">input_w</span><span class="p">,</span> <span class="n">input_h</span><span class="p">))</span>
    <span class="n">outfile_name</span> <span class="o">=</span> <span class="s2">&quot;1_</span><span class="si">%s</span><span class="s2">_</span><span class="si">%s</span><span class="s2">_3_nhwc.bin&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">argvs</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">argvs</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">img_hwc</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="n">outfile_name</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generated &quot;</span> <span class="o">+</span> <span class="n">outfile_name</span> <span class="o">+</span> <span class="s2">&quot; file success in current dir.&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">usage</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXIT&quot;</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input argument is invalid.&quot;</span><span class="p">)</span>
        <span class="n">usage</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;EXIT&quot;</span><span class="p">)</span>
        <span class="n">exit</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">main</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="executing-the-benchmark">
<h4>Executing the Benchmark<a class="headerlink" href="#executing-the-benchmark" title="Permalink to this headline"></a></h4>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cd /user/mindspore
./benchmark --modelFile=${model_path}
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">model_path</span></code> indicates the path of the MS model file after conversion.</p>
<p>After this command is executed, random input of the model is generated and forward inference is performed. For details about how to use the benchmark, such as time consumption analysis and inference error analysis, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/benchmark_tool.html">benchmark</a>.</p>
<p>For details about the input data format requirements of the model, see <a class="reference internal" href="#svp-tool-chain-related-functions-and-precautions-advanced-options"><span class="std std-doc">(Optional) SVP Tool Chain-related Functions and Precautions</span></a>.</p>
</section>
</section>
</section>
<section id="integration">
<h2>Integration<a class="headerlink" href="#integration" title="Permalink to this headline"></a></h2>
<p>After the transformation model is obtained, the MindSpore Lite inference framework supporting the board can be used for integrated inference on the associated embedded board.
Before reading this section, users need to have a certain understanding of the c++ interface integration development of MindSpore Lite.
Users can understand the basic usage of the integration of MindSpore Lite through chapter <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/runtime_cpp.html">Using C++ Interface to Perform Inference</a>.
For the integrated use of NNIE, there are the following precautions:</p>
<ol class="arabic">
<li><p>Link <code class="docutils literal notranslate"><span class="pre">libmslite_nnie.so</span></code> at compile time</p>
<p>The integration of MindSpore Lite with NNIE is developed by registering MindSpore Lite Custom Operators. The developed Custom Operators <code class="docutils literal notranslate"><span class="pre">NNIE</span> <span class="pre">Custom</span> <span class="pre">Operators</span></code> is compiled into libmslite_nnie.so.
Therefore, if users want to use NNIE inference through MindSpore Lite integration, they must link the so at compile time to complete the registration of <code class="docutils literal notranslate"><span class="pre">NNIE</span> <span class="pre">Custom</span> <span class="pre">Operators</span></code>.</p>
</li>
<li><p>(Optional) Use configuration items on demand</p>
<p>MindSpore Lite provides <code class="docutils literal notranslate"><span class="pre">mindspore::Model::LoadConfig</span></code> and <code class="docutils literal notranslate"><span class="pre">mindspore::Model::UpdateConfig</span></code> interfaces to receive configuration parameters or configuration files, so that users can pass configuration items to all operators (including custom operators) through them.
<code class="docutils literal notranslate"><span class="pre">NNIE</span> <span class="pre">Custom</span> <span class="pre">Operators</span></code> opens four configuration items, as shown below:</p>
<ul>
<li><p>KeepOriginalOutput ：Keep the original NNIE hardware inference output results.</p>
<p>The output of NNIE hardware chip inference is quantized int32 data (actual value multiplied by 4096). When the model output is given, the chip inference result will be inversely quantized to the real float value output. When this option is configured as <code class="docutils literal notranslate"><span class="pre">on</span></code>, the output of the model will remain quantized int32 output. By default, this option is <code class="docutils literal notranslate"><span class="pre">off</span></code>.</p>
</li>
<li><p>MaxROINum ：Maximum number of ROIs supported by a single image. The value is a positive integer. The default value is 300.</p>
<p>If the user model contains the proposal operator, configure the MAX_ROI_NUM environment variable based on the implementation of the proposal operator.
If the user model doesn’t contain the proposal operator, you don’t need to configure the environment.</p>
</li>
<li><p>TimeStep ：Number of steps for loop or LSTM network running. The value is a positive integer. The default value is 1.</p>
<p>If the user model is a loop or LSTM network, you need to configure the TIME_STEP environment variable based on the actual network running status.
If the user model is not a loop or LSTM network, you don’t need to configure the TIME_STEP environment varialbe.
For details about other requirements, see <a class="reference internal" href="#multi-image-batch-running-and-multi-step-running"><span class="std std-doc">Multi-image Batch Running and Multi-step Running</span></a>.</p>
</li>
<li><p>CoreIds：Core ID for NNIE running. Model segments can be configured independently and are separated by commas (,), for example, export CORE_IDS=1,1. The default value is 0.</p>
<p>If there are multiple NNIE hardware devices on the board, you can specify the NNIE device on which the model runs by using the CORE_IDS environment variable.
If the model is segmented (you can open the model by using the Netron to observe the segmentation status), you can configure the device on which each segment runs in sequence. The segments that are not configured run on the last configured NNIE device.</p>
</li>
</ul>
<p>The configuration process of NNIE is as follows:</p>
<ul>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">mindspore::Model::LoadConfig</span></code> interface for configuration</p>
<p>After loading the model, before the user calls the <code class="docutils literal notranslate"><span class="pre">mindspore::Model::Build</span></code> interface to compile the model, the path of the configuration file is passed into the implementation configuration by calling the <code class="docutils literal notranslate"><span class="pre">mindspore::Model::LoadConfig</span></code> interface. An example of the content of a NNIE configuration file is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[nnie]
TimeStep=1
MaxROINum=0
CoreIds=0
KeepOriginalOutput=off
</pre></div>
</div>
</li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">mindspore::Model::UpdateConfig</span></code> interface for configuration</p>
<p>After loading the model, the user can also configure the above configuration items by calling the <code class="docutils literal notranslate"><span class="pre">mindspore::Model::UpdateConfig</span></code> interface before calling the <code class="docutils literal notranslate"><span class="pre">mindspore::Model::Build</span></code> interface for model compilation, as shown in the following example :</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>// ms_model is an instance of the `mindspore::Model` class
ms_model.UpdateConfig(&quot;nnie&quot;, std::make_pair(&quot;TimeStep&quot;, &quot;1&quot;));
ms_model.UpdateConfig(&quot;nnie&quot;, std::make_pair(&quot;MaxROINum&quot;, &quot;0&quot;));
ms_model.UpdateConfig(&quot;nnie&quot;, std::make_pair(&quot;CoreIds&quot;, &quot;0&quot;));
ms_model.UpdateConfig(&quot;nnie&quot;, std::make_pair(&quot;KeepOriginalOutput&quot;, &quot;off&quot;));
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
</section>
<section id="svp-tool-chain-related-functions-and-precautions-advanced-options">
<h2>SVP Tool Chain-related Functions and Precautions (Advanced Options)<a class="headerlink" href="#svp-tool-chain-related-functions-and-precautions-advanced-options" title="Permalink to this headline"></a></h2>
<p>During model conversion, the <code class="docutils literal notranslate"><span class="pre">nnie.cfg</span></code> file declared by the NNIE_CONFIG_PATH environment variable provides functions related to the SVP tool chain and supports the configuration of fields except caffemodel_file, prototxt_file, is_simulation and instructions_name. The implementation is as follows:</p>
<section id="nhwc-the-format-of-the-running-input-on-the-board">
<h3>NHWC the Format of the Running Input on the Board<a class="headerlink" href="#nhwc-the-format-of-the-running-input-on-the-board" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">ms</span></code> model after conversion accepts only the data input in NHWC format. If image_type is declared as 0, float32 data in NHWC format is received. If image_type is declared as 1, uint8 data input in NHWC format is received.</p>
</section>
<section id="image-list-description">
<h3>image_list Description<a class="headerlink" href="#image-list-description" title="Permalink to this headline"></a></h3>
<p>The meaning of the image_list field in the <code class="docutils literal notranslate"><span class="pre">nnie.cfg</span></code> file remains unchanged. When image_type is declared as 0, data in the CHW format is provided by row, regardless of whether the original model is the NCHW input.</p>
</section>
<section id="image-type-restrictions">
<h3>image_type Restrictions<a class="headerlink" href="#image-type-restrictions" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite does not support the network input when image_type is set to 3 or 5. You can set it to 0 or 1.</p>
</section>
<section id="image-list-and-roi-coordinate-file-quantity">
<h3>image_list and roi_coordinate_file Quantity<a class="headerlink" href="#image-list-and-roi-coordinate-file-quantity" title="Permalink to this headline"></a></h3>
<p>You only need to provide image_list whose quantity is the same as that of model inputs. If the model contains the ROI pooling or PSROI pooling layer, you need to provide roi_coordinate_file, the quantity and sequence correspond to the number and sequence of the ROI pooling or PSROI pooling layer in the .prototxt file.</p>
</section>
<section id="suffix-cpu-of-the-node-name-in-the-prototxt-file">
<h3>Suffix cpu of the Node Name in the prototxt File<a class="headerlink" href="#suffix-cpu-of-the-node-name-in-the-prototxt-file" title="Permalink to this headline"></a></h3>
<p>In the .prototxt file, you can add _cpu to the end of the node name to declare CPU custom operator. The_cpu suffix is ignored in MindSpore Lite and is not supported. If you want to redefine the implementation of an existing operator or add an operator, you can register the operator in custom operator mode.</p>
</section>
<section id="custom-operator-in-the-prototxt-file">
<h3>Custom Operator in the prototxt File<a class="headerlink" href="#custom-operator-in-the-prototxt-file" title="Permalink to this headline"></a></h3>
<p>In the SVP tool chain, the custom layer is declared in the .prototxt file to implement inference by segment and implement the CPU code by users. In MindSpore Lite, you need to add the op_type attribute to the custom layer and register the online inference code in custom operator mode.</p>
<p>An example of modifying the custom layer is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>layer {
  name: &quot;custom1&quot;
  type: &quot;Custom&quot;
  bottom: &quot;conv1&quot;
  top: &quot;custom1_1&quot;
  custom_param {
    type: &quot;MY_CUSTOM&quot;
    shape {
        dim: 1
        dim: 256
        dim: 64
        dim: 64
    }
}
}
</pre></div>
</div>
<p>In this example, a custom operator of the MY_CUSTOM type is defined. During inference, you need to register a custom operator of the MY_CUSTOM type.</p>
</section>
<section id="suffix-report-of-the-top-domain-in-the-prototxt-file">
<h3>Suffix report of the Top Domain in the prototxt File<a class="headerlink" href="#suffix-report-of-the-top-domain-in-the-prototxt-file" title="Permalink to this headline"></a></h3>
<p>When converting the NNIE model, MindSpore Lite fuses most operators into the binary file for NNIE running. Users cannot view the output of the intermediate operators. In this case, you can add the _report suffix to the top domain, during image composition conversion, the output of the intermediate operator is added to the output of the fused layer. If the operator has output (not fused), the output remains unchanged.</p>
<p>During the inference running, you can obtain the output of the intermediate operator by referring to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/runtime_cpp.html#using-c++-interface-to-perform-inference">Using C++ Interface to Perform Inference</a>.</p>
<p>MindSpore Lite parses the corresponding rules of _report and resolves the conflict with the <a class="reference internal" href="#inplace-mechanism"><span class="std std-doc">Inplace Mechanism</span></a>. For details, see the definition in the HiSVP Development Guide.</p>
</section>
<section id="inplace-mechanism">
<h3>Inplace Mechanism<a class="headerlink" href="#inplace-mechanism" title="Permalink to this headline"></a></h3>
<p>The inplace layer can be used to run the chip in efficient mode. By default, the conversion tool rewrites all layers in the .prototxt file that support the inplace layer. To disable this function, you can declare the following environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NNIE_DISABLE_INPLACE_FUSION</span><span class="o">=</span>off<span class="w">         </span><span class="c1"># When this parameter is set to on or is not set, inplace automatic rewriting is enabled.</span>
</pre></div>
</div>
<p>When the automatic rewriting function is disabled, you can manually rewrite the corresponding layer in the .prototxt file to enable the efficient mode for some layers.</p>
</section>
<section id="multi-image-batch-running-and-multi-step-running">
<h3>Multi-image Batch Running and Multi-step Running<a class="headerlink" href="#multi-image-batch-running-and-multi-step-running" title="Permalink to this headline"></a></h3>
<p>If you need to infer multiple input data (multiple images) at the same time, you can resize the first dimension of the model input to the quantity of input data by referring to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.11/use/runtime_cpp.html#resizing-the-input-dimension">Resizing the Input Dimension</a>. In the NNIE model, only the first dimension (‘n’ dimension) can be resized, and other dimensions (‘hwc’) cannot be resized.</p>
<p>For the loop or LSTM network, you need to configure the TIME_STEP environment variable based on the step value and resize the model input.
Assume that the number of data records for forward inference at a time is <code class="docutils literal notranslate"><span class="pre">input_num</span></code>. The resize value of the input node of sequence data is <code class="docutils literal notranslate"><span class="pre">input_num</span> <span class="pre">x</span> <span class="pre">step</span></code>, and the resize value of the input node of non-sequence data is <code class="docutils literal notranslate"><span class="pre">input_num</span></code>.</p>
<p>Models with the proposal operator do not support batch running or the resizie operation.</p>
</section>
<section id="node-name-change">
<h3>Node Name Change<a class="headerlink" href="#node-name-change" title="Permalink to this headline"></a></h3>
<p>After a model is converted into an NNIE model, the name of each node may change. You can use the Netron to open the model and obtain the new node name.</p>
</section>
<section id="proposal-operator-usage-description">
<h3>Proposal Operator Usage Description<a class="headerlink" href="#proposal-operator-usage-description" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite provides the sample code of the proposal operator. In this sample, the proposal operator and its infer shape are registered in custom operator mode. You can change it to the implementation that matches your own model, and then perform <a class="reference internal" href="#integration"><span class="std std-doc">integration</span></a>.</p>
<blockquote>
<div><p>Download address of the complete sample code:</p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/tree/r1.11/docs/sample_code/nnie_proposal">https://gitee.com/mindspore/docs/tree/r1.11/docs/sample_code/nnie_proposal</a></p>
</div></blockquote>
</section>
<section id="segmentation-mechanism-and-restrictions">
<h3>Segmentation Mechanism and Restrictions<a class="headerlink" href="#segmentation-mechanism-and-restrictions" title="Permalink to this headline"></a></h3>
<p>Due to the restrictions on the operators supported by the NNIE chip, if there are operators that are not supported by the NNIE chip, the model needs to be divided into supported layers and unsupported layers.
The chip on the board supports a maximum of eight supported layers. If the number of supported layers after segmentation is greater than 8, the model cannot run. You can observe the custom operator (whose attribute contains type:NNIE) by using Netron to obtain the number of supported layers after conversion.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="npu_info.html" class="btn btn-neutral float-left" title="NPU Integration Information" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="tensorrt_info.html" class="btn btn-neutral float-right" title="TensorRT Integration Information" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>