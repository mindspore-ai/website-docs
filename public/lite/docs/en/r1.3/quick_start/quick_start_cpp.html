

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Experience C++ Simple Inference Demo &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Experience Java Simple Inference Demo" href="quick_start_java.html" />
    <link rel="prev" title="Building MindSpore Lite" href="../use/build.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Experience C++ Simple Inference Demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#building-and-running">Building and Running</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linux-x86">Linux x86</a></li>
<li class="toctree-l3"><a class="reference internal" href="#windows">Windows</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#configure-cmake">Configure CMake</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-loading">Model Loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-build">Model Build</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-inference">Model Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#memory-release">Memory Release</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Android Application Development Based on JNI Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption"><span class="caption-text">Inference on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/post_training_quantization.html">Optimizing the Model (Quantization After Training)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/micro.html">Perform Inference on Mini and Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/visual_tool.html">Visual Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/obfuscator_tool.html">Obfuscator Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Experience C++ Simple Inference Demo</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/quick_start/quick_start_cpp.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="experience-c-simple-inference-demo">
<h1>Experience C++ Simple Inference Demo<a class="headerlink" href="#experience-c-simple-inference-demo" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Windows</span></code> <code class="docutils literal notranslate"><span class="pre">x86</span></code> <code class="docutils literal notranslate"><span class="pre">C++</span></code> <code class="docutils literal notranslate"><span class="pre">Whole</span> <span class="pre">Process``Inference</span> <span class="pre">Application</span></code> <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Preparation</span></code> <code class="docutils literal notranslate"><span class="pre">Beginner</span></code></p>
<!-- TOC -->
<ul class="simple">
<li><p><a class="reference external" href="#experience-c-simple-inference-demo">Experience C++ Simple Inference Demo</a></p>
<ul>
<li><p><a class="reference external" href="#overview">Overview</a></p></li>
<li><p><a class="reference external" href="#building-and-running">Building and Running</a></p>
<ul>
<li><p><a class="reference external" href="#linux-x86">Linux x86</a></p></li>
<li><p><a class="reference external" href="#windows">Windows</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#cmake-integration">CMake Integration</a></p></li>
<li><p><a class="reference external" href="#model-loading">Model Loading</a></p></li>
<li><p><a class="reference external" href="#model-build">Model Build</a></p></li>
<li><p><a class="reference external" href="#model-inference">Model Inference</a></p></li>
<li><p><a class="reference external" href="#memory-release">Memory Release</a></p></li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p><a href="https://gitee.com/mindspore/docs/blob/r1.3/docs/lite/docs/source_en/quick_start/quick_start_cpp.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>This tutorial provides a MindSpore Lite inference demo. It demonstrates the basic on-device inference process using C++ by inputting random data, executing inference, and printing the inference result. You can quickly understand how to use inference-related APIs on MindSpore Lite. In this tutorial, the randomly generated data is used as the input data to perform the inference on the MobileNetV2 model and print the output data. The code is stored in the <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.3/mindspore/lite/examples/quick_start_cpp">mindspore/lite/examples/quick_start_cpp</a> directory.</p>
<p>The MindSpore Lite inference steps are as follows:</p>
<ol class="simple">
<li><p>Load the model: Read the <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model converted by the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/converter_tool.html">model conversion tool</a> from the file system, import the model by using <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.3/api_cpp/lite.html#import">mindspore::lite::Model::Import</a>, parse the model, and create the <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">*</span></code>.</p></li>
<li><p>Create and configure context: Create and configure <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.3/api_cpp/lite.html#context">context</a> to save some basic configuration parameters required by a session to guide graph build and execution.</p></li>
<li><p>Create a session: Create <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.3/api_cpp/session.html#litesession">LiteSession</a> and configure the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.3/api_cpp/lite.html#context">context</a> obtained in the previous step to the session.</p></li>
<li><p>Build a graph: Before performing inference, call the <code class="docutils literal notranslate"><span class="pre">CompileGraph</span></code> API of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.3/api_cpp/session.html#litesession">LiteSession</a> to build a graph. In the graph build phase, subgraph partition and operator selection and scheduling are performed, which takes a long time. Therefore, it is recommended that with one <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.3/api_cpp/session.html#litesession">LiteSession</a> created, one graph be built. In this case, the inference will be performed for multiple times.</p></li>
<li><p>Input data: Before the graph is executed, data needs to be filled in the <code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">Tensor</span></code>.</p></li>
<li><p>Perform inference: Use <code class="docutils literal notranslate"><span class="pre">RunGraph</span></code> of the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.3/api_cpp/session.html#litesession">LiteSession</a> to perform model inference.</p></li>
<li><p>Obtain the output: After the graph execution is complete, you can obtain the inference result by <code class="docutils literal notranslate"><span class="pre">outputting</span> <span class="pre">the</span> <span class="pre">tensor</span></code>.</p></li>
<li><p>Release the memory: If the MindSpore Lite inference framework is not required, release the created <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.3/api_cpp/session.html#litesession">LiteSession</a> and <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.3/api_cpp/lite.html#model">Model</a>.</p></li>
</ol>
<p><img alt="img" src="../_images/lite_runtime.png" /></p>
<blockquote>
<div><p>To view the advanced usage of MindSpore Lite, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/runtime_cpp.html">Using Runtime to Perform Inference (C++)</a>].</p>
</div></blockquote>
</div>
<div class="section" id="building-and-running">
<h2>Building and Running<a class="headerlink" href="#building-and-running" title="Permalink to this headline">¶</a></h2>
<div class="section" id="linux-x86">
<h3>Linux x86<a class="headerlink" href="#linux-x86" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Environment requirements</p>
<ul class="simple">
<li><p>System environment: Linux x86_64 (Ubuntu 18.04.02LTS is recommended.)</p></li>
<li><p>Build dependency:</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Build</p>
<p>Run the <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.3/mindspore/lite/examples/quick_start_cpp/build.sh">build script</a> in the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp</span></code> directory to automatically download the MindSpore Lite inference framework library and model files and build the demo.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash build.sh
</pre></div>
</div>
<blockquote>
<div><p>If the MindSpore Lite inference framework fails to be downloaded by using this build script, manually download the MindSpore Lite model inference framework <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/downloads.html">mindspore-lite-{version}-linux-x64.tar.gz</a> whose hardware platform is CPU and operating system is Ubuntu-x64, and copy the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> file in the decompressed lib directory to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/lib</span></code> directory. Also copy the files from <code class="docutils literal notranslate"><span class="pre">runtime/include</span></code> to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/include</span></code> directory.</p>
<p>If the MobileNetV2 model fails to be downloaded, manually download the model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.ms">mobilenetv2.ms</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/model</span></code> directory.</p>
<p>After manually downloading and placing the file in the specified location, you need to execute the build.sh script again to complete the compilation.</p>
</div></blockquote>
</li>
<li><p>Inference</p>
<p>After the build, go to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/build</span></code> directory and run the following command to experience MindSpore Lite inference on the MobileNetV2 model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./mindspore_quick_start_cpp ../model/mobilenetv2.ms
</pre></div>
</div>
<p>After the execution, the following information is displayed, including the tensor name, tensor size, number of output tensors, and the first 50 pieces of data.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tensor name is:Default/head-MobileNetV2Head/Softmax-op204 tensor size is:4000 tensor elements num is:1000
output data is:5.26823e-05 0.00049752 0.000296722 0.000377607 0.000177048 8.02107e-05 0.000212864 0.000422286 0.000273189 0.000234105 0.00099807 0.0042331 0.00204993 0.00124968 0.00294458 0.00139795 0.00111545 0.000656357 0.000809457 0.00153731 0.000621049 0.00224637 0.00127045 0.00187557 0.000420144 0.000150638 0.000266477 0.000438628 0.000187773 0.00054668 0.000212853 0.000921661 0.000127179 0.000565873 0.00100394 0.000300159 0.000282677 0.000358067 0.00215288 0.000477845 0.00107596 0.00065134 0.000722132 0.000807501 0.000631415 0.00043247 0.00125898 0.000255094 8.2606e-05 9.91917e-05 0.000794512
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="windows">
<h3>Windows<a class="headerlink" href="#windows" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Environment requirements</p>
<ul class="simple">
<li><p>System environment: 64-bit Windows 7 or 64-bit Windows 10</p></li>
<li><p>Build dependency:</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://sourceforge.net/projects/mingw-w64/files/ToolchainstargettingWin64/PersonalBuilds/mingw-builds/7.3.0/threads-posix/seh/x86_64-7.3.0-release-posix-seh-rt_v5-rev0.7z/download">MinGW GCC</a> = 7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Build</p>
<ul class="simple">
<li><p>Download the library: Manually download the MindSpore Lite model inference framework <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/downloads.html">mindspore-lite-{version}-win-x64.zip</a> whose hardware platform is CPU and operating system is Windows-x64. Copy all the files in the decompressed <code class="docutils literal notranslate"><span class="pre">runtime/lib</span></code> directory to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/lib</span></code> project directory, and change the include directory to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/include</span></code> project directory. (Note: The <code class="docutils literal notranslate"><span class="pre">lib</span></code> and <code class="docutils literal notranslate"><span class="pre">include</span></code> directories under the project need to be created manually)</p></li>
<li><p>Download the model: Manually download the model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.ms">mobilenetv2.ms</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/model</span></code> directory.</p></li>
<li><p>Build the demo: Run the <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.3/mindspore/lite/examples/quick_start_cpp/build.bat">build script</a> in the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp</span></code> directory to automatically download related files and build the Demo.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call build.bat
</pre></div>
</div>
</li>
<li><p>Inference</p>
<p>After the build, go to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_cpp/build</span></code> directory and run the following command to experience MindSpore Lite inference on the MobileNetV2 model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">set</span> <span class="nv">PATH</span><span class="o">=</span>../lib<span class="p">;</span>%PATH%
call mindspore_quick_start_cpp.exe ../model/mobilenetv2.ms
</pre></div>
</div>
<p>After the execution, the following information is displayed, including the tensor name, tensor size, number of output tensors, and the first 50 pieces of data.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tensor name is:Default/head-MobileNetV2Head/Softmax-op204 tensor size is:4000 tensor elements num is:1000
output data is:5.26823e-05 0.00049752 0.000296722 0.000377607 0.000177048 8.02107e-05 0.000212864 0.000422286 0.000273189 0.000234105 0.00099807 0.0042331 0.00204993 0.00124968 0.00294458 0.00139795 0.00111545 0.000656357 0.000809457 0.00153731 0.000621049 0.00224637 0.00127045 0.00187557 0.000420144 0.000150638 0.000266477 0.000438628 0.000187773 0.00054668 0.000212853 0.000921661 0.000127179 0.000565873 0.00100394 0.000300159 0.000282677 0.000358067 0.00215288 0.000477845 0.00107596 0.00065134 0.000722132 0.000807501 0.000631415 0.00043247 0.00125898 0.000255094 8.2606e-05 9.91917e-05 0.000794512
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="section" id="configure-cmake">
<h2>Configure CMake<a class="headerlink" href="#configure-cmake" title="Permalink to this headline">¶</a></h2>
<p>The following is the sample code when integrating <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.a</span></code> static library through CMake.</p>
<blockquote>
<div><p>When CMake integrates the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.a</span></code> static library, the <code class="docutils literal notranslate"><span class="pre">-Wl,--whole-archive</span></code> option needs to be passed to the linker.</p>
<p>In addition, the build option for stack protection <code class="docutils literal notranslate"><span class="pre">-fstack-protector-strong</span></code> is added during the build of MindSpore Lite. Therefore, the <code class="docutils literal notranslate"><span class="pre">ssp</span></code> library in MinGW needs to be linked on the Windows platform.</p>
<p>In addition, the support of processing .so file is added during the build of MindSpore Lite. Therefore, the <code class="docutils literal notranslate"><span class="pre">dl</span></code> library needs to be linked on the Linux platform.</p>
</div></blockquote>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span> <span class="s">3.18.3</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">QuickStartCpp</span><span class="p">)</span>

<span class="nb">if</span><span class="p">(</span><span class="s">CMAKE_CXX_COMPILER_ID</span> <span class="s">STREQUAL</span> <span class="s2">&quot;GNU&quot;</span> <span class="s">AND</span> <span class="s">CMAKE_CXX_COMPILER_VERSION</span> <span class="s">VERSION_LESS</span> <span class="s">7.3.0</span><span class="p">)</span>
    <span class="nb">message</span><span class="p">(</span><span class="s">FATAL_ERROR</span> <span class="s2">&quot;GCC version ${CMAKE_CXX_COMPILER_VERSION} must not be less than 7.3.0&quot;</span><span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span>

<span class="c"># Add the directory to include search path</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="p">)</span>

<span class="c"># Add the directory to link search path</span>
<span class="nb">link_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="s">/lib</span><span class="p">)</span>

<span class="nb">file</span><span class="p">(</span><span class="s">GLOB_RECURSE</span> <span class="s">QUICK_START_CXX</span> <span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="s">/*.cc</span><span class="p">)</span>
<span class="nb">add_executable</span><span class="p">(</span><span class="s">mindspore_quick_start_cpp</span> <span class="o">${</span><span class="nv">QUICK_START_CXX</span><span class="o">}</span><span class="p">)</span>

<span class="nb">target_link_libraries</span><span class="p">(</span>
        <span class="s">mindspore_quick_start_cpp</span>
        <span class="s">-Wl,--whole-archive</span> <span class="s">mindspore-lite</span> <span class="s">-Wl,--no-whole-archive</span>
        <span class="s">pthread</span>
<span class="p">)</span>

<span class="c"># Due to the increased compilation options for stack protection,</span>
<span class="c"># it is necessary to target link ssp library when Use the static library in Windows.</span>
<span class="nb">if</span><span class="p">(</span><span class="s">WIN32</span><span class="p">)</span>
    <span class="nb">target_link_libraries</span><span class="p">(</span>
            <span class="s">mindspore_quick_start_cpp</span>
            <span class="s">ssp</span>
    <span class="p">)</span>
<span class="nb">else</span><span class="p">()</span>
    <span class="nb">target_link_libraries</span><span class="p">(</span>
            <span class="s">mindspore_quick_start_cpp</span>
            <span class="s">dl</span>
    <span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="model-loading">
<h2>Model Loading<a class="headerlink" href="#model-loading" title="Permalink to this headline">¶</a></h2>
<p>Read the MindSpore Lite model from the file system and use the <code class="docutils literal notranslate"><span class="pre">mindspore::lite::Model::Import</span></code> function to import the model for parsing.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Read model file.</span>
<span class="kt">size_t</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="kt">char</span> <span class="o">*</span><span class="n">model_buf</span> <span class="o">=</span> <span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">model_buf</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Read model file failed.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Load the .ms model.</span>
<span class="k">auto</span> <span class="n">model</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Model</span><span class="o">::</span><span class="n">Import</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span> <span class="n">size</span><span class="p">);</span>
<span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">model</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Import model file failed.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="k">return</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="model-build">
<h2>Model Build<a class="headerlink" href="#model-build" title="Permalink to this headline">¶</a></h2>
<p>Model build includes context configuration creation, session creation, and graph build.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span> <span class="o">*</span><span class="n">Compile</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Model</span> <span class="o">*</span><span class="n">model</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// Create and init context.</span>
  <span class="k">auto</span> <span class="n">context</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">context</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;New context failed while.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="k">nullptr</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Create the session.</span>
  <span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span> <span class="o">*</span><span class="n">session</span> <span class="o">=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span><span class="o">::</span><span class="n">CreateSession</span><span class="p">(</span><span class="n">context</span><span class="p">.</span><span class="n">get</span><span class="p">());</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">session</span> <span class="o">==</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;CreateSession failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="k">nullptr</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Build a graph.</span>
  <span class="k">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">CompileGraph</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">delete</span> <span class="n">session</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Compile failed while running.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="k">nullptr</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Note: when use model-&gt;Free(), the model can not be compiled again.</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">model</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">model</span><span class="o">-&gt;</span><span class="n">Free</span><span class="p">();</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">session</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="model-inference">
<h2>Model Inference<a class="headerlink" href="#model-inference" title="Permalink to this headline">¶</a></h2>
<p>Model inference includes data input, inference execution, and output obtaining. In this example, the input data is randomly generated, and the output result is printed after inference.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span> <span class="nf">Run</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">session</span><span class="o">::</span><span class="n">LiteSession</span> <span class="o">*</span><span class="n">session</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">auto</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>

  <span class="c1">// Generate random data as input data.</span>
  <span class="k">auto</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Generate Random Input Data failed.&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Run Inference.</span>
  <span class="n">ret</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">RunGraph</span><span class="p">();</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">ret</span> <span class="o">!=</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cerr</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;Inference error &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">ret</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">return</span> <span class="n">ret</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="c1">// Get Output Tensor Data.</span>
  <span class="k">auto</span> <span class="n">out_tensors</span> <span class="o">=</span> <span class="n">session</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
  <span class="k">for</span> <span class="p">(</span><span class="k">auto</span> <span class="nl">tensor</span> <span class="p">:</span> <span class="n">out_tensors</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;tensor name is:&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">tensor</span><span class="p">.</span><span class="n">first</span> <span class="o">&lt;&lt;</span> <span class="s">&quot; tensor size is:&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">tensor</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">()</span>
              <span class="o">&lt;&lt;</span> <span class="s">&quot; tensor elements num is:&quot;</span> <span class="o">&lt;&lt;</span> <span class="n">tensor</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">ElementsNum</span><span class="p">()</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
    <span class="k">auto</span> <span class="n">out_data</span> <span class="o">=</span> <span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span> <span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">());</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;output data is:&quot;</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">tensor</span><span class="p">.</span><span class="n">second</span><span class="o">-&gt;</span><span class="n">ElementsNum</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">50</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">&quot; &quot;</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="memory-release">
<h2>Memory Release<a class="headerlink" href="#memory-release" title="Permalink to this headline">¶</a></h2>
<p>If the MindSpore Lite inference framework is not required, release the created <code class="docutils literal notranslate"><span class="pre">LiteSession</span></code> and <code class="docutils literal notranslate"><span class="pre">Model</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Delete model buffer.</span>
<span class="k">delete</span> <span class="n">model</span><span class="p">;</span>
<span class="c1">// Delete session buffer.</span>
<span class="k">delete</span> <span class="n">session</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="quick_start_java.html" class="btn btn-neutral float-right" title="Experience Java Simple Inference Demo" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../use/build.html" class="btn btn-neutral float-left" title="Building MindSpore Lite" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, MindSpore Lite

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>