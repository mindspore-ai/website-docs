<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optimizing the Model (Quantization After Training) &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Data Preprocessing" href="data_preprocessing.html" />
    <link rel="prev" title="Converting Models for Inference" href="converter_tool.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Android Application Development Based on JNI Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimizing the Model (Quantization After Training)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#weight-quantization">Weight Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#parameter-description">Parameter Description</a></li>
<li class="toctree-l3"><a class="reference internal" href="#procedure">Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#partial-model-accuracy-result">Partial Model Accuracy Result</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#full-quantization">Full Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Parameter Description</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Procedure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">Partial Model Accuracy Result</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Perform Inference on Mini and Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visual Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Obfuscator Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Optimizing the Model (Quantization After Training)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/post_training_quantization.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="optimizing-the-model-quantization-after-training">
<h1>Optimizing the Model (Quantization After Training)<a class="headerlink" href="#optimizing-the-model-quantization-after-training" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Windows</span></code> <code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Converting</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Optimization</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.3/docs/lite/docs/source_en/use/post_training_quantization.md"><img alt="View Source On Gitee" src="https://gitee.com/mindspore/docs/raw/r1.3/resource/_static/logo_source.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Converting a trained <code class="docutils literal notranslate"><span class="pre">float32</span></code> model into an <code class="docutils literal notranslate"><span class="pre">int8</span></code> model through quantization after training can reduce the model size and improve the inference performance. In MindSpore Lite, this function is integrated into the model conversion tool <code class="docutils literal notranslate"><span class="pre">conveter_lite</span></code>. You can add command line parameters to convert a model into a quantization model.</p>
<p>MindSpore Lite quantization after training is classified into two types:</p>
<ol class="arabic simple">
<li><p>Weight quantization: quantizes a weight of a model and compresses only the model size. <code class="docutils literal notranslate"><span class="pre">float32</span></code> inference is still performed during inference.</p></li>
<li><p>Full quantization: quantizes the weight and activation value of a model. The <code class="docutils literal notranslate"><span class="pre">int</span></code> operation is performed during inference to improve the model inference speed and reduce power consumption.</p></li>
</ol>
<p>Data types and parameters required for the two types are different, but both can be set by using the conversion tool. For details about how to use the conversion tool <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code>, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/converter_tool.html">Converting Training Models</a>. After the tool configuration is completed, you can enable quantization after training.</p>
</section>
<section id="weight-quantization">
<h2>Weight Quantization<a class="headerlink" href="#weight-quantization" title="Permalink to this headline"></a></h2>
<p>Quantization of 1 to 16 bits is supported. A smaller number of quantization bits indicates a higher model compression ratio and a large accuracy loss. You can use the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/benchmark_tool.html">Benchmark tool</a> to evaluate the accuracy and determine the number of quantization bits. Generally, the average relative error (accuracyThreshold) is within 4% which is small. The following describes the usage and effect of weight quantization.</p>
<section id="parameter-description">
<h3>Parameter Description<a class="headerlink" href="#parameter-description" title="Permalink to this headline"></a></h3>
<p>Generally, the weight quantization conversion command is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--quantType<span class="o">=</span>WeightQuant<span class="w"> </span>--bitNum<span class="o">=</span>BitNumValue<span class="w"> </span>--quantWeightSize<span class="o">=</span>ConvWeightQuantSizeThresholdValue<span class="w"> </span>--quantWeightChannel<span class="o">=</span>ConvWeightQuantChannelThresholdValue
</pre></div>
</div>
<p>Parameters of this command are described as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Attribute</p></th>
<th class="head"><p>Function Description</p></th>
<th class="head"><p>Parameter Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Value Range</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--quantType=&lt;QUANTTYPE&gt;</span></code></p></td>
<td><p>Mandatory</p></td>
<td><p>Set this parameter to WeightQuant to enable weight quantization.</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>WeightQuant</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--bitNum=&lt;BITNUM&gt;</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Number of bits for weight quantization. Currently, 1 to 16 bits are supported.</p></td>
<td><p>Integer</p></td>
<td><p>8</p></td>
<td><p>[1, 16]</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--quantWeightSize=&lt;QUANTWEIGHTSIZE&gt;</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set the threshold of the convolution kernel size for weight quantization. If the size of the convolution kernel is greater than the threshold, the weight is quantized. Recommended value: 500</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
<td><p>[0, +∞)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--quantWeightChannel=&lt;QUANTWEIGHTCHANNEL&gt;</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set the threshold of the number of convolution channels for weight quantization. If the number of convolution channels is greater than the threshold, the weight is quantized. Recommended value: 16</p></td>
<td><p>Integer</p></td>
<td><p>16</p></td>
<td><p>[0, +∞)</p></td>
</tr>
</tbody>
</table>
<p>You can adjust the weight quantization parameters based on the model and your requirements.</p>
<blockquote>
<div><p>To ensure the accuracy of weight quantization, you are advised to set the value range of the <code class="docutils literal notranslate"><span class="pre">--bitNum</span></code> parameter to 8 bits to 16 bits.</p>
</div></blockquote>
</section>
<section id="procedure">
<h3>Procedure<a class="headerlink" href="#procedure" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Correctly build the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> executable file. For details about how to obtain the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> tool and configure environment variables, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/build.html">Building MindSpore Lite</a>.</p></li>
<li><p>Take the TensorFlow Lite model as an example. Run the following command to convert the weight quantization model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>Inception_v3.tflite<span class="w"> </span>--outputFile<span class="o">=</span>Inception_v3.tflite<span class="w"> </span>--quantType<span class="o">=</span>WeightQuant<span class="w"> </span>--bitNum<span class="o">=</span><span class="m">8</span><span class="w"> </span>--quantWeightSize<span class="o">=</span><span class="m">0</span><span class="w"> </span>--quantWeightChannel<span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
</li>
<li><p>After the preceding command is successfully executed, the quantization model <code class="docutils literal notranslate"><span class="pre">Inception_v3.tflite.ms</span></code> is obtained. The size of the quantization model usually decreases to one fourth of the FP32 model.</p></li>
</ol>
</section>
<section id="partial-model-accuracy-result">
<h3>Partial Model Accuracy Result<a class="headerlink" href="#partial-model-accuracy-result" title="Permalink to this headline"></a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Test Dataset</p></th>
<th class="head"><p>FP32 Model Accuracy</p></th>
<th class="head"><p>Weight Quantization Accuracy (8 bits)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>77.60%</p></td>
<td><p>77.53%</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>70.96%</p></td>
<td><p>70.56%</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>71.56%</p></td>
<td><p>71.53%</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>All the preceding results are obtained in the x86 environment.</p>
</div></blockquote>
</section>
</section>
<section id="full-quantization">
<h2>Full Quantization<a class="headerlink" href="#full-quantization" title="Permalink to this headline"></a></h2>
<p>In scenarios where the model running speed needs to be improved and the model running power consumption needs to be reduced, the full quantization after training can be used. The following describes the usage and effect of full quantization.</p>
<section id="id1">
<h3>Parameter Description<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>Generally, the full quantization conversion command is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--quantType<span class="o">=</span>PostTraining<span class="w"> </span>--bitNum<span class="o">=</span><span class="m">8</span><span class="w"> </span>--configFile<span class="o">=</span>config.cfg
</pre></div>
</div>
<p>Parameters of this command are described as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Attribute</p></th>
<th class="head"><p>Function Description</p></th>
<th class="head"><p>Parameter Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Value Range</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--quantType=&lt;QUANTTYPE&gt;</span></code></p></td>
<td><p>Mandatory</p></td>
<td><p>Set this parameter to PostTraining to enable full quantization.</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>PostTraining</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--configFile=&lt;CONFIGFILE&gt;</span></code></p></td>
<td><p>Mandatory</p></td>
<td><p>Path of a calibration dataset configuration file</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--bitNum=&lt;BITNUM&gt;</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Number of bits for full quantization. Currently, 1 to 8 bits are supported.</p></td>
<td><p>Integer</p></td>
<td><p>8</p></td>
<td><p>[1, 8]</p></td>
</tr>
</tbody>
</table>
<p>To compute a quantization parameter of an activation value, you need to provide a calibration dataset. It is recommended that the calibration dataset be obtained from the actual inference scenario and can represent the actual input of a model. The number of data records is about 100. Please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/converter_tool.html#parameter-description">Parameter Description</a> for <code class="docutils literal notranslate"><span class="pre">configFile</span></code> configuration.</p>
<blockquote>
<div><p>For a multi-input model, different input data must be stored in different directories. In addition, names of all files in each directory must be sorted in ascending lexicographic order to ensure one-to-one mapping. For example, a model has two inputs input0 and input1, and there are two calibration datasets (batch_count=2). The data of input0 is stored in the /dir/input0/ directory. The input data files are data_1.bin and data_2.bin. The data of input1 is stored in the /dir/input1/ directory. The input data files are data_a.bin and data_b.bin. The (data_1.bin, data_a.bin) is regarded as a group of inputs and the (data_2.bin, data_b.bin) is regarded as another group of inputs.</p>
</div></blockquote>
</section>
<section id="id2">
<h3>Procedure<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Correctly build the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> executable file.</p></li>
<li><p>Prepare a calibration dataset. Assume that the dataset is stored in the <code class="docutils literal notranslate"><span class="pre">/dir/images</span></code> directory. Configure the <code class="docutils literal notranslate"><span class="pre">config.cfg</span></code> file. The content is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>image_path=/dir/images
batch_count=100
method_x=MAX_MIN
thread_num=1
bias_correction=true
</pre></div>
</div>
<p>The calibration dataset can be a subset of the test dataset. Each file stored in the <code class="docutils literal notranslate"><span class="pre">/dir/images</span></code> directory must be pre-processed input data, and each file can be directly used as the input for inference.</p>
</li>
<li><p>Take the MindSpore model as an example. Run the following command to convert the full quantization model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>lenet.mindir<span class="w"> </span>--outputFile<span class="o">=</span>lenet_quant<span class="w"> </span>--quantType<span class="o">=</span>PostTraining<span class="w"> </span>--configFile<span class="o">=</span>config.cfg
</pre></div>
</div>
</li>
<li><p>After the preceding command is successfully executed, the quantization model <code class="docutils literal notranslate"><span class="pre">lenet_quant.ms</span></code> is obtained. Generally, the size of the quantization model decreases to one fourth of the FP32 model.</p></li>
</ol>
</section>
<section id="id3">
<h3>Partial Model Accuracy Result<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Test Dataset</p></th>
<th class="head"><p>method_x</p></th>
<th class="head"><p>FP32 Model Accuracy</p></th>
<th class="head"><p>Full Quantization Accuracy (8 bits)</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>KL</p></td>
<td><p>77.60%</p></td>
<td><p>77.40%</p></td>
<td><p>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>KL</p></td>
<td><p>70.96%</p></td>
<td><p>70.31%</p></td>
<td><p>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>MAX_MIN</p></td>
<td><p>71.56%</p></td>
<td><p>71.16%</p></td>
<td><p>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>All the preceding results are obtained in the x86 environment, and <code class="docutils literal notranslate"><span class="pre">bias_correction=true</span></code> is set.</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="converter_tool.html" class="btn btn-neutral float-left" title="Converting Models for Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data_preprocessing.html" class="btn btn-neutral float-right" title="Data Preprocessing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>