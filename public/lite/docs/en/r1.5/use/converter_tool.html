<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Converting Models for Inference &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Optimizing the Model (Quantization After Training)" href="post_training_quantization.html" />
    <link rel="prev" title="Implement Device Training Based On Java Interface" href="../quick_start/train_lenet_java.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Android Application Development Based on JNI Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Converting Models for Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#linux-environment-instructions">Linux Environment Instructions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#directory-structure">Directory Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#parameter-description">Parameter Description</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example">Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#windows-environment-instructions">Windows Environment Instructions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Environment Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Directory Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">Parameter Description</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-usage">Advanced Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pass-extension">Pass Extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="#operator-infershape-extension">Operator Infershape Extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">Example</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#compile">Compile</a></li>
<li class="toctree-l4"><a class="reference internal" href="#execute-program">Execute Program</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Optimizing the Model (Quantization After Training)</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Perform Inference on Mini and Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="register_kernel.html">Register Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Converting Models for Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/converter_tool.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="converting-models-for-inference">
<h1>Converting Models for Inference<a class="headerlink" href="#converting-models-for-inference" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Windows</span></code> <code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Converting</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.5/docs/lite/docs/source_en/use/converter_tool.md"><img alt="View Source On Gitee" src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite provides a tool for offline model conversion. It supports conversion of multiple types of models. The converted models can be used for inference. The command line parameters contain multiple personalized options, providing a convenient conversion method for users.</p>
<p>Currently, the following input formats are supported: MindSpore, TensorFlow Lite, Caffe, TensorFlow and ONNX.</p>
<p>The ms model converted by the conversion tool supports the conversion tool and the higher version of the Runtime framework to perform inference.</p>
</section>
<section id="linux-environment-instructions">
<h2>Linux Environment Instructions<a class="headerlink" href="#linux-environment-instructions" title="Permalink to this headline"></a></h2>
<section id="environment-preparation">
<h3>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline"></a></h3>
<p>To use the MindSpore Lite model conversion tool, you need to prepare the environment as follows:</p>
<ul>
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/build.html">Compile</a> or <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/downloads.html">download</a> model transfer tool.</p></li>
<li><p>Add the path of dynamic library required by the conversion tool to the environment variables LD_LIBRARY_PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} is the decompressed package path obtained by compiling or downloading.</p>
</li>
</ul>
</section>
<section id="directory-structure">
<h3>Directory Structure<a class="headerlink" href="#directory-structure" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
└── tools
    └── converter
        ├── include
        │   └── registry             # Header files of customized op, model parser, node parser and pass registration
        ├── converter                # Model conversion tool
        │   └── converter_lite       # Executable program
        └── lib                      # The dynamic link library that converter depends
            ├── libglog.so.0         # Dynamic library of Glog
            ├── libmslite_converter_plugin.so  # Dynamic library of plugin registry
            ├── libopencv_core.so.4.5          # Dynamic library of OpenCV
            ├── libopencv_imgcodecs.so.4.5     # Dynamic library of OpenCV
            └── libopencv_imgproc.so.4.5       # Dynamic library of OpenCV
</pre></div>
</div>
</section>
<section id="parameter-description">
<h3>Parameter Description<a class="headerlink" href="#parameter-description" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite model conversion tool provides multiple parameters.
You can enter <code class="docutils literal notranslate"><span class="pre">./converter_lite</span> <span class="pre">--help</span></code> to obtain the help information in real time.</p>
<p>The following describes the parameters in detail.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Mandatory or Not</p></th>
<th class="head"><p>Parameter Description</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Default Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--help</span></code></p></td>
<td><p>No</p></td>
<td><p>Prints all the help information.</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--fmk=&lt;FMK&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Original format of the input model.</p></td>
<td><p>MINDIR, CAFFE, TFLITE, TF, or ONNX</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--modelFile=&lt;MODELFILE&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Path of the input model.</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--outputFile=&lt;OUTPUTFILE&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Path of the output model. The suffix <code class="docutils literal notranslate"><span class="pre">.ms</span></code> can be automatically generated.</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--weightFile=&lt;WEIGHTFILE&gt;</span></code></p></td>
<td><p>Yes (for Caffe models only)</p></td>
<td><p>Path of the weight file of the input model.</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--configFile=&lt;CONFIGFILE&gt;</span></code></p></td>
<td><p>No</p></td>
<td><p>1) Configure quantization parameter; 2) Profile path for extension.</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--fp16=&lt;FP16&gt;</span></code></p></td>
<td><p>No</p></td>
<td><p>Serialize const tensor in Float16 data type, only effective for const tensor in Float32 data type.</p></td>
<td><p>on or off</p></td>
<td><p>off</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--inputShape=&lt;INPUTSHAPE&gt;</span></code></p></td>
<td><p>No</p></td>
<td><p>Set the dimension of the model input, the order of input dimensions is consistent with the original model. For some models, the model structure can be further optimized, but the transformed model may lose the characteristics of dynamic shape. Multiple inputs are separated by <code class="docutils literal notranslate"><span class="pre">;</span></code>, and surround with <code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code></p></td>
<td><p>e.g.  “inTensorName_1: 1,32,32,4;inTensorName_2:1,64,64,4;”</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--inputDataFormat=&lt;INPUTDATAFORMAT&gt;</span></code></p></td>
<td><p>No</p></td>
<td><p>Set the input format of exported model. Only valid for 4-dimensional inputs.</p></td>
<td><p>NHWC, NCHW</p></td>
<td><p>NHWC</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>The parameter name and parameter value are separated by an equal sign (=) and no space is allowed between them.</p></li>
<li><p>The Caffe model is divided into two files: model structure <code class="docutils literal notranslate"><span class="pre">*.prototxt</span></code>, corresponding to the <code class="docutils literal notranslate"><span class="pre">--modelFile</span></code> parameter; model weight <code class="docutils literal notranslate"><span class="pre">*.caffemodel</span></code>, corresponding to the <code class="docutils literal notranslate"><span class="pre">--weightFile</span></code> parameter.</p></li>
<li><p>The priority of <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> option is very low. For example, if quantization is enabled, <code class="docutils literal notranslate"><span class="pre">--fp16</span></code> will no longer take effect on const tensors that have been quantized. All in all, this option only takes effect on const tensors of Float32 when serializing model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">inputDataFormat</span></code>: generally, in the scenario of integrating third-party hardware of NCHW specification(<a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/nnie.html#nnie">Usage Description of the Integrated NNIE</a>), designated as NCHW will have a significant performance improvement over NHWC. In other scenarios, users can also set as needed.</p></li>
<li><p>The calibration dataset configuration file uses the <code class="docutils literal notranslate"><span class="pre">key=value</span></code> mode to define related parameters. For the configuration parameters related to quantization, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/post_training_quantization.html">post training quantization</a>. For the configuration parameters related to extension, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/nnie.html#extension-configuration">Extension Configuration</a>.</p></li>
</ul>
</div></blockquote>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline"></a></h3>
<p>The following describes how to use the conversion command by using several common examples.</p>
<ul>
<li><p>Take the Caffe model LeNet as an example. Run the following conversion command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>CAFFE<span class="w"> </span>--modelFile<span class="o">=</span>lenet.prototxt<span class="w"> </span>--weightFile<span class="o">=</span>lenet.caffemodel<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
<p>In this example, the Caffe model is used. Therefore, the model structure and model weight files are required. Two more parameters <code class="docutils literal notranslate"><span class="pre">fmk</span></code> and <code class="docutils literal notranslate"><span class="pre">outputFile</span></code> are also required.</p>
<p>The output is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
<p>This indicates that the Caffe model is successfully converted into the MindSpore Lite model and the new file <code class="docutils literal notranslate"><span class="pre">lenet.ms</span></code> is generated.</p>
</li>
<li><p>The following uses the MindSpore, TensorFlow Lite, TensorFlow and ONNX models as examples to describe how to run the conversion command.</p>
<ul>
<li><p>MindSpore model <code class="docutils literal notranslate"><span class="pre">model.mindir</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>model.mindir<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> model exported by MindSpore v1.1.1 or earlier is recommended to be converted to the <code class="docutils literal notranslate"><span class="pre">ms</span></code> model using the converter tool of the corresponding version. MindSpore v1.1.1 and later versions, the converter tool will be forward compatible.</p>
</div></blockquote>
<ul>
<li><p>TensorFlow Lite model <code class="docutils literal notranslate"><span class="pre">model.tflite</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>model.tflite<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
</li>
<li><p>TensorFlow model <code class="docutils literal notranslate"><span class="pre">model.pb</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TF<span class="w"> </span>--modelFile<span class="o">=</span>model.pb<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
</li>
<li><p>ONNX model <code class="docutils literal notranslate"><span class="pre">model.onnx</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span>model.onnx<span class="w"> </span>--outputFile<span class="o">=</span>model
</pre></div>
</div>
</li>
</ul>
<p>In the preceding scenarios, the following information is displayed, indicating that the conversion is successful. In addition, the target file <code class="docutils literal notranslate"><span class="pre">model.ms</span></code> is obtained.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="windows-environment-instructions">
<h2>Windows Environment Instructions<a class="headerlink" href="#windows-environment-instructions" title="Permalink to this headline"></a></h2>
<section id="id1">
<h3>Environment Preparation<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<p>To use the MindSpore Lite model conversion tool, the following environment preparations are required.</p>
<ul>
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/build.html">Compile</a> or <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/downloads.html">download</a> model transfer tool.</p></li>
<li><p>Add the path of dynamic library required by the conversion tool to the environment variables PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">set</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>%PACKAGE_ROOT_PATH%<span class="se">\t</span>ools<span class="se">\c</span>onverter<span class="se">\l</span>ib<span class="p">;</span>%PATH%
</pre></div>
</div>
</li>
</ul>
</section>
<section id="id2">
<h3>Directory Structure<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-win-x64
└── tools
    └── converter # Model conversion tool
        ├── converter
        │   └── converter_lite.exe    # Executable program
        └── lib
            ├── libgcc_s_seh-1.dll    # Dynamic library of MinGW
            ├── libglog.dll           # Dynamic library of Glog
            ├── libmslite_converter_plugin.dll   # Dynamic library of plugin registry
            ├── libmslite_converter_plugin.dll.a # Link file of Dynamic library of plugin registry
            ├── libssp-0.dll          # Dynamic library of MinGW
            ├── libstdc++-6.dll       # Dynamic library of MinGW
            └── libwinpthread-1.dll   # Dynamic library of MinGW
</pre></div>
</div>
</section>
<section id="id3">
<h3>Parameter Description<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<p>Refer to the Linux environment model conversion tool <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/converter_tool.html#parameter-description">parameter description</a>.</p>
</section>
<section id="id4">
<h3>Example<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<p>Set the log printing level to INFO.</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">set</span> <span class="nv">GLOG_v</span><span class="p">=</span>1
</pre></div>
</div>
<blockquote>
<div><p>Log level: 0 is DEBUG, 1 is INFO, 2 is WARNING, 3 is ERROR.</p>
</div></blockquote>
<p>Several common examples are selected below to illustrate the use of conversion commands.</p>
<ul>
<li><p>Take the Caffe model LeNet as an example to execute the conversion command.</p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">call</span> converter_lite --fmk=CAFFE --modelFile=lenet.prototxt --weightFile=lenet.caffemodel --outputFile=lenet
</pre></div>
</div>
<p>In this example, because the Caffe model is used, two input files of model structure and model weight are required. Then with the fmk type and output path two parameters which are required, you can successfully execute.</p>
<p>The result is shown as:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
<p>This means that the Caffe model has been successfully converted to the MindSpore Lite model and the new file <code class="docutils literal notranslate"><span class="pre">lenet.ms</span></code> has been obtained.</p>
</li>
<li><p>Take MindSpore, TensorFlow Lite, ONNX model format and perceptual quantization model as examples to execute conversion commands.</p>
<ul>
<li><p>MindSpore model <code class="docutils literal notranslate"><span class="pre">model.mindir</span></code></p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">call</span> converter_lite --fmk=MINDIR --modelFile=model.mindir --outputFile=model
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> model exported by MindSpore v1.1.1 or earlier is recommended to be converted to the <code class="docutils literal notranslate"><span class="pre">ms</span></code> model using the converter tool of the corresponding version. MindSpore v1.1.1 and later versions, the converter tool will be forward compatible.</p>
</div></blockquote>
<ul>
<li><p>TensorFlow Lite model<code class="docutils literal notranslate"><span class="pre">model.tflite</span></code></p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">call</span> converter_lite --fmk=TFLITE --modelFile=model.tflite --outputFile=model
</pre></div>
</div>
</li>
<li><p>TensorFlow model <code class="docutils literal notranslate"><span class="pre">model.pb</span></code></p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">call</span> converter_lite --fmk=TF --modelFile=model.pb --outputFile=model
</pre></div>
</div>
</li>
<li><p>ONNX model<code class="docutils literal notranslate"><span class="pre">model.onnx</span></code></p>
<div class="highlight-bat notranslate"><div class="highlight"><pre><span></span><span class="k">call</span> converter_lite --fmk=ONNX --modelFile=model.onnx --outputFile=model
</pre></div>
</div>
</li>
</ul>
<p>In the above cases, the following conversion success prompt is displayed, and the <code class="docutils literal notranslate"><span class="pre">model.ms</span></code> target file is obtained at the same time.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline"></a></h2>
<p>The extension ability is only supported in Linux, including node-parse extension, model-parse extension and graph-optimization extension. The users can combined them as needed to achieve their own intention.</p>
<blockquote>
<div><ul class="simple">
<li><p>node-parse extension: The users can define the process to parse a certain node of a model by themselves, which only support ONNX, CAFFE, TF and TFLITE. The related interface is <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/api_cpp/mindspore_converter.html#nodeparser">NodeParser</a>.</p></li>
<li><p>model-parse extension: The users can define the process to parse a model by themselves, which only support ONNX, CAFFE, TF and TFLITE. The related interface is <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/api_cpp/mindspore_converter.html#modelparser">ModelParser</a>.</p></li>
<li><p>graph-optimization extension: After parsing a model, the users can define the process to optimize the parsed graph. The related interface is <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/api_cpp/mindspore_registry.html#passbase">PassBase</a>.</p></li>
</ul>
<p>The node-parse extension needs to rely on the flatbuffers, protobuf and the serialization files of third-party frameworks, at the same time, the version of flatbuffers and the protobuf needs to be consistent with that of the released package, the serialized files must be compatible with that used by the released package. Note that the flatbuffers, protobuf and the serialization files are not provided in the released package, users need to compile and generate the serialized files by themselves. The users can obtain the basic information about <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/cmake/external_libs/flatbuffers.cmake">flabuffers</a>, <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/cmake/external_libs/protobuf.cmake">probobuf</a>, <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/third_party/proto/onnx">ONNX prototype file</a>, <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/third_party/proto/caffe">CAFFE prototype file</a>, <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/third_party/proto/tensorflow">TF prototype file</a> and <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/mindspore/lite/tools/converter/parser/tflite/schema.fbs">TFLITE prototype file</a> from the <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5">MindSpore WareHouse</a>.</p>
</div></blockquote>
<p>In this chapter, we will show the users an example of extending Mindspore Lite converter tool, covering the whole process of creating pass, compiling and linking. The example will help the users understand the graph-optimization extension as soon as possible.</p>
<p>The chapter takes a <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/add.tflite">add.tflite</a>, which only includes an opreator of adding, as an example. We will show the users how to convert the single operator of adding to that of <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/register_kernel.html#custom">Custom</a> and finally, obtain a model, which only includs a single operator of custom.</p>
<p>The code related to the example can be obtained from the directory <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.5/mindspore/lite/examples/converter_extend">mindspore/lite/examples/converter_extend</a>.</p>
<section id="pass-extension">
<h3>Pass Extension<a class="headerlink" href="#pass-extension" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Self-defined Pass: The users need to inherit the base class <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/api_cpp/mindspore_registry.html#passbase">PassBase</a>, and override the interface function <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/api_cpp/mindspore_registry.html#execute">Execute</a>。</p></li>
<li><p>Pass Registration: The users can directly call the registration interface <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/api_cpp/mindspore_registry.html#reg-pass">REG_PASS</a>, so that the self-defined pass can be registered in the converter tool of MindSpore Lite.</p></li>
</ol>
</section>
<section id="operator-infershape-extension">
<h3>Operator Infershape Extension<a class="headerlink" href="#operator-infershape-extension" title="Permalink to this headline"></a></h3>
<p>In the offline phase of conversion, we will infer the basic information of output tensors of each node of the model, including the format, data type and shape. So, in this phase, users need to provide the inferring process of self-defined operator. Here, users can refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/runtime_cpp.html#id19">Operator Infershape Extension</a>。</p>
</section>
<section id="id5">
<h3>Example<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<section id="compile">
<h4>Compile<a class="headerlink" href="#compile" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Environment Requirements</p>
<ul class="simple">
<li><p>System environment: Linux x86_64; Recommend Ubuntu 18.04.02LTS</p></li>
<li><p>compilation dependencies:</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Compilation and Build</p>
<p>Execute the script <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/mindspore/lite/examples/converter_extend/build.sh">build.sh</a> in the directory of <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/converter_extend</span></code>. And then, the released package of Mindspore Lite will be downloaded and the demo will be compiled automatically.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>build.sh
</pre></div>
</div>
<blockquote>
<div><p>If the automatic download is failed, users can download the specified package manually, of which the hardware platform is CPU and the system is Ubuntu-x64 <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/downloads.html">mindspore-lite-{version}-linux-x64.tar.gz</a>, After unzipping, please copy the directory of <code class="docutils literal notranslate"><span class="pre">tools/converter/lib</span></code> and <code class="docutils literal notranslate"><span class="pre">tools/converter/include</span></code> to the directory of <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/converter_extend</span></code>.</p>
<p>After manually downloading and storing the specified file, users need to execute the <code class="docutils literal notranslate"><span class="pre">build.sh</span></code> script to complete the compilation and build process.</p>
</div></blockquote>
</li>
<li><p>Compilation Result</p>
<p>The dynamic library <code class="docutils literal notranslate"><span class="pre">libconverter_extend_tutorial.so</span></code> will be generated in the directory of <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/converter_extend/build</span></code>.</p>
</li>
</ul>
</section>
<section id="execute-program">
<h4>Execute Program<a class="headerlink" href="#execute-program" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Copy library</p>
<p>Copy the dynamic library <code class="docutils literal notranslate"><span class="pre">libconverter_extend_tutorial.so</span></code> to the directory of <code class="docutils literal notranslate"><span class="pre">tools/converter/lib</span></code> of the released package.</p>
</li>
<li><p>Enter the conversion directory of the released package.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/converter
</pre></div>
</div>
</li>
<li><p>Create extension configuration file(converter.cfg), the content is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[registry]
plugin_path=libconverter_extend_tutorial.so      # users need to configure the correct path of the dynamic library
</pre></div>
</div>
</li>
<li><p>Add the required dynamic library to the environment variable <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>:/tools/converter/lib
</pre></div>
</div>
</li>
<li><p>Execute the script</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>add.tflite<span class="w"> </span>--configFile<span class="o">=</span>converter.cfg<span class="w"> </span>--outputFile<span class="o">=</span>add_extend
</pre></div>
</div>
</li>
</ol>
<p>The model file <code class="docutils literal notranslate"><span class="pre">add_extend.ms</span></code> will be generated, the place of which is up to the parameter <code class="docutils literal notranslate"><span class="pre">outputFile</span></code>.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../quick_start/train_lenet_java.html" class="btn btn-neutral float-left" title="Implement Device Training Based On Java Interface" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="post_training_quantization.html" class="btn btn-neutral float-right" title="Optimizing the Model (Quantization After Training)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>