<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Optimizing the Model (Quantization After Training) &mdash; MindSpore Lite master documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Data Preprocessing" href="data_preprocessing.html" />
    <link rel="prev" title="Converting Models for Inference" href="converter_tool.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Android Application Development Based on JNI Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimizing the Model (Quantization After Training)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuration-parameter">Configuration Parameter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common-quantization-parameter">Common Quantization Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mixed-bit-weight-quantization-parameter">Mixed Bit Weight Quantization Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#full-quantization-parameters">Full Quantization Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-preprocessing">Data Preprocessing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#weight-quantization">Weight Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mixed-bit-weight-quantization">Mixed Bit Weight Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fixed-bit-weight-quantization">Fixed Bit Weight Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#partial-model-accuracy-result">Partial Model Accuracy Result</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#full-quantization">Full Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Partial Model Accuracy Result</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Perform Inference on Mini and Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="register_kernel.html">Register Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Optimizing the Model (Quantization After Training)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/post_training_quantization.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="optimizing-the-model-quantization-after-training">
<h1>Optimizing the Model (Quantization After Training)<a class="headerlink" href="#optimizing-the-model-quantization-after-training" title="Permalink to this headline"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Windows</span></code> <code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Converting</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Optimization</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.5/docs/lite/docs/source_en/use/post_training_quantization.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Converting a trained <code class="docutils literal notranslate"><span class="pre">float32</span></code> model into an <code class="docutils literal notranslate"><span class="pre">int8</span></code> model through quantization after training can reduce the model size and improve the inference performance. In MindSpore Lite, this function is integrated into the model conversion tool <code class="docutils literal notranslate"><span class="pre">conveter_lite</span></code>. You can add command line parameters to convert a model into a quantization model.</p>
<p>MindSpore Lite quantization after training is classified into two types:</p>
<ol class="arabic simple">
<li><p>Weight quantization: quantizes a weight of a model and compresses only the model size. <code class="docutils literal notranslate"><span class="pre">float32</span></code> inference is still performed during inference.</p></li>
<li><p>Full quantization: quantizes the weight and activation value of a model. The <code class="docutils literal notranslate"><span class="pre">int</span></code> operation is performed during inference to improve the model inference speed and reduce power consumption.</p></li>
</ol>
</section>
<section id="configuration-parameter">
<h2>Configuration Parameter<a class="headerlink" href="#configuration-parameter" title="Permalink to this headline"></a></h2>
<p>Post training quantization can be enabled by configuring <code class="docutils literal notranslate"><span class="pre">configFile</span></code> through <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/converter_tool.html">Conversion Tool</a>. The configuration file adopts the style of <code class="docutils literal notranslate"><span class="pre">INI</span></code>, For quantization, configurable parameters include <code class="docutils literal notranslate"><span class="pre">common</span> <span class="pre">quantization</span> <span class="pre">parameter</span> <span class="pre">[common_quant_param]</span></code>, <code class="docutils literal notranslate"><span class="pre">mixed</span> <span class="pre">bit</span> <span class="pre">weight</span> <span class="pre">quantization</span> <span class="pre">parameter</span> <span class="pre">[mixed_bit_weight_quant_param]</span></code>,<code class="docutils literal notranslate"><span class="pre">full</span> <span class="pre">quantization</span> <span class="pre">parameter</span> <span class="pre">[full_quant_param]</span></code>, and <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">preprocess</span> <span class="pre">parameter</span> <span class="pre">[data_preprocess_param]</span></code>.</p>
<section id="common-quantization-parameter">
<h3>Common Quantization Parameter<a class="headerlink" href="#common-quantization-parameter" title="Permalink to this headline"></a></h3>
<p>common quantization parameters are the basic settings for post training quantization, mainly including <code class="docutils literal notranslate"><span class="pre">quant_type</span></code>, <code class="docutils literal notranslate"><span class="pre">bit_num</span></code>, <code class="docutils literal notranslate"><span class="pre">min_quant_weight_size</span></code>, and <code class="docutils literal notranslate"><span class="pre">min_quant_weight_channel</span></code>. The detailed description of the parameters is as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Attribute</p></th>
<th class="head"><p>Function Description</p></th>
<th class="head"><p>Parameter Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Value Range</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">quant_type</span></code></p></td>
<td><p>Mandatory</p></td>
<td><p>The quantization type. When set to WEIGHT_QUANT, weight quantization is enabled; when set to FULL_QUANT, full quantization is enabled.</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>WEIGHT_QUAN, FULL_QUANT</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">bit_num</span></code></p></td>
<td><p>Optional</p></td>
<td><p>The number of quantized bits. Currently, weight quantization supports 0-16bit quantization. When it is set to 1-16bit, it is fixed-bit quantization. When it is set to 0bit, mixed-bit quantization is enabled. Full quantization supports 1-8bit quantization.</p></td>
<td><p>Integer</p></td>
<td><p>8</p></td>
<td><p>WEIGHT_QUAN:[0，16]<br/>FULL_QUANT:[1，8]</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">min_quant_weight_size</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set the threshold of the weight size for quantization. If the number of weights is greater than this value, the weight will be quantized.</p></td>
<td><p>Integer</p></td>
<td><p>0</p></td>
<td><p>[0, 65535]</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">min_quant_weight_channel</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set the threshold of the number of weight channels for quantization. If the number of weight channels is greater than this value, the weight will be quantized.</p></td>
<td><p>Integer</p></td>
<td><p>16</p></td>
<td><p>[0, 65535]</p></td>
</tr>
</tbody>
</table>
<p>The common quantization parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support the number of bits [1,8]</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>
</pre></div>
</div>
</section>
<section id="mixed-bit-weight-quantization-parameter">
<h3>Mixed Bit Weight Quantization Parameter<a class="headerlink" href="#mixed-bit-weight-quantization-parameter" title="Permalink to this headline"></a></h3>
<p>The mixed bit weight quantization parameters include <code class="docutils literal notranslate"><span class="pre">init_scale</span></code>. When enable the mixed bit weight quantization, the optimal number of bits will be automatically searched for different layers. The detailed description of the parameters is as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Attribute</p></th>
<th class="head"><p>Function Description</p></th>
<th class="head"><p>Parameter Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Value Range</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>init_scale</p></td>
<td><p>Optional</p></td>
<td><p>Initialize the scale. The larger the value, the greater the compression rate, but it will also cause varying degrees of accuracy loss.</p></td>
<td><p>float</p></td>
<td><p>0.02</p></td>
<td><p>(0 , 1)</p></td>
</tr>
</tbody>
</table>
<p>The mixed bit quantization parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[mixed_bit_weight_quant_param]</span>
<span class="na">init_scale</span><span class="o">=</span><span class="s">0.02</span>
</pre></div>
</div>
</section>
<section id="full-quantization-parameters">
<h3>Full Quantization Parameters<a class="headerlink" href="#full-quantization-parameters" title="Permalink to this headline"></a></h3>
<p>The full quantization parameters mainly include <code class="docutils literal notranslate"><span class="pre">activation_quant_method</span></code> and <code class="docutils literal notranslate"><span class="pre">bias_correction</span></code>. The detailed description of the parameters is as follows:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Attribute</p></th>
<th class="head"><p>Function Description</p></th>
<th class="head"><p>Parameter Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Value Range</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>activation_quant_method</p></td>
<td><p>Optional</p></td>
<td><p>Activation quantization algorithm</p></td>
<td><p>String</p></td>
<td><p>MAX_MIN</p></td>
<td><p>KL, MAX_MIN, or RemovalOutlier.<br/>KL: quantizes and calibrates the data range based on <a class="reference external" href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">KL divergence</a>.<br/>MAX_MIN: data quantization parameter computed based on the maximum and minimum values.<br/>RemovalOutlier: removes the maximum and minimum values of data based on a certain proportion and then calculates the quantization parameters.<br/>If the calibration dataset is consistent with the input data during actual inference, MAX_MIN is recommended. If the noise of the calibration dataset is large, KL or RemovalOutlier is recommended.</p></td>
</tr>
<tr class="row-odd"><td><p>bias_correction</p></td>
<td><p>Optional</p></td>
<td><p>Indicate whether to correct the quantization error.</p></td>
<td><p>Boolean</p></td>
<td><p>True</p></td>
<td><p>True or False. After this parameter is enabled, the accuracy of the converted model can be improved. You are advised to set this parameter to true.</p></td>
</tr>
</tbody>
</table>
<p>The full quantization parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
</section>
<section id="data-preprocessing">
<h3>Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Permalink to this headline"></a></h3>
<p>To calculate the full quantization activation quantized parameter, the user needs to provide a calibration dataset. For the image calibration dataset, data preprocessing functions such as channel conversion, normalization, resize, and center crop will be provided.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Attribute</p></th>
<th class="head"><p>Function Description</p></th>
<th class="head"><p>Parameter Type</p></th>
<th class="head"><p>Default Value</p></th>
<th class="head"><p>Value Range</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>calibrate_path</p></td>
<td><p>Mandatory</p></td>
<td><p>The directory where the calibration dataset is stored; if the model has multiple inputs, please fill in the directory where the corresponding data is located one by one, and separate the directory paths with <code class="docutils literal notranslate"><span class="pre">,</span></code></p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</p></td>
</tr>
<tr class="row-odd"><td><p>calibrate_size</p></td>
<td><p>Mandatory</p></td>
<td><p>Calibration data size</p></td>
<td><p>Integer</p></td>
<td><p>-</p></td>
<td><p>[1, 65535]</p></td>
</tr>
<tr class="row-even"><td><p>input_type</p></td>
<td><p>Mandatory</p></td>
<td><p>Correction data file format type</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>IMAGE、BIN <br>IMAGE：image file data <br>BIN：binary <code class="docutils literal notranslate"><span class="pre">.bin</span></code> file data</p></td>
</tr>
<tr class="row-odd"><td><p>image_to_format</p></td>
<td><p>Optional</p></td>
<td><p>Image format conversion</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>RGB、GRAY、BGR</p></td>
</tr>
<tr class="row-even"><td><p>normalize_mean</p></td>
<td><p>Optional</p></td>
<td><p>Normalized mean<br/>dst = (src - mean) / std</p></td>
<td><p>Vector</p></td>
<td><p>-</p></td>
<td><p>Channel 3: [mean_1, mean_2, mean_3] <br/>Channel 1: [mean_1]</p></td>
</tr>
<tr class="row-odd"><td><p>normalize_std</p></td>
<td><p>Optional</p></td>
<td><p>Normalized standard deviation<br/>dst = (src - mean) / std</p></td>
<td><p>Vector</p></td>
<td><p>-</p></td>
<td><p>Channel 3: [std_1, std_2, std_3] <br/>Channel 1: [std_1]</p></td>
</tr>
<tr class="row-even"><td><p>resize_width</p></td>
<td><p>Optional</p></td>
<td><p>Resize width</p></td>
<td><p>Integer</p></td>
<td><p>-</p></td>
<td><p>[1, 65535]</p></td>
</tr>
<tr class="row-odd"><td><p>resize_height</p></td>
<td><p>Optional</p></td>
<td><p>Resize height</p></td>
<td><p>Integer</p></td>
<td><p>-</p></td>
<td><p>[1, 65535]</p></td>
</tr>
<tr class="row-even"><td><p>resize_method</p></td>
<td><p>Optional</p></td>
<td><p>Resize algorithm</p></td>
<td><p>String</p></td>
<td><p>-</p></td>
<td><p>LINEAR, NEAREST, CUBIC<br/>LINEAR：Bilinear interpolation<br/>NEARST：Nearest neighbor interpolation<br/>CUBIC：Bicubic interpolation</p></td>
</tr>
<tr class="row-odd"><td><p>center_crop_width</p></td>
<td><p>Optional</p></td>
<td><p>Center crop width</p></td>
<td><p>Integer</p></td>
<td><p>-</p></td>
<td><p>[1, 65535]</p></td>
</tr>
<tr class="row-even"><td><p>center_crop_height</p></td>
<td><p>Optional</p></td>
<td><p>Center crop height</p></td>
<td><p>Integer</p></td>
<td><p>-</p></td>
<td><p>[1, 65535]</p></td>
</tr>
</tbody>
</table>
<p>The data preprocessing parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[data_preprocess_param]</span>
<span class="c1"># Calibration dataset path, the format is input_name_1:input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Full quantification must provide correction dataset</span>
<span class="na">calibrate_path</span><span class="o">=</span><span class="s">input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Calibration data size</span>
<span class="na">calibrate_size</span><span class="o">=</span><span class="s">100</span>
<span class="c1"># Input type supports IMAGE or BIN</span>
<span class="c1"># When set to IMAGE, the image data will be read</span>
<span class="c1"># When set to BIN, the `.bin` binary file will be read</span>
<span class="na">input_type</span><span class="o">=</span><span class="s">IMAGE</span>
<span class="c1"># The output format of the preprocessed image</span>
<span class="c1"># Supports RGB or GRAY or BGR</span>
<span class="na">image_to_format</span><span class="o">=</span><span class="s">RGB</span>
<span class="c1"># Image normalization</span>
<span class="c1"># dst = (src - mean) / std</span>
<span class="na">normalize_mean</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="na">normalize_std</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="c1"># Image resize</span>
<span class="na">resize_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">resize_height</span><span class="o">=</span><span class="s">224</span>
<span class="c1"># Resize method supports LINEAR or NEAREST or CUBIC</span>
<span class="na">resize_method</span><span class="o">=</span><span class="s">LINEAR</span>
<span class="c1"># Image center crop</span>
<span class="na">center_crop_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">center_crop_height</span><span class="o">=</span><span class="s">224</span>
</pre></div>
</div>
</section>
</section>
<section id="weight-quantization">
<h2>Weight Quantization<a class="headerlink" href="#weight-quantization" title="Permalink to this headline"></a></h2>
<p>Weight quantization supports mixed bit quantization, as well as fixed bit quantization between 1 and 16. The lower the number of bits, the greater the model compression rate, but the accuracy loss is usually larger. The following describes the use and effects of weighting.</p>
<section id="mixed-bit-weight-quantization">
<h3>Mixed Bit Weight Quantization<a class="headerlink" href="#mixed-bit-weight-quantization" title="Permalink to this headline"></a></h3>
<p>Currently, weight quantization supports mixed bit quantization. According to the distribution of model parameters and the initial value of <code class="docutils literal notranslate"><span class="pre">init_scale</span></code> set by the user, the number of bits that is most suitable for the current layer will be automatically searched out. When the <code class="docutils literal notranslate"><span class="pre">bit_num</span></code> of the configuration parameter is set to 0, mixed bit quantization will be enabled.</p>
<p>The general form of the mixed bit weight requantization command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--quantType<span class="o">=</span>WeightQuant<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/mixed_bit_weight_quant.cfg
</pre></div>
</div>
<p>The mixed bit weight quantification configuration file is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support the number of bits [1,8]</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">5000</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">5</span>

<span class="k">[mixed_bit_weight_quant_param]</span>
<span class="c1"># Initialization scale in (0,1).</span>
<span class="c1"># A larger value can get a larger compression ratio, but it may also cause a larger error.</span>
<span class="na">init_scale</span><span class="o">=</span><span class="s">0.02</span>
</pre></div>
</div>
<p>Users can adjust the weighted parameters according to the model and their own needs.</p>
<blockquote>
<div><p>The init_scale default value is 0.02, and the compression rate is equivalent to the compression effect of 6-7 fixed bits quantization.</p>
<p>For the sparse structure model, it is recommended to set init_scale to 0.00003.</p>
</div></blockquote>
</section>
<section id="fixed-bit-weight-quantization">
<h3>Fixed Bit Weight Quantization<a class="headerlink" href="#fixed-bit-weight-quantization" title="Permalink to this headline"></a></h3>
<p>Fixed-bit weighting supports fixed-bit quantization between 1 and 16, and users can adjust the weighting parameters according to the requirement.</p>
<p>The general form of the fixed bit weight quantization conversion command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/fixed_bit_weight_quant.cfg
</pre></div>
</div>
<p>The fixed bit weight quantization configuration file is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support the number of bits [1,8]</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>
</pre></div>
</div>
</section>
<section id="partial-model-accuracy-result">
<h3>Partial Model Accuracy Result<a class="headerlink" href="#partial-model-accuracy-result" title="Permalink to this headline"></a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Test Dataset</p></th>
<th class="head"><p>FP32 Model Accuracy</p></th>
<th class="head"><p>Weight Quantization Accuracy (8 bits)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>77.60%</p></td>
<td><p>77.53%</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>70.96%</p></td>
<td><p>70.56%</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>71.56%</p></td>
<td><p>71.53%</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>All the preceding results are obtained in the x86 environment.</p>
</div></blockquote>
</section>
</section>
<section id="full-quantization">
<h2>Full Quantization<a class="headerlink" href="#full-quantization" title="Permalink to this headline"></a></h2>
<p>In scenarios where the model running speed needs to be improved and the model running power consumption needs to be reduced, the full quantization after training can be used.</p>
<p>To calculate a quantization parameter of an activation value, you need to provide a calibration dataset. It is recommended that the calibration dataset be obtained from the actual inference scenario and can represent the actual input of a model. The number of data records is about 100.</p>
<p>For image data, currently supports channel pack, normalization, resize, center crop processing. The user can set the corresponding <span class="xref myst">parameter</span> according to the preprocessing operation requirements.</p>
<p>The general form of the full quantization conversion command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ModelType<span class="w"> </span>--modelFile<span class="o">=</span>ModelFilePath<span class="w"> </span>--outputFile<span class="o">=</span>ConvertedModelPath<span class="w"> </span>--configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/full_quant.cfg
</pre></div>
</div>
<p>The full quantization profile is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">FULL_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support the number of bits [1,8]</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>

<span class="k">[data_preprocess_param]</span>
<span class="c1"># Calibration dataset path, the format is input_name_1:input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Full quantification must provide correction dataset</span>
<span class="na">calibrate_path</span><span class="o">=</span><span class="s">input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Calibration data size</span>
<span class="na">calibrate_size</span><span class="o">=</span><span class="s">100</span>
<span class="c1"># Input type supports IMAGE or BIN</span>
<span class="c1"># When set to IMAGE, the image data will be read</span>
<span class="c1"># When set to BIN, the `.bin` binary file will be read</span>
<span class="na">input_type</span><span class="o">=</span><span class="s">IMAGE</span>
<span class="c1"># The output format of the preprocessed image</span>
<span class="c1"># Supports RGB or GRAY or BGR</span>
<span class="na">image_to_format</span><span class="o">=</span><span class="s">RGB</span>
<span class="c1"># Image normalization</span>
<span class="c1"># dst = (src - mean) / std</span>
<span class="na">normalize_mean</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="na">normalize_std</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="c1"># Image resize</span>
<span class="na">resize_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">resize_height</span><span class="o">=</span><span class="s">224</span>
<span class="c1"># Resize method supports LINEAR or NEAREST or CUBIC</span>
<span class="na">resize_method</span><span class="o">=</span><span class="s">LINEAR</span>
<span class="c1"># Image center crop</span>
<span class="na">center_crop_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">center_crop_height</span><span class="o">=</span><span class="s">224</span>

<span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<section id="id1">
<h3>Partial Model Accuracy Result<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h3>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Test Dataset</p></th>
<th class="head"><p>method_x</p></th>
<th class="head"><p>FP32 Model Accuracy</p></th>
<th class="head"><p>Full Quantization Accuracy (8 bits)</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>KL</p></td>
<td><p>77.60%</p></td>
<td><p>77.40%</p></td>
<td><p>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>KL</p></td>
<td><p>70.96%</p></td>
<td><p>70.31%</p></td>
<td><p>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></p></td>
<td><p><a class="reference external" href="http://image-net.org/">ImageNet</a></p></td>
<td><p>MAX_MIN</p></td>
<td><p>71.56%</p></td>
<td><p>71.16%</p></td>
<td><p>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>All the preceding results are obtained in the x86 environment.</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="converter_tool.html" class="btn btn-neutral float-left" title="Converting Models for Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data_preprocessing.html" class="btn btn-neutral float-right" title="Data Preprocessing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>