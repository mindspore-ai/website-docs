

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Optimizing the Model (Quantization After Training) &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Data Preprocessing" href="data_preprocessing.html" />
    <link rel="prev" title="Converting Models for Inference" href="converter_tool.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Android Application Development Based on JNI Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption"><span class="caption-text">Inference on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimizing the Model (Quantization After Training)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configuration-parameter">Configuration Parameter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common-quantization-parameter">Common Quantization Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mixed-bit-weight-quantization-parameter">Mixed Bit Weight Quantization Parameter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#full-quantization-parameters">Full Quantization Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-preprocessing">Data Preprocessing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#weight-quantization">Weight Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mixed-bit-weight-quantization">Mixed Bit Weight Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fixed-bit-weight-quantization">Fixed Bit Weight Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#partial-model-accuracy-result">Partial Model Accuracy Result</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#full-quantization">Full Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Partial Model Accuracy Result</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Perform Inference on Mini and Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="register_kernel.html">Register Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Optimizing the Model (Quantization After Training)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/use/post_training_quantization.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="optimizing-the-model-quantization-after-training">
<h1>Optimizing the Model (Quantization After Training)<a class="headerlink" href="#optimizing-the-model-quantization-after-training" title="Permalink to this headline">Â¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Windows</span></code> <code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Converting</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Optimization</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<!-- TOC -->
<ul class="simple">
<li><p><a class="reference external" href="#optimizing-the-model-quantization-after-training">Optimizing the Model (Quantization After Training)</a></p>
<ul>
<li><p><a class="reference external" href="#overview">Overview</a></p></li>
<li><p><a class="reference external" href="#configuration-parameter">Configuration Parameter</a></p>
<ul>
<li><p><a class="reference external" href="#common-quantization-parameter">Common Quantization Parameter</a></p></li>
<li><p><a class="reference external" href="#mixed-bit-weight-quantization-parameter">Mixed Bit Weight Quantization Parameter</a></p></li>
<li><p><a class="reference external" href="#full-quantization-parameters">Full Quantization Parameters</a></p></li>
<li><p><a class="reference external" href="#data-preprocessing">Data Preprocessing</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#weight-quantization">Weight Quantization</a></p>
<ul>
<li><p><a class="reference external" href="#mixed-bit-weight-quantization">Mixed Bit Weight Quantization</a></p></li>
<li><p><a class="reference external" href="#fixed-bit-weight-quantization">Fixed Bit Weight Quantization</a></p></li>
<li><p><a class="reference external" href="#partial-model-accuracy-result">Partial Model Accuracy Result</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#full-quantization">Full Quantization</a></p>
<ul>
<li><p><a class="reference external" href="#partial-model-accuracy-result-1">Partial Model Accuracy Result</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /TOC -->
<p><a href="https://gitee.com/mindspore/docs/blob/r1.5/docs/lite/docs/source_en/use/post_training_quantization.md" target="_blank"><img src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">Â¶</a></h2>
<p>Converting a trained <code class="docutils literal notranslate"><span class="pre">float32</span></code> model into an <code class="docutils literal notranslate"><span class="pre">int8</span></code> model through quantization after training can reduce the model size and improve the inference performance. In MindSpore Lite, this function is integrated into the model conversion tool <code class="docutils literal notranslate"><span class="pre">conveter_lite</span></code>. You can add command line parameters to convert a model into a quantization model.</p>
<p>MindSpore Lite quantization after training is classified into two types:</p>
<ol class="simple">
<li><p>Weight quantization: quantizes a weight of a model and compresses only the model size. <code class="docutils literal notranslate"><span class="pre">float32</span></code> inference is still performed during inference.</p></li>
<li><p>Full quantization: quantizes the weight and activation value of a model. The <code class="docutils literal notranslate"><span class="pre">int</span></code> operation is performed during inference to improve the model inference speed and reduce power consumption.</p></li>
</ol>
</div>
<div class="section" id="configuration-parameter">
<h2>Configuration Parameter<a class="headerlink" href="#configuration-parameter" title="Permalink to this headline">Â¶</a></h2>
<p>Post training quantization can be enabled by configuring <code class="docutils literal notranslate"><span class="pre">configFile</span></code> through <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/converter_tool.html">Conversion Tool</a>. The configuration file adopts the style of <code class="docutils literal notranslate"><span class="pre">INI</span></code>, For quantization, configurable parameters include <code class="docutils literal notranslate"><span class="pre">common</span> <span class="pre">quantization</span> <span class="pre">parameter</span> <span class="pre">[common_quant_param]</span></code>, <code class="docutils literal notranslate"><span class="pre">mixed</span> <span class="pre">bit</span> <span class="pre">weight</span> <span class="pre">quantization</span> <span class="pre">parameter</span> <span class="pre">[mixed_bit_weight_quant_param]</span></code>,<code class="docutils literal notranslate"><span class="pre">full</span> <span class="pre">quantization</span> <span class="pre">parameter</span> <span class="pre">[full_quant_param]</span></code>, and <code class="docutils literal notranslate"><span class="pre">data</span> <span class="pre">preprocess</span> <span class="pre">parameter</span> <span class="pre">[data_preprocess_param]</span></code>.</p>
<div class="section" id="common-quantization-parameter">
<h3>Common Quantization Parameter<a class="headerlink" href="#common-quantization-parameter" title="Permalink to this headline">Â¶</a></h3>
<p>common quantization parameters are the basic settings for post training quantization, mainly including <code class="docutils literal notranslate"><span class="pre">quant_type</span></code>, <code class="docutils literal notranslate"><span class="pre">bit_num</span></code>, <code class="docutils literal notranslate"><span class="pre">min_quant_weight_size</span></code>, and <code class="docutils literal notranslate"><span class="pre">min_quant_weight_channel</span></code>. The detailed description of the parameters is as follows:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Attribute</th>
<th>Function Description</th>
<th>Parameter Type</th>
<th>Default Value</th>
<th>Value Range</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>quant_type</code></td>
<td>Mandatory</td>
<td>The quantization type. When set to WEIGHT_QUANT, weight quantization is enabled; when set to FULL_QUANT, full quantization is enabled.</td>
<td>String</td>
<td>-</td>
<td>WEIGHT_QUAN, FULL_QUANT</td>
</tr>
<tr>
<td><code>bit_num</code></td>
<td>Optional</td>
<td>The number of quantized bits. Currently, weight quantization supports 0-16bit quantization. When it is set to 1-16bit, it is fixed-bit quantization. When it is set to 0bit, mixed-bit quantization is enabled. Full quantization supports 1-8bit quantization.</td>
<td>Integer</td>
<td>8</td>
<td>WEIGHT_QUAN:[0ï¼16]<br/>FULL_QUANT:[1ï¼8]</td>
</tr>
<tr>
<td><code>min_quant_weight_size</code></td>
<td>Optional</td>
<td>Set the threshold of the weight size for quantization. If the number of weights is greater than this value, the weight will be quantized.</td>
<td>Integer</td>
<td>0</td>
<td>[0, 65535]</td>
</tr>
<tr>
<td><code>min_quant_weight_channel</code></td>
<td>Optional</td>
<td>Set the threshold of the number of weight channels for quantization. If the number of weight channels is greater than this value, the weight will be quantized.</td>
<td>Integer</td>
<td>16</td>
<td>[0, 65535]</td>
</tr>
</tbody>
</table>
<p>The common quantization parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support the number of bits [1,8]</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>
</pre></div>
</div>
</div>
<div class="section" id="mixed-bit-weight-quantization-parameter">
<h3>Mixed Bit Weight Quantization Parameter<a class="headerlink" href="#mixed-bit-weight-quantization-parameter" title="Permalink to this headline">Â¶</a></h3>
<p>The mixed bit weight quantization parameters include <code class="docutils literal notranslate"><span class="pre">init_scale</span></code>. When enable the mixed bit weight quantization, the optimal number of bits will be automatically searched for different layers. The detailed description of the parameters is as follows:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Attribute</th>
<th>Function Description</th>
<th>Parameter Type</th>
<th>Default Value</th>
<th>Value Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>init_scale</td>
<td>Optional</td>
<td>Initialize the scale. The larger the value, the greater the compression rate, but it will also cause varying degrees of accuracy loss.</td>
<td>float</td>
<td>0.02</td>
<td>(0 , 1)</td>
</tr>
</tbody>
</table>
<p>The mixed bit quantization parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[mixed_bit_weight_quant_param]</span>
<span class="na">init_scale</span><span class="o">=</span><span class="s">0.02</span>
</pre></div>
</div>
</div>
<div class="section" id="full-quantization-parameters">
<h3>Full Quantization Parameters<a class="headerlink" href="#full-quantization-parameters" title="Permalink to this headline">Â¶</a></h3>
<p>The full quantization parameters mainly include <code class="docutils literal notranslate"><span class="pre">activation_quant_method</span></code> and <code class="docutils literal notranslate"><span class="pre">bias_correction</span></code>. The detailed description of the parameters is as follows:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Attribute</th>
<th>Function Description</th>
<th>Parameter Type</th>
<th>Default Value</th>
<th>Value Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>activation_quant_method</td>
<td>Optional</td>
<td>Activation quantization algorithm</td>
<td>String</td>
<td>MAX_MIN</td>
<td>KL, MAX_MIN, or RemovalOutlier.<br/>KL: quantizes and calibrates the data range based on <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">KL divergence</a>.<br/>MAX_MIN: data quantization parameter computed based on the maximum and minimum values.<br/>RemovalOutlier: removes the maximum and minimum values of data based on a certain proportion and then calculates the quantization parameters.<br/>If the calibration dataset is consistent with the input data during actual inference, MAX_MIN is recommended. If the noise of the calibration dataset is large, KL or RemovalOutlier is recommended.</td>
</tr>
<tr>
<td>bias_correction</td>
<td>Optional</td>
<td>Indicate whether to correct the quantization error.</td>
<td>Boolean</td>
<td>True</td>
<td>True or False. After this parameter is enabled, the accuracy of the converted model can be improved. You are advised to set this parameter to true.</td>
</tr>
</tbody>
</table>
<p>The full quantization parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
</div>
<div class="section" id="data-preprocessing">
<h3>Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Permalink to this headline">Â¶</a></h3>
<p>To calculate the full quantization activation quantized parameter, the user needs to provide a calibration dataset. For the image calibration dataset, data preprocessing functions such as channel conversion, normalization, resize, and center crop will be provided.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameter</th>
<th>Attribute</th>
<th>Function Description</th>
<th>Parameter Type</th>
<th>Default Value</th>
<th>Value Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>calibrate_path</td>
<td>Mandatory</td>
<td>The directory where the calibration dataset is stored; if the model has multiple inputs, please fill in the directory where the corresponding data is located one by one, and separate the directory paths with <code>,</code></td>
<td>String</td>
<td>-</td>
<td>input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</td>
</tr>
<tr>
<td>calibrate_size</td>
<td>Mandatory</td>
<td>Calibration data size</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>input_type</td>
<td>Mandatory</td>
<td>Correction data file format type</td>
<td>String</td>
<td>-</td>
<td>IMAGEãBIN <br>IMAGEï¼image file data <br>BINï¼binary <code>.bin</code> file data</td>
</tr>
<tr>
<td>image_to_format</td>
<td>Optional</td>
<td>Image format conversion</td>
<td>String</td>
<td>-</td>
<td>RGBãGRAYãBGR</td>
</tr>
<tr>
<td>normalize_mean</td>
<td>Optional</td>
<td>Normalized mean<br/>dst = (src - mean) / std</td>
<td>Vector</td>
<td>-</td>
<td>Channel 3: [mean_1, mean_2, mean_3] <br/>Channel 1: [mean_1]</td>
</tr>
<tr>
<td>normalize_std</td>
<td>Optional</td>
<td>Normalized standard deviation<br/>dst = (src - mean) / std</td>
<td>Vector</td>
<td>-</td>
<td>Channel 3: [std_1, std_2, std_3] <br/>Channel 1: [std_1]</td>
</tr>
<tr>
<td>resize_width</td>
<td>Optional</td>
<td>Resize width</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>resize_height</td>
<td>Optional</td>
<td>Resize height</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>resize_method</td>
<td>Optional</td>
<td>Resize algorithm</td>
<td>String</td>
<td>-</td>
<td>LINEAR, NEAREST, CUBIC<br/>LINEARï¼Bilinear interpolation<br/>NEARSTï¼Nearest neighbor interpolation<br/>CUBICï¼Bicubic interpolation</td>
</tr>
<tr>
<td>center_crop_width</td>
<td>Optional</td>
<td>Center crop width</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
<tr>
<td>center_crop_height</td>
<td>Optional</td>
<td>Center crop height</td>
<td>Integer</td>
<td>-</td>
<td>[1, 65535]</td>
</tr>
</tbody>
</table>
<p>The data preprocessing parameter configuration is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[data_preprocess_param]</span>
<span class="c1"># Calibration dataset path, the format is input_name_1:input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Full quantification must provide correction dataset</span>
<span class="na">calibrate_path</span><span class="o">=</span><span class="s">input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Calibration data size</span>
<span class="na">calibrate_size</span><span class="o">=</span><span class="s">100</span>
<span class="c1"># Input type supports IMAGE or BIN</span>
<span class="c1"># When set to IMAGE, the image data will be read</span>
<span class="c1"># When set to BIN, the `.bin` binary file will be read</span>
<span class="na">input_type</span><span class="o">=</span><span class="s">IMAGE</span>
<span class="c1"># The output format of the preprocessed image</span>
<span class="c1"># Supports RGB or GRAY or BGR</span>
<span class="na">image_to_format</span><span class="o">=</span><span class="s">RGB</span>
<span class="c1"># Image normalization</span>
<span class="c1"># dst = (src - mean) / std</span>
<span class="na">normalize_mean</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="na">normalize_std</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="c1"># Image resize</span>
<span class="na">resize_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">resize_height</span><span class="o">=</span><span class="s">224</span>
<span class="c1"># Resize method supports LINEAR or NEAREST or CUBIC</span>
<span class="na">resize_method</span><span class="o">=</span><span class="s">LINEAR</span>
<span class="c1"># Image center crop</span>
<span class="na">center_crop_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">center_crop_height</span><span class="o">=</span><span class="s">224</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="weight-quantization">
<h2>Weight Quantization<a class="headerlink" href="#weight-quantization" title="Permalink to this headline">Â¶</a></h2>
<p>Weight quantization supports mixed bit quantization, as well as fixed bit quantization between 1 and 16. The lower the number of bits, the greater the model compression rate, but the accuracy loss is usually larger. The following describes the use and effects of weighting.</p>
<div class="section" id="mixed-bit-weight-quantization">
<h3>Mixed Bit Weight Quantization<a class="headerlink" href="#mixed-bit-weight-quantization" title="Permalink to this headline">Â¶</a></h3>
<p>Currently, weight quantization supports mixed bit quantization. According to the distribution of model parameters and the initial value of <code class="docutils literal notranslate"><span class="pre">init_scale</span></code> set by the user, the number of bits that is most suitable for the current layer will be automatically searched out. When the <code class="docutils literal notranslate"><span class="pre">bit_num</span></code> of the configuration parameter is set to 0, mixed bit quantization will be enabled.</p>
<p>The general form of the mixed bit weight requantization command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite --fmk<span class="o">=</span>ModelType --modelFile<span class="o">=</span>ModelFilePath --outputFile<span class="o">=</span>ConvertedModelPath --quantType<span class="o">=</span>WeightQuant --configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/mixed_bit_weight_quant.cfg
</pre></div>
</div>
<p>The mixed bit weight quantification configuration file is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support the number of bits [1,8]</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">5000</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">5</span>

<span class="k">[mixed_bit_weight_quant_param]</span>
<span class="c1"># Initialization scale in (0,1).</span>
<span class="c1"># A larger value can get a larger compression ratio, but it may also cause a larger error.</span>
<span class="na">init_scale</span><span class="o">=</span><span class="s">0.02</span>
</pre></div>
</div>
<p>Users can adjust the weighted parameters according to the model and their own needs.</p>
<blockquote>
<div><p>The init_scale default value is 0.02, and the compression rate is equivalent to the compression effect of 6-7 fixed bits quantization.</p>
<p>For the sparse structure model, it is recommended to set init_scale to 0.00003.</p>
</div></blockquote>
</div>
<div class="section" id="fixed-bit-weight-quantization">
<h3>Fixed Bit Weight Quantization<a class="headerlink" href="#fixed-bit-weight-quantization" title="Permalink to this headline">Â¶</a></h3>
<p>Fixed-bit weighting supports fixed-bit quantization between 1 and 16, and users can adjust the weighting parameters according to the requirement.</p>
<p>The general form of the fixed bit weight quantization conversion command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite --fmk<span class="o">=</span>ModelType --modelFile<span class="o">=</span>ModelFilePath --outputFile<span class="o">=</span>ConvertedModelPath --configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/fixed_bit_weight_quant.cfg
</pre></div>
</div>
<p>The fixed bit weight quantization configuration file is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">WEIGHT_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support the number of bits [1,8]</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>
</pre></div>
</div>
</div>
<div class="section" id="partial-model-accuracy-result">
<h3>Partial Model Accuracy Result<a class="headerlink" href="#partial-model-accuracy-result" title="Permalink to this headline">Â¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model</th>
<th>Test Dataset</th>
<th>FP32 Model Accuracy</th>
<th>Weight Quantization Accuracy (8 bits)</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>77.60%</td>
<td>77.53%</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>70.96%</td>
<td>70.56%</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>71.56%</td>
<td>71.53%</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>All the preceding results are obtained in the x86 environment.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="full-quantization">
<h2>Full Quantization<a class="headerlink" href="#full-quantization" title="Permalink to this headline">Â¶</a></h2>
<p>In scenarios where the model running speed needs to be improved and the model running power consumption needs to be reduced, the full quantization after training can be used.</p>
<p>To calculate a quantization parameter of an activation value, you need to provide a calibration dataset. It is recommended that the calibration dataset be obtained from the actual inference scenario and can represent the actual input of a model. The number of data records is about 100.</p>
<p>For image data, currently supports channel pack, normalization, resize, center crop processing. The user can set the corresponding <a class="reference external" href="#data-preprocessing">parameter</a> according to the preprocessing operation requirements.</p>
<p>The general form of the full quantization conversion command is:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite --fmk<span class="o">=</span>ModelType --modelFile<span class="o">=</span>ModelFilePath --outputFile<span class="o">=</span>ConvertedModelPath --configFile<span class="o">=</span>/mindspore/lite/tools/converter/quantizer/config/full_quant.cfg
</pre></div>
</div>
<p>The full quantization profile is as follows:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[common_quant_param]</span>
<span class="c1"># Supports WEIGHT_QUANT or FULL_QUANT</span>
<span class="na">quant_type</span><span class="o">=</span><span class="s">FULL_QUANT</span>
<span class="c1"># Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization</span>
<span class="c1"># Full quantization support the number of bits [1,8]</span>
<span class="na">bit_num</span><span class="o">=</span><span class="s">8</span>
<span class="c1"># Layers with size of weights exceeds threshold `min_quant_weight_size` will be quantized.</span>
<span class="na">min_quant_weight_size</span><span class="o">=</span><span class="s">0</span>
<span class="c1"># Layers with channel size of weights exceeds threshold `min_quant_weight_channel` will be quantized.</span>
<span class="na">min_quant_weight_channel</span><span class="o">=</span><span class="s">16</span>

<span class="k">[data_preprocess_param]</span>
<span class="c1"># Calibration dataset path, the format is input_name_1:input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Full quantification must provide correction dataset</span>
<span class="na">calibrate_path</span><span class="o">=</span><span class="s">input_name_1:/mnt/image/input_1_dir,input_name_2:input_2_dir</span>
<span class="c1"># Calibration data size</span>
<span class="na">calibrate_size</span><span class="o">=</span><span class="s">100</span>
<span class="c1"># Input type supports IMAGE or BIN</span>
<span class="c1"># When set to IMAGE, the image data will be read</span>
<span class="c1"># When set to BIN, the `.bin` binary file will be read</span>
<span class="na">input_type</span><span class="o">=</span><span class="s">IMAGE</span>
<span class="c1"># The output format of the preprocessed image</span>
<span class="c1"># Supports RGB or GRAY or BGR</span>
<span class="na">image_to_format</span><span class="o">=</span><span class="s">RGB</span>
<span class="c1"># Image normalization</span>
<span class="c1"># dst = (src - mean) / std</span>
<span class="na">normalize_mean</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="na">normalize_std</span><span class="o">=</span><span class="s">[127.5, 127.5, 127.5]</span>
<span class="c1"># Image resize</span>
<span class="na">resize_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">resize_height</span><span class="o">=</span><span class="s">224</span>
<span class="c1"># Resize method supports LINEAR or NEAREST or CUBIC</span>
<span class="na">resize_method</span><span class="o">=</span><span class="s">LINEAR</span>
<span class="c1"># Image center crop</span>
<span class="na">center_crop_width</span><span class="o">=</span><span class="s">224</span>
<span class="na">center_crop_height</span><span class="o">=</span><span class="s">224</span>

<span class="k">[full_quant_param]</span>
<span class="c1"># Activation quantized method supports MAX_MIN or KL or REMOVAL_OUTLIER</span>
<span class="na">activation_quant_method</span><span class="o">=</span><span class="s">MAX_MIN</span>
<span class="c1"># Whether to correct the quantization error. Recommended to set to true.</span>
<span class="na">bias_correction</span><span class="o">=</span><span class="s">true</span>
</pre></div>
</div>
<div class="section" id="id1">
<h3>Partial Model Accuracy Result<a class="headerlink" href="#id1" title="Permalink to this headline">Â¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model</th>
<th>Test Dataset</th>
<th>method_x</th>
<th>FP32 Model Accuracy</th>
<th>Full Quantization Accuracy (8 bits)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/inception_v3_2018_04_27.tgz">Inception_V3</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>KL</td>
<td>77.60%</td>
<td>77.40%</td>
<td>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_02_22/mobilenet_v1_1.0_224.tgz">Mobilenet_V1_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>KL</td>
<td>70.96%</td>
<td>70.31%</td>
<td>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</td>
</tr>
<tr>
<td><a href="https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224.tgz">Mobilenet_V2_1.0_224</a></td>
<td><a href="http://image-net.org/">ImageNet</a></td>
<td>MAX_MIN</td>
<td>71.56%</td>
<td>71.16%</td>
<td>Randomly select 100 images from the ImageNet Validation dataset as a calibration dataset.</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>All the preceding results are obtained in the x86 environment.</p>
</div></blockquote>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="data_preprocessing.html" class="btn btn-neutral float-right" title="Data Preprocessing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="converter_tool.html" class="btn btn-neutral float-left" title="Converting Models for Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>