<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using C++ Interface to Perform Training &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Benchmark Tool" href="benchmark.html" />
    <link rel="prev" title="Executing Model Training" href="runtime_train.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Android Application Development Based on JNI Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Optimizing the Model (Quantization After Training)</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Perform Inference on Mini and Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="register_kernel.html">Register Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime_train.html">Executing Model Training</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using C++ Interface to Perform Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-creating-loading-and-building">Model Creating Loading and Building</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#reading-models">Reading Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-contexts">Creating Contexts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-trainloop">Creating TrainLoop</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#data-processing">Data Processing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#data-reading-pipeline">Data Reading Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="#data-preprocessing-pipeline">Data Preprocessing Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example">Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#execute-training">Execute Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#evaluating">Evaluating</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#others">Others</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#session-mode-switching">Session Mode Switching</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resizing-the-input-dimension">Resizing the Input Dimension</a></li>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-input-tensors">Obtaining Input Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-output-tensors">Obtaining Output Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#execute-session">Execute Session</a></li>
<li class="toctree-l4"><a class="reference internal" href="#saving-model">Saving Model</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime_train.html">Executing Model Training</a> &raquo;</li>
      <li>Using C++ Interface to Perform Training</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/runtime_train_cpp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-c-interface-to-perform-training">
<h1>Using C++ Interface to Perform Training<a class="headerlink" href="#using-c-interface-to-perform-training" title="Permalink to this headline">ÔÉÅ</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">Linux</span></code> <code class="docutils literal notranslate"><span class="pre">Android</span></code> <code class="docutils literal notranslate"><span class="pre">C++</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Training</span></code> <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">Loading</span></code> <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">Preparation</span></code> <code class="docutils literal notranslate"><span class="pre">Intermediate</span></code> <code class="docutils literal notranslate"><span class="pre">Expert</span></code></p>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.5/docs/lite/docs/source_en/use/runtime_train_cpp.md"><img alt="View Source On Gitee" src="https://gitee.com/mindspore/docs/raw/r1.5/resource/_static/logo_source_en.png" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>The principal procedures of lite training is as follows:</p>
<ol class="arabic simple">
<li><p>Design the network and export the <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> model file by using the cloud side APIs.</p></li>
<li><p>Transfer the <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> file to .ms model file.</p></li>
<li><p>Train, evaluate and save <code class="docutils literal notranslate"><span class="pre">ms</span></code> model files.</p></li>
</ol>
<blockquote>
<div><p>The model structure is saved in the transferred <code class="docutils literal notranslate"><span class="pre">ms</span></code> model file which will be load to the device platform for training.</p>
</div></blockquote>
<p>The following figure shows the detailed training process:</p>
<p><img alt="img" src="../_images/side_train_sequence_unify_api.png" /></p>
</section>
<section id="model-creating-loading-and-building">
<h2>Model Creating Loading and Building<a class="headerlink" href="#model-creating-loading-and-building" title="Permalink to this headline">ÔÉÅ</a></h2>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/generate/classmindspore_Model.html#model">Model</a> is the main entrance of the MindSpore Lite framework. We can compile and execute graph models through <code class="docutils literal notranslate"><span class="pre">Model</span></code> class.</p>
<section id="reading-models">
<h3>Reading Models<a class="headerlink" href="#reading-models" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>A Model file is flatbuffer-serialized file which was converted using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/generate/classmindspore_Serialization.html">MindSpore Model Converter Tool</a>. These files have a <code class="docutils literal notranslate"><span class="pre">.ms</span></code> extension. Before model training and/or inference, the model needs to be loaded from the file system and parsed. Related operations are mainly implemented in the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/api_cpp/mindspore.html">Serialization</a> class which holds the model data such as the network structure, weights data and operators attributes.</p>
<blockquote>
<div><p>In MindSpore Lite the user is not allowed to access the training model object, since it is being used by <code class="docutils literal notranslate"><span class="pre">Model</span></code> during training. All interactions with training model object including instantiation, compilation and deletion are handled within <code class="docutils literal notranslate"><span class="pre">Model</span></code>.</p>
</div></blockquote>
</section>
<section id="creating-contexts">
<h3>Creating Contexts<a class="headerlink" href="#creating-contexts" title="Permalink to this headline">ÔÉÅ</a></h3>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/generate/classmindspore_Context.html">Context</a> is a MindSpore Lite Object which contains basic configuration parameters required by the sessions to guide graph compilation and execution. It allows to define the device to run the model, e.g., CPU or GPU, the number of threads used for training and inference and the memory allocation scheme.
Currently, only single threaded CPU device is supported by <code class="docutils literal notranslate"><span class="pre">TrainSession</span></code>.</p>
<p>Once the <code class="docutils literal notranslate"><span class="pre">Model</span></code> is created with the <code class="docutils literal notranslate"><span class="pre">Context</span></code> object, it is no longer needed and can be deleted.</p>
</section>
<section id="creating-trainloop">
<h3>Creating TrainLoop<a class="headerlink" href="#creating-trainloop" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>User can create the object of the class <code class="docutils literal notranslate"><span class="pre">Model</span></code> by using the function <code class="docutils literal notranslate"><span class="pre">Build</span></code> to call MindData APIs. The member function <code class="docutils literal notranslate"><span class="pre">Build</span></code> of the class <code class="docutils literal notranslate"><span class="pre">Model</span></code> whose prototype is as follows:</p>
<p><code class="docutils literal notranslate"><span class="pre">Status</span> <span class="pre">Build(GraphCell</span> <span class="pre">graph,</span> <span class="pre">const</span> <span class="pre">std::shared_ptr&lt;Context&gt;</span> <span class="pre">&amp;model_context</span> <span class="pre">=</span> <span class="pre">nullptr,</span> <span class="pre">const</span> <span class="pre">std::shared_ptr&lt;TrainCfg&gt;</span> <span class="pre">&amp;train_cfg</span> <span class="pre">=</span> <span class="pre">nullptr);</span></code></p>
<p>The following codes show ho to create a training session based on the multi-threads CPU by using the class <code class="docutils literal notranslate"><span class="pre">Model</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">CreateSession</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">cpu_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="n">cpu_context</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="n">enable_fp16_</span><span class="p">);</span>
<span class="w">  </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_context</span><span class="p">);</span>

<span class="w">  </span><span class="n">graph_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Graph</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Serialization</span><span class="o">::</span><span class="n">Load</span><span class="p">(</span><span class="n">ms_file_</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kFlatBuffer</span><span class="p">,</span><span class="w"> </span><span class="n">graph_</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; during serialization of graph &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ms_file_</span><span class="p">;</span>
<span class="w">    </span><span class="n">MS_ASSERT</span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">cfg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">TrainCfg</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">enable_fp16_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">cfg</span><span class="p">.</span><span class="n">get</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">optimization_level_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kO2</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">model_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="w">  </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GraphCell</span><span class="p">(</span><span class="o">*</span><span class="n">graph_</span><span class="p">),</span><span class="w"> </span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="n">cfg</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; during build of model &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ms_file_</span><span class="p">;</span>
<span class="w">    </span><span class="n">MS_ASSERT</span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>Refer <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.5/mindspore/lite/examples/unified_api/src/net_runner.cc">Train a LeNet</a> for more details.</p>
</div></blockquote>
</section>
</section>
<section id="data-processing">
<h2>Data Processing<a class="headerlink" href="#data-processing" title="Permalink to this headline">ÔÉÅ</a></h2>
<section id="data-reading-pipeline">
<h3>Data Reading Pipeline<a class="headerlink" href="#data-reading-pipeline" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The class <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and its extension class (e.g., <code class="docutils literal notranslate"><span class="pre">MnistDataset</span></code> and <code class="docutils literal notranslate"><span class="pre">AlbumDataset</span></code>) have provided abundant data procssing API. Users only need to specify the dataset path and set the data processing operations for the model training by using the shared pointers from the related API. Reading pipeline will decode and load dataset during model training. Refer <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/api_cpp/mindspore_dataset.html">Dataset</a> for more detials.</p>
</section>
<section id="data-preprocessing-pipeline">
<h3>Data Preprocessing Pipeline<a class="headerlink" href="#data-preprocessing-pipeline" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The class <code class="docutils literal notranslate"><span class="pre">TensorTransform</span></code> has provided abundant data preprocssing API and has the same function as the cloud side, (e.g., Dimension reshaping, data type casting and one-hot coding). The users only need to create the objects of the extension classes of <code class="docutils literal notranslate"><span class="pre">TensorTransform</span></code> and transfer them to the function <code class="docutils literal notranslate"><span class="pre">Map</span></code>. Refer <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/api_cpp/mindspore_dataset_vision.html">Vision</a> for more detials.</p>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The following codes show how to read and process dataset by using the class <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">TensorTransform</span></code>:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">DataSetPipeline</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">train_ds_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Mnist</span><span class="p">(</span><span class="n">data_dir_</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">&quot;/train&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;all&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">SequentialSampler</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">));</span>

<span class="w">    </span><span class="n">TypeCast</span><span class="w"> </span><span class="n">typecast_f</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">);</span>
<span class="w">    </span><span class="n">Resize</span><span class="w"> </span><span class="n">resize</span><span class="p">({</span><span class="n">h_</span><span class="p">,</span><span class="w"> </span><span class="n">w_</span><span class="p">});</span>
<span class="w">    </span><span class="n">train_ds_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_ds_</span><span class="o">-&gt;</span><span class="n">Map</span><span class="p">({</span><span class="o">&amp;</span><span class="n">resize</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">typecast_f</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;image&quot;</span><span class="p">});</span>

<span class="w">    </span><span class="n">TypeCast</span><span class="w"> </span><span class="n">typecast</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeInt32</span><span class="p">);</span>
<span class="w">    </span><span class="n">train_ds_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_ds_</span><span class="o">-&gt;</span><span class="n">Map</span><span class="p">({</span><span class="o">&amp;</span><span class="n">typecast</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;label&quot;</span><span class="p">});</span>

<span class="w">    </span><span class="n">train_ds_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train_ds_</span><span class="o">-&gt;</span><span class="n">Batch</span><span class="p">(</span><span class="n">batch_size_</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">verbose_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;DatasetSize is &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">train_ds_</span><span class="o">-&gt;</span><span class="n">GetDatasetSize</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">train_ds_</span><span class="o">-&gt;</span><span class="n">GetDatasetSize</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;No relevant data was found in &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">data_dir_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="n">MS_ASSERT</span><span class="p">(</span><span class="n">train_ds_</span><span class="o">-&gt;</span><span class="n">GetDatasetSize</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="execute-training">
<h2>Execute Training<a class="headerlink" href="#execute-training" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>MindSpore has provided some off-the-shelf callback classes for users (e.g., <code class="docutils literal notranslate"><span class="pre">AccuracyMetrics</span></code>, <code class="docutils literal notranslate"><span class="pre">CkptSaver</span></code>, <code class="docutils literal notranslate"><span class="pre">TrainAccuracy</span></code>, <code class="docutils literal notranslate"><span class="pre">LossMonitor</span></code> and <code class="docutils literal notranslate"><span class="pre">Metrics</span></code>). The function <code class="docutils literal notranslate"><span class="pre">Train</span></code> and <code class="docutils literal notranslate"><span class="pre">Evaluate</span></code> of the class <code class="docutils literal notranslate"><span class="pre">Model</span></code> can set the model to the training or evaluation mode separately, specify the methods of the data processing and monitor the session status.</p>
<section id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Create the objects of the off-the-shelf functions and call the Train function of the class Model to training:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">Train</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">mindspore</span><span class="o">::</span><span class="n">LossMonitor</span><span class="w"> </span><span class="n">lm</span><span class="p">(</span><span class="n">kPrintTimes</span><span class="p">);</span>
<span class="w">  </span><span class="n">mindspore</span><span class="o">::</span><span class="n">TrainAccuracy</span><span class="w"> </span><span class="n">am</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="n">mindspore</span><span class="o">::</span><span class="n">CkptSaver</span><span class="w"> </span><span class="n">cs</span><span class="p">(</span><span class="n">kSaveEpochs</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">(</span><span class="s">&quot;lenet&quot;</span><span class="p">));</span>
<span class="w">  </span><span class="n">Rescaler</span><span class="w"> </span><span class="n">rescale</span><span class="p">(</span><span class="n">kScalePoint</span><span class="p">);</span>
<span class="w">  </span><span class="n">Measurement</span><span class="w"> </span><span class="n">measure</span><span class="p">(</span><span class="n">epochs_</span><span class="p">);</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">virtual_batch_</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">model_</span><span class="o">-&gt;</span><span class="n">Train</span><span class="p">(</span><span class="n">epochs_</span><span class="p">,</span><span class="w"> </span><span class="n">train_ds_</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="o">&amp;</span><span class="n">rescale</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">cs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">measure</span><span class="p">});</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">mindspore</span><span class="o">::</span><span class="n">StepLRLambda</span><span class="w"> </span><span class="n">step_lr_lambda</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">kGammaFactor</span><span class="p">);</span>
<span class="w">    </span><span class="n">mindspore</span><span class="o">::</span><span class="n">LRScheduler</span><span class="w"> </span><span class="n">step_lr_sched</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">StepLRLambda</span><span class="p">,</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">step_lr_lambda</span><span class="p">),</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="n">model_</span><span class="o">-&gt;</span><span class="n">Train</span><span class="p">(</span><span class="n">epochs_</span><span class="p">,</span><span class="w"> </span><span class="n">train_ds_</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="o">&amp;</span><span class="n">rescale</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lm</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">cs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">am</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">step_lr_sched</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">measure</span><span class="p">});</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="evaluating">
<h3>Evaluating<a class="headerlink" href="#evaluating" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Also  call the <code class="docutils literal notranslate"><span class="pre">Evaluate</span></code> function of the class <code class="docutils literal notranslate"><span class="pre">Model</span></code> to evaluate model.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">float</span><span class="w"> </span><span class="nf">Evaluate</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">test_ds_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Mnist</span><span class="p">(</span><span class="n">data_dir_</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">&quot;/test&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;all&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="n">TypeCast</span><span class="w"> </span><span class="n">typecast_f</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">);</span>
<span class="w">  </span><span class="n">Resize</span><span class="w"> </span><span class="n">resize</span><span class="p">({</span><span class="n">h_</span><span class="p">,</span><span class="w"> </span><span class="n">w_</span><span class="p">});</span>
<span class="w">  </span><span class="n">test_ds_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test_ds_</span><span class="o">-&gt;</span><span class="n">Map</span><span class="p">({</span><span class="o">&amp;</span><span class="n">resize</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">typecast_f</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;image&quot;</span><span class="p">});</span>

<span class="w">  </span><span class="n">TypeCast</span><span class="w"> </span><span class="n">typecast</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeInt32</span><span class="p">);</span>
<span class="w">  </span><span class="n">test_ds_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test_ds_</span><span class="o">-&gt;</span><span class="n">Map</span><span class="p">({</span><span class="o">&amp;</span><span class="n">typecast</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;label&quot;</span><span class="p">});</span>
<span class="w">  </span><span class="n">test_ds_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test_ds_</span><span class="o">-&gt;</span><span class="n">Batch</span><span class="p">(</span><span class="n">batch_size_</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>

<span class="w">  </span><span class="n">model_</span><span class="o">-&gt;</span><span class="n">Evaluate</span><span class="p">(</span><span class="n">test_ds_</span><span class="p">,</span><span class="w"> </span><span class="p">{});</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Accuracy is &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">acc_metrics_</span><span class="o">-&gt;</span><span class="n">Eval</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>With TrainSessions, a network can be used for both inference and training. These two modes differ in several aspects:</p>
<ul class="simple">
<li><p>The input of the network: Running inference requires only the data, while running training requires both data and labels.</p></li>
<li><p>The output of the network: Running inference returns the predicted values in the output, while running in training mode returns the loss.</p></li>
<li><p>In training mode, the weights of the layers are updated in each Run, while in inference mode they are static.</p></li>
<li><p>Some layers behave differently in inference vs. training mode, e.g., updating the accumulated batch mean and variance in Batch Normalization layers.</p></li>
</ul>
</div></blockquote>
</section>
</section>
<section id="others">
<h2>Others<a class="headerlink" href="#others" title="Permalink to this headline">ÔÉÅ</a></h2>
<section id="session-mode-switching">
<h3>Session Mode Switching<a class="headerlink" href="#session-mode-switching" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The functions <code class="docutils literal notranslate"><span class="pre">Train</span></code> and <code class="docutils literal notranslate"><span class="pre">Evaluate</span></code>  in the class <code class="docutils literal notranslate"><span class="pre">Model</span></code> are called by the functions <code class="docutils literal notranslate"><span class="pre">Train</span></code> and <code class="docutils literal notranslate"><span class="pre">Evaluate</span></code> in the class <code class="docutils literal notranslate"><span class="pre">Model</span></code> . User can switch session mode by calling the two functions directly, the prototypes are as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief Set model to train mode</span>
<span class="c1">/// \return STATUS as an error code of compiling graph, STATUS is defined in errorcode.h</span>
<span class="n">Status</span><span class="w"> </span><span class="nf">Train</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">epochs</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">dataset</span><span class="o">::</span><span class="n">Dataset</span><span class="o">&gt;</span><span class="w"> </span><span class="n">ds</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TrainCallBack</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">cbs</span><span class="p">);</span>

<span class="c1">/// \brief Set model to Evaluate mode</span>
<span class="c1">/// \return STATUS as an error code of compiling graph, STATUS is defined in errorcode.h</span>
<span class="n">Status</span><span class="w"> </span><span class="nf">Evaluate</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">dataset</span><span class="o">::</span><span class="n">Dataset</span><span class="o">&gt;</span><span class="w"> </span><span class="n">ds</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TrainCallBack</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">cbs</span><span class="p">);</span>
</pre></div>
</div>
<p>The following sample code shows how to set a <code class="docutils literal notranslate"><span class="pre">Model</span></code> object to train mode.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Train</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Could not set to train mode&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Evaluate</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Could not set to evaluate mode&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="resizing-the-input-dimension">
<h3>Resizing the Input Dimension<a class="headerlink" href="#resizing-the-input-dimension" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>When MindSpore Lite is used for inference, if the input shape needs to be resized, you can call the Resize API of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/generate/classmindspore_Model.html#class-model">Model</a> to resize the shape of the input tensor after a model is created and built.</p>
<blockquote>
<div><p>Some networks do not support variable dimensions. As a result, an error message is displayed and the model exits unexpectedly. For example, the model contains the MatMul operator, one input tensor of the MatMul operator is the weight, and the other input tensor is the input. If a variable dimension API is called, the input tensor does not match the shape of the weight tensor. As a result, the training fails.</p>
</div></blockquote>
<p>The following sample code demonstrates how to perform Resize on the input tensor of MindSpore Lite:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">resize_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">};</span>
<span class="c1">// Assume the model has only one input,resize input shape to [16, 32, 32, 1]</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">;</span>
<span class="n">new_shapes</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">resize_shape</span><span class="p">);</span>
<span class="k">return</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Resize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="obtaining-input-tensors">
<h3>Obtaining Input Tensors<a class="headerlink" href="#obtaining-input-tensors" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Before graph execution, whether it is during training or inference, the input data must be filled-in into the model input tensors.
MindSpore Lite provides the following methods to obtain model input tensors:</p>
<ol class="arabic">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetInputByTensorName</span></code> method to obtain model input tensors that are connected to the model input node based on the tensor name.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  Get MindSpore input Tensors of model by the tensor name.</span>
<span class="c1">///</span>
<span class="c1">/// \param[in] tensor_name  Define tensor name.</span>
<span class="c1">///</span>
<span class="c1">/// \return  MindSpore Lite MSTensor.</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">MSTensor</span><span class="w"> </span><span class="nf">GetInputByTensorName</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor_name</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetInputs</span></code> method to directly obtain the vectors of all model input tensors.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  Get input MindSpore Lite MSTensors of model.</span>
<span class="c1">///</span>
<span class="c1">/// \return  The vector of MindSpore Lite MSTensor.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">GetInputs</span><span class="p">();</span>
</pre></div>
</div>
<p>If the model requires more than one input tensor (this is certainly the case during training, where both data and labels serve as inputs of the network) it is the user‚Äôs responsibility to know the inputs order or their tensorName. This can be obtained from the Python model.
Alternatively, one can deduce this information from the sizes of the input tensors.</p>
</li>
<li><p>Copying Data</p>
<p>After model input tensors are obtained, the data must be copied into the tensors. The following methods allows to access the size of the data, the number of elements, the data type and the writable pointer. See also detailed description in the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.5/generate/classmindspore_MSTensor.html">MSTensor</a> API documentation.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief Obtains the length of the data of the MSTensor, in bytes.</span>
<span class="c1">///</span>
<span class="c1">/// \return The length of the data of the MSTensor, in bytes.</span>
<span class="kt">size_t</span><span class="w"> </span><span class="nf">DataSize</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="c1">/// \brief Obtains the number of elements of the MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return The number of elements of the MSTensor.</span>
<span class="kt">int64_t</span><span class="w"> </span><span class="nf">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="c1">/// \brief Obtains the data type of the MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return The data type of the MSTensor.</span>
<span class="k">enum</span><span class="w"> </span><span class="nc">DataType</span><span class="w"> </span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="c1">/// \brief Obtains the pointer to the data of the MSTensor. If the MSTensor is a device tensor, the data cannot be</span>
<span class="c1">/// accessed directly on host.</span>
<span class="c1">///</span>
<span class="c1">/// \return A pointer to the data of the MSTensor.</span>
<span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="nf">MutableData</span><span class="p">();</span>
</pre></div>
</div>
</li>
<li><p>Example</p>
<p>The following sample code shows how to obtain the entire graph input <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">Model</span></code> and enter the model input data to <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assuming model is a valid instance of Model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>

<span class="c1">// Assuming the model has two input tensors, the first is for data and the second for labels</span>
<span class="kt">int</span><span class="w"> </span><span class="n">data_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="kt">int</span><span class="w"> </span><span class="n">label_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>

<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Unexpected amount of input tensors. Expected 2, model requires &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Assuming batch_size and data_size variables holds the Batch size and the size of a single data tensor, respectively:</span>
<span class="c1">// And assuming sparse labels are used</span>
<span class="k">if</span><span class="w"> </span><span class="p">((</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">data_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">batch_size</span><span class="o">*</span><span class="n">data_size</span><span class="p">)</span><span class="w"> </span><span class="o">||</span>
<span class="w">    </span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">label_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">ElementsNum</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">batch_size</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input data size does not match model input&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Assuming data_ptr is the pointer to a batch of data tensors</span>
<span class="c1">// and assuming label_ptr is a pointer to a batch of label indices (obtained by the DataLoder)</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">in_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">data_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">in_labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">label_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">((</span><span class="n">in_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="o">||</span><span class="w"> </span><span class="p">(</span><span class="n">in_labels</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model&#39;s input tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span><span class="w"> </span><span class="n">data_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">data_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">());</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_labels</span><span class="p">,</span><span class="w"> </span><span class="n">label_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">label_index</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">Size</span><span class="p">());</span>
<span class="c1">// After filling the input tensors the data_ptr and label_ptr may be freed</span>
<span class="c1">// The input tensors themselves are managed by MindSpore Lite and users are not allowed to access them or delete them</span>
</pre></div>
</div>
</li>
</ol>
<p>Note:</p>
<ul class="simple">
<li><p>The data layout in the model input tensors of MindSpore Lite must be NHWC (bathc size, height, weight and channel).</p></li>
<li><p>The Tensors returned by <code class="docutils literal notranslate"><span class="pre">GetInputs</span></code> and <code class="docutils literal notranslate"><span class="pre">GetInputByTensorName</span></code> methods shuold not be released by users.</p></li>
</ul>
</section>
<section id="obtaining-output-tensors">
<h3>Obtaining Output Tensors<a class="headerlink" href="#obtaining-output-tensors" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>After each execution of the graph, the user might want to read the model‚Äôs outputs, whether it is the loss in the case of training mode, or the predicted output in the case of evaluation mode.</p>
<p>MindSpore Lite provides the following methods to obtain the model‚Äôs output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p>
<ol class="arabic">
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code> method to obtain the output tensors that belong to a certain node:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief Get output MSTensors of model by node name.</span>
<span class="c1">///</span>
<span class="c1">/// \param[in] node_name Define node name.</span>
<span class="c1">///</span>
<span class="c1">/// \note Deprecated, replace with GetOutputByTensorName</span>
<span class="c1">///</span>
<span class="c1">/// \return The vector of output MSTensor.</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">node_name</span><span class="p">);</span>
</pre></div>
</div>
<p>The following sample code shows how to obtain the output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">Model</span></code> using the <code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume that model is a vlaid model instance</span>
<span class="c1">// Assume that model has a output node named output_node_name_0.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">output_vec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="s">&quot;output_node_name_0&quot;</span><span class="p">);</span>
<span class="c1">// Assume that output node named output_node_name_0 has only one output tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_vec</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code> method to obtain an output tensor, based on the tensor name.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief Obtains the output tensor of the model by name.</span>
<span class="c1">///</span>
<span class="c1">/// \return The output tensor with the given name, if the name is not found, an invalid tensor is returned.</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">MSTensor</span><span class="w"> </span><span class="nf">GetOutputByTensorName</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor_name</span><span class="p">);</span>
</pre></div>
</div>
<p>The following sample code shows how to obtain the output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">Model</span></code> using the <code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume that model is a vlaid model instance</span>
<span class="c1">// We can use GetOutputByTensorName method to get the names of all the output tensors of the model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tensor_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span>
<span class="c1">// Use output tensor name returned by GetOutputTensorNames as key</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor_name</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">tensor_names</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code> method to obtain all the output tensors, ordered by their tensor name:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief Obtains all output tensors of the model.</span>
<span class="c1">///</span>
<span class="c1">/// \return The vector that includes all output tensors.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">GetOutputs</span><span class="p">();</span>

<span class="c1">/// \brief Obtains the number of elements of the MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return The number of elements of the MSTensor.</span>
<span class="kt">int64_t</span><span class="w"> </span><span class="nf">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="c1">/// \brief Obtains the data type of the MSTensor.</span>
<span class="c1">///</span>
<span class="c1">/// \return The data type of the MSTensor.</span>
<span class="k">enum</span><span class="w"> </span><span class="nc">DataType</span><span class="w"> </span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="c1">/// \brief Obtains the pointer to the data of the MSTensor. If the MSTensor is a device tensor, the data cannot be</span>
<span class="c1">/// accessed directly on host.</span>
<span class="c1">///</span>
<span class="c1">/// \return A pointer to the data of the MSTensor.</span>
<span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="nf">MutableData</span><span class="p">();</span>
</pre></div>
</div>
<p>The following sample code shows how to obtain the output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> from <code class="docutils literal notranslate"><span class="pre">Model</span></code> using the <code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code> method and print the first ten data or all data records of each output <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">out_tensors</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tensor name is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_tensor</span><span class="p">.</span><span class="n">Name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; tensor size is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span>
<span class="w">             </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; tensor elements num is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">   </span><span class="c1">// The model output data is float 32.</span>
<span class="w">   </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="p">.</span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output should in float32&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">     </span><span class="k">return</span><span class="p">;</span>
<span class="w">   </span><span class="p">}</span>
<span class="w">   </span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">out_tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">());</span>
<span class="w">   </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data of out_tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">     </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">   </span><span class="p">}</span>
<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;output data is:&quot;</span><span class="p">;</span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">out_tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="w">   </span><span class="p">}</span>
<span class="w">   </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<p>Note that the vectors or map returned by the <code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code>, <code class="docutils literal notranslate"><span class="pre">GetOutputByTensorName</span></code> and <code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code> methods do not need to be released by users.</p>
</section>
<section id="execute-session">
<h3>Execute Session<a class="headerlink" href="#execute-session" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Whether a <code class="docutils literal notranslate"><span class="pre">Model</span></code> object is in the training mode or in eval mode, the way to make it execute, i.e., to run the data through the graph, is to call the <code class="docutils literal notranslate"><span class="pre">Train</span></code> method.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief Run session with callbacks.</span>
<span class="c1">///</span>
<span class="c1">/// \param[in] before Define a call_back_function to be called before running each node.</span>
<span class="c1">/// \param[in] after Define a call_back_function called after running each node.</span>
<span class="c1">///</span>
<span class="c1">/// \note RunGraph should be called after CompileGraph.</span>
<span class="c1">///</span>
<span class="c1">/// \return STATUS as an error code of running graph, STATUS is defined in errorcode.h.</span>
<span class="k">virtual</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="nf">RunGraph</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">KernelCallBack</span><span class="w"> </span><span class="o">&amp;</span><span class="n">before</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">KernelCallBack</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
<p>Prior to run each graph, the user must make sure that the data is properly loaded to the input tensors.</p>
<section id="execute-callback">
<h4>Execute Callback<a class="headerlink" href="#execute-callback" title="Permalink to this headline">ÔÉÅ</a></h4>
<p>MindSpore Lite framework allows the user to set two callback functions that will be called before and after running each node. Such functions can assist the developer in tracing the network, debugging it and measuring how long it took run each node. The callback parameters are as follows:</p>
<ul class="simple">
<li><p>The current input tensors of the running node</p></li>
<li><p>The current output tensors of the running node</p></li>
<li><p>Name and type of the running node</p></li>
</ul>
<p>While the node name and type will be the same before and after running the node, the output tensors will differ between the two callbacks invocations.
For some operators, also the input tesnors will vary.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief  CallBackParam defines input arguments for callback function.</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">CallBackParam</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">node_name</span><span class="p">;</span><span class="w"> </span><span class="cm">/**&lt; node name argument */</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">node_type</span><span class="p">;</span><span class="w"> </span><span class="cm">/**&lt; node type argument */</span>
<span class="p">};</span>

<span class="c1">/// \brief KernelCallBack defined the function pointer for callBack.</span>
<span class="k">using</span><span class="w"> </span><span class="n">KernelCallBack</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="kt">bool</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">CallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">opInfo</span><span class="p">)</span><span class="o">&gt;</span><span class="p">;</span>
</pre></div>
</div>
<p>The following sample code demonstrates how to define two callback functions, the first will be called before running each layer, and the second after running it.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assuming model is a valid instance of Model and that data was assigned to the input tensors</span>

<span class="c1">// Definition of a callback function that will be called before forwarding operator</span>
<span class="kt">bool</span><span class="w"> </span><span class="nf">before_callback</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSCallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_name</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Before forwarding: input size is &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>

<span class="c1">// Definition of callback function that will be called after forwarding operator</span>
<span class="kt">bool</span><span class="w"> </span><span class="nf">after_callback</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSCallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;After forwarding: output size is &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>

<span class="c1">// Hand over the callback functions to RunGraph when performing the training or inference</span>
<span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_</span><span class="o">-&gt;</span><span class="n">Train</span><span class="p">(</span><span class="n">epochs_</span><span class="p">,</span><span class="w"> </span><span class="n">train_ds_</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="o">&amp;</span><span class="n">before_callback</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after_callback</span><span class="p">});</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Run graph failed.&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="saving-model">
<h3>Saving Model<a class="headerlink" href="#saving-model" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The function <code class="docutils literal notranslate"><span class="pre">Serialization</span></code> calls the function <code class="docutils literal notranslate"><span class="pre">ExportModel</span></code> actually. The user can also call <code class="docutils literal notranslate"><span class="pre">ExportModel</span></code> directly to save the trained model.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">Status</span><span class="w"> </span><span class="nf">ExportModel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Model</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">ModelType</span><span class="w"> </span><span class="n">model_type</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_file</span><span class="p">,</span>
<span class="w">                            </span><span class="n">QuantizationType</span><span class="w"> </span><span class="n">quantization_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kNoQuant</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">export_inference_only</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">,</span>
<span class="w">                            </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_tensor_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{});</span>
</pre></div>
</div>
<p>You can load the saved model to do re-training or inference.</p>
<blockquote>
<div><p>Please use <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.5/use/benchmark_train_tool.html">benchmark_train</a> to measure the performance and accuarcy of the trained models.</p>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_train.html" class="btn btn-neutral float-left" title="Executing Model Training" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="benchmark.html" class="btn btn-neutral float-right" title="Benchmark Tool" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>