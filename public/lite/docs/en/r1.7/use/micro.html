<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Perform Inference on Mini and Small Systems &mdash; MindSpore Lite master documentation</title>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Application Specific Integrated Circuit Integration Instructions" href="asic.html" />
    <link rel="prev" title="Using Java Interface to Perform Inference" href="runtime_java.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Getting Started in One Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_cpp.html">Experience C++ Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_java.html">Experience Java Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Android Application Development Based on JNI Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Perform Inference on Mini and Small Systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#obtaining-codegen">Obtaining codegen</a></li>
<li class="toctree-l2"><a class="reference internal" href="#codegen-directory-structure">Codegen Directory Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#performing-inference-on-stm-boards">Performing Inference on STM Boards</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#stm32f746-compile-dependencies">STM32F746 Compile Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stm32f746-project-construction">STM32F746 Project Construction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#project-compiling">Project Compiling</a></li>
<li class="toctree-l4"><a class="reference internal" href="#compiling-model">Compiling Model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#stm32f746-project-deployment">STM32F746 Project Deployment</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performing-inference-on-harmonyos-lite">Performing Inference on HarmonyOS Lite</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#installing-build-environment">Installing build environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#connecting-to-the-board">Connecting to the board</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiling-the-model">Compiling the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#writing-build-scripts">Writing build scripts</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-benchmark">Building benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-benchmark">Running benchmark</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#register-kernel">Register Kernel</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#run-codegen">Run codegen</a></li>
<li class="toctree-l3"><a class="reference internal" href="#implement-custom-kernel-by-users">Implement custom kernel by users</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#more-details">More Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linux-x86-64-platform-compile-and-deploy">Linux_x86_64 platform compile and deploy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#android-platform-compile-and-deploy">Android platform compile and deploy</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Perform Inference on Mini and Small Systems</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/micro.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="perform-inference-on-mini-and-small-systems">
<h1>Perform Inference on Mini and Small Systems<a class="headerlink" href="#perform-inference-on-mini-and-small-systems" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.7/docs/lite/docs/source_en/use/micro.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>Compared with mobile devices, IOT devices are equipped with MicroControllerUnits(MCUs),
and are very resource-constrained due to low RAM and computation power.
Therefore, AI application on IOT devices have strict restrictions on RAM and power consumption of AI model Inference.</p>
<p>MindSpore Lite provides a light-weight Micro solution for deploying AI models to IOT devices: converter AI
models to source code of target HW, and don’t need parsing model from flatterbuf to database and compiling graph anymore.
The generated source codes are very intuitive and with very small footprint and code size.
It is easy to use MindSpore Lite converter tool to generates source codes for x86/ARM64/ARM32A/ARM32M platforms.
For x86/ARM64/ARM32A, generated source codes call NNACL NN lib to do inference, and call CMSIS-NN lib on ARM32M instead.</p>
<ol class="arabic simple">
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.7/use/converter_tool.html">MindSpore Lite Converter</a> to
convert the pre-trained model into target device codes with specifying configuration.</p></li>
</ol>
<p><img alt="img" src="../_images/lite_codegen.png" /></p>
</section>
<section id="obtaining-codegen">
<h2>Obtaining codegen<a class="headerlink" href="#obtaining-codegen" title="Permalink to this headline"></a></h2>
<p>You can obtain codegen by any of the following ways:</p>
<ol class="arabic simple">
<li><p>Download pre-compiled <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.7/use/downloads.html">Release Package</a> from MindSpore.</p></li>
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.7/use/build.html">Build</a> from the source.</p></li>
</ol>
<p>With below command, it converters MNIST model into codes of x86 target.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_dir</span><span class="si">}</span>/mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">SOURCE_CODE_DIR</span><span class="si">}</span><span class="w"> </span>--configFile<span class="o">=</span><span class="si">${</span><span class="nv">COFIG_FILE</span><span class="si">}</span>
</pre></div>
</div>
<p>The explicit form of configuration file please see below:</p>
<div class="highlight-txt notranslate"><div class="highlight"><pre><span></span>[micro_param]

# enable code-generation for MCU HW

enable_micro=true

# specify HW target, support x86,ARM32M, AMR32A, ARM64 only

target=x86

# code generation for Inference or Train

codegen_mode=Inference

# enable parallel inference or not

support_parallel=false

# enable debug

debug_mode=false
</pre></div>
</div>
<p>Here is the detailed description of parameters:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Mandatory or Not</p></th>
<th class="head"><p>Parameter Description</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Default value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>enable_micro</p></td>
<td><p>Yes</p></td>
<td><p>enable code generation or not</p></td>
<td><p>true, false</p></td>
<td><p>false</p></td>
</tr>
<tr class="row-odd"><td><p>target</p></td>
<td><p>Yes</p></td>
<td><p>target platform for the generated code</p></td>
<td><p>x86, ARM32M, ARM32A, ARM64</p></td>
<td><p>x86</p></td>
</tr>
<tr class="row-even"><td><p>codegen_mode</p></td>
<td><p>No</p></td>
<td><p>generate inference or training codes</p></td>
<td><p>Inference, Train</p></td>
<td><p>Inference</p></td>
</tr>
<tr class="row-odd"><td><p>supportParallel</p></td>
<td><p>No</p></td>
<td><p>generate parallel codes or not</p></td>
<td><p>true, false</p></td>
<td><p>false</p></td>
</tr>
<tr class="row-even"><td><p>debugMode</p></td>
<td><p>No</p></td>
<td><p>generate debug codes or not</p></td>
<td><p>true, false</p></td>
<td><p>false</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>debugMode is not available when the filesystem is not supported by os.</p>
<p>Please check the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/index.html">API Document</a> to get the detailed API description.</p>
<p>The following 3 interfaces are currently not supported：</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">std::unordered_map&lt;String,</span> <span class="pre">mindspore::tensor::MSTensor</span> <span class="pre">*&gt;</span> <span class="pre">GetOutputs()</span> <span class="pre">const</span> <span class="pre">=</span> <span class="pre">0;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">Vector&lt;tensor::MSTensor</span> <span class="pre">*&gt;</span> <span class="pre">GetOutputsByNodeName(const</span> <span class="pre">String</span> <span class="pre">&amp;node_name)</span> <span class="pre">const</span> <span class="pre">=</span> <span class="pre">0;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">virtual</span> <span class="pre">int</span> <span class="pre">Resize(const</span> <span class="pre">Vector&lt;tensor::MSTensor</span> <span class="pre">*&gt;</span> <span class="pre">&amp;inputs,</span> <span class="pre">const</span> <span class="pre">Vector&lt;Vector&lt;int&gt;&gt;</span> <span class="pre">&amp;dims)</span> <span class="pre">=</span> <span class="pre">0;</span></code></p></li>
</ol>
</div></blockquote>
<p>Currently the code generator is only available on Linux x86_64.</p>
<p>After successful execution, codegen would generate a folder named mnist at the specified path. The structure of the project file is shown as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mnist
├── benchmark                  # integrate debugging-related routines
│   ├── benchmark.c
│   ├── calib_output.c
│   ├── calib_output.h
│   ├── load_input.c
│   └── load_input.h
├── CMakeLists.txt
└── src                        # source files
    ├── CMakeLists.txt
    ├── net.bin                # binary model weights
    ├── net.c
    ├── net.cmake
    ├── net.h
    ├── model.c
    ├── context.c
    ├── context.h
    ├── tensor.c
    ├── tensor.h
    ├── weight.c
    └── weight.h
</pre></div>
</div>
</section>
<section id="codegen-directory-structure">
<h2>Codegen Directory Structure<a class="headerlink" href="#codegen-directory-structure" title="Permalink to this headline"></a></h2>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
└── tools
    └── codegen # dependency header files and library
        ├── include          # Header files of inference framework
        │   ├── nnacl        # nnacl operator header file
        │   └── wrapper
        ├── lib
        │   └── libwrapper.a # MindSpore Lite codegen generates code-dependent operator static library
        └── third_party
            ├── include
            │   └── CMSIS    # ARM CMSIS NN operator header files
            └── lib
                └── libcmsis_nn.a # ARM CMSIS NN operator static library
</pre></div>
</div>
</section>
<section id="performing-inference-on-stm-boards">
<h2>Performing Inference on STM Boards<a class="headerlink" href="#performing-inference-on-stm-boards" title="Permalink to this headline"></a></h2>
<p>This guide takes the deployment on STM32F746 as an example to show how the pre-complied model is built and deployed on Cortex-M platform. More information about Arm Cortex-M could be found in their <a class="reference external" href="https://developer.arm.com/ip-products/processors/cortex-m">Official Web Site</a>.</p>
<section id="stm32f746-compile-dependencies">
<h3>STM32F746 Compile Dependencies<a class="headerlink" href="#stm32f746-compile-dependencies" title="Permalink to this headline"></a></h3>
<p>The generated program compilation and deployment need to install the following tools on Windows: <a class="reference external" href="https://www.segger.com/">J-Link</a>, <a class="reference external" href="https://www.st.com/content/st_com/en.html">STM32CubeMX</a> and <a class="reference external" href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm">GNU Arm Embedded Toolchain</a> to perform Cross-compilation.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.st.com/content/ccc/resource/technical/software/sw_development_suite/group0/0b/05/f0/25/c7/2b/42/9d/stm32cubemx_v6-1-1/files/stm32cubemx_v6-1-1.zip/jcr:content/translations/en.stm32cubemx_v6-1-1.zip">STM32CubeMX Windows Version</a> &gt;= 6.0.1</p></li>
<li><p><a class="reference external" href="https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm/downloads">GNU Arm Embedded Toolchain</a>  &gt;= 9-2019-q4-major-win32</p></li>
<li><p><a class="reference external" href="https://www.segger.com/downloads/jlink/">J-Link Windows Version</a> &gt;= 6.56</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
</ul>
</section>
<section id="stm32f746-project-construction">
<h3>STM32F746 Project Construction<a class="headerlink" href="#stm32f746-project-construction" title="Permalink to this headline"></a></h3>
<ul>
<li><p>The structure of the project files that needs to be managed as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── mnist              # generated inference code by codegen
├── include            # API header files (needs to be managed)
└── operator_library   # operator source code (needs to be managed)
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>API header files could be found in the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.7/use/downloads.html">Release Package</a> provided by the MindSpore team.</p>
<p>You need to obtain the source code corresponding to the target platform because the pre-compiled static library is not provided since the Cross compilation on Cortex-M platform is complicated. The corresponding project file structure is provided in the example and you could follow the instructions shown below to copy the source code and finish the compilation.</p>
</div></blockquote>
<ul class="simple">
<li><p>Use codegen to compile <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/micro/mnist.tar.gz">MNIST handwriting number identification model</a>, generate corresponding inference codes for STM32F46. The command is as follows:</p></li>
</ul>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">SOURCE_CODE_DIR</span><span class="si">}</span><span class="w"> </span>--configFile<span class="o">=</span><span class="si">${</span><span class="nv">COFIG_FILE</span><span class="si">}</span>
</pre></div>
</div>
<p>where target is specified to ARM32M in configure file.</p>
<ul>
<li><p>The generated project file structure is shown below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── mnist               # root of the generated code
    ├── benchmark       # generated benchmark code
    └── src             # generated model inference code
</pre></div>
</div>
</li>
<li><p>The file structure of the prepared static operator library is shown below:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── operator_library    # operator library
    ├── include         # header files of operator library
    └── nnacl           # operator source code provided by MindSpore team
    └── wrapper         # operator source code provided by MindSpore team
    └── CMSIS           # CMSIS source code provided by Arm
</pre></div>
</div>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">arm_nnfunctions.h</span></code> needs to be added when using CMSIS v5.7.0 Softmax operator.</p>
</div></blockquote>
</li>
</ul>
<section id="project-compiling">
<h4>Project Compiling<a class="headerlink" href="#project-compiling" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Environment testing</p>
<p>When programs needed for Cross-compilation are installed, add them to the Windows PATH one by one, and test them with the following instructions:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>gcc -v               # Check GCC
arm-none-eabi-gdb -v # Check Cross compiler
jlink -v             # Check J-Link
make -v              # Check Make
</pre></div>
</div>
<p>If all success, the environment preparation is done.</p>
</li>
<li><p>Generate the initialization codes run on the STM32F746 board. (<a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.7/mindspore/lite/examples/quick_start_micro/mnist_stm32f746">detailed code example</a>)</p>
<ul class="simple">
<li><p>start STM32CubeMX, new project and choose STM32F746IG.</p></li>
<li><p>Choose <code class="docutils literal notranslate"><span class="pre">Makefile</span></code> and <code class="docutils literal notranslate"><span class="pre">generator</span> <span class="pre">code</span></code>.</p></li>
<li><p>Launch <code class="docutils literal notranslate"><span class="pre">cmd</span></code> on the generated project root, execute <code class="docutils literal notranslate"><span class="pre">make</span></code> to test whether the initialization code compilation is successful.</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># make success result
arm-none-eabi-size build/test_stm32f746.elf
  text    data     bss     dec     hex filename
  3660      20    1572    5252    1484 build/test_stm32f746.elf
arm-none-eabi-objcopy -O ihex build/test_stm32f746.elf build/test_stm32f746.hex
arm-none-eabi-objcopy -O binary -S build/test_stm32f746.elf build/test_stm32f746.bin
</pre></div>
</div>
</li>
</ol>
</section>
<section id="compiling-model">
<h4>Compiling Model<a class="headerlink" href="#compiling-model" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Copy operator library source code and header files provided by MindSpore team to the project folder generated by STM32CubeMX.</p></li>
<li><p>Copy model inference code generated by codegen to the project folder generated by STM32CubeMX.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>├── .mxproject
├── build             # compile output folder
├── Core
├── Drivers
├── mnist             # cortex-m7 model inference code generated by codegen
├── Makefile          # modify makefile to organize mnist &amp;&amp; operator_library source code
├── startup_stm32f746xx.s
├── STM32F746IGKx_FLASH.ld
└── test_stm32f746.ioc
</pre></div>
</div>
</li>
<li><p>Modify makefile, organize operator library source code and generated inference code, check <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.7/mindspore/lite/examples/quick_start_micro/mnist_stm32f746">example</a> to get detailed information about makefile.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># C includes
C_INCLUDES =  \
-ICore/Inc \
-IDrivers/STM32F7xx_HAL_Driver/Inc \
-IDrivers/STM32F7xx_HAL_Driver/Inc/Legacy \
-IDrivers/CMSIS/Device/ST/STM32F7xx/Include \
-Imnist/operator_library/include \                # Added, header files for operator library
-Imnist/include \                                 # Added, header files of model inference code
-Imnist/src                                       # Added, source code of model inference code
......
</pre></div>
</div>
</li>
<li><p>Add code in <code class="docutils literal notranslate"><span class="pre">Core/Src/main.c</span></code> to call inference API. The code can be referenced is shown below:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="cm">/* USER CODE END WHILE */</span>
<span class="w">    </span><span class="n">SEGGER_RTT_printf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;***********mnist test start***********</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSContextHandle</span><span class="w"> </span><span class="n">ms_context_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span>
<span class="w">    </span><span class="n">ms_context_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSContextCreate</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ms_context_handle</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">MSContextSetThreadNum</span><span class="p">(</span><span class="n">ms_context_handle</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">      </span><span class="n">MSContextSetThreadAffinityMode</span><span class="p">(</span><span class="n">ms_context_handle</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">model_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="c1">// read net.bin</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">model_buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadInputData</span><span class="p">(</span><span class="s">&quot;net.bin&quot;</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_size</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSModelHandle</span><span class="w"> </span><span class="n">model_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelCreate</span><span class="p">();</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelBuild</span><span class="p">(</span><span class="n">model_handle</span><span class="p">,</span><span class="w"> </span><span class="n">model_buffer</span><span class="p">,</span><span class="w"> </span><span class="n">model_size</span><span class="p">,</span><span class="w"> </span><span class="n">kMSModelTypeMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">ms_context_handle</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSContextDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ms_context_handle</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_buffer</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">free</span><span class="p">(</span><span class="n">model_buffer</span><span class="p">);</span>
<span class="w">      </span><span class="n">model_buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// read input_data.bin</span>
<span class="w">    </span><span class="n">MSTensorHandleArray</span><span class="w"> </span><span class="n">inputs_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelGetInputs</span><span class="p">(</span><span class="n">model_handle</span><span class="p">);</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">inputs_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs_handle</span><span class="p">.</span><span class="n">handle_num</span><span class="p">;</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="n">inputs_num</span><span class="p">];</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">inputs_size</span><span class="p">[</span><span class="n">inputs_num</span><span class="p">];</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs_num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">MSTensorHandle</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs_handle</span><span class="p">.</span><span class="n">handle_list</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">      </span><span class="n">inputs_size</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">MSTensorGetDataSize</span><span class="p">(</span><span class="n">tensor</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadInputsFile</span><span class="p">(</span><span class="s">&quot;input.bin&quot;</span><span class="w"> </span><span class="n">inputs_binbuf</span><span class="p">,</span><span class="w"> </span><span class="n">inputs_size</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">inputs_num</span><span class="p">);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs_num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSTensorGetMutableData</span><span class="p">(</span><span class="n">inputs_handle</span><span class="p">.</span><span class="n">handle_list</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">      </span><span class="n">memcpy</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">inputs_size</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">      </span><span class="n">free</span><span class="p">(</span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="w">      </span><span class="n">inputs_binbuf</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">MSTensorHandleArray</span><span class="w"> </span><span class="n">outputs_handle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelGetOutputs</span><span class="p">(</span><span class="n">model_handle</span><span class="p">);</span>
<span class="w">    </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelPredict</span><span class="p">(</span><span class="n">model_handle</span><span class="p">,</span><span class="w"> </span><span class="n">inputs_handle</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs_handle</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kMSStatusSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model_handle</span><span class="p">);</span>
<span class="w">      </span><span class="n">SEGGER_RTT_printf</span><span class="p">(</span><span class="s">&quot;MSModelPredict failed, ret: %d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">kMSStatusSuccess</span><span class="p">);</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">outputs_handle</span><span class="p">.</span><span class="n">handle_num</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">MSTensorHandle</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outputs_handle</span><span class="p">.</span><span class="n">handle_list</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">      </span><span class="n">PrintTensorHandle</span><span class="p">(</span><span class="n">output</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">SEGGER_RTT_printf</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;***********mnist test end***********</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Launch <code class="docutils literal notranslate"><span class="pre">cmd</span></code> as admin and run <code class="docutils literal notranslate"><span class="pre">make</span></code> to compile.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>make
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="stm32f746-project-deployment">
<h3>STM32F746 Project Deployment<a class="headerlink" href="#stm32f746-project-deployment" title="Permalink to this headline"></a></h3>
<p>Deploy executable files to the board using J-Link and perform inference.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>jlinkgdbserver           # start jlinkgdbserver set target device as STM32F746IG
jlinkRTTViewer           # start jlinkRTTViewer set target devices as STM32F746IG
arm-none-eabi-gdb        # start arm-gcc gdb service
file build/target.elf    # open debugging file
target remote 127.0.0.1  # connect jlink server
monitor reset            # reset board
monitor halt             # halt board
load                     # load executable to board
c                        # perform model inference
</pre></div>
</div>
</section>
</section>
<section id="performing-inference-on-harmonyos-lite">
<h2>Performing Inference on HarmonyOS Lite<a class="headerlink" href="#performing-inference-on-harmonyos-lite" title="Permalink to this headline"></a></h2>
<section id="installing-build-environment">
<h3>Installing build environment<a class="headerlink" href="#installing-build-environment" title="Permalink to this headline"></a></h3>
<p>For the environment preparation, please refer to <a class="reference external" href="https://device.harmonyos.com/en/docs/start/introduce/quickstart-lite-env-setup-lin-0000001105407498">HarmonyOS quick start</a>, including gn/ninja/llvm.</p>
</section>
<section id="connecting-to-the-board">
<h3>Connecting to the board<a class="headerlink" href="#connecting-to-the-board" title="Permalink to this headline"></a></h3>
<p>For Hardware environment preparation, please refer to the HarmonyOS quick start <a class="reference external" href="https://device.harmonyos.com/en/docs/start/introduce/quickstart-lite-steps-board3516-setting-0000001105829366">How to Develop</a> of board Hi3516 as example.</p>
</section>
<section id="compiling-the-model">
<h3>Compiling the model<a class="headerlink" href="#compiling-the-model" title="Permalink to this headline"></a></h3>
<p>Compile MNIST model for HarmonyOS lite by using codegen:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">SOURCE_CODE_DIR</span><span class="si">}</span><span class="w"> </span>--configFile<span class="o">=</span><span class="si">${</span><span class="nv">COFIG_FILE</span><span class="si">}</span>
</pre></div>
</div>
</section>
<section id="writing-build-scripts">
<h3>Writing build scripts<a class="headerlink" href="#writing-build-scripts" title="Permalink to this headline"></a></h3>
<p>For the HarmonyOS application development, please refer to <a class="reference external" href="https://device.harmonyos.com/en/docs/start/introduce/quickstart-lite-steps-board3516-running-0000001151888681">demo</a>. Copy the mnist directory generated in the previous step to any HarmonyOS source code path, assuming it is applications/sample/, and then create a new BUILD.gn file</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> &lt;harmony-source-path&gt;/applications/sample/mnist
 ├── benchmark
 ├── CMakeLists.txt
 ├── BUILD.gn
 └── src
</pre></div>
</div>
<p>Download the precompile runtime component for openharmony in <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.7/use/downloads.html">Download page</a>. This is a BUILD.gn example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>import(&quot;//build/lite/config/component/lite_component.gni&quot;)
   import(&quot;//build/lite/ndk/ndk.gni&quot;)

   lite_component(&quot;mnist_benchmark&quot;) {
       target_type = &quot;executable&quot;
       sources = [
            &quot;benchmark/benchmark.cc&quot;,
            &quot;benchmark/calib_output.cc&quot;,
            &quot;benchmark/load_input.c&quot;,
            &quot;src/net.c&quot;,
            &quot;src/weight.c&quot;,
            &quot;src/session.cc&quot;,
            &quot;src/tensor.cc&quot;,
       ]
       features = []
       include_dirs = [
            &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/runtime&quot;,
            &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/tools/codegen/include&quot;,
            &quot;//applications/sample/mnist/benchmark&quot;,
            &quot;//applications/sample/mnist/src&quot;,
       ]
       ldflags = [
            &quot;-fno-strict-aliasing&quot;,
            &quot;-Wall&quot;,
            &quot;-pedantic&quot;,
            &quot;-std=gnu99&quot;,
       ]
       libs = [
            &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/runtime/lib/libmindspore-lite.a&quot;,
            &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/tools/codegen/lib/libwrapper.a&quot;,
       ]
       defines = [
           &quot;NOT_USE_STL&quot;,
           &quot;ENABLE_NEON&quot;,
           &quot;ENABLE_ARM&quot;,
           &quot;ENABLE_ARM32&quot;
       ]
       cflags = [
            &quot;-fno-strict-aliasing&quot;,
            &quot;-Wall&quot;,
            &quot;-pedantic&quot;,
            &quot;-std=gnu99&quot;,
       ]
       cflags_cc = [
           &quot;-fno-strict-aliasing&quot;,
           &quot;-Wall&quot;,
           &quot;-pedantic&quot;,
           &quot;-std=c++17&quot;,
       ]
   }
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&lt;YOUR</span> <span class="pre">MINDSPORE</span> <span class="pre">LITE</span> <span class="pre">RUNTIME</span> <span class="pre">PATH&gt;</span></code> is the path where the runtime was unzipped, e.g. “//applications/sample/mnist/mindspore-lite-1.3.0-ohos-aarch32”.
Add the configuration of the mnist_benchmark component to the build/lite/components/applications.json file.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{
   &quot;component&quot;: &quot;mnist_benchmark&quot;,
   &quot;description&quot;: &quot;Communication related samples.&quot;,
   &quot;optional&quot;: &quot;true&quot;,
   &quot;dirs&quot;: [
     &quot;applications/sample/mnist&quot;
   ],
   &quot;targets&quot;: [
     &quot;//applications/sample/mnist:mnist_benchmark&quot;
   ],
   &quot;rom&quot;: &quot;&quot;,
   &quot;ram&quot;: &quot;&quot;,
   &quot;output&quot;: [],
   &quot;adapted_kernel&quot;: [ &quot;liteos_a&quot; ],
   &quot;features&quot;: [],
   &quot;deps&quot;: {
     &quot;components&quot;: [],
     &quot;third_party&quot;: []
   }
 },
</pre></div>
</div>
<p>Add the configuration of the mnist_benchmark component to the vendor/hisilicon/hispark_taurus/config.json.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{ &quot;component&quot;: &quot;mnist_benchmark&quot;, &quot;features&quot;:[] }
</pre></div>
</div>
</section>
<section id="building-benchmark">
<h3>Building benchmark<a class="headerlink" href="#building-benchmark" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cd &lt;OPENHARMONY SOURCE PATH&gt;
hb set
.
(select ipcamera_hispark_taurus@hisilicon)
hb build mnist_benchmark
</pre></div>
</div>
<p>The result file is generated in out/hispark_taurus/ipcamera_hispark_taurus directory.</p>
</section>
<section id="running-benchmark">
<h3>Running benchmark<a class="headerlink" href="#running-benchmark" title="Permalink to this headline"></a></h3>
<p>Copy mnist_benchmark, net.bin and test data(https://download.mindspore.cn/model_zoo/official/lite/quick_start/micro/mnist.tar.gz) to the board, and run:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> OHOS # ./mnist_benchmark mnist_input.bin net.bin 1
 OHOS # =======run benchmark======
 input 0: mnist_input.bin

 loop count: 1
 total time: 10.11800ms, per time: 10.11800ms

 outputs:
 name: int8toft32_Softmax-7_post0/output-0, DataType: 43, Elements: 10, Shape: [1 10 ], Data:
 0.000000, 0.000000, 0.003906, 0.000000, 0.000000, 0.992188, 0.000000, 0.000000, 0.000000, 0.000000,
 ========run success=======
</pre></div>
</div>
</section>
</section>
<section id="register-kernel">
<h2>Register Kernel<a class="headerlink" href="#register-kernel" title="Permalink to this headline"></a></h2>
<p>Currently, Users can only register their own kernels for custom operator. We will support registering the built-in operators’ kernels in the future. We use Hi3516D board as an example to show you how to use kernel register in codegen.</p>
<p>For how to register custom operators, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.7/use/nnie.html">Usage Description of the Integrated NNIE</a>.</p>
<section id="run-codegen">
<h3>Run codegen<a class="headerlink" href="#run-codegen" title="Permalink to this headline"></a></h3>
<p>Codegen can generate custom kernel’s function declaration and reference code if the model has custom operators.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">SOURCE_CODE_DIR</span><span class="si">}</span><span class="w"> </span>--configFile<span class="o">=</span><span class="si">${</span><span class="nv">COFIG_FILE</span><span class="si">}</span>
</pre></div>
</div>
<p>where target sets to be ARM32A.</p>
</section>
<section id="implement-custom-kernel-by-users">
<h3>Implement custom kernel by users<a class="headerlink" href="#implement-custom-kernel-by-users" title="Permalink to this headline"></a></h3>
<p>A header file named registered_kernel.h in the generated files. The custom kernel function is declared in this file:</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">CustomKernel</span><span class="p">(</span><span class="n">TensorC</span><span class="w"> </span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">input_num</span><span class="p">,</span><span class="w"> </span><span class="n">TensorC</span><span class="w"> </span><span class="o">*</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">output_num</span><span class="p">,</span><span class="w"> </span><span class="n">CustomParameter</span><span class="w"> </span><span class="o">*</span><span class="n">param</span><span class="p">);</span>
</pre></div>
</div>
<p>Users need to implement this function then add their source files to the cmake project. For example, we provide a sample library named libmicro_nnie.so in the nnie runtime package, <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.7/use/downloads.html">download</a>. The library contains the implementation of custom kernel for NNIE. Users can download it and modify the CMakeLists.txt：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>link_directories<span class="o">(</span>&lt;YOUR_PATH&gt;/mindspore-lite-1.5.0-linux-aarch32/providers/Hi3516D<span class="o">)</span>
link_directories<span class="o">(</span>&lt;HI3516D_SDK_PATH&gt;<span class="o">)</span>
target_link_libraries<span class="o">(</span>benchmark<span class="w"> </span>net<span class="w"> </span>micro_nnie<span class="w"> </span>nnie<span class="w"> </span>mpi<span class="w"> </span>VoiceEngine<span class="w"> </span>upvqe<span class="w"> </span>securec<span class="w"> </span>-lm<span class="w"> </span>-pthread<span class="o">)</span>
</pre></div>
</div>
<p>Finally, we build the benchmark:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>nnie<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>mkdir<span class="w"> </span>buid<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>-DCMAKE_TOOLCHAIN_FILE<span class="o">=</span>&lt;MS_SRC_PATH&gt;/mindspore/lite/cmake/himix200.toolchain.cmake<span class="w"> </span>-DPLATFORM_ARM32<span class="o">=</span>ON<span class="w"> </span>-DPKG_PATH<span class="o">=</span>&lt;RUNTIME_PKG_PATH&gt;<span class="w"> </span>..
make
</pre></div>
</div>
</section>
</section>
<section id="more-details">
<h2>More Details<a class="headerlink" href="#more-details" title="Permalink to this headline"></a></h2>
<section id="linux-x86-64-platform-compile-and-deploy">
<h3><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.7/mindspore/lite/examples/quick_start_micro/mnist_x86">Linux_x86_64 platform compile and deploy</a><a class="headerlink" href="#linux-x86-64-platform-compile-and-deploy" title="Permalink to this headline"></a></h3>
</section>
<section id="android-platform-compile-and-deploy">
<h3><a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r1.7/mindspore/lite/examples/quick_start_micro/mobilenetv2_arm64">Android platform compile and deploy</a><a class="headerlink" href="#android-platform-compile-and-deploy" title="Permalink to this headline"></a></h3>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_java.html" class="btn btn-neutral float-left" title="Using Java Interface to Perform Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="asic.html" class="btn btn-neutral float-right" title="Application Specific Integrated Circuit Integration Instructions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>