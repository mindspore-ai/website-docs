<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Building Custom Operators Online &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Using Delegate to Support Third-party AI Framework" href="delegate.html" />
    <link rel="prev" title="Construct custom kernel by registering conversion tool" href="converter_register.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Getting Started in One Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_cpp.html">Experience C++ Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_java.html">Experience Java Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start.html">Android Application Development Based on JNI Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Perform Inference on Mini and Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="register.html">Custom Kernel</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="converter_register.html">Construct custom kernel by registering conversion tool</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Building Custom Operators Online</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#implementing-custom-operators">Implementing Custom Operators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#determining-operator-types">Determining Operator Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#common-operators">Common Operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="#custom-operators">Custom Operators</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#custom-gpu-operators">Custom GPU Operators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#registering-operators">Registering Operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="#implementing-operators">Implementing Operators</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="register.html">Custom Kernel</a> &raquo;</li>
      <li>Building Custom Operators Online</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/register_kernel.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="building-custom-operators-online">
<h1>Building Custom Operators Online<a class="headerlink" href="#building-custom-operators-online" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r1.7/docs/lite/docs/source_en/use/register_kernel.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source_en.png" /></a></p>
<section id="implementing-custom-operators">
<h2>Implementing Custom Operators<a class="headerlink" href="#implementing-custom-operators" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite provides a southbound operator registration mechanism. This document describes how to schedule your own operators through the MindSpore Lite framework.</p>
<p>To implement custom operators, perform the following steps:</p>
<ol class="arabic simple">
<li><p>Determine operator types: Classify operators into common and custom operators.</p></li>
<li><p>Implement operators: Inherit the Kernel class to implement custom operators and register them in MindSpore Lite.</p></li>
<li><p>Implement the InferShape capability: Inherit mindspore::kernel::KernelInteface to implement the InferShape capability of custom operators and register them in MindSpore Lite.</p></li>
</ol>
<section id="determining-operator-types">
<h3>Determining Operator Types<a class="headerlink" href="#determining-operator-types" title="Permalink to this headline"></a></h3>
<p>View the operator prototype definition in mindspore/lite/schema/ops.fbs. Check whether the operator prototype to be registered is defined in PrimitiveType. If yes, the operator is a common operator, and you can implement and register the operator based on the existing IR. Otherwise, the operator is a custom operator.</p>
</section>
<section id="common-operators">
<h3>Common Operators<a class="headerlink" href="#common-operators" title="Permalink to this headline"></a></h3>
<p>For details about code related to implementation, registration, and InferShape of an operator, see <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.7/mindspore/lite/test/ut/src/registry/registry_test.cc">the code repository</a>.</p>
<section id="implementing-common-operators">
<h4>Implementing Common Operators<a class="headerlink" href="#implementing-common-operators" title="Permalink to this headline"></a></h4>
<p>Inherit <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/api_cpp/mindspore_kernel.html">mindspore::kernel::Kernel</a> and overload necessary APIs. The following describes how to customize an Add operator:</p>
<ol class="arabic simple">
<li><p>An operator inherits a kernel.</p></li>
<li><p>PreProcess() pre-allocates memory.</p></li>
<li><p>Execute() adds inputs.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kernel</span><span class="o">::</span><span class="n">Kernel</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_PARAM_INVALID</span><span class="p">;</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TestCustomAdd</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">Kernel</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">TestCustomAdd</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">Kernel</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">)</span><span class="w"> </span><span class="p">{}</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">Prepare</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="p">}</span>

<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">Execute</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>

<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="nf">ReSize</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="p">}</span>

<span class="w"> </span><span class="k">private</span><span class="o">:</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">PreProcess</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// malloc data for output tensor</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Get data failed&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">TestCustomAdd::Execute</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">RET_PARAM_INVALID</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">PreProcess</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">in0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">inputs_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">());</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">in1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">inputs_</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">());</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">outputs_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">MutableData</span><span class="p">());</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outputs_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">ElementNum</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in0</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">in1</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="registering-common-operators">
<h4>Registering Common Operators<a class="headerlink" href="#registering-common-operators" title="Permalink to this headline"></a></h4>
<p>Currently, the generated macro <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/generate/classmindspore_registry_RegisterKernel.html">REGISTER_KERNEL</a> is provided for operator registration. The implementation procedure is as follows:</p>
<ol class="arabic simple">
<li><p>The TestCustomAddCreator function is used to create a kernel.</p></li>
<li><p>Use the macro REGISTER_KERNEL to register the kernel. Assume that the vendor is BuiltInTest.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">schema</span><span class="o">::</span><span class="n">PrimitiveType_AddFusion</span><span class="p">;</span>

<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Kernel</span><span class="o">&gt;</span><span class="w"> </span><span class="n">TestCustomAddCreator</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                                             </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                                             </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">TestCustomAdd</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">);</span>
<span class="p">}</span>
<span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">kFloat32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">;</span>

<span class="n">REGISTER_KERNEL</span><span class="p">(</span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">BuiltInTest</span><span class="p">,</span><span class="w"> </span><span class="n">kFloat32</span><span class="p">,</span><span class="w"> </span><span class="n">PrimitiveType_AddFusion</span><span class="p">,</span><span class="w"> </span><span class="n">TestCustomAddCreator</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="common-operator-infershape">
<h4>Common Operator InferShape<a class="headerlink" href="#common-operator-infershape" title="Permalink to this headline"></a></h4>
<p>Reload the Infer function after inheriting KernelInterface to implement the InferShape capability. The implementation procedure is as follows:</p>
<ol class="arabic simple">
<li><p>Inherit <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/generate/classmindspore_kernel_KernelInterface.html">KernelInterface</a>.</p></li>
<li><p>Overload the Infer function to derive the shape, format, and data_type of the output tensor.</p></li>
</ol>
<p>The following uses the custom Add operator as an example:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kernel</span><span class="o">::</span><span class="n">KernelInterface</span><span class="p">;</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TestCustomAddInfer</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">KernelInterface</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">TestCustomAddInfer</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="o">~</span><span class="n">TestCustomAddInfer</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="n">Status</span><span class="w"> </span><span class="nf">Infer</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">               </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetFormat</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">format</span><span class="p">());</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetDataType</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">());</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetShape</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">());</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
</section>
<section id="registering-the-common-operator-infershape">
<h4>Registering the Common Operator InferShape<a class="headerlink" href="#registering-the-common-operator-infershape" title="Permalink to this headline"></a></h4>
<p>Currently, the generated macro <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/generate/classmindspore_registry_RegisterKernelInterface.html">REGISTER_KERNEL_INTERFACE</a> is provided for registering the operator InferShape. The procedure is as follows:</p>
<ol class="arabic simple">
<li><p>Use the CustomAddInferCreator function to create a KernelInterface instance.</p></li>
<li><p>Call the REGISTER_KERNEL_INTERFACE macro to register the common operator InferShape. Assume that the vendor is BuiltInTest.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">KernelInterface</span><span class="o">&gt;</span><span class="w"> </span><span class="n">CustomAddInferCreator</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">TestCustomAddInfer</span><span class="o">&gt;</span><span class="p">();</span><span class="w"> </span><span class="p">}</span>

<span class="n">REGISTER_KERNEL_INTERFACE</span><span class="p">(</span><span class="n">BuiltInTest</span><span class="p">,</span><span class="w"> </span><span class="n">PrimitiveType_AddFusion</span><span class="p">,</span><span class="w"> </span><span class="n">CustomAddInferCreator</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="custom-operators">
<h3>Custom Operators<a class="headerlink" href="#custom-operators" title="Permalink to this headline"></a></h3>
<p>For details about code related to parsing, creating, and operating custom operators, see <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.7/mindspore/lite/test/ut/tools/converter/registry/pass_registry_test.cc">the code repository</a>.</p>
<section id="defining-custom-operators">
<h4>Defining Custom Operators<a class="headerlink" href="#defining-custom-operators" title="Permalink to this headline"></a></h4>
<div class="highlight-css notranslate"><div class="highlight"><pre><span></span><span class="nt">table</span><span class="w"> </span><span class="nt">Attribute</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">name</span><span class="p">:</span><span class="w"> </span><span class="n">string</span><span class="p">;</span>
<span class="w">    </span><span class="n">data</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">ubyte</span><span class="p">];</span>
<span class="p">}</span>

<span class="nt">table</span><span class="w"> </span><span class="nt">Custom</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">type</span><span class="p">:</span><span class="w"> </span><span class="n">string</span><span class="p">;</span>
<span class="w">    </span><span class="n">attr</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="n">Attribute</span><span class="p">];</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Attributes are stored in a dictionary. <code class="docutils literal notranslate"><span class="pre">name</span></code> indicates the attribute name. <code class="docutils literal notranslate"><span class="pre">data</span></code> indicates the byte stream of the attribute content.
<code class="docutils literal notranslate"><span class="pre">type</span></code> indicates the custom operator type.</p>
</section>
<section id="creating-custom-operators">
<h4>Creating Custom Operators<a class="headerlink" href="#creating-custom-operators" title="Permalink to this headline"></a></h4>
<p>You can register your own Pass using the Pass registration API of the <code class="docutils literal notranslate"><span class="pre">Converter</span></code> to export the required operator structure. The following describes how to convert the AddN operator into a custom operator:</p>
<ol class="arabic simple">
<li><p>Assume that the custom operator has the input_num and op_kind attributes.</p></li>
<li><p>Customize Pass subclasses to convert and create custom operators.</p></li>
<li><p>Register the custom Pass class.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">namespace</span><span class="w"> </span><span class="nn">mindspore</span><span class="o">::</span><span class="nn">opt</span><span class="w"> </span><span class="p">{</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Test2Fusion</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">Pass</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">AnfNodePtr</span><span class="w"> </span><span class="n">CreateCustomOp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">FuncGraphPtr</span><span class="w"> </span><span class="n">func_graph</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">CNodePtr</span><span class="w"> </span><span class="n">cnode</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">func_graph</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">cnode</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">primc</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">ops</span><span class="o">::</span><span class="n">Custom</span><span class="o">&gt;</span><span class="p">();</span><span class="w">      </span><span class="c1">// Create a primitive to store operator attributes.</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">primc</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">primc</span><span class="o">-&gt;</span><span class="n">set_type</span><span class="p">(</span><span class="s">&quot;Custom_AddN&quot;</span><span class="p">);</span><span class="w">        </span><span class="c1">// Set the custom operator type.</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">custom_attrs</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">input_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">cnode</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">input_num_attr</span><span class="p">(</span><span class="n">input_num</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">input_num</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
<span class="w">    </span><span class="n">custom_attrs</span><span class="p">[</span><span class="s">&quot;input_num&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_num_attr</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">op_kind</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;custom op&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">op_kind_attr</span><span class="p">(</span><span class="n">op_kind</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">op_kind</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
<span class="w">    </span><span class="n">custom_attrs</span><span class="p">[</span><span class="s">&quot;op_kind&quot;</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">op_kind_attr</span><span class="p">;</span>
<span class="w">    </span><span class="n">primc</span><span class="o">-&gt;</span><span class="n">set_attr</span><span class="p">(</span><span class="n">custom_attrs</span><span class="p">);</span><span class="w">         </span><span class="c1">// Set the custom operator attributes.</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cnode</span><span class="o">-&gt;</span><span class="n">inputs</span><span class="p">();</span>
<span class="w">    </span><span class="n">inputs</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">begin</span><span class="p">());</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">custom_cnode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">func_graph</span><span class="o">-&gt;</span><span class="n">NewCNode</span><span class="p">(</span><span class="n">primc</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">);</span><span class="w">         </span><span class="c1">// Create a CNode.</span>
<span class="w">    </span><span class="n">custom_cnode</span><span class="o">-&gt;</span><span class="n">set_fullname_with_scope</span><span class="p">(</span><span class="n">cnode</span><span class="o">-&gt;</span><span class="n">fullname_with_scope</span><span class="p">());</span><span class="w">     </span><span class="c1">// Set the node name.</span>
<span class="w">    </span><span class="n">custom_cnode</span><span class="o">-&gt;</span><span class="n">set_abstract</span><span class="p">(</span><span class="n">cnode</span><span class="o">-&gt;</span><span class="n">abstract</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">Clone</span><span class="p">());</span><span class="w">          </span><span class="c1">// Set basic attributes of the operator output and store them in abstract.</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">custom_cnode</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">Run</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">FuncGraphPtr</span><span class="w"> </span><span class="o">&amp;</span><span class="n">func_graph</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">manager</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Manage</span><span class="p">(</span><span class="n">func_graph</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span><span class="w">       </span><span class="c1">// Create a FuncGrap manager.</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">manager</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">node_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TopoSort</span><span class="p">(</span><span class="n">func_graph</span><span class="o">-&gt;</span><span class="n">get_return</span><span class="p">());</span><span class="w">      </span><span class="c1">// Obtain all nodes.</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">node</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">node_list</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">utils</span><span class="o">::</span><span class="n">isa</span><span class="o">&lt;</span><span class="n">CNode</span><span class="o">&gt;</span><span class="p">(</span><span class="n">node</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">continue</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">opt</span><span class="o">::</span><span class="n">CheckPrimitiveType</span><span class="p">(</span><span class="n">node</span><span class="p">,</span><span class="w"> </span><span class="n">prim</span><span class="o">::</span><span class="n">kPrimAddN</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">     </span><span class="c1">// Check whether the current node is an AddN operator.</span>
<span class="w">        </span><span class="k">continue</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">cnode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">node</span><span class="o">-&gt;</span><span class="n">cast</span><span class="o">&lt;</span><span class="n">CNodePtr</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">custom_cnode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCustomOp</span><span class="p">(</span><span class="n">func_graph</span><span class="p">,</span><span class="w"> </span><span class="n">cnode</span><span class="p">);</span><span class="w">    </span><span class="c1">// Create a custom operator.</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">custom_cnode</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="n">manager</span><span class="o">-&gt;</span><span class="n">Replace</span><span class="p">(</span><span class="n">node</span><span class="p">,</span><span class="w"> </span><span class="n">custom_cnode</span><span class="p">)</span><span class="w">        </span><span class="c1">// Replace old nodes with new nodes through the manager.</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>

<span class="n">REG_PASS</span><span class="p">(</span><span class="n">Test1Fusion</span><span class="p">,</span><span class="w"> </span><span class="n">Test1Fusion</span><span class="p">)</span><span class="w">    </span><span class="c1">// Register Test1Fusion.</span>
<span class="n">REG_PASS</span><span class="p">(</span><span class="n">Test2Fusion</span><span class="p">,</span><span class="w"> </span><span class="n">Test2Fusion</span><span class="p">)</span><span class="w">    </span><span class="c1">// Register Test2Fusion.</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">schedule</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;Test1Fusion&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Test2Fusion&quot;</span><span class="p">};</span>
<span class="n">REG_SCHEDULED_PASS</span><span class="p">(</span><span class="n">POSITION_BEGIN</span><span class="p">,</span><span class="w"> </span><span class="n">schedule</span><span class="p">)</span><span class="w">       </span><span class="c1">// Set the external Pass scheduling logic and run the external Pass before the built-in fusion.</span>
<span class="p">}</span><span class="w">  </span><span class="c1">// namespace mindspore::opt</span>
</pre></div>
</div>
<p>For details about code related to implementation, registration, and InferShape of a custom operator, see <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.7/mindspore/lite/test/ut/src/registry/registry_custom_op_test.cc">the code repository</a>.</p>
</section>
<section id="implementing-custom-operators-1">
<h4>Implementing Custom Operators<a class="headerlink" href="#implementing-custom-operators-1" title="Permalink to this headline"></a></h4>
<p>The implementation procedure of a custom operator is the same as that of a common operator, because they are specific subclasses of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/api_cpp/mindspore_kernel.html">Kernel</a>.
If the custom operator does not run on the CPU platform, the result needs to be copied back to the output tensor after the running is complete. The following describes how to create a custom operator with the Add capability:</p>
<ol class="arabic simple">
<li><p>An operator inherits a kernel.</p></li>
<li><p>PreProcess() pre-allocates memory.</p></li>
<li><p>Execute() adds inputs.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kernel</span><span class="o">::</span><span class="n">Kernel</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TestCustomOp</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">Kernel</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">TestCustomOp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">               </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">Kernel</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">)</span><span class="w"> </span><span class="p">{}</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">Prepare</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="p">}</span>

<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">Execute</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="p">;</span>

<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="nf">ReSize</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="p">}</span>

<span class="w"> </span><span class="k">private</span><span class="o">:</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">PreProcess</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// malloc data for output tensor</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output</span><span class="o">-&gt;</span><span class="n">MutableData</span><span class="p">();</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Get data failed&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">TestCustomOp</span><span class="o">::</span><span class="n">Execute</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">RET_PARAM_INVALID</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">PreProcess</span><span class="p">();</span>
<span class="w">  </span><span class="n">GetAttrData</span><span class="p">();</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">in0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">inputs_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">());</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">in1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">inputs_</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">());</span>
<span class="w">  </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">outputs_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">MutableData</span><span class="p">());</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outputs_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">ElementNum</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in0</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">in1</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">RET_OK</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="custom-operator-attribute-decoding-example">
<h4>Custom Operator Attribute Decoding Example<a class="headerlink" href="#custom-operator-attribute-decoding-example" title="Permalink to this headline"></a></h4>
<p>In the example, the byte stream in the attribute is copied to the buffer.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">prim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">primitive_</span><span class="o">-&gt;</span><span class="n">value_as_Custom</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">prim</span><span class="o">-&gt;</span><span class="n">attr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_bytes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">prim</span><span class="o">-&gt;</span><span class="n">attr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">Get</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">();</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data_bytes</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span>
<span class="w">    </span><span class="kt">char</span><span class="w"> </span><span class="n">buf</span><span class="p">[</span><span class="mi">100</span><span class="p">];</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">data_size</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">buf</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="p">(</span><span class="n">data_bytes</span><span class="o">-&gt;</span><span class="n">Get</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">buf</span><span class="p">[</span><span class="n">data_size</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="registering-custom-operators">
<h4>Registering Custom Operators<a class="headerlink" href="#registering-custom-operators" title="Permalink to this headline"></a></h4>
<p>Currently, the generated macro <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/generate/define_register_kernel.h_REGISTER_CUSTOM_KERNEL-1.html">REGISTER_CUSTOM_KERNEL</a> is provided for operator registration. The procedure is as follows:</p>
<ol class="arabic simple">
<li><p>The TestCustomAddCreator function is used to create a kernel.</p></li>
<li><p>Use the macro REGISTER_CUSTOM_KERNEL to register an operator. Assume that the vendor is BuiltInTest and the operator type is Add.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">schema</span><span class="o">::</span><span class="n">PrimitiveType_AddFusion</span><span class="p">;</span>

<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Kernel</span><span class="o">&gt;</span><span class="w"> </span><span class="n">TestCustomAddCreator</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                                             </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">tensor</span><span class="o">::</span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                                             </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">TestCustomOp</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">);</span>
<span class="p">}</span>
<span class="k">constexpr</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">kFloat32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">;</span>
<span class="n">REGISTER_CUSTOM_KERNEL</span><span class="p">(</span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">BuiltInTest</span><span class="p">,</span><span class="w"> </span><span class="n">kFloat32</span><span class="p">,</span><span class="w"> </span><span class="n">Add</span><span class="p">,</span><span class="w"> </span><span class="n">TestCustomAddCreator</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="custom-operator-infershape">
<h4>Custom Operator InferShape<a class="headerlink" href="#custom-operator-infershape" title="Permalink to this headline"></a></h4>
<p>The overall implementation is the same as that of the common operator InferShape. The procedure is as follows:</p>
<ol class="arabic simple">
<li><p>Inherit <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/generate/classmindspore_kernel_KernelInterface.html">KernelInterface</a>.</p></li>
<li><p>Overload the Infer function to derive the shape, format, and data_type of the output tensor.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TestCustomOpInfer</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">KernelInterface</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">TestCustomOpInfer</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="o">~</span><span class="n">TestCustomOpInfer</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="n">Status</span><span class="w"> </span><span class="nf">Infer</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">             </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetFormat</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">format</span><span class="p">());</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetDataType</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">());</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetShape</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">());</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
</pre></div>
</div>
</section>
<section id="registering-the-custom-operator-infershape">
<h4>Registering the Custom Operator InferShape<a class="headerlink" href="#registering-the-custom-operator-infershape" title="Permalink to this headline"></a></h4>
<p>Currently, the generated macro <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/generate/define_register_kernel_interface.h_REGISTER_CUSTOM_KERNEL_INTERFACE-1.html">REGISTER_CUSTOM_KERNEL_INTERFACE</a> is provided for registering the custom operator InferShape. The procedure is as follows:</p>
<ol class="arabic simple">
<li><p>Use the CustomAddInferCreator function to create a custom KernelInterface.</p></li>
<li><p>The macro <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/generate/define_register_kernel_interface.h_REGISTER_CUSTOM_KERNEL_INTERFACE-1.html">REGISTER_CUSTOM_KERNEL_INTERFACE</a> is provided for registering the InferShape capability. The operator type Add must be the same as that in REGISTER_CUSTOM_KERNEL.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">KernelInterface</span><span class="o">&gt;</span><span class="w"> </span><span class="n">CustomAddInferCreator</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">TestCustomOpInfer</span><span class="o">&gt;</span><span class="p">();</span><span class="w"> </span><span class="p">}</span>

<span class="n">REGISTER_CUSTOM_KERNEL_INTERFACE</span><span class="p">(</span><span class="n">BuiltInTest</span><span class="p">,</span><span class="w"> </span><span class="n">Add</span><span class="p">,</span><span class="w"> </span><span class="n">CustomAddInferCreator</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="custom-gpu-operators">
<h2>Custom GPU Operators<a class="headerlink" href="#custom-gpu-operators" title="Permalink to this headline"></a></h2>
<p>A set of GPU-related functional APIs are provided to facilitate the development of the GPU-based custom operator and enable the GPU-based custom operator to share the same resources with the internal GPU-based operators to improve the scheduling efficiency. For details about the APIs, see <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a>.
This document describes how to develop a custom GPU operator by parsing sample code. Before reading this document, you need to understand <a class="reference internal" href="#implementing-custom-operators"><span class="std std-doc">Implement Custom Operators</span></a>.
The <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r1.7/mindspore/lite/test/ut/src/registry/registry_gpu_custom_op_test.cc">code repository</a> contains implementation and registration of custom GPU operators.</p>
<section id="registering-operators">
<h3>Registering Operators<a class="headerlink" href="#registering-operators" title="Permalink to this headline"></a></h3>
<p>In this example, the custom operator <code class="docutils literal notranslate"><span class="pre">Custom_Add</span></code> is registered. For details about how to create and implement this operator, see <a class="reference internal" href="#defining-custom-operators"><span class="std std-doc">Defining Custom Operators</span></a> and <a class="reference internal" href="#implementing-custom-operators"><span class="std std-doc">Implementing Custom Operators</span></a>.</p>
<section id="implementing-a-function-for-creating-an-operator-instance">
<h4>Implementing a Function for Creating an Operator Instance<a class="headerlink" href="#implementing-a-function-for-creating-an-operator-instance" title="Permalink to this headline"></a></h4>
<p>Implement a function for creating an operator instance to implement the first step of custom operator registration. The function type is declared in <code class="docutils literal notranslate"><span class="pre">include/registry/register_kernel.h</span></code>, as shown in the following:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">/// \brief CreateKernel Defined a functor to create a kernel.</span>
<span class="c1">///</span>
<span class="c1">/// \param[in] inputs Define input tensors of kernel.</span>
<span class="c1">/// \param[in] outputs Define output tensors of kernel.</span>
<span class="c1">/// \param[in] primitive Define attributes of op.</span>
<span class="c1">/// \param[in] ctx Define for holding environment variables during runtime.</span>
<span class="c1">///</span>
<span class="c1">/// \return Smart Pointer of kernel.</span>
<span class="k">using</span><span class="w"> </span><span class="n">CreateKernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kernel</span><span class="o">::</span><span class="n">Kernel</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">,</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">)</span><span class="o">&gt;</span><span class="p">;</span>
</pre></div>
</div>
<p>In this example, the operator instance creation function is implemented as follows. The function returns a <code class="docutils literal notranslate"><span class="pre">CustomAddKernel</span></code> class instance. This class is the user-defined operator class that inherits the <code class="docutils literal notranslate"><span class="pre">kernel::Kernel</span></code> class. For details about the implementation of this class, see <a class="reference internal" href="#implementing-operators"><span class="std std-doc">Implementing Operators</span></a>.
In the function, in addition to transferring the function parameters to the constructor function of the <code class="docutils literal notranslate"><span class="pre">CustomAddKernel</span></code> class, a Boolean variable is also transferred. The variable is used to control whether the data type processed by the created <code class="docutils literal notranslate"><span class="pre">CustomAddKernel</span></code> instance is FLOAT32 or FLOAT16.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">namespace</span><span class="w"> </span><span class="nn">custom_gpu_demo</span><span class="w"> </span><span class="p">{</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kernel</span><span class="o">::</span><span class="n">Kernel</span><span class="o">&gt;</span><span class="w"> </span><span class="n">CustomAddCreator</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                                                 </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                                                 </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">fp16_enable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;using fp32 add.</span><span class="se">\n</span><span class="s">&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">CustomAddKernel</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">fp16_enable</span><span class="p">);</span>
<span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="registering-operators-1">
<h4>Registering Operators<a class="headerlink" href="#registering-operators-1" title="Permalink to this headline"></a></h4>
<p>When registering GPU operators, you must declare the device type as GPU and transfer the operator instance creation function <code class="docutils literal notranslate"><span class="pre">CustomAddCreator</span></code> implemented in the previous step.
In this example, the Float32 implementation of the Custom_Add operator is registered. The registration code is as follows. For details about other parameters in the registration macro, see the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/api_cpp/mindspore_registry.html">API</a>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">kFloat32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">;</span>
<span class="c1">// Register custom &quot;Custom_Add&quot; operator</span>
<span class="n">REGISTER_CUSTOM_KERNEL</span><span class="p">(</span><span class="n">GPU</span><span class="p">,</span><span class="w"> </span><span class="n">BuiltInTest</span><span class="p">,</span><span class="w"> </span><span class="n">kFloat32</span><span class="p">,</span><span class="w"> </span><span class="n">Custom_Add</span><span class="p">,</span><span class="w"> </span><span class="n">CustomAddCreator</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="implementing-operators">
<h3>Implementing Operators<a class="headerlink" href="#implementing-operators" title="Permalink to this headline"></a></h3>
<p>In this example, the operator is implemented as the <code class="docutils literal notranslate"><span class="pre">CustomAddKernel</span></code> class. This class inherits <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/api_cpp/mindspore_kernel.html">mindspore::kernel::Kernel</a> and reloads necessary APIs to implement the custom operator computation.</p>
<section id="constructor-and-destructor-functions">
<h4>Constructor and Destructor Functions<a class="headerlink" href="#constructor-and-destructor-functions" title="Permalink to this headline"></a></h4>
<p>In the constructor function of the <code class="docutils literal notranslate"><span class="pre">CustomAddKernel</span></code> class, the Boolean variable <code class="docutils literal notranslate"><span class="pre">fp16_enable</span></code> is saved, and other parameters are transferred to the constructor function of the base class.
In the destructor function of the <code class="docutils literal notranslate"><span class="pre">CustomAddKernel</span></code> class, <code class="docutils literal notranslate"><span class="pre">FreeWeight()</span></code> is called to release the memory that is temporarily allocated for computation.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CustomAddKernel</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">kernel</span><span class="o">::</span><span class="n">Kernel</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">CustomAddKernel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                  </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">,</span>
<span class="w">                  </span><span class="kt">bool</span><span class="w"> </span><span class="n">fp16_enable</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">Kernel</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">),</span><span class="w"> </span><span class="n">fp16_enable_</span><span class="p">(</span><span class="n">fp16_enable</span><span class="p">)</span><span class="w"> </span><span class="p">{}</span>
<span class="w">  </span><span class="o">~</span><span class="n">CustomAddKernel</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">FreeWeight</span><span class="p">();</span><span class="w"> </span><span class="p">}</span>

<span class="w">  </span><span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="class-member-variable-description">
<h4>Class Member Variable Description<a class="headerlink" href="#class-member-variable-description" title="Permalink to this headline"></a></h4>
<ul>
<li><p>opencl_runtime_</p>
<p>An instance of the OpenCLRuntimeWrapper class. In an operator, this object can be used to call the OpenCL-related API <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a> provided by MindSpore Lite.</p>
</li>
<li><p>fp16_enable_</p>
<p>Determines whether the operator uses FP16 for computation. To use FP16 for computation, you need to register the operator as an FP16 operator. In this example, the FP32 operator is registered.</p>
</li>
<li><p>weight_ptrs_</p>
<p>Pointer to the temporary memory required for operator computation.</p>
</li>
<li><p>Other variables</p>
<p>Other variables are required for OpenCL operations. For details, see <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r1.7/api_cpp/mindspore_registry_opencl.html">mindspore::registry::opencl</a>.</p>
</li>
</ul>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CustomAddKernel</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">kernel</span><span class="o">::</span><span class="n">Kernel</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="p">...</span>
<span class="w"> </span><span class="k">private</span><span class="o">:</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">fp16_enable_</span><span class="p">;</span>
<span class="w">  </span><span class="n">cl</span><span class="o">::</span><span class="n">Kernel</span><span class="w"> </span><span class="n">kernel_</span><span class="p">;</span>
<span class="w">  </span><span class="n">cl</span><span class="o">::</span><span class="n">Event</span><span class="w"> </span><span class="n">event_</span><span class="p">;</span>
<span class="w">  </span><span class="n">cl</span><span class="o">::</span><span class="n">NDRange</span><span class="w"> </span><span class="n">global_range_</span><span class="p">{</span><span class="n">cl</span><span class="o">::</span><span class="n">NullRange</span><span class="p">};</span>
<span class="w">  </span><span class="n">cl</span><span class="o">::</span><span class="n">NDRange</span><span class="w"> </span><span class="n">local_range_</span><span class="p">{</span><span class="n">cl</span><span class="o">::</span><span class="n">NullRange</span><span class="p">};</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">weight_ptrs_</span><span class="p">;</span>
<span class="w">  </span><span class="n">registry</span><span class="o">::</span><span class="n">opencl</span><span class="o">::</span><span class="n">OpenCLRuntimeWrapper</span><span class="w"> </span><span class="n">opencl_runtime_</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="code-and-description-of-the-prepare-implementation">
<h4>Code and Description of the Prepare Implementation<a class="headerlink" href="#code-and-description-of-the-prepare-implementation" title="Permalink to this headline"></a></h4>
<p>In the graph build phase <code class="docutils literal notranslate"><span class="pre">mindspore::Model::Build</span></code>, the Prepare function of the operator is called. You can perform some time-consuming and one-off operations to save the operator computation time of <code class="docutils literal notranslate"><span class="pre">mindspore::Model::Predict</span></code>.
In this example, the Prepare API is overloaded to load and build the custom OpenCL code.</p>
<ol class="arabic simple">
<li><p>Check the environment.</p></li>
</ol>
<p>In the example, <code class="docutils literal notranslate"><span class="pre">CheckSpecs</span></code> is called to check the running environment of the operator.
In <code class="docutils literal notranslate"><span class="pre">CheckSpecs</span></code>, the input and output data types and the number of input and output tensors are checked.
The <code class="docutils literal notranslate"><span class="pre">MSTensor::IsConst()</span></code> API can be used to determine whether the data of a tensor is a constant. The data type of the non-constant input is also compared with the data type declared during operator registration. For details about how to process constant data, see the subsequent tutorials.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">Prepare</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CheckSpecs</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Prepare failed for check kernel specs!&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="p">...</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">CheckSpecs</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;ArithmeticOpenCLKernel only support fp32/fp16 input&quot;</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;ArithmeticOpenCLKernel only support fp32/fp16 output&quot;</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">outputs_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;in size: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">inputs_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;, out size: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">outputs_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">in_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs_</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">in_tensor</span><span class="p">.</span><span class="n">IsConst</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">fp16_enable_</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">in_tensor</span><span class="p">.</span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inputs data type error, expectation kNumberTypeFloat16 but kNumberTypeFloat32.&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">fp16_enable_</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">in_tensor</span><span class="p">.</span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inputs data type error, expectation kNumberTypeFloat32 but kNumberTypeFloat16.&quot;</span><span class="p">;</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Load the custom OpenCL code.</p></li>
</ol>
<p>Use <code class="docutils literal notranslate"><span class="pre">opencl_runtime_</span></code> to call the <code class="docutils literal notranslate"><span class="pre">OpenCLRuntimeWrapper::LoadSource</span></code> API to load the custom OpenCL code.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">Prepare</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="p">...</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">kernel_name_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;ElementAdd&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">program_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Arithmetic&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">arithmetic_source</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">opencl_runtime_</span><span class="p">.</span><span class="n">LoadSource</span><span class="p">(</span><span class="n">program_name</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Load source failed.&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">arithmetic_source</span></code> is the user-defined OpenCL code, as shown in the following:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">static</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">arithmetic_source</span><span class="w"> </span><span class="o">=</span>
<span class="w">  </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;#pragma OPENCL EXTENSION cl_khr_fp16 : enable</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;__constant sampler_t smp_none = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_NONE | CLK_FILTER_NEAREST;</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;__kernel void ElementAdd(__read_only image2d_t input_a, __read_only image2d_t input_b, __write_only image2d_t &quot;</span>
<span class="w">  </span><span class="s">&quot;output,</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;                         const int2 output_shape) {</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;  int X = get_global_id(0);</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;  int Y = get_global_id(1);</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;  if (X &gt;= output_shape.x || Y &gt;= output_shape.y) {</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;    return;</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;  }</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;  FLT4 a = READ_IMAGE(input_a, smp_none, (int2)(X, Y));</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;  FLT4 b = READ_IMAGE(input_b, smp_none, (int2)(X, Y));</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;  FLT4 result = a + b;</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;  WRITE_IMAGE(output, (int2)(X, Y), result);</span><span class="se">\n</span><span class="s">&quot;</span>
<span class="w">  </span><span class="s">&quot;}</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Build the OpenCL code.</p></li>
</ol>
<p>Use <code class="docutils literal notranslate"><span class="pre">fp16_enable_</span></code> to specify different build options to generate the code for processing FLOAT16 or FLOAT32 data.
Use <code class="docutils literal notranslate"><span class="pre">opencl_runtime_</span></code> to call the <code class="docutils literal notranslate"><span class="pre">OpenCLRuntimeWrapper::BuildKernel</span></code> API, obtain the built <code class="docutils literal notranslate"><span class="pre">cl::Kernel</span></code> variable, and save it in <code class="docutils literal notranslate"><span class="pre">kernel_</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">Prepare</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="p">...</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">build_options_ext</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="s">&quot;-cl-mad-enable -cl-fast-relaxed-math -Werror&quot;</span><span class="p">};</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">fp16_enable_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">build_options_ext</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="s">&quot; -DFLT4=half4 -DWRITE_IMAGE=write_imageh -DREAD_IMAGE=read_imageh&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">build_options_ext</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="s">&quot; -DFLT4=float4 -DWRITE_IMAGE=write_imagef -DREAD_IMAGE=read_imagef&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">opencl_runtime_</span><span class="p">.</span><span class="n">BuildKernel</span><span class="p">(</span><span class="o">&amp;</span><span class="n">kernel_</span><span class="p">,</span><span class="w"> </span><span class="n">program_name</span><span class="p">,</span><span class="w"> </span><span class="n">kernel_name_</span><span class="p">,</span><span class="w"> </span><span class="n">build_options_ext</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build kernel failed.&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Set the OpenCL working group and work items.</p></li>
</ol>
<p>For an operator registered as a GPU, the input data received is in image format except that the input is a constant. The format is NHWC4 (C-axis 4-byte aligned NHWC format data).
In this example, all data is converted to this format for computation and output.
In the routine, a simple addition custom operator is implemented. Therefore, the <code class="docutils literal notranslate"><span class="pre">GpuTensorInfo</span></code> function is used to compute the width and height of the <code class="docutils literal notranslate"><span class="pre">Image</span></code> memory used by the output data to set the work items.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">Prepare</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="p">...</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GpuTensorInfo</span><span class="p">(</span><span class="o">&amp;</span><span class="n">outputs_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="o">&amp;</span><span class="n">opencl_runtime_</span><span class="p">);</span>
<span class="w">  </span><span class="n">local_range_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cl</span><span class="o">::</span><span class="n">NullRange</span><span class="p">;</span>
<span class="w">  </span><span class="n">global_range_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cl</span><span class="o">::</span><span class="n">NDRange</span><span class="p">(</span><span class="n">out_shape</span><span class="p">.</span><span class="n">width</span><span class="p">,</span><span class="w"> </span><span class="n">out_shape</span><span class="p">.</span><span class="n">height</span><span class="p">);</span>
<span class="w">  </span><span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The implementation of <code class="docutils literal notranslate"><span class="pre">GpuTensorInfo</span></code> is as follows: Use the <code class="docutils literal notranslate"><span class="pre">Broadcast2GpuShape</span></code> function to convert the shape of a tensor to four dimensions, and then compute the shape value when the format is NHWC4.
Then, obtain the maximum width and height supported by the image memory by calling <code class="docutils literal notranslate"><span class="pre">OpenCLRuntimeWrapper::GetMaxImage2DWidth</span></code> and <code class="docutils literal notranslate"><span class="pre">OpenCLRuntimeWrapper::GetMaxImage2DHeight</span></code>, and determine the image memory width and height actually used by the operator.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">GpuTensorInfo</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">GpuTensorInfo</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="k">explicit</span><span class="w"> </span><span class="n">GpuTensorInfo</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">MSTensor</span><span class="w"> </span><span class="o">*</span><span class="n">tensor</span><span class="p">,</span><span class="w"> </span><span class="n">registry</span><span class="o">::</span><span class="n">opencl</span><span class="o">::</span><span class="n">OpenCLRuntimeWrapper</span><span class="w"> </span><span class="o">*</span><span class="n">opencl_run</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">shape_ori</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">Shape</span><span class="p">();</span>
<span class="w">    </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">shape</span><span class="p">[</span><span class="mi">4</span><span class="p">];</span>
<span class="w">    </span><span class="n">Broadcast2GpuShape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span><span class="w"> </span><span class="n">shape_ori</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">shape_ori</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="mf">1l</span><span class="p">);</span>
<span class="w">    </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="n">H</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="n">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
<span class="w">    </span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>
<span class="w">    </span><span class="n">Slice</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">UP_DIV</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">C4NUM</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensor</span><span class="o">-&gt;</span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">FLT_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">cl_half</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">FLT_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">cl_float</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">FLT4_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">FLT_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">W</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Slice</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">opencl_run</span><span class="o">-&gt;</span><span class="n">GetMaxImage2DWidth</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">H</span><span class="p">;</span>
<span class="w">      </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">W</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Slice</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">H</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">W</span><span class="p">;</span>
<span class="w">      </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Slice</span><span class="p">;</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">height</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">opencl_run</span><span class="o">-&gt;</span><span class="n">GetMaxImage2DHeight</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">        </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">ElementsNum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">H</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">W</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">C</span><span class="p">;</span>
<span class="w">    </span><span class="n">Image2DSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">height</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">FLT4_size</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">N</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">H</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">W</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">C</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">Slice</span><span class="p">{};</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">width</span><span class="p">{};</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">height</span><span class="p">{};</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">FLT_size</span><span class="p">{</span><span class="mi">4</span><span class="p">};</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">FLT4_size</span><span class="p">{</span><span class="mi">16</span><span class="p">};</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">ElementsNum</span><span class="p">{};</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">Image2DSize</span><span class="p">{};</span>
<span class="p">};</span>
<span class="p">}</span><span class="w">  </span><span class="c1">// namespace</span>
</pre></div>
</div>
<p>The implementation of <code class="docutils literal notranslate"><span class="pre">Broadcast2GpuShape</span></code> is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">SrcT</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">DstT</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">Broadcast2GpuShape</span><span class="p">(</span><span class="n">DstT</span><span class="w"> </span><span class="o">*</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">SrcT</span><span class="w"> </span><span class="o">*</span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">src_num</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">src</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">src_num</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dst</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">H</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dst</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dst</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dst</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">src_num</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// 1 1 1 C</span>
<span class="w">    </span><span class="o">*</span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">src_num</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// N 1 1 C</span>
<span class="w">    </span><span class="o">*</span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="o">*</span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">src_num</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// N 1 W C</span>
<span class="w">    </span><span class="o">*</span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="o">*</span><span class="n">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="o">*</span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">src_num</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">  </span><span class="c1">// N H W C</span>
<span class="w">    </span><span class="o">*</span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="o">*</span><span class="n">H</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">    </span><span class="o">*</span><span class="n">W</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
<span class="w">    </span><span class="o">*</span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">src_num</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;GPU doesn&#39;t support ndim&gt;=&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">src_num</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">SrcT</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">DstT</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">Broadcast2GpuShape</span><span class="p">(</span><span class="n">DstT</span><span class="w"> </span><span class="o">*</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">SrcT</span><span class="w"> </span><span class="o">*</span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">src_num</span><span class="p">,</span><span class="w"> </span><span class="n">DstT</span><span class="w"> </span><span class="n">default_value</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">dst</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">default_value</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">src</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">src_num</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Broadcast2GpuShape</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">src_num</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Convert the constant input into data in a proper format and allocate the GPU memory.</p></li>
</ol>
<p>For an operator registered as a GPU, the input data is GPU memory data in image format except when the input is a constant.
To meet the operator computation requirements, you need to set a proper format for the constant input and allocate GPU memory if necessary. In this example, the operations on a constant tensor are as follows:</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">MSTensor::IsConst()</span></code> API to check whether the input is a constant, and use <code class="docutils literal notranslate"><span class="pre">GpuTensorInfo</span></code> to compute the memory size required for converting the image format.
Allocate the local memory <code class="docutils literal notranslate"><span class="pre">weight</span></code> of this size, and use the <code class="docutils literal notranslate"><span class="pre">PackNHWCToNHWC4</span></code> function to transfer the tensor memory to <code class="docutils literal notranslate"><span class="pre">weight</span></code> for storage.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">in_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs_</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_tensor</span><span class="p">.</span><span class="n">IsConst</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">GpuTensorInfo</span><span class="w"> </span><span class="n">in_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GpuTensorInfo</span><span class="p">(</span><span class="o">&amp;</span><span class="n">in_tensor</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">opencl_runtime_</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">char</span><span class="o">&gt;</span><span class="w"> </span><span class="n">weight</span><span class="p">(</span><span class="n">in_shape</span><span class="p">.</span><span class="n">Image2DSize</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">src_is_fp16</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in_tensor</span><span class="p">.</span><span class="n">DataType</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat16</span><span class="p">;</span>
<span class="w">    </span><span class="n">PackNHWCToNHWC4</span><span class="p">(</span><span class="n">in_tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">(),</span><span class="w"> </span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">src_is_fp16</span><span class="p">,</span><span class="w"> </span><span class="n">fp16_enable_</span><span class="p">,</span><span class="w"> </span><span class="n">in_shape</span><span class="p">,</span>
<span class="w">                    </span><span class="n">in_tensor</span><span class="p">.</span><span class="n">DataType</span><span class="p">());</span>
<span class="w">    </span><span class="p">...</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">PackNHWCToNHWC4</span></code> function is implemented as follows, including the conversion between the FLOAT16 and FLOAT32 types.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">PackNHWCToNHWC4</span><span class="p">(</span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">src_is_fp16</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">dst_is_fp16</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">GpuTensorInfo</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="p">,</span>
<span class="w">                     </span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="w"> </span><span class="n">data_type</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">src_fp16</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">float16_t</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">src</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">src_fp32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">float32_t</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">src</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">src_int32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">src</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">dst_fp16</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">float16_t</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">dst</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">dst_fp32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">float32_t</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">dst</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">dst_int32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">dst</span><span class="p">);</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">src_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">N</span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">h</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">H</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">h</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">W</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">w</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">C</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="o">++</span><span class="n">src_idx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">          </span><span class="kt">int</span><span class="w"> </span><span class="n">dst_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">((</span><span class="n">n</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">H</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">h</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">W</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">w</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">Slice</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">C4NUM</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">c</span><span class="p">;</span>
<span class="w">          </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeInt32</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">dst_int32</span><span class="p">[</span><span class="n">dst_idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src_int32</span><span class="p">[</span><span class="n">src_idx</span><span class="p">];</span>
<span class="w">          </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">dst_is_fp16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">dst_fp16</span><span class="p">[</span><span class="n">dst_idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src_is_fp16</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">src_fp16</span><span class="p">[</span><span class="n">src_idx</span><span class="p">]</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">float16_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">src_fp32</span><span class="p">[</span><span class="n">src_idx</span><span class="p">]);</span>
<span class="w">          </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">dst_fp32</span><span class="p">[</span><span class="n">dst_idx</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">src_is_fp16</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">float32_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">src_fp16</span><span class="p">[</span><span class="n">src_idx</span><span class="p">])</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">src_fp32</span><span class="p">[</span><span class="n">src_idx</span><span class="p">];</span>
<span class="w">          </span><span class="p">}</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementsNum</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">dst_is_fp16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">dst_fp16</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dst_fp16</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dst_fp16</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dst_fp16</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">dst_fp32</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dst_fp32</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dst_fp32</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dst_fp32</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">OpenCLRuntimeWrapper::GetAllocator</span></code> is used to obtain the memory allocator that allocates the memory.
Then, the <code class="docutils literal notranslate"><span class="pre">mindspore::Allocator::Malloc</span></code> API of the allocator can be used to apply for the GPU memory in image format.
Write the <code class="docutils literal notranslate"><span class="pre">weight</span></code> data in the NHWC4 format to the GPU memory through the <code class="docutils literal notranslate"><span class="pre">OpenCLRuntimeWrapper::WriteImage(void</span> <span class="pre">*buffer,</span> <span class="pre">void</span> <span class="pre">*src_data)</span></code> API.
The pointer to the requested GPU memory is stored in weight_ptrs_ so that it can be released during destruction.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">DataType</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span>
<span class="w">  </span><span class="n">fp16_enable_</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat16</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">allocator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">opencl_runtime_</span><span class="p">.</span><span class="n">GetAllocator</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">allocator</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;GetAllocator fail.&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">FreeWeight</span><span class="p">();</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="n">weight_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">allocator</span><span class="o">-&gt;</span><span class="n">Malloc</span><span class="p">(</span><span class="n">in_shape</span><span class="p">.</span><span class="n">width</span><span class="p">,</span><span class="w"> </span><span class="n">in_shape</span><span class="p">.</span><span class="n">height</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">weight_ptr</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Malloc fail.&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">FreeWeight</span><span class="p">();</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">weight_ptrs_</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">weight_ptr</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">opencl_runtime_</span><span class="p">.</span><span class="n">WriteImage</span><span class="p">(</span><span class="n">weight_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">())</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;WriteImage fail.&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">FreeWeight</span><span class="p">();</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>During destruction, the function called for releasing the GPU memory is as follows. The memory allocator that allocates the GPU memory is obtained by using <code class="docutils literal notranslate"><span class="pre">OpenCLRuntimeWrapper::GetAllocator</span></code>.
Then, the <code class="docutils literal notranslate"><span class="pre">mindspore::Allocator::Free</span></code> API of the allocator can be used to release the applied GPU memory.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">FreeWeight</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">allocator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">opencl_runtime_</span><span class="p">.</span><span class="n">GetAllocator</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">allocator</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;GetAllocator fail.&quot;</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">weight_ptr</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">weight_ptrs_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">weight_ptr</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">allocator</span><span class="o">-&gt;</span><span class="n">Free</span><span class="p">(</span><span class="n">weight_ptr</span><span class="p">);</span>
<span class="w">        </span><span class="n">weight_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>Set values of OpenCL kernel runtime parameters.</p></li>
</ol>
<p>Some unchanged parameters during the OpenCL kernel running can be set in the <code class="docutils literal notranslate"><span class="pre">Prepare</span></code> phase.
In this example, <code class="docutils literal notranslate"><span class="pre">OpenCLRuntimeWrapper::SetKernelArg</span></code> is used to set the third parameter (computation range) of the <code class="docutils literal notranslate"><span class="pre">ElementAdd</span></code> runtime.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="n">arg_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span>
<span class="n">cl_int2</span><span class="w"> </span><span class="n">output_shape</span><span class="p">{</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">global_range_</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">global_range_</span><span class="p">[</span><span class="mi">1</span><span class="p">])};</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">opencl_runtime_</span><span class="p">.</span><span class="n">SetKernelArg</span><span class="p">(</span><span class="n">kernel_</span><span class="p">,</span><span class="w"> </span><span class="n">arg_idx</span><span class="p">,</span><span class="w"> </span><span class="n">output_shape</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Set kernel arg&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">arg_idx</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;failed.&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">FreeWeight</span><span class="p">();</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="code-and-description-of-the-resize-and-execute-implementations">
<h4>Code and Description of the ReSize and Execute Implementations<a class="headerlink" href="#code-and-description-of-the-resize-and-execute-implementations" title="Permalink to this headline"></a></h4>
<p>By overloading <code class="docutils literal notranslate"><span class="pre">Execute</span></code>, you can customize the computation operations of the operator during inference.</p>
<ol class="arabic simple">
<li><p>Call the <code class="docutils literal notranslate"><span class="pre">ReSize</span></code> function to support shape changes during running.</p></li>
</ol>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">PreProcess</span></code> is called to prepare for the computation.
In <code class="docutils literal notranslate"><span class="pre">PreProcess()</span></code>, call the <code class="docutils literal notranslate"><span class="pre">ReSize</span></code> function first. This function is the runtime shape change adaptation API that needs to be overloaded.
In the <code class="docutils literal notranslate"><span class="pre">ReSize</span></code> function, call <code class="docutils literal notranslate"><span class="pre">CheckOutputs</span></code> to check whether the shape of the output tensor of the operator contains invalid values to determine whether shape inference needs to be performed again. If no, the function returns directly.
When shape inference is required, call <code class="docutils literal notranslate"><span class="pre">registry::RegisterKernelInterface::GetKernelInterface</span></code> to obtain the shape inference function registered by the operator. The obtained function is the InferShape function implemented and registered by the user in this routine.
After re-inference, call the previously implemented <code class="docutils literal notranslate"><span class="pre">Prepare</span></code> API to re-apply for and allocate the memory and related variables required for operator computation.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">ReSize</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">CheckOutputs</span><span class="p">(</span><span class="n">outputs_</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span><span class="n">registry</span><span class="o">::</span><span class="n">RegisterKernelInterface</span><span class="o">::</span><span class="n">GetKernelInterface</span><span class="p">(</span><span class="s">&quot;&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">primitive_</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">Infer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">inputs_</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs_</span><span class="p">,</span><span class="w"> </span><span class="n">primitive_</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;infer failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Prepare</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;ReSize failed for kernel prepare!&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">PreProcess</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="kt">int</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">   </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReSize</span><span class="p">();</span>
<span class="w">   </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">     </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">   </span><span class="p">}</span>
<span class="w">   </span><span class="p">...</span>
<span class="w"> </span><span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">Execute</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs_</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_PARAM_INVALID</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">PreProcess</span><span class="p">();</span>
<span class="w">  </span><span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Allocate memory for the output tensor.</p></li>
</ol>
<p>Before running the operator, you need to apply for GPU memory for the output tensor. Due to the limitation of the framework, the GPU memory needs to be hosted by the framework for management and cannot be manually released. The process is as follows:</p>
<ol class="arabic simple">
<li><p>Call the <code class="docutils literal notranslate"><span class="pre">allocator()</span></code> API of the output tensor to obtain the memory allocator that manages the tensor in the framework. In the GPU registration operator, the memory allocator is responsible for allocating the GPU memory.</p></li>
<li><p>Compute the size of the memory to be allocated. In this example, the <code class="docutils literal notranslate"><span class="pre">GpuTensorInfo</span></code> function is used to compute the size of the memory to be allocated.</p></li>
<li><p>Apply for memory by using the <code class="docutils literal notranslate"><span class="pre">Malloc</span></code> API of the memory allocator. You can obtain the memory in image or buffer format by using the <code class="docutils literal notranslate"><span class="pre">void</span> <span class="pre">*Malloc(size_t</span> <span class="pre">weight,</span> <span class="pre">size_t</span> <span class="pre">height,</span> <span class="pre">DataType</span> <span class="pre">type)</span></code> and <code class="docutils literal notranslate"><span class="pre">void</span> <span class="pre">*Malloc(size_t</span> <span class="pre">size)</span></code> APIs.</p></li>
<li><p>Use the <code class="docutils literal notranslate"><span class="pre">SetData</span></code> API to assign the requested memory to the tensor. After that, the memory is managed by the framework in a unified manner and cannot be manually released.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">PreProcess</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="p">...</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">outputs_</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs_</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">img_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GpuTensorInfo</span><span class="p">(</span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">opencl_runtime_</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">allocator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output</span><span class="o">-&gt;</span><span class="n">allocator</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">allocator</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;The output tensor of OpenCL kernel must have an allocator.&quot;</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">allocator</span><span class="o">-&gt;</span><span class="n">Malloc</span><span class="p">(</span><span class="n">img_info</span><span class="p">.</span><span class="n">width</span><span class="p">,</span><span class="w"> </span><span class="n">img_info</span><span class="p">.</span><span class="n">height</span><span class="p">,</span><span class="w"> </span><span class="n">output</span><span class="o">-&gt;</span><span class="n">DataType</span><span class="p">());</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data_ptr</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Malloc data failed&quot;</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">output</span><span class="o">-&gt;</span><span class="n">SetData</span><span class="p">(</span><span class="n">data_ptr</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Run the OpenCL kernel.</p></li>
</ol>
<p>The <code class="docutils literal notranslate"><span class="pre">SetKernelArg</span></code> API is used to set parameters for running the OpenCL kernel, and the <code class="docutils literal notranslate"><span class="pre">RunKernel</span></code> API is used to run the OpenCL kernel.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">Execute</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="p">...</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="k">this</span><span class="o">-&gt;</span><span class="n">name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; Running!&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">input_0_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weight_ptrs_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">inputs_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">MutableData</span><span class="p">()</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">weight_ptrs_</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">input_1_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weight_ptrs_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">inputs_</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">MutableData</span><span class="p">()</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">weight_ptrs_</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">arg_idx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">opencl_runtime_</span><span class="o">-&gt;</span><span class="n">SetKernelArg</span><span class="p">(</span><span class="n">kernel_</span><span class="p">,</span><span class="w"> </span><span class="n">arg_idx</span><span class="o">++</span><span class="p">,</span><span class="w"> </span><span class="n">input_0_ptr</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Set kernel arg&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">arg_idx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;failed.&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">opencl_runtime_</span><span class="o">-&gt;</span><span class="n">SetKernelArg</span><span class="p">(</span><span class="n">kernel_</span><span class="p">,</span><span class="w"> </span><span class="n">arg_idx</span><span class="o">++</span><span class="p">,</span><span class="w"> </span><span class="n">input_1_ptr</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Set kernel arg&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">arg_idx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;failed.&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">opencl_runtime_</span><span class="o">-&gt;</span><span class="n">SetKernelArg</span><span class="p">(</span><span class="n">kernel_</span><span class="p">,</span><span class="w"> </span><span class="n">arg_idx</span><span class="o">++</span><span class="p">,</span><span class="w"> </span><span class="n">outputs_</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">MutableData</span><span class="p">())</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Set kernel arg&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">arg_idx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;failed.&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">opencl_runtime_</span><span class="o">-&gt;</span><span class="n">RunKernel</span><span class="p">(</span><span class="n">kernel_</span><span class="p">,</span><span class="w"> </span><span class="n">global_range_</span><span class="p">,</span><span class="w"> </span><span class="n">local_range_</span><span class="p">,</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">event_</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Run kernel failed.&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_ERROR</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lite</span><span class="o">::</span><span class="n">RET_OK</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="converter_register.html" class="btn btn-neutral float-left" title="Construct custom kernel by registering conversion tool" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="delegate.html" class="btn btn-neutral float-right" title="Using Delegate to Support Third-party AI Framework" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>