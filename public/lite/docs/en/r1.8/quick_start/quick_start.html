

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Android Application Development Based on JNI Interface &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Android Application Development Based on Java Interface" href="image_segmentation.html" />
    <link rel="prev" title="Experience Java Minimalist Concurrent Reasoning Demo" href="quick_start_server_inference_java.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="one_hour_introduction.html">Getting Started in One Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_server_inference_cpp.html">Experience C++ Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_server_inference_java.html">Experience Java Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Android Application Development Based on JNI Interface</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#selecting-a-model">Selecting a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#converting-a-model">Converting a Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deploying-an-application">Deploying an Application</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#running-dependencies">Running Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-and-running">Building and Running</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#detailed-description-of-the-sample-program">Detailed Description of the Sample Program</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sample-program-structure">Sample Program Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuring-mindspore-lite-dependencies">Configuring MindSpore Lite Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloading-and-deploying-a-model-file">Downloading and Deploying a Model File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#writing-on-device-inference-code">Writing On-Device Inference Code</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption"><span class="caption-text">Inference on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter_tool.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/micro.html">Perform Inference on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter_train.html">Creating MindSpore Lite Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Server Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_server_inference.html">Executing Server Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Android Application Development Based on JNI Interface</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/quick_start/quick_start.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="android-application-development-based-on-jni-interface">
<h1>Android Application Development Based on JNI Interface<a class="headerlink" href="#android-application-development-based-on-jni-interface" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r1.8/docs/lite/docs/source_en/quick_start/quick_start.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.8/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>It is recommended that you start from the image classification demo on the Android device to understand how to build the MindSpore Lite application project, configure dependencies, and use related APIs.</p>
<p>This tutorial demonstrates the on-device deployment process based on the image classification sample program on the Android device provided by the MindSpore team.</p>
<ol class="simple">
<li><p>Select an image classification model.</p></li>
<li><p>Convert the model into a MindSpore Lite model.</p></li>
<li><p>Use the MindSpore Lite inference model on the device side. The following describes how to use the MindSpore Lite C++ APIs (Android JNIs) and MindSpore Lite image classification models to perform on-device inference, implement the classification of individual images, and display the most possible classification result on the application’s image preview screen.</p></li>
</ol>
<blockquote>
<div><p>Click to find <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/mobilenetv2_openimage_lite/1.5">Android image classification models</a> and <a class="reference external" href="https://gitee.com/mindspore/vision/tree/r0.1/android">image classification sample code</a>.</p>
<p>In this example, we explain how to use C++ API. Besides, MindSpore Lite also supports Java API. Please refer to <a class="reference external" href="https://gitee.com/mindspore/models/tree/r1.8/official/lite/image_segmentation">image segmentation demo</a> to learn more about Java API.</p>
<p>The application development environment is Windows.</p>
</div></blockquote>
<p>We provide the APK file corresponding to this example. You can scan the QR code below or download the <a class="reference external" href="https://download.mindspore.cn/vision/android/mindvision-0.1.0.apk">APK file</a> directly, and deploy it to Android devices for use.</p>
<p><img alt="apk" src="../_images/vision_apk.png" /></p>
</div>
<div class="section" id="selecting-a-model">
<h2>Selecting a Model<a class="headerlink" href="#selecting-a-model" title="Permalink to this headline">¶</a></h2>
<p>The MindSpore team provides a series of preset device models that you can use in your application.<br />
Click to download <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/mobilenetv2_openimage_lite/1.5/mobilenetv2.ms">image classification models</a> in MindSpore ModelZoo.
In addition, you can use the preset model to perform transfer learning to implement your image classification tasks.</p>
</div>
<div class="section" id="converting-a-model">
<h2>Converting a Model<a class="headerlink" href="#converting-a-model" title="Permalink to this headline">¶</a></h2>
<p>After you retrain a model provided by MindSpore, export the model in the <a class="reference external" href="https://www.mindspore.cn/tutorials/en/r1.8/advanced/train/save.html#export-mindir-model">.mindir format</a>. Use the MindSpore Lite <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.8/use/converter_tool.html">model conversion tool</a> to convert the .mindir format to a .ms model.</p>
<p>Take the mobilenetv2 model as an example. Execute the following script to convert a model into a MindSpore Lite model for on-device inference.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call converter_lite --fmk<span class="o">=</span>MINDIR --modelFile<span class="o">=</span>mobilenetv2.mindir --outputFile<span class="o">=</span>mobilenetv2
</pre></div>
</div>
</div>
<div class="section" id="deploying-an-application">
<h2>Deploying an Application<a class="headerlink" href="#deploying-an-application" title="Permalink to this headline">¶</a></h2>
<p>The following section describes how to build and execute an on-device image classification task on MindSpore Lite.</p>
<div class="section" id="running-dependencies">
<h3>Running Dependencies<a class="headerlink" href="#running-dependencies" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Android Studio 3.2 or later and Android 4.0 or later is recommended.</p></li>
<li><p>Native development kit (NDK) 21.3</p></li>
<li><p><a class="reference external" href="https://cmake.org/download">CMake</a> &gt;= 3.18.3</p></li>
<li><p>Android software development kit (SDK) 26 or later</p></li>
<li><p>JDK 1.8 or later</p></li>
</ul>
</div>
<div class="section" id="building-and-running">
<h3>Building and Running<a class="headerlink" href="#building-and-running" title="Permalink to this headline">¶</a></h3>
<ol>
<li><p>Load the <a class="reference external" href="https://gitee.com/mindspore/vision/tree/r0.1/android">sample source code</a> to Android Studio and install the corresponding SDK. (After the SDK version is specified, Android Studio automatically installs the SDK.)</p>
<p><img alt="start_home" src="../_images/lite_quick_start_home.png" /></p>
<p>Start Android Studio, click <code class="docutils literal notranslate"><span class="pre">File</span> <span class="pre">&gt;</span> <span class="pre">Settings</span> <span class="pre">&gt;</span> <span class="pre">System</span> <span class="pre">Settings</span> <span class="pre">&gt;</span> <span class="pre">Android</span> <span class="pre">SDK</span></code>, and select the corresponding SDK. As shown in the following figure, select an SDK and click <code class="docutils literal notranslate"><span class="pre">OK</span></code>. Android Studio automatically installs the SDK.</p>
<p><img alt="start_sdk" src="../_images/lite_quick_start_sdk.png" /></p>
<p>(Optional) If an NDK version issue occurs during the installation, manually download the corresponding <a class="reference external" href="https://developer.android.com/ndk/downloads">NDK version</a> (the version used in the sample code is 21.3). Specify the NDK location in <code class="docutils literal notranslate"><span class="pre">Android</span> <span class="pre">NDK</span> <span class="pre">location</span></code> of <code class="docutils literal notranslate"><span class="pre">Project</span> <span class="pre">Structure</span></code>.</p>
<p><img alt="project_structure" src="../_images/lite_quick_start_project_structure.png" /></p>
</li>
<li><p>Connect to an Android device and runs the image classification application.</p>
<p>Connect to the Android device through a USB cable for debugging. Click <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">'app'</span></code> to run the sample project on your device.</p>
<p><img alt="run_app" src="../_images/lite_quick_start_run_app.PNG" /></p>
<p>For details about how to connect the Android Studio to a device for debugging, see <a class="reference external" href="https://developer.android.com/studio/run/device">https://developer.android.com/studio/run/device</a>.</p>
<p>The mobile phone needs to turn on “USB debugging mode” for Android Studio to recognize the phone. In general, Huawei mobile phones turn on “USB debugging mode” in Settings -&gt; System and Update -&gt; Developer Options -&gt; USB Debugging.</p>
</li>
<li><p>After opening the APP, you can click the classification module on the home page, and then click the middle button to take a picture and obtain an image, or click the image button on the upper sidebar to select the picture album for the image classification function.</p>
<p><img alt="install" src="../_images/app1.png" /></p>
<p>By default, the MindSpore Vision classification module has a built-in general AI network model to identify and classify images. You can also <a class="reference external" href="https://mindspore.cn/tutorials/en/r1.8/beginner/infer.html">custom model</a> for debugging on the APP.</p>
<p><img alt="result" src="../_images/app2.png" /></p>
</li>
</ol>
</div>
</div>
<div class="section" id="detailed-description-of-the-sample-program">
<h2>Detailed Description of the Sample Program<a class="headerlink" href="#detailed-description-of-the-sample-program" title="Permalink to this headline">¶</a></h2>
<p>The Android sample program for image classification on the device is divided into the JAVA layer and the JNI layer. The JAVA layer mainly completes the rendering function of the Android page and the subsequent inference operation of obtaining an image by taking a photo or opening the mobile phone album, while the JNI layer is in <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.8/use/runtime.html">Runtime</a> to complete the process of model inference.</p>
<blockquote>
<div><p>The JNI layer implementation of the sample program, the implementation of the JAVA layer page rendering function, and the image frame processing and other functions are described in detail here. The reader needs to have a certain basic knowledge of Android development.</p>
</div></blockquote>
<div class="section" id="sample-program-structure">
<h3>Sample Program Structure<a class="headerlink" href="#sample-program-structure" title="Permalink to this headline">¶</a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>enginelibrary
├── src/main
│   ├── assets # resource files
|   |   └── mobilenetv2.ms # stored model file
│   |
│   ├── cpp # main logic encapsulation classes for model loading and prediction
|   |   └── classification
|   |       ├── CommonMindSporeNetnative.cpp # common MindSpore calls the JNI method
│   |       ├── CommonMindSporeNetnative.h # header file
|   |       ├── CustomMindSporeNetnative.cpp # customized MindSpore calls the JNI method
│   |       ├── CustomMindSporeNetnative.h # header file
|   |
|   |   └── mindspore-lite-{version}-android-{arch} # MindSpore Lite version
|   |
|   |   └── CMakeList.txt # CMake compilation entry file
|   |
|   |   └── MSNetWork.cpp # MindSpore interface encapsulation
│   |
│   ├── java # Java-layer application code
│   │   └── com.mindspore.enginelibrary
│   │       └── train # Implementation of image processing and MindSpore JNI Calling
│   │
│   ├── res # resource files related to Android
│   └── AndroidManifest.xml # Android configuration file
│
│
├── build.gradle # Other Android configuration file
├── download.gradle # MindSpore version download
└── ...
</pre></div>
</div>
</div>
<div class="section" id="configuring-mindspore-lite-dependencies">
<h3>Configuring MindSpore Lite Dependencies<a class="headerlink" href="#configuring-mindspore-lite-dependencies" title="Permalink to this headline">¶</a></h3>
<p>When MindSpore C++ APIs are called at the Android JNI layer, related library files are required. You can use MindSpore Lite <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.8/use/build.html">source code compilation</a> to generate the MindSpore Lite version. In this case, you need to use the compile command of generate with image preprocessing module.</p>
<p>In this example, the build process automatically downloads the <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-android-{arch}.tar.gz</span></code> by the <code class="docutils literal notranslate"><span class="pre">app/download.gradle</span></code> file and saves in the <code class="docutils literal notranslate"><span class="pre">app/src/main/cpp</span></code> directory.</p>
<blockquote>
<div><p>version: Version number of the .tar package, which is the same as the version of the compiled branch code.</p>
<p>arch: Operating system arm64 or arm32.</p>
</div></blockquote>
<p>Note: if the automatic download fails, please manually download the relevant library files <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.8/use/downloads.html">mindspore-lite-{version}-android-{arch}.tar.gz</a>. After decompression, copy the <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-android-{arch}</span></code> folder to the directory of <code class="docutils literal notranslate"><span class="pre">src/main/cpp</span></code>.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>android{
    defaultConfig{
        externalNativeBuild{
            cmake{
                arguments &quot;-DANDROID_STL=c++_shared&quot;
            }
        }

        ndk{
            abiFilters&#39;armeabi-v7a&#39;, &#39;arm64-v8a&#39;
        }
    }
}
</pre></div>
</div>
<p>Create a link to the <code class="docutils literal notranslate"><span class="pre">.so</span></code> library file in the <code class="docutils literal notranslate"><span class="pre">app/CMakeLists.txt</span></code> file:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span># ============== Set MindSpore Dependencies. =============
include_directories(${CMAKE_SOURCE_DIR})
include_directories(${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION})
include_directories(${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime/third_party)
include_directories(${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime/include)
include_directories(${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime/include/dataset)
include_directories(${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime/include/dataset/lite_cv)
include_directories(${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime)
include_directories(${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime/include/ir/dtype)
include_directories(${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime/include/schema)

add_library(mindspore-lite SHARED IMPORTED)
add_library(minddata-lite SHARED IMPORTED)
add_library(libjpeg SHARED IMPORTED)
add_library(libturbojpeg SHARED IMPORTED)

set_target_properties(mindspore-lite PROPERTIES IMPORTED_LOCATION
        ${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime/lib/libmindspore-lite.so)
set_target_properties(minddata-lite PROPERTIES IMPORTED_LOCATION
        ${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime/lib/libminddata-lite.so)
set_target_properties(libjpeg PROPERTIES IMPORTED_LOCATION
        ${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime/third_party/libjpeg-turbo/lib/libjpeg.so)
set_target_properties(libturbojpeg PROPERTIES IMPORTED_LOCATION
        ${CMAKE_SOURCE_DIR}/${MINDSPORELITE_VERSION}/runtime/third_party/libjpeg-turbo/lib/libturbojpeg.so)

# --------------- MindSpore Lite set End. --------------------

# Link target library.
target_link_libraries( # Specifies the target library.
        mlkit-label-MS

        mindspore-lite
        minddata-lite
        libjpeg
        libturbojpeg

        # --- other dependencies.---
        -ljnigraphics
        android

        # Links the target library to the log library
        ${log-lib}
        )
</pre></div>
</div>
</div>
<div class="section" id="downloading-and-deploying-a-model-file">
<h3>Downloading and Deploying a Model File<a class="headerlink" href="#downloading-and-deploying-a-model-file" title="Permalink to this headline">¶</a></h3>
<p>In this example, the build process automatically downloads the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> by referring to the <code class="docutils literal notranslate"><span class="pre">app/download.gradle</span></code> file and saves in the <code class="docutils literal notranslate"><span class="pre">app/src/main/assets</span></code> directory.</p>
<p>Note: if the automatic download fails, please manually download the relevant library files <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/mobilenetv2_openimage_lite/1.5/mobilenetv2.ms">mobilenetv2.ms</a> and put them in the corresponding location.</p>
</div>
<div class="section" id="writing-on-device-inference-code">
<h3>Writing On-Device Inference Code<a class="headerlink" href="#writing-on-device-inference-code" title="Permalink to this headline">¶</a></h3>
<p>Call MindSpore Lite C++ APIs at the JNI layer to implement on-device inference.</p>
<p>The inference process code is as follows. For details about the complete code, see <a class="reference external" href="https://gitee.com/mindspore/vision/blob/master/android/enginelibrary/src/main/cpp/classification/CommonMindSporeNetnative.cpp">CommonMindSporeNetnative.cpp</a>.</p>
<ol>
<li><p>Load the MindSpore Lite model file and build the context, model, and computational graph for inference.</p>
<ul>
<li><p>Load model file:</p>
<p>Read the model file in the Java layer of Android and convert it into a ByteBuffer type file <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">_</span> <span class="pre">Buffer</span></code>, which is transferred to C++ layer by calling JNI. Finally, the <code class="docutils literal notranslate"><span class="pre">model_</span> <span class="pre">Buffer</span></code> is converted to char type file <code class="docutils literal notranslate"><span class="pre">modelbuffer</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Buffer is the model data passed in by the Java layer</span>
<span class="n">jlong</span><span class="w"> </span><span class="n">bufferLen</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">env</span><span class="o">-&gt;</span><span class="n">GetDirectBufferCapacity</span><span class="p">(</span><span class="n">model_buffer</span><span class="p">);</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="mi">0</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">bufferLen</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;error, bufferLen is 0!&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">jlong</span><span class="p">)</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">modelBuffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateLocalModelBuffer</span><span class="p">(</span><span class="n">env</span><span class="p">,</span><span class="w"> </span><span class="n">model_buffer</span><span class="p">);</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">modelBuffer</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;modelBuffer create failed!&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">jlong</span><span class="p">)</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Build context, model, and computational graph for inference:</p>
<p>Build context and set model parameters. Create a model from context and model data.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// To create a Mindspore network inference environment.</span>
<span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">labelEnv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="p">;</span><span class="w"></span>
<span class="n">MSNetWork</span><span class="w"> </span><span class="o">*</span><span class="n">labelNet</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MSNetWork</span><span class="p">;</span><span class="w"></span>
<span class="o">*</span><span class="n">labelEnv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">labelNet</span><span class="p">;</span><span class="w"></span>

<span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;context create failed!&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">labelNet</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">labelEnv</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">jlong</span><span class="p">)</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadNum</span><span class="p">(</span><span class="n">num_thread</span><span class="p">);</span><span class="w"></span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadAffinity</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpuDeviceInfo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="n">cpuDeviceInfo</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span><span class="w"></span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpuDeviceInfo</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Based on the model file <code class="docutils literal notranslate"><span class="pre">modelBuffer</span></code>, the computational graph for inference is constructed.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">bool</span><span class="w"> </span><span class="nf">MSNetWork::BuildModel</span><span class="p">(</span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">modelBuffer</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">bufferLen</span><span class="p">,</span><span class="w"></span>
<span class="w">                           </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="w"> </span><span class="n">ctx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="n">model_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span><span class="p">();</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;MindSpore build model failed!.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model_</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">modelBuffer</span><span class="p">,</span><span class="w"> </span><span class="n">bufferLen</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">);</span><span class="w"></span>
<span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">.</span><span class="n">IsOk</span><span class="p">();</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Convert the input image into the Tensor format of the MindSpore model.</p>
<ul>
<li><p>Cut the size of the image <code class="docutils literal notranslate"><span class="pre">srcbitmap</span></code> to be detected and convert it to LiteMat format <code class="docutils literal notranslate"><span class="pre">lite_norm_mat_cut</span></code>. The width, height and channel number information are converted into float format data <code class="docutils literal notranslate"><span class="pre">dataHWC</span></code>. Finally, copy the <code class="docutils literal notranslate"><span class="pre">dataHWC</span></code> to the input <code class="docutils literal notranslate"><span class="pre">inTensor</span></code> of MindSpore model.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="o">**</span><span class="n">labelEnv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">**&gt;</span><span class="p">(</span><span class="n">netEnv</span><span class="p">);</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">labelEnv</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;MindSpore error, labelEnv is a nullptr.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="n">MSNetWork</span><span class="w"> </span><span class="o">*</span><span class="n">labelNet</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">MSNetWork</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="o">*</span><span class="n">labelEnv</span><span class="p">);</span><span class="w"></span>

<span class="k">auto</span><span class="w"> </span><span class="n">mModel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">labelNet</span><span class="o">-&gt;</span><span class="n">model</span><span class="p">();</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">mModel</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;MindSpore error, Model is a nullptr.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;MindSpore get model.&quot;</span><span class="p">);</span><span class="w"></span>

<span class="k">auto</span><span class="w"> </span><span class="n">msInputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mModel</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span><span class="w"></span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">msInputs</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;MindSpore error, msInputs.size() equals 0.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="k">auto</span><span class="w"> </span><span class="n">inTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">msInputs</span><span class="p">.</span><span class="n">front</span><span class="p">();</span><span class="w"></span>

<span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">dataHWC</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">lite_norm_mat_cut</span><span class="p">.</span><span class="n">data_ptr_</span><span class="p">);</span><span class="w"></span>
<span class="c1">// Copy dataHWC to the model input tensor.</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">inTensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">(),</span><span class="w"> </span><span class="n">dataHWC</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="n">inputDims</span><span class="p">.</span><span class="n">channel</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">inputDims</span><span class="p">.</span><span class="n">width</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">inputDims</span><span class="p">.</span><span class="n">height</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Adjust the size of the input image, as well as the detailed algorithm of data processing.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">bool</span><span class="w"> </span><span class="nf">PreProcessImageData</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">LiteMat</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lite_mat_bgr</span><span class="p">,</span><span class="w"> </span><span class="n">LiteMat</span><span class="w"> </span><span class="o">*</span><span class="n">lite_norm_mat_ptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">LiteMat</span><span class="w"> </span><span class="n">lite_mat_resize</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">LiteMat</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lite_norm_mat_cut</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">*</span><span class="n">lite_norm_mat_ptr</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ResizeBilinear</span><span class="p">(</span><span class="n">lite_mat_bgr</span><span class="p">,</span><span class="w"> </span><span class="n">lite_mat_resize</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">ret</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;ResizeBilinear error&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">LiteMat</span><span class="w"> </span><span class="n">lite_mat_convert_float</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ConvertTo</span><span class="p">(</span><span class="n">lite_mat_resize</span><span class="p">,</span><span class="w"> </span><span class="n">lite_mat_convert_float</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">255.0</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">ret</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;ConvertTo error&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">LiteMat</span><span class="w"> </span><span class="n">lite_mat_cut</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Crop</span><span class="p">(</span><span class="n">lite_mat_convert_float</span><span class="p">,</span><span class="w"> </span><span class="n">lite_mat_cut</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">,</span><span class="w"> </span><span class="mi">224</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">ret</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;Crop error&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">means</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">0.485</span><span class="p">,</span><span class="w"> </span><span class="mf">0.456</span><span class="p">,</span><span class="w"> </span><span class="mf">0.406</span><span class="p">};</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="w"> </span><span class="n">stds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">0.229</span><span class="p">,</span><span class="w"> </span><span class="mf">0.224</span><span class="p">,</span><span class="w"> </span><span class="mf">0.225</span><span class="p">};</span><span class="w"></span>
<span class="w">    </span><span class="n">SubStractMeanNormalize</span><span class="p">(</span><span class="n">lite_mat_cut</span><span class="p">,</span><span class="w"> </span><span class="n">lite_norm_mat_cut</span><span class="p">,</span><span class="w"> </span><span class="n">means</span><span class="p">,</span><span class="w"> </span><span class="n">stds</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>The input tensor is inferred according to the model, and the output tensor is obtained and post processed.</p>
<ul>
<li><p>The graph and model are loaded and on device inference is performed.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span><span class="w"></span>
<span class="c1">// After the model and image tensor data is loaded, run inference.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mModel</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">msInputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Get the tensor output <code class="docutils literal notranslate"><span class="pre">msOutputs</span></code> of MindSpore model. The text information <code class="docutils literal notranslate"><span class="pre">resultCharData</span></code> displayed in the APP is calculated through <code class="docutils literal notranslate"><span class="pre">msOutputs</span></code> and classification array information.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mModel</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">msOutputs</span><span class="p">;</span><span class="w"></span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="nl">name</span> <span class="p">:</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">temp_dat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mModel</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">name</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">msOutputs</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="p">{</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">temp_dat</span><span class="p">});</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">resultStr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ProcessRunnetResult</span><span class="p">(</span><span class="o">::</span><span class="n">RET_CATEGORY_SUM</span><span class="p">,</span><span class="o">::</span><span class="n">labels_name_map</span><span class="p">,</span><span class="w"> </span><span class="n">msOutputs</span><span class="p">);</span><span class="w"></span>

<span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">resultCharData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">resultStr</span><span class="p">.</span><span class="n">c_str</span><span class="p">();</span><span class="w"></span>
<span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">env</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">NewStringUTF</span><span class="p">(</span><span class="n">resultCharData</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Perform post-processing of the output data. Obtain the output object <code class="docutils literal notranslate"><span class="pre">outputTensor</span></code> through <code class="docutils literal notranslate"><span class="pre">msOutputs</span></code>, and parse it with the thing category array <code class="docutils literal notranslate"><span class="pre">labels_name_map</span></code> to obtain the training score array <code class="docutils literal notranslate"><span class="pre">scores[]</span></code> of each element. Set the credibility threshold value to <code class="docutils literal notranslate"><span class="pre">unifiedThre</span></code>, and count the credibility threshold value according to the training data. Above the threshold, it belongs to this type. On the contrary, it is not. Finally, a corresponding category name and corresponding score data <code class="docutils literal notranslate"><span class="pre">categoryScore</span></code> are returned.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="nf">ProcessRunnetResult</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">RET_CATEGORY_SUM</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="k">const</span><span class="w"> </span><span class="n">labels_name_map</span><span class="p">[],</span><span class="w"></span>
<span class="w">                                </span><span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">msOutputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="c1">// Get the branch of the model output.</span>
<span class="c1">// Use iterators to get map elements.</span>
<span class="n">std</span><span class="o">::</span><span class="n">unordered_map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;::</span><span class="n">iterator</span><span class="w"> </span><span class="n">iter</span><span class="p">;</span><span class="w"></span>
<span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">msOutputs</span><span class="p">.</span><span class="n">begin</span><span class="p">();</span><span class="w"></span>

<span class="c1">// The mobilenetv2.ms model output just one branch.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="o">-&gt;</span><span class="n">second</span><span class="p">;</span><span class="w"></span>

<span class="kt">int</span><span class="w"> </span><span class="n">tensorNum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outputTensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">();</span><span class="w"></span>
<span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;Number of tensor elements:%d&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">tensorNum</span><span class="p">);</span><span class="w"></span>

<span class="c1">// Get a pointer to the first score.</span>
<span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">temp_scores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">outputTensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">());</span><span class="w"></span>
<span class="kt">float</span><span class="w"> </span><span class="n">scores</span><span class="p">[</span><span class="n">RET_CATEGORY_SUM</span><span class="p">];</span><span class="w"></span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">RET_CATEGORY_SUM</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">temp_scores</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">unifiedThre</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5</span><span class="p">;</span><span class="w"></span>
<span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">probMax</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">;</span><span class="w"></span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">RET_CATEGORY_SUM</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">threshold</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g_thres_map</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">tmpProb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tmpProb</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">threshold</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">tmpProb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tmpProb</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threshold</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">unifiedThre</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">tmpProb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">tmpProb</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">threshold</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">probMax</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">threshold</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">unifiedThre</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">unifiedThre</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tmpProb</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">RET_CATEGORY_SUM</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">0.5</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">MS_PRINT</span><span class="p">(</span><span class="s">&quot;MindSpore scores[%d] : [%f]&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="c1">// Score for each category.</span>
<span class="c1">// Converted to text information that needs to be displayed in the APP.</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">categoryScore</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"></span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">RET_CATEGORY_SUM</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">categoryScore</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">labels_name_map</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="n">categoryScore</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="s">&quot;:&quot;</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">score_str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">to_string</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span><span class="w"></span>
<span class="w">    </span><span class="n">categoryScore</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">score_str</span><span class="p">;</span><span class="w"></span>
<span class="w">    </span><span class="n">categoryScore</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="s">&quot;;&quot;</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="k">return</span><span class="w"> </span><span class="n">categoryScore</span><span class="p">;</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="image_segmentation.html" class="btn btn-neutral float-right" title="Android Application Development Based on Java Interface" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="quick_start_server_inference_java.html" class="btn btn-neutral float-left" title="Experience Java Minimalist Concurrent Reasoning Demo" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>