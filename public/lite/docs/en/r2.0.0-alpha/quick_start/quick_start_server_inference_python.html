

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Experiencing the Python Simplified Concurrent Inference Demo &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Expriencing Simpcified Inference Demo with C-language" href="quick_start_c.html" />
    <link rel="prev" title="Experiencing the Python Simplified Inference Demo" href="quick_start_python.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="one_hour_introduction.html">Getting Started in One Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_server_inference_cpp.html">Experience C++ Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_server_inference_java.html">Experience Java Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_python.html">Experiencing the Python Simplified Inference Demo</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Experiencing the Python Simplified Concurrent Inference Demo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#one-click-installation">One-click Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#executing-demo">Executing Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="#demo-content-description">Demo Content Description</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#key-variable-description">Key Variable Description</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-concurrent-inference-configuration">Creating Concurrent Inference Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#concurrent-runner-loading-and-compilation">Concurrent Runner Loading and Compilation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#setting-concurrent-inference-task">Setting Concurrent Inference Task</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performing-concurrent-inference">Performing Concurrent Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="quick_start_c.html">Expriencing Simpcified Inference Demo with C-language</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Server Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_server_inference.html">Executing Server Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Experiencing the Python Simplified Concurrent Inference Demo</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quick_start/quick_start_server_inference_python.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section class="tex2jax_ignore mathjax_ignore" id="experiencing-the-python-simplified-concurrent-inference-demo">
<h1>Experiencing the Python Simplified Concurrent Inference Demo<a class="headerlink" href="#experiencing-the-python-simplified-concurrent-inference-demo" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0.0-alpha/docs/lite/docs/source_en/quick_start/quick_start_server_inference_python.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial provides a sample program for MindSpore Lite to perform concurrent inference. By creating concurrent inference configuration, loading and compiling concurrent Runner, setting concurrent inference task, and executing concurrent inference, it demonstrates the basic process of <a class="reference external" href="https://mindspore.cn/lite/api/en/r2.0.0-alpha/mindspore_lite.html">Python interface</a> for server-side concurrent inference, so that users can quickly understand the use of MindSpore Lite to perform concurrent inference-related APIs. The related code is located in <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.0.0-alpha/mindspore/lite/examples/quick_start_server_inference_python">mindspore/lite/examples/quick_start_server_inference_python</a> directory.</p>
<p>The usage scenario simulated by this tutorial: When the server receives inference tasks requested by multiple clients at the same time, it uses the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/mindspore_lite/mindspore_lite.ModelParallelRunner.html">ModelParallelRunner</a> interface that supports concurrent inference. Multiple inference tasks are performed at the same time and the results are returned to the client.</p>
<p>The following is an example of how to use the Python simplified concurrent inference demo on a Linux X86 operating system and a CPU hardware platform, taking Ubuntu 18.04 as an example.</p>
<ul class="simple">
<li><p>One-click installation of inference-related model files, MindSpore Lite and its required dependencies. See the <span class="xref myst">One-click installation</span> section for details.</p></li>
<li><p>Execute the Python Simplified Inference Demo. See the <span class="xref myst">Execute Demo</span> section for details.</p></li>
<li><p>For a description of the Python Simplified Inference Demo content, see the <span class="xref myst">Demo Content Description</span> section for details.</p></li>
</ul>
</section>
<section id="one-click-installation">
<h2>One-click Installation<a class="headerlink" href="#one-click-installation" title="Permalink to this headline"></a></h2>
<p>This session introduces the installation of MindSpore Lite for Python version 3.7 via pip on a Linux-x86_64 system with a CPU environment, taking the new Ubuntu 18.04 as an example.</p>
<p>Go to <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.0.0-alpha/mindspore/lite/examples/quick_start_server_inference_python">mindspore/lite/examples/quick_start_server_inference_python</a> directory, and execute the <code class="docutils literal notranslate"><span class="pre">lite-server-cpu-pip.sh</span></code> script to install in one click, taking MindSpore Lite version 1.9.0 as an example. The installation script downloads the model and input data files required for concurrent inference, installs the dependencies required for MindSpore_Lite, and downloads and installs MindSpore Lite that supports concurrent inference.</p>
<p>Note: This command sets the installed version of MindSpore Lite. Since the Python interface is supported from MindSpore Lite version 1.8.0, the version cannot be set lower than 1.8.0. For the versions that can be set, see the version provided in <a class="reference external" href="https://www.mindspore.cn/lite/docs/zh-CN/r2.0.0-alpha/use/downloads.html">Download MindSpore Lite</a> for details.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MINDSPORE_LITE_VERSION</span><span class="o">=</span><span class="m">1</span>.9.0<span class="w"> </span>bash<span class="w"> </span>./lite-server-cpu-pip.sh
</pre></div>
</div>
<blockquote>
<div><p>If the MobileNetV2 model download fails, please manually download the relevant model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.ms">mobilenetv2.ms</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_server_inference_python/model</span></code> directory.</p>
<p>If the input.bin input data files download fails, please manually download the relevant model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/input.bin">input.bin</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_server_inference_python/model</span></code> directory.</p>
<p>If you fail to download MindSpore Lite Concurrent Inference Framework by using the script, please manually download <a class="reference external" href="https://ms-release.obs.cn-north-4.myhuaweicloud.com/1.9.0/MindSpore/lite/release/linux/x86_64/server/mindspore_lite-1.9.0-cp37-cp37m-linux_x86_64.whl">MindSpore Lite Concurrent Model Inference Framework</a> for the corresponding hardware platform of CPU and operating system of Linux-x86_64 or <a class="reference external" href="https://ms-release.obs.cn-north-4.myhuaweicloud.com/1.9.0/MindSpore/lite/release/linux/aarch64/server/mindspore_lite-1.9.0-cp37-cp37m-linux_aarch64.whl">MindSpore Lite Concurrent Model Inference Framework</a> for the corresponding hardware platform of CPU and operating system of Linux-aarch64. Users can use the <code class="docutils literal notranslate"><span class="pre">uname</span> <span class="pre">-m</span></code> command to query the operating system on the terminal, and copy it to <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_server_inference_python</span></code> directory.</p>
<p>If you need to use MindSpore Lite corresponding to Python 3.7 or above, please <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/build.html">compile</a> locally, noting that the Python API module compilation depends on Python &gt;= 3.7.0, NumPy &gt;= 1.17.0, wheel &gt;= 0.32.0. It should be noted that to generate a MindSpore Lite installation package that supports concurrent inference, and you need to configure the environment variable before compiling: export MSLITE_ENABLE_SERVER_INFERENCE=on. After successful compilation, copy the Whl installation package generated in the <code class="docutils literal notranslate"><span class="pre">output/</span></code> directory to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_server_inference_python</span></code> directory.</p>
<p>If the MindSpore Lite installation package does not exist in the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_server_inference_python</span></code> directory, the one-click installation script will uninstall the currently installed MindSpore Lite and then download and install MindSpore Lite from the Huawei image. Otherwise, if the MindSpore Lite installation package exists in the directory, it will install it first.</p>
<p>After manually downloading and placing the files in the specified directory, you need to execute the lite-server-cpu-pip.sh script again to complete the one-click installation.</p>
</div></blockquote>
<p>A successful execution will show the following results. The model files and input data files can be found in the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_server_inference_python/model</span></code> directory.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Successfully installed mindspore-lite-1.9.0
</pre></div>
</div>
</section>
<section id="executing-demo">
<h2>Executing Demo<a class="headerlink" href="#executing-demo" title="Permalink to this headline"></a></h2>
<p>After one-click installation, go to the [mindspore/lite/examples/quick_start_server_inference_python](https://gitee.com/mindspore/mindspore/tree/r2.0.0-alpha/mindspore/ lite/examples/quick_start_server_inference_python) directory and execute the following command to experience the MindSpore Lite concurrent inference MobileNetV2 model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>quick_start_server_inferece_python.py
</pre></div>
</div>
<p>The following results will be obtained when the execution is completed. During the execution of concurrent inference tasks in multiple threads, the single time-consuming inference and inference results in concurrent inference are printed, and the total concurrent inference time-consuming is printed after ending the thread.</p>
<p>Description of inference results:</p>
<ul>
<li><p>Simulates 5 clients and sends concurrent inference task requests to the server at the same time, with <code class="docutils literal notranslate"><span class="pre">parallel</span> <span class="pre">id</span></code> denoting the client id.</p></li>
<li><p>Simulates each client to send 2 requests for inference tasks to the server, and <code class="docutils literal notranslate"><span class="pre">task</span> <span class="pre">index</span></code> denotes the serial number of the task.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">run</span> <span class="pre">once</span> <span class="pre">time</span></code> indicates the inference time for a single requested inference task per client.</p></li>
<li><p>Next, the inference results of each client for single requested inference task are displayed, including the name of the output Tensor, the data size of the output Tensor, the number of elements of the output Tensor, and the first 5 pieces of data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">total</span> <span class="pre">run</span> <span class="pre">time</span></code> indicates the total time taken by the server to complete all concurrent inference tasks.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>parallel id:  0  | task index:  1  | run once time:  0.024082660675048828  s
tensor name is:Softmax-65 tensor size is:4004 tensor elements num is:1001
output data is: 1.02271215e-05 9.92699e-06 1.6968432e-05 6.8573616e-05 9.731416e-05
parallel id:  2  | task index:  1  | run once time:  0.029989957809448242  s
tensor name is:Softmax-65 tensor size is:4004 tensor elements num is:1001
output data is: 1.02271215e-05 9.92699e-06 1.6968432e-05 6.8573616e-05 9.731416e-05
parallel id:  1  | task index:  1  | run once time:  0.03409552574157715  s
tensor name is:Softmax-65 tensor size is:4004 tensor elements num is:1001
output data is: 1.02271215e-05 9.92699e-06 1.6968432e-05 6.8573616e-05 9.731416e-05
parallel id:  3  | task index:  1  | run once time:  0.04005265235900879  s
tensor name is:Softmax-65 tensor size is:4004 tensor elements num is:1001
output data is: 1.02271215e-05 9.92699e-06 1.6968432e-05 6.8573616e-05 9.731416e-05
parallel id:  4  | task index:  1  | run once time:  0.04981422424316406  s
tensor name is:Softmax-65 tensor size is:4004 tensor elements num is:1001
output data is: 1.02271215e-05 9.92699e-06 1.6968432e-05 6.8573616e-05 9.731416e-05
parallel id:  0  | task index:  2  | run once time:  0.028667926788330078  s
tensor name is:Softmax-65 tensor size is:4004 tensor elements num is:1001
output data is: 1.02271215e-05 9.92699e-06 1.6968432e-05 6.8573616e-05 9.731416e-05
parallel id:  2  | task index:  2  | run once time:  0.03010392189025879  s
tensor name is:Softmax-65 tensor size is:4004 tensor elements num is:1001
output data is: 1.02271215e-05 9.92699e-06 1.6968432e-05 6.8573616e-05 9.731416e-05
parallel id:  3  | task index:  2  | run once time:  0.030695676803588867  s
tensor name is:Softmax-65 tensor size is:4004 tensor elements num is:1001
output data is: 1.02271215e-05 9.92699e-06 1.6968432e-05 6.8573616e-05 9.731416e-05
parallel id:  1  | task index:  2  | run once time:  0.04117941856384277  s
tensor name is:Softmax-65 tensor size is:4004 tensor elements num is:1001
output data is: 1.02271215e-05 9.92699e-06 1.6968432e-05 6.8573616e-05 9.731416e-05
parallel id:  4  | task index:  2  | run once time:  0.028671741485595703  s
tensor name is:Softmax-65 tensor size is:4004 tensor elements num is:1001
output data is: 1.02271215e-05 9.92699e-06 1.6968432e-05 6.8573616e-05 9.731416e-05
total run time:  0.08787751197814941  s
</pre></div>
</div>
</li>
</ul>
</section>
<section id="demo-content-description">
<h2>Demo Content Description<a class="headerlink" href="#demo-content-description" title="Permalink to this headline"></a></h2>
<p>Performing concurrent inference with MindSpore Lite consists of the following main steps:</p>
<ol class="arabic simple">
<li><p><span class="xref myst">key variable description</span>: Description of the key variables used in concurrent inference.</p></li>
<li><p><span class="xref myst">Create concurrent inference configuration</span>: Create the concurrent inference configuration option <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/mindspore_lite/mindspore_lite.RunnerConfig.html">RunnerConfig</a> to save some basic configuration parameters used to perform the initialization of concurrent inference.</p></li>
<li><p><span class="xref myst">Concurrent Runner loading and compilation</span>: Before performing concurrent inference, you need to call <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.init">init</a> interface of <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code> for concurrent Runner initialization, mainly for model reading, loading <code class="docutils literal notranslate"><span class="pre">RunnerConfig</span></code> configuration, creating concurrency, and subgraph slicing and operator selection scheduling. The <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code> can be understood as a <code class="docutils literal notranslate"><span class="pre">model</span></code> that supports concurrent inference. This phase can take more time, so it is recommended that <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code> be initialized once and perform concurrent inference multiple times.</p></li>
<li><p><span class="xref myst">Set concurrent inference task</span>: Create multiple threads and bind concurrent inference tasks. The inference tasks include populating the <code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">Tensor</span></code> with data, using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.predict">predict</a> interface of <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code> for concurrent inference and getting inference results via the <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">Tensor</span></code>.</p></li>
<li><p><span class="xref myst">perform concurrent inference</span>: Start multiple threads and execute the configured concurrent inference tasks. During execution, the single inference time consumption and inference result in concurrent inference are printed, and the total concurrent inference time consumption is printed after the end of the thread.</p></li>
</ol>
<p>For more advanced usage and examples of Python interfaces, please refer to the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/mindspore_lite.html">Python API</a>.</p>
<p><img alt="img" src="../_images/server_inference.png" /></p>
<section id="key-variable-description">
<h3>Key Variable Description<a class="headerlink" href="#key-variable-description" title="Permalink to this headline"></a></h3>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">THREAD_NUM</span></code>: the number of threads in a single worker. <code class="docutils literal notranslate"><span class="pre">WORKERS_NUM</span> <span class="pre">*</span> <span class="pre">THREAD_NUM</span></code> should be less than the number of machine cores.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WORKERS_NUM</span></code>: On the server side, specify the number of workers in a <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code>, i.e., the units that perform concurrent inference. If you want to compare the difference in inference time between concurrent inference and non-concurrent inference, you can set <code class="docutils literal notranslate"><span class="pre">WORKERS_NUM</span></code> to 1 for comparison.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PARALLEL_NUM</span></code>: The number of concurrent, i.e., the number of clients that are sending inference task requests at the same time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TASK_NUM</span></code>: The number of tasks, i.e., the number of inference task requests sent by a single client.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>

<span class="c1"># the number of threads of one worker.</span>
<span class="c1"># WORKERS_NUM * THREAD_NUM should not exceed the number of cores of the machine.</span>
<span class="n">THREAD_NUM</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># In parallel inference, the number of workers in one `ModelParallelRunner` in server.</span>
<span class="c1"># If you prepare to compare the time difference between parallel inference and serial inference,</span>
<span class="c1"># you can set WORKERS_NUM = 1 as serial inference.</span>
<span class="n">WORKERS_NUM</span> <span class="o">=</span> <span class="mi">3</span>

<span class="c1"># Simulate 5 clients, and each client sends 2 inference tasks to the server at the same time.</span>
<span class="n">PARALLEL_NUM</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">TASK_NUM</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="creating-concurrent-inference-configuration">
<h3>Creating Concurrent Inference Configuration<a class="headerlink" href="#creating-concurrent-inference-configuration" title="Permalink to this headline"></a></h3>
<p>Create concurrent inference configuration <code class="docutils literal notranslate"><span class="pre">RunnerConfig</span></code>. Since this tutorial demonstrates a scenario where inference is performed on a CPU device, it is necessary to add the created CPU device hardware information to the context <code class="docutils literal notranslate"><span class="pre">Conterxt</span></code> and then add <code class="docutils literal notranslate"><span class="pre">Conterxt</span></code> to <code class="docutils literal notranslate"><span class="pre">RunnerConfig</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Init RunnerConfig and context, and add CPU device info</span>
<span class="n">cpu_device_info</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">CPUDeviceInfo</span><span class="p">(</span><span class="n">enable_fp16</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">(</span><span class="n">thread_num</span><span class="o">=</span><span class="n">THREAD_NUM</span><span class="p">,</span> <span class="n">inter_op_parallel_num</span><span class="o">=</span><span class="n">THREAD_NUM</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">append_device_info</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">)</span>
<span class="n">parallel_runner_config</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">RunnerConfig</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="n">context</span><span class="p">,</span> <span class="n">workers_num</span><span class="o">=</span><span class="n">WORKERS_NUM</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="concurrent-runner-loading-and-compilation">
<h3>Concurrent Runner Loading and Compilation<a class="headerlink" href="#concurrent-runner-loading-and-compilation" title="Permalink to this headline"></a></h3>
<p>Call <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.init">init</a> interface of <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code> for concurrent Runner initialization, mainly for model reading, loading <code class="docutils literal notranslate"><span class="pre">RunnerConfig</span></code> configuration, creating concurrency, and subgraph slicing and operator selection scheduling. The <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code> can be understood as a <code class="docutils literal notranslate"><span class="pre">Model</span></code> that supports concurrent inference. This phase can take more time, so it is recommended that <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code> be initialized once and perform concurrent inference multiple times.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build ModelParallelRunner from file</span>
<span class="n">model_parallel_runner</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelParallelRunner</span><span class="p">()</span>
<span class="n">model_parallel_runner</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="s2">&quot;./model/mobilenetv2.ms&quot;</span><span class="p">,</span> <span class="n">runner_config</span><span class="o">=</span><span class="n">parallel_runner_config</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="setting-concurrent-inference-task">
<h3>Setting Concurrent Inference Task<a class="headerlink" href="#setting-concurrent-inference-task" title="Permalink to this headline"></a></h3>
<p>Create multiple threads and bind concurrent inference tasks. The inference tasks include populating the <code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">Tensor</span></code> with data, using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/mindspore_lite/mindspore_lite.ModelParallelRunner.html#mindspore_lite.ModelParallelRunner.predict">predict</a> interface of <code class="docutils literal notranslate"><span class="pre">ModelParallelRunner</span></code> for concurrent inference and getting inference results via the <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">Tensor</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parallel_runner_predict</span><span class="p">(</span><span class="n">parallel_runner</span><span class="p">,</span> <span class="n">parallel_id</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    One Runner with 3 workers, set model input, execute inference and get output.</span>

<span class="sd">    Args:</span>
<span class="sd">        parallel_runner (mindspore_lite.ModelParallelRunner): Actuator Supporting Parallel inference.</span>
<span class="sd">        parallel_id (int): Simulate which client&#39;s task to process</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">task_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">task_index</span> <span class="o">==</span> <span class="n">TASK_NUM</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">task_index</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Set model input</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">parallel_runner</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()</span>
        <span class="n">in_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="s2">&quot;./model/input.bin&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">)</span>
        <span class="n">once_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="c1"># Execute inference</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">parallel_runner</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="n">once_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;parallel id: &quot;</span><span class="p">,</span> <span class="n">parallel_id</span><span class="p">,</span> <span class="s2">&quot; | task index: &quot;</span><span class="p">,</span> <span class="n">task_index</span><span class="p">,</span> <span class="s2">&quot; | run once time: &quot;</span><span class="p">,</span>
              <span class="n">once_end_time</span> <span class="o">-</span> <span class="n">once_start_time</span><span class="p">,</span> <span class="s2">&quot; s&quot;</span><span class="p">)</span>
        <span class="c1"># Get output</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
            <span class="n">tensor_name</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get_tensor_name</span><span class="p">()</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>
            <span class="n">data_size</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get_data_size</span><span class="p">()</span>
            <span class="n">element_num</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get_element_num</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor name is:</span><span class="si">%s</span><span class="s2"> tensor size is:</span><span class="si">%s</span><span class="s2"> tensor elements num is:</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">tensor_name</span><span class="p">,</span> <span class="n">data_size</span><span class="p">,</span>
                                                                                     <span class="n">element_num</span><span class="p">))</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get_data_to_numpy</span><span class="p">()</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output data is:&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>


<span class="c1"># The server creates 5 threads to store the inference tasks of 5 clients.</span>
<span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">total_start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">PARALLEL_NUM</span><span class="p">):</span>
    <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">parallel_runner_predict</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">model_parallel_runner</span><span class="p">,</span> <span class="n">i</span><span class="p">,)))</span>
</pre></div>
</div>
</section>
<section id="performing-concurrent-inference">
<h3>Performing Concurrent Inference<a class="headerlink" href="#performing-concurrent-inference" title="Permalink to this headline"></a></h3>
<p>Start multiple threads and execute the configured concurrent inference tasks. During execution, the single inference time consumption and inference result in concurrent inference are printed, and the total concurrent inference time consumption is printed after the end of the thread.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start threads to perform parallel inference.</span>
<span class="k">for</span> <span class="n">th</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
    <span class="n">th</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="k">for</span> <span class="n">th</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
    <span class="n">th</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
<span class="n">total_end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;total run time: &quot;</span><span class="p">,</span> <span class="n">total_end_time</span> <span class="o">-</span> <span class="n">total_start_time</span><span class="p">,</span> <span class="s2">&quot; s&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="quick_start_python.html" class="btn btn-neutral float-left" title="Experiencing the Python Simplified Inference Demo" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="quick_start_c.html" class="btn btn-neutral float-right" title="Expriencing Simpcified Inference Demo with C-language" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>