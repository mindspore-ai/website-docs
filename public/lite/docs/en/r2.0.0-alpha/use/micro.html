

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Performing Inference or Training on MCU or Small Systems &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Application Specific Integrated Circuit Integration Instructions" href="asic.html" />
    <link rel="prev" title="Using Java Interface to Perform Inference" href="runtime_java.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Getting Started in One Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_cpp.html">Experience C++ Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_java.html">Experience Java Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_python.html">Experiencing the Python Simplified Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_python.html">Experiencing the Python Simplified Concurrent Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_c.html">Expriencing Simpcified Inference Demo with C-language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Performing Inference or Training on MCU or Small Systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#generating-model-inference-code">Generating Model Inference Code</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview-1">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparing-environment">Preparing Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generating-inference-code-by-running-converter-lite">Generating Inference Code by Running converter_lite</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optional-model-input-shape-configuration">(Optional) Model Input Shape Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optional-generating-multithreaded-parallel-inference-code">(Optional) Generating Multithreaded Parallel Inference Code</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuration">Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#involved-calling-interfaces">Involved Calling Interfaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integration-considerations">Integration Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#restrictions">Restrictions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#optional-generate-int8-quantitative-inference-code">(Optional) Generate Int8 Quantitative Inference Code</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuration-1">Configuration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#generating-model-training-code">Generating Model Training Code</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview-2">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparing-environment-1">Preparing Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generating-training-code-by-running-converter-lite">Generating Training Code by Running converter_lite</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#obtaining-micro-lib">Obtaining <code class="docutils literal notranslate"><span class="pre">Micro</span></code> Lib</a></li>
<li class="toctree-l2"><a class="reference internal" href="#code-integration-and-compilation-deployment">Code Integration and Compilation Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#calling-interface-of-inference-code">Calling Interface of Inference Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#calling-interface-of-training-code">Calling Interface of Training Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#integration-differences-of-different-platforms">Integration Differences of Different Platforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performing-inference-on-the-mcu">Performing Inference on the MCU</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview-3">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generating-mcu-inference-code">Generating MCU Inference Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloading-micro-lib-of-cortex-m-architecture">Downloading <code class="docutils literal notranslate"><span class="pre">Micro</span></code> Lib of Cortex-M Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="#code-integration-and-compilation-deployment-on-windows-integrated-development-through-iar">Code Integration and Compilation Deployment on Windows: Integrated Development Through IAR</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-the-mcu-startup-code-and-project">Obtaining the MCU Startup Code and Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integrating-model-inference-code-and-micro-lib">Integrating Model Inference Code and <code class="docutils literal notranslate"><span class="pre">Micro</span></code> Lib</a></li>
<li class="toctree-l4"><a class="reference internal" href="#compiling-and-simulation-run">Compiling and Simulation Run</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#code-integration-and-compilation-deployment-on-linux-code-integration-via-makefile">Code Integration and Compilation Deployment on Linux: Code Integration via MakeFile</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#environment-preparation-1">Environment Preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-the-mcu-startup-code-and-project-1">Obtaining the MCU Startup Code and Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integrating-model-inference-code-and-micro-lib-1">Integrating Model Inference Code and <code class="docutils literal notranslate"><span class="pre">Micro</span></code> Lib</a></li>
<li class="toctree-l4"><a class="reference internal" href="#compiling-and-burning-the-project">Compiling and Burning the Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference-result-verification">Inference Result Verification</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#executing-inference-on-light-harmony-devices">Executing Inference on Light Harmony Devices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#preparing-the-light-harmony-compilation-environment">Preparing the Light Harmony Compilation Environment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiling-models">Compiling Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiling-the-build-script">Compiling the Build Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiling-benchmark">Compiling benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performing-benchmark">Performing benchmark</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#custom-kernel">Custom Kernel</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#implementing-custom-kernel-by-users">Implementing custom kernel by users</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Server Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime_server_inference.html">Executing Server Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Performing Inference or Training on MCU or Small Systems</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/micro.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section class="tex2jax_ignore mathjax_ignore" id="performing-inference-or-training-on-mcu-or-small-systems">
<h1>Performing Inference or Training on MCU or Small Systems<a class="headerlink" href="#performing-inference-or-training-on-mcu-or-small-systems" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0.0-alpha/docs/lite/docs/source_en/use/micro.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/resource/_static/logo_source_en.png"></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial describes an ultra-lightweight AI deployment solution for IoT edge devices.</p>
<p>Compared with mobile devices, MCUs are usually used on IoT devices. The ROM resources of the device are limited, and the memory and computing power of the hardware resources are weak.
Therefore, AI applications on IoT devices have strict limits on runtime memory and power consumption of AI model inference.
For MCUs deploying hardware backends, MindSpore Lite provides the ultra-lightweight Micro AI deployment solution. In the offline phase, models are directly generated into lightweight code without online model parsing and graph compilation. The generated Micro code is easy to understand, with less memory at runtime and smaller code size.
You can use a MindSpore Lite conversion tool <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> to easily generate inference or training code that can be deployed on the x86/ARM64/ARM32/Cortex-M platform.</p>
<p>Deploying a model for inference or training via the Micro involves the following four steps: model code generation, <code class="docutils literal notranslate"><span class="pre">Micro</span></code> lib obtaining, code integration, and compilation and deployment.</p>
</section>
<section id="generating-model-inference-code">
<h2>Generating Model Inference Code<a class="headerlink" href="#generating-model-inference-code" title="Permalink to this headline"></a></h2>
<section id="overview-1">
<h3>Overview<a class="headerlink" href="#overview-1" title="Permalink to this headline"></a></h3>
<p>The Micro configuration item in the parameter configuration file is configured via the MindSpore Lite conversion tool <code class="docutils literal notranslate"><span class="pre">convert_lite</span></code>.
This chapter describes the functions related to code generation in the conversion tool. For details about how to use the conversion tool, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/converter_tool.html">Converting Models for Inference</a>.</p>
</section>
<section id="preparing-environment">
<h3>Preparing Environment<a class="headerlink" href="#preparing-environment" title="Permalink to this headline"></a></h3>
<p>The following describes how to prepare the environment for using the conversion tool in the Linux environment.</p>
<ol class="arabic">
<li><p>System environment required for running the conversion tool</p>
<p>In this example, the Linux operating system is used. Ubuntu 18.04.02LTS is recommended.</p>
</li>
<li><p>Obtain the conversion tool</p>
<p>You can obtain the conversion tool in either of the following ways:</p>
<ul>
<li><p>Download <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/downloads.html">Release Version</a> from the MindSpore official website.</p>
<p>Download the release package whose OS is Linux-x86_64 and hardware platform is CPU.</p>
</li>
<li><p>Start from the source code for <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/build.html">Building MindSpore Lite</a>.</p></li>
</ul>
</li>
<li><p>Decompress the downloaded package.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>-zxf<span class="w"> </span>mindspore-lite-<span class="si">${</span><span class="nv">version</span><span class="si">}</span>-linux-x64.tar.gz
</pre></div>
</div>
<p>${version} is the version number of the release package.</p>
</li>
<li><p>Add the dynamic link library required by the conversion tool to the environment variable LD_LIBRARY_PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} is the path of the decompressed folder.</p>
</li>
</ol>
</section>
<section id="generating-inference-code-by-running-converter-lite">
<h3>Generating Inference Code by Running converter_lite<a class="headerlink" href="#generating-inference-code-by-running-converter-lite" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Go to the conversion directory</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/converter
</pre></div>
</div>
</li>
<li><p>Set the Micro configuration item</p>
<p>Create the micro.cfg file in the current directory. The file content is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[micro_param]

# enable code-generation for MCU HW

enable_micro=true

# specify HW target, support x86,Cortex-M, AMR32A, ARM64 only.

target=x86

# enable parallel inference or not.

support_parallel=false
</pre></div>
</div>
<p>In the configuration file, <code class="docutils literal notranslate"><span class="pre">[micro_param]</span></code> in the first line indicates that the subsequent variable parameters belong to the micro configuration item <code class="docutils literal notranslate"><span class="pre">micro_param</span></code>. These parameters are used to control code generation. Table 1 describes the parameters.
In this example, we will generate inference code for Linux systems with the underlying architecture x86_64, so set <code class="docutils literal notranslate"><span class="pre">target=x86</span></code> to declare that the generated inference code will be used for Linux systems with the underlying architecture x86_64.</p>
</li>
<li><p>Prepare the model to generate inference code</p>
<p>Click here to download the <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/micro/mnist.tar.gz">MNIST Handwritten Digit Recognition Model</a> used in this example.
After downloading, decompress the package to obtain <code class="docutils literal notranslate"><span class="pre">mnist.tflite</span></code>. This model is a trained MNIST classification model, that is, a TFLITE model. Copy the <code class="docutils literal notranslate"><span class="pre">mnist.tflite</span></code> model to the current conversion tool directory.</p>
</li>
<li><p>Execute converter_lite and generate code</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span>mnist<span class="w"> </span>--configFile<span class="o">=</span>micro.cfg
</pre></div>
</div>
<p>The following information is displayed when the code is run successfully:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
<p>For details about the parameters related to converter_lite, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/converter_tool.html#parameter-description">Converter Parameter Description</a>.</p>
<p>After the conversion tool is successfully executed, the generated code is saved in the specified <code class="docutils literal notranslate"><span class="pre">outputFile</span></code> directory. In this example, the mnist folder is in the current conversion directory. The content is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mnist                          # Specified name of generated code root directory
├── benchmark                  # Benchmark routines for integrated calls to model inference code
│   ├── benchmark.c
│   ├── calib_output.c
│   ├── calib_output.h
│   ├── load_input.c
│   └── load_input.h
├── CMakeLists.txt             # cmake project file of the benchmark routine
└── src                        # Model inference code directory
    ├── CMakeLists.txt
    ├── net.bin                # Model weights in binary form
    ├── net.c
    ├── net.cmake
    ├── net.h
    ├── model.c
    ├── context.c
    ├── context.h
    ├── tensor.c
    ├── tensor.h
    ├── weight.c
    └── weight.h
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">src</span></code> directory in the generated code is the directory where the model inference code is located. The <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> is just a routine for calling the <code class="docutils literal notranslate"><span class="pre">src</span></code> directory code integratedly.
For more details on integrated calls, please refer to the section on <span class="xref myst">Code Integration and Compilation Deployment</span>.</p>
</li>
</ol>
<p>Table 1: micro_param Parameter Definition</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Mandatory or not</p></th>
<th class="head"><p>Parameter Description</p></th>
<th class="head"><p>Range</p></th>
<th class="head"><p>Default value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>enable_micro</p></td>
<td><p>Yes</p></td>
<td><p>The model generates code, otherwise it generates .ms.</p></td>
<td><p>true, false</p></td>
<td><p>false</p></td>
</tr>
<tr class="row-odd"><td><p>target</p></td>
<td><p>Yes</p></td>
<td><p>Platform for which code is generated</p></td>
<td><p>x86, Cortex-M, ARM32, ARM64</p></td>
<td><p>x86</p></td>
</tr>
<tr class="row-even"><td><p>support_parallel</p></td>
<td><p>No</p></td>
<td><p>Generate multi-threaded inference codes or not, which can be set to true only on x86/ARM32/ARM64 platforms</p></td>
<td><p>true, false</p></td>
<td><p>false</p></td>
</tr>
</tbody>
</table>
</section>
<section id="optional-model-input-shape-configuration">
<h3>(Optional) Model Input Shape Configuration<a class="headerlink" href="#optional-model-input-shape-configuration" title="Permalink to this headline"></a></h3>
<p>Usually, when generating code, you can reduce the probability of errors in the deployment process by configuring the model input shape as the input shape for actual inference.
When the model contains a <code class="docutils literal notranslate"><span class="pre">Shape</span></code> operator or the original model has a non-fixed input shape value, the input shape value of the model must be configured to support the relevant shape optimization and code generation.
The <code class="docutils literal notranslate"><span class="pre">--inputShape=</span></code> command of the conversion tool can be used to configure the input shape of the generated code. For specific parameter meanings, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/converter_tool.html">Conversion Tool Instructions</a>.</p>
</section>
<section id="optional-generating-multithreaded-parallel-inference-code">
<h3>(Optional) Generating Multithreaded Parallel Inference Code<a class="headerlink" href="#optional-generating-multithreaded-parallel-inference-code" title="Permalink to this headline"></a></h3>
<p>In the usual Linux-x86/android scenario, with multi-core CPUs, Micro multi-threaded inference is enabled to leverage device performance and speed up model inference.</p>
<section id="configuration">
<h4>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline"></a></h4>
<p>By setting the <code class="docutils literal notranslate"><span class="pre">support_parallel</span></code> to true in the configuration file, the code supporting multi-threaded inference will be generated. Please refer to Table 1 for the meaning of each option in the configuration file.
An example of a ‘x86’ multithreaded code generation configuration file is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[micro_param]

# enable code-generation for MCU HW

enable_micro=true

# specify HW target, support x86,Cortex-M, AMR32A, ARM64 only.

target=x86

# enable parallel inference or not.

support_parallel=true
</pre></div>
</div>
<p>For the meaning of each option in the configuration file, refer to Table 1.</p>
</section>
<section id="involved-calling-interfaces">
<h4>Involved Calling Interfaces<a class="headerlink" href="#involved-calling-interfaces" title="Permalink to this headline"></a></h4>
<p>By integrating the code and calling the following interfaces, the user can configure the multi-threaded inference of the model.
For specific interface parameters, refer to <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/index.html">API Document</a>.</p>
<p>Table 2: API Interface for Multi-threaded Configuration</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Function definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>set the number of threads during inference</p></td>
<td><p>void MSContextSetThreadNum(MSContextHandle context, int32_t thread_num)</p></td>
</tr>
<tr class="row-odd"><td><p>set the thread affinity to CPU cores</p></td>
<td><p>void MSContextSetThreadAffinityMode(MSContextHandle context, int mode)</p></td>
</tr>
<tr class="row-even"><td><p>Obtain the current thread number setting during inference</p></td>
<td><p>int32_t MSContextGetThreadNum(const MSContextHandle context);</p></td>
</tr>
<tr class="row-odd"><td><p>obtain the thread affinity of CPU cores</p></td>
<td><p>int MSContextGetThreadAffinityMode(const MSContextHandle context)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="integration-considerations">
<h4>Integration Considerations<a class="headerlink" href="#integration-considerations" title="Permalink to this headline"></a></h4>
<p>After generating multithreaded code, users need to link to the <code class="docutils literal notranslate"><span class="pre">pthread</span></code> standard library and the <code class="docutils literal notranslate"><span class="pre">libwrapper.a</span></code> static library in the Micro library.
Please refer to the <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> file in the generated code for details.</p>
</section>
<section id="restrictions">
<h4>Restrictions<a class="headerlink" href="#restrictions" title="Permalink to this headline"></a></h4>
<p>At present, this function is only enabled when the <code class="docutils literal notranslate"><span class="pre">target</span></code> is configured as x86/ARM32/ARM64. The maximum number of inference threads can be set to 4.</p>
</section>
</section>
<section id="optional-generate-int8-quantitative-inference-code">
<h3>(Optional) Generate Int8 Quantitative Inference Code<a class="headerlink" href="#optional-generate-int8-quantitative-inference-code" title="Permalink to this headline"></a></h3>
<p>In MCU scenarios such as Cortex-M, limited by the memory size and computing power of the device, Int8 quantization operators are usually used for deployment inference to reduce the runtime memory size and speed up operations.</p>
<p>If the user already has an Int8 full quantitative model, you can refer to the section on <span class="xref myst">Generating Inference Code by Running converter_lite</span> to try to generate Int8 quantitative inference code directly without reading this chapter.
In general, the user has only one trained Float32 model. To generate Int8 quantitative inference code at this time, it is necessary to cooperate with the post quantization function of the conversion tool to generate code. See the following for specific steps.</p>
<section id="configuration-1">
<h4>Configuration<a class="headerlink" href="#configuration-1" title="Permalink to this headline"></a></h4>
<p>Int8 quantization inference code can be generated by configuring quantization control parameters in the configuration file. For the description of quantization control parameters (<code class="docutils literal notranslate"><span class="pre">universal</span> <span class="pre">quantization</span> <span class="pre">parameters</span></code> and <code class="docutils literal notranslate"><span class="pre">full</span> <span class="pre">quantization</span> <span class="pre">parameters</span></code>), please refer to the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/post_training_quantization.html">Post Training Quantization</a>.</p>
<p>An example of Int8 quantitative inference code generation configuration file for a <code class="docutils literal notranslate"><span class="pre">Cortex-M</span></code> platform is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[micro_param]
# enable code-generation for MCU HW
enable_micro=true

# specify HW target, support x86,Cortex-M, ARM32, ARM64 only.
target=Cortex-M

# code generation for Inference or Train
codegen_mode=Inference

# enable parallel inference or not
support_parallel=false

[common_quant_param]
# Supports WEIGHT_QUANT or FULL_QUANT
quant_type=FULL_QUANT

# Weight quantization support the number of bits [0,16], Set to 0 is mixed bit quantization, otherwise it is fixed bit quantization
# Full quantization support the number of bits [1,8]
bit_num=8

[data_preprocess_param]

calibrate_path=inputs:/home/input_dir

calibrate_size=100

input_type=BIN

[full_quant_param]

activation_quant_method=MAX_MIN

bias_correction=true

target_device=DSP
</pre></div>
</div>
<section id="restrictions-1">
<h5>Restrictions<a class="headerlink" href="#restrictions-1" title="Permalink to this headline"></a></h5>
<ul class="simple">
<li><p>Currently, it only supports full quantitative inference code generation.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">target_device</span></code> of the <code class="docutils literal notranslate"><span class="pre">full</span> <span class="pre">quantization</span> <span class="pre">parameter</span></code> in the configuration file usually needs to be set to DSP to support more operators for post quantization.</p></li>
<li><p>At present, Micro has supported 34 Int8 quantization operators. If a related quantization operator does not support it when generating code, you can circumvent the operator through the <code class="docutils literal notranslate"><span class="pre">skip_quant_node</span></code> of the <code class="docutils literal notranslate"><span class="pre">universal</span> <span class="pre">quantization</span> <span class="pre">parameter</span></code>. The circumvented operator node still uses Float32 inference.</p></li>
</ul>
</section>
</section>
</section>
</section>
<section id="generating-model-training-code">
<h2>Generating Model Training Code<a class="headerlink" href="#generating-model-training-code" title="Permalink to this headline"></a></h2>
<section id="overview-2">
<h3>Overview<a class="headerlink" href="#overview-2" title="Permalink to this headline"></a></h3>
<p>The training code can be generated for the input model by using the MindSpore Lite conversion tool <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> and configuring the Micro configuration item in the parameter configuration file of the conversion tool.
This chapter describes the functions related to code generation in the conversion tool. For details about how to use the conversion tool, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/converter_train.html">Converting Models for Training</a>.</p>
</section>
<section id="preparing-environment-1">
<h3>Preparing Environment<a class="headerlink" href="#preparing-environment-1" title="Permalink to this headline"></a></h3>
<p>For preparing environment section, refer to the <span class="xref myst">above</span>, which will not be repeated here.</p>
</section>
<section id="generating-training-code-by-running-converter-lite">
<h3>Generating Training Code by Running converter_lite<a class="headerlink" href="#generating-training-code-by-running-converter-lite" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Go to the conversion directory</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/converter
</pre></div>
</div>
</li>
<li><p>Set the Micro configuration item</p>
<p>Create the micro.cfg file in the current directory. The file content is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[micro_param]

# enable code-generation for MCU HW

enable_micro=true

# specify HW target, support x86,Cortex-M, AMR32A, ARM64 only.

target=x86

# code generation for Inference or Train. Cortex-M is unsupported when codegen_mode is Train.

codegen_mode=Train
</pre></div>
</div>
</li>
<li><p>Execute converter_lite and generate code</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--trainModel<span class="o">=</span>True<span class="w"> </span>--modelFile<span class="o">=</span>my_model.mindir<span class="w"> </span>--outputFile<span class="o">=</span>my_model<span class="w"> </span>--configFile<span class="o">=</span>micro.cfg
</pre></div>
</div>
<p>The following information is displayed when the code is run successfully:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
<p>After the conversion tool is successfully executed, the generated code is saved in the specified <code class="docutils literal notranslate"><span class="pre">outputFile</span></code> directory. In this example, the my_model folder is in the current conversion directory. The content is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>my_model                       # Specified name of generated code root directory
├── benchmark                  # Benchmark routines for integrated calls to model train code
│   ├── benchmark.c
│   ├── calib_output.c
│   ├── calib_output.h
│   ├── load_input.c
│   └── load_input.h
├── CMakeLists.txt             # cmake project file of the benchmark routine
└── src                        # Model inference code directory
    ├── CMakeLists.txt
    ├── net.bin                # Model weights in binary form
    ├── net.c
    ├── net.cmake
    ├── net.h
    ├── model.c
    ├── context.c
    ├── context.h
    ├── tensor.c
    ├── tensor.h
    ├── weight.c
    └── weight.h
</pre></div>
</div>
<p>For the API involved in the training process, please refer to the <span class="xref myst">Introduction to training interface</span></p>
</li>
</ol>
</section>
</section>
<section id="obtaining-micro-lib">
<h2>Obtaining <code class="docutils literal notranslate"><span class="pre">Micro</span></code> Lib<a class="headerlink" href="#obtaining-micro-lib" title="Permalink to this headline"></a></h2>
<p>After generating model inference code, you need to obtain the <code class="docutils literal notranslate"><span class="pre">Micro</span></code> lib on which the generated inference code depends before performing integrated development on the code.</p>
<p>The inference code of different platforms depends on the <code class="docutils literal notranslate"><span class="pre">Micro</span></code> lib of the corresponding platform. You need to specify the platform via the micro configuration item <code class="docutils literal notranslate"><span class="pre">target</span></code> based on the platform in use when generating code, and obtain the <code class="docutils literal notranslate"><span class="pre">Micro</span></code> lib of the platform when obtaining the inference package.
You can download the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/downloads.html">Release Version</a> of the corresponding platform from the MindSpore official website.</p>
<p>In chapter <span class="xref myst">Generating Model Inference Code</span>, we obtain the model inference code of the Linux platform with the x86_64 architecture. The <code class="docutils literal notranslate"><span class="pre">Micro</span></code> lib on which the code depends is the release package used by the conversion tool.
In the release package, the following content depended by the inference code:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
├── runtime
│   └── include
│       └── c_api            # C API header file integrated with MindSpore Lite
└── tools
    └── codegen # The source code generated by code depends on include and lib
        ├── include          # Inference framework header file
        │   ├── nnacl        # nnacl operator header file
        │   └── wrapper      # wrapper operator header file
        ├── lib
        │   ├── libwrapper.a # The MindSpore Lite codegen generates some operator static libraries on which the code depends
        │   └── libnnacl.a   # The MindSpore Lite codegen generates the nnacl operator static library on which the code depends
        └── third_party
            ├── include
            │   └── CMSIS    # ARM CMSIS NN operator header file
            └── lib
                └── libcmsis_nn.a # ARM CMSIS NN operator static library
</pre></div>
</div>
</section>
<section id="code-integration-and-compilation-deployment">
<h2>Code Integration and Compilation Deployment<a class="headerlink" href="#code-integration-and-compilation-deployment" title="Permalink to this headline"></a></h2>
<p>In the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> directory where the code is generated, there is an interface call example for the inference code.
Users can refer to the benchmark routine to integrate and develop the <code class="docutils literal notranslate"><span class="pre">src</span></code> inference code to realize their own applications.</p>
<section id="calling-interface-of-inference-code">
<h3>Calling Interface of Inference Code<a class="headerlink" href="#calling-interface-of-inference-code" title="Permalink to this headline"></a></h3>
<p>The following is the general calling interface of the inference code. For a detailed description of the interface, please refer to the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/index.html">API documentation</a>.</p>
<p>Table 3: Inference Common API Interface</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Function definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Create a model object</p></td>
<td><p>MSModelHandle MSModelCreate()</p></td>
</tr>
<tr class="row-odd"><td><p>Destroy the model object</p></td>
<td><p>void MSModelDestroy(MSModelHandle *model)</p></td>
</tr>
<tr class="row-even"><td><p>Calculate the workspace size required for model inference, valid only on cortex-M architecture</p></td>
<td><p>size_t MSModelCalcWorkspaceSize(MSModelHandle model)</p></td>
</tr>
<tr class="row-odd"><td><p>Set workspace for the model object, valid only on cortex-M architecture</p></td>
<td><p>void MSModelSetWorkspace(MSModelHandle model, void *workspace, size_t workspace_size)</p></td>
</tr>
<tr class="row-even"><td><p>Compile model</p></td>
<td><p>MSStatus MSModelBuild(MSModelHandle model, const void *model_data, size_t data_size, MSModelType model_type, const MSContextHandle model_context)</p></td>
</tr>
<tr class="row-odd"><td><p>Inference model</p></td>
<td><p>MSStatus MSModelPredict(MSModelHandle model, const MSTensorHandleArray inputs, MSTensorHandleArray *outputs, const MSKernelCallBackC before, const MSKernelCallBackC after)</p></td>
</tr>
<tr class="row-even"><td><p>Obtain all input tensor handles of the model</p></td>
<td><p>MSTensorHandleArray MSModelGetInputs(const MSModelHandle model)</p></td>
</tr>
<tr class="row-odd"><td><p>Obtain all output tensor handles of the model</p></td>
<td><p>MSTensorHandleArray MSModelGetOutputs(const MSModelHandle model)</p></td>
</tr>
<tr class="row-even"><td><p>Obtain the input tensor handle of the model by name</p></td>
<td><p>MSTensorHandle MSModelGetInputByTensorName(const MSModelHandle model, const char *tensor_name)</p></td>
</tr>
<tr class="row-odd"><td><p>Obtain the output tensor handle of the model by name</p></td>
<td><p>MSTensorHandle MSModelGetOutputByTensorName(const MSModelHandle model, const char *tensor_name)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="calling-interface-of-training-code">
<h3>Calling Interface of Training Code<a class="headerlink" href="#calling-interface-of-training-code" title="Permalink to this headline"></a></h3>
<p>The following is the general calling interface of the Training code.</p>
<p>Table 4: Training Common API Interface (only training-related interfaces are listed here)</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Function definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Run model by step</p></td>
<td><p>MSStatus MSModelRunStep(MSModelHandle model, const MSKernelCallBackC before, const MSKernelCallBackC after)</p></td>
</tr>
<tr class="row-odd"><td><p>Set the model running mode</p></td>
<td><p>MSStatus MSModelSetTrainMode(MSModelHandle model, bool train)</p></td>
</tr>
<tr class="row-even"><td><p>Export the weights of model to file</p></td>
<td><p>MSStatus MSModelExportWeight(MSModelHandle model, const char *export_path)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="integration-differences-of-different-platforms">
<h3>Integration Differences of Different Platforms<a class="headerlink" href="#integration-differences-of-different-platforms" title="Permalink to this headline"></a></h3>
<p>Different platforms have differences in code integration and compilation deployment.</p>
<ul class="simple">
<li><p>For the MCU of the cortex-M architecture, see <span class="xref myst">Performing Inference on the MCU</span></p></li>
<li><p>For the Linux platform with the x86_64 architecture, see <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.0.0-alpha/mindspore/lite/examples/quick_start_micro/mnist_x86">Compilation and Deployment on Linux_x86_64 Platform</a></p></li>
<li><p>For details about how to compile and deploy arm32 or arm64 on the Android platform, see <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.0.0-alpha/mindspore/lite/examples/quick_start_micro/mobilenetv2_arm64">Compilation and Deployment on Android Platform</a></p></li>
<li><p>For compilation and deployment on the OpenHarmony platform, see <span class="xref myst">Executing Inference on Light Harmony Devices</span></p></li>
</ul>
</section>
</section>
<section id="performing-inference-on-the-mcu">
<h2>Performing Inference on the MCU<a class="headerlink" href="#performing-inference-on-the-mcu" title="Permalink to this headline"></a></h2>
<section id="overview-3">
<h3>Overview<a class="headerlink" href="#overview-3" title="Permalink to this headline"></a></h3>
<p>This tutorial takes the deployment of the <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/micro/mnist.tar.gz">MNIST model</a> on the STM32F767 chip as an example to demonstrate how to deploy the inference model on the MCU of the Cortex-M architecture, including the following steps:</p>
<ul>
<li><p>Use the converter_lite conversion tool to generate model inference code that adapts to the Cortex-M architecture</p></li>
<li><p>Download the <code class="docutils literal notranslate"><span class="pre">Micro</span></code> lib corresponding to the Cortex-M architecture</p></li>
<li><p>Integrate and compile the obtained inference code and <code class="docutils literal notranslate"><span class="pre">Micro</span></code> lib, and deploy verification</p>
<p>On the Windows platform, we demonstrate how to develop inference code through IAR, while on the Linux platform, we demonstrate how to develop inference code through MakeFile cross-compilation.</p>
</li>
</ul>
</section>
<section id="generating-mcu-inference-code">
<h3>Generating MCU Inference Code<a class="headerlink" href="#generating-mcu-inference-code" title="Permalink to this headline"></a></h3>
<p>Generate inference code for the MCU. For details, see the chapter <span class="xref myst">Generating Model Inference Code</span>. You only need to change <code class="docutils literal notranslate"><span class="pre">target=x86</span></code> in the Micro configuration item to <code class="docutils literal notranslate"><span class="pre">target=Cortex-M</span></code> to generate inference code for the MCU.
After the code is generated, the contents of the folder are as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mnist                          # Specified name of generated code root directory
├── benchmark                  # Benchmark routines for integrated calls to model inference code
│   ├── benchmark.c
│   ├── calib_output.c
│   ├── calib_output.h
│   ├── data.c
│   ├── data.h
│   ├── load_input.c
│   └── load_input.h
├── build.sh                   # One-click compilation script
├── CMakeLists.txt             # cmake project file of the benchmark routine
├── cortex-m7.toolchain.cmake  # Cross-compilation cmake file for cortex-m7
└── src                        # Model inference code directory
    ├── CMakeLists.txt
    ├── context.c
    ├── context.h
    ├── model.c
    ├── net.c
    ├── net.cmake
    ├── net.h
    ├── tensor.c
    ├── tensor.h
    ├── weight.c
    └── weight.h
</pre></div>
</div>
</section>
<section id="downloading-micro-lib-of-cortex-m-architecture">
<h3>Downloading <code class="docutils literal notranslate"><span class="pre">Micro</span></code> Lib of Cortex-M Architecture<a class="headerlink" href="#downloading-micro-lib-of-cortex-m-architecture" title="Permalink to this headline"></a></h3>
<p>The STM32F767 uses the Cortex-M7 architecture. You can obtain the <code class="docutils literal notranslate"><span class="pre">Micro</span></code> lib of the architecture in either of the following ways:</p>
<ul>
<li><p>Download <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/downloads.html">Release Version</a> from the MindSpore official website.</p>
<p>You need to download the release package whose OS is None and hardware platform is Cortex-M7.</p>
</li>
<li><p>Start from the source code for <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/build.html">Building MindSpore Lite</a>.</p>
<p>You can run the <code class="docutils literal notranslate"><span class="pre">MSLITE_MICRO_PLATFORM=cortex-m7</span> <span class="pre">bash</span> <span class="pre">build.sh</span> <span class="pre">-I</span> <span class="pre">x86_64</span></code> command to compile the Cortex-M7 release package.</p>
</li>
</ul>
<p>For other Cortex-M architecture platforms that do not provide release packages for download, you can modify MindSpore source code and manually compile the code to obtain the release package by referring to the method of compiling and building from source code.</p>
</section>
<section id="code-integration-and-compilation-deployment-on-windows-integrated-development-through-iar">
<h3>Code Integration and Compilation Deployment on Windows: Integrated Development Through IAR<a class="headerlink" href="#code-integration-and-compilation-deployment-on-windows-integrated-development-through-iar" title="Permalink to this headline"></a></h3>
<p>This example shows code integration and burning through IAR and demonstrates how to develop the generated inference code in Windows. The main steps are as follows:</p>
<ul class="simple">
<li><p>Download the required software and prepare the integrated environment.</p></li>
<li><p>Generate the required MCU startup code and demonstration project by using the <code class="docutils literal notranslate"><span class="pre">STM32CubeMX</span></code> software.</p></li>
<li><p>Integrate model inference code and <code class="docutils literal notranslate"><span class="pre">Micro</span></code> lib within <code class="docutils literal notranslate"><span class="pre">IAR</span></code>.</p></li>
<li><p>Perform complilation and simulation run</p></li>
</ul>
<section id="environment-preparation">
<h4>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://www.st.com/en/development-tools/stm32cubemx.html">STM32CubeMX Windows Version</a> &gt;= 6.0.1</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">STM32CubeMX</span></code> is a graphical configuration tool for STM32 chips provided by <code class="docutils literal notranslate"><span class="pre">STM</span></code>. This tool is used to generate the startup code and project of STM chips.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.iar.com/ewarm">IAR EWARM</a> &gt;= 9.1</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">IAR</span> <span class="pre">EWARM</span></code> is an integrated development environment developed by IAR Systems for ARM microprocessors.</p></li>
</ul>
</li>
</ul>
</section>
<section id="obtaining-the-mcu-startup-code-and-project">
<h4>Obtaining the MCU Startup Code and Project<a class="headerlink" href="#obtaining-the-mcu-startup-code-and-project" title="Permalink to this headline"></a></h4>
<p>If you have an MCU project, skip this chapter.
This chapter uses the STM32F767 startup project as an example to describe how to generate an MCU project for an STM32 chip via <code class="docutils literal notranslate"><span class="pre">STM32CubeMX</span></code>.</p>
<ul class="simple">
<li><p>Start <code class="docutils literal notranslate"><span class="pre">STM32CubeMX</span></code> and select <code class="docutils literal notranslate"><span class="pre">New</span> <span class="pre">Project</span></code> from <code class="docutils literal notranslate"><span class="pre">File</span></code> to create a project.</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">MCU/MPU</span> <span class="pre">Selector</span></code> window, search for and select <code class="docutils literal notranslate"><span class="pre">STM32F767IGT6</span></code>, and click <code class="docutils literal notranslate"><span class="pre">Start</span> <span class="pre">Project</span></code> to create a project for the chip</p></li>
<li><p>On the <code class="docutils literal notranslate"><span class="pre">Project</span> <span class="pre">Manager</span></code> page, configure the project name and the path of the generated project, and select <code class="docutils literal notranslate"><span class="pre">EWARM</span></code> in <code class="docutils literal notranslate"><span class="pre">Toolchain</span> <span class="pre">/</span> <span class="pre">IDE</span></code> to generate the IAR project</p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">GENERATE</span> <span class="pre">CODE</span></code> above to generate code</p></li>
<li><p>On the PC where the <code class="docutils literal notranslate"><span class="pre">IAR</span></code> is installed, double-click <code class="docutils literal notranslate"><span class="pre">Project.eww</span></code> in the <code class="docutils literal notranslate"><span class="pre">EWARM</span></code> directory of the generated project to open the IAR project.</p></li>
</ul>
</section>
<section id="integrating-model-inference-code-and-micro-lib">
<h4>Integrating Model Inference Code and <code class="docutils literal notranslate"><span class="pre">Micro</span></code> Lib<a class="headerlink" href="#integrating-model-inference-code-and-micro-lib" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Copy the generated inference code to the project, decompress the package obtained in <span class="xref myst">Downloading <code class="docutils literal notranslate"><span class="pre">Micro</span></code> Lib of Cortex-M Architecture</span>, and place it in the generated inference code directory, as shown in the following figure:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>test_stm32f767                                   # MCU project directory
├── Core
│   ├── Inc
│   └── Src
│       ├── main.c
│       └── ...
├── Drivers
├── EWARM                                        # IAR project file directory
└── mnist                                        # Generated Code Root Directory
    ├── benchmark                                # Benchmark routines for integrated calls to model inference code
    │   ├── benchmark.c
    │   ├── data.c
    │   ├── data.h
    │   └── ...
    │── mindspore-lite-1.8.0-none-cortex-m7      # Downloaded Cortex-M7 Architecture `Micro` Lib
    ├── src                                      # Model inference code directory
    └── ...
</pre></div>
</div>
</li>
<li><p>Import source files to the IAR project</p>
<p>Open the IAR project. On the <code class="docutils literal notranslate"><span class="pre">Workspace</span></code> page, right-click the project, choose <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">&gt;</span> <span class="pre">Add</span> <span class="pre">Group</span></code> and add a <code class="docutils literal notranslate"><span class="pre">mnist</span></code> group. Right-click the group and repeat the operations to create groups <code class="docutils literal notranslate"><span class="pre">src</span></code> and <code class="docutils literal notranslate"><span class="pre">benchmark</span></code>.
Choose <code class="docutils literal notranslate"><span class="pre">Add</span> <span class="pre">-&gt;</span> <span class="pre">Add</span> <span class="pre">Files</span></code> to import the source files in the <code class="docutils literal notranslate"><span class="pre">src</span></code> and <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> directories in the <code class="docutils literal notranslate"><span class="pre">mnist</span></code> folder to the groups.</p>
</li>
<li><p>Add the dependent header file path and static library</p>
<p>On the <code class="docutils literal notranslate"><span class="pre">Workspace</span></code> page, right-click the project and choose <code class="docutils literal notranslate"><span class="pre">Options</span></code> from the shortcut menu. Select <code class="docutils literal notranslate"><span class="pre">C/C++</span> <span class="pre">Compiler</span></code> in the left pane of the project options window. In the right pane, select <code class="docutils literal notranslate"><span class="pre">Preprocessor</span></code> and add the path of the header file on which the inference code depends to the list. In this example, the path of the header file is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$PROJ_DIR$/../mnist/mindspore-lite-1.8.0-none-cortex-m7/runtime
$PROJ_DIR$/../mnist/mindspore-lite-1.8.0-none-cortex-m7/runtime/include
$PROJ_DIR$/../mnist/mindspore-lite-1.8.0-none-cortex-m7/tools/codegen/include
$PROJ_DIR$/../mnist/mindspore-lite-1.8.0-none-cortex-m7/tools/codegen/third_party/include/CMSIS/Core
$PROJ_DIR$/../mnist/mindspore-lite-1.8.0-none-cortex-m7/tools/codegen/third_party/include/CMSIS/DSP
$PROJ_DIR$/../mnist/mindspore-lite-1.8.0-none-cortex-m7/tools/codegen/third_party/include/CMSIS/NN
$PROJ_DIR$/../mnist
</pre></div>
</div>
<p>In the left pane of the Project Options window, select <code class="docutils literal notranslate"><span class="pre">Linker</span></code>. In the right pane, select <code class="docutils literal notranslate"><span class="pre">Library</span></code> and add the operator static library file on which the inference code depends to the list. The static library file added in this example is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$PROJ_DIR$/../mnist/mindspore-lite-1.8.0-none-cortex-m7/tools/codegen/lib/libwrapper.a
$PROJ_DIR$/../mnist/mindspore-lite-1.8.0-none-cortex-m7/tools/codegen/lib/libnnacl.a
$PROJ_DIR$/../mnist/mindspore-lite-1.8.0-none-cortex-m7/tools/codegen/third_party/lib/libcmsis_nn.a  
</pre></div>
</div>
</li>
<li><p>Modify the main.c file and invoke the benchmark function</p>
<p>Add a header file reference at the beginning of <code class="docutils literal notranslate"><span class="pre">main.c</span></code> and invoke the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> function in <code class="docutils literal notranslate"><span class="pre">benchmark.c</span></code> in the main function. The program in the benchmark folder is a sample program that invokes the inference code in the generated <code class="docutils literal notranslate"><span class="pre">src</span></code> and compares the output, which can be modified freely.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;benchmark/benchmark.h&quot;</span>
<span class="p">...</span>
<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">  </span><span class="p">...</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">benchmark</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">run success.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">run failed.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="p">...</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Modify the <code class="docutils literal notranslate"><span class="pre">mnist/benchmark/data.c</span></code> file to store benchmark input and output data in the program for comparison</p>
<p>In the benchmark routine, the input data of the model is set and the inference result is compared with the expected result to obtain the error offset.
In this example, set the input data of the model by modifying the <code class="docutils literal notranslate"><span class="pre">calib_input0_data</span></code> array of <code class="docutils literal notranslate"><span class="pre">data.c</span></code>, and set the expected result by modifying the <code class="docutils literal notranslate"><span class="pre">calib_output0_data</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">float</span><span class="w"> </span><span class="n">calib_input0_data</span><span class="p">[</span><span class="n">NET_INPUT0_SIZE</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">0.54881352186203</span><span class="p">,</span><span class="mf">0.7151893377304077</span><span class="p">,</span><span class="mf">0.6027633547782898</span><span class="p">,</span><span class="mf">0.5448831915855408</span><span class="p">,</span><span class="mf">0.42365479469299316</span><span class="p">,</span><span class="mf">0.6458941102027893</span><span class="p">,</span><span class="mf">0.4375872015953064</span><span class="p">,</span><span class="mf">0.891772985458374</span><span class="p">,</span><span class="mf">0.9636627435684204</span><span class="p">,</span><span class="mf">0.3834415078163147</span><span class="p">,</span><span class="mf">0.7917250394821167</span><span class="p">,</span><span class="mf">0.5288949012756348</span><span class="p">,</span><span class="mf">0.5680445432662964</span><span class="p">,</span><span class="mf">0.9255966544151306</span><span class="p">,</span><span class="mf">0.07103605568408966</span><span class="p">,</span><span class="mf">0.08712930232286453</span><span class="p">,</span><span class="mf">0.020218396559357643</span><span class="p">,</span><span class="mf">0.832619845867157</span><span class="p">,</span><span class="mf">0.7781567573547363</span><span class="p">,</span><span class="mf">0.8700121641159058</span><span class="p">,</span><span class="mf">0.978618323802948</span><span class="p">,</span><span class="mf">0.7991585731506348</span><span class="p">,</span><span class="mf">0.4614793658256531</span><span class="p">,</span><span class="mf">0.7805292010307312</span><span class="p">,</span><span class="mf">0.11827442795038223</span><span class="p">,</span><span class="mf">0.6399210095405579</span><span class="p">,</span><span class="mf">0.14335328340530396</span><span class="p">,</span><span class="mf">0.9446688890457153</span><span class="p">,</span><span class="mf">0.5218483209609985</span><span class="p">,</span><span class="mf">0.4146619439125061</span><span class="p">,</span><span class="mf">0.26455560326576233</span><span class="p">,</span><span class="mf">0.7742336988449097</span><span class="p">,</span><span class="mf">0.4561503231525421</span><span class="p">,</span><span class="mf">0.568433940410614</span><span class="p">,</span><span class="mf">0.018789799883961678</span><span class="p">,</span><span class="mf">0.6176354885101318</span><span class="p">,</span><span class="mf">0.6120957136154175</span><span class="p">,</span><span class="mf">0.6169340014457703</span><span class="p">,</span><span class="mf">0.9437480568885803</span><span class="p">,</span><span class="mf">0.681820273399353</span><span class="p">,</span><span class="mf">0.35950788855552673</span><span class="p">,</span><span class="mf">0.43703195452690125</span><span class="p">,</span><span class="mf">0.6976311802864075</span><span class="p">,</span><span class="mf">0.0602254718542099</span><span class="p">,</span><span class="mf">0.6667667031288147</span><span class="p">,</span><span class="mf">0.670637845993042</span><span class="p">,</span><span class="mf">0.21038256585597992</span><span class="p">,</span><span class="mf">0.12892629206180573</span><span class="p">,</span><span class="mf">0.31542834639549255</span><span class="p">,</span><span class="mf">0.36371076107025146</span><span class="p">,</span><span class="mf">0.5701967477798462</span><span class="p">,</span><span class="mf">0.4386015236377716</span><span class="p">,</span><span class="mf">0.9883738160133362</span><span class="p">,</span><span class="mf">0.10204481333494186</span><span class="p">,</span><span class="mf">0.20887675881385803</span><span class="p">,</span><span class="mf">0.16130951046943665</span><span class="p">,</span><span class="mf">0.6531082987785339</span><span class="p">,</span><span class="mf">0.25329160690307617</span><span class="p">,</span><span class="mf">0.4663107693195343</span><span class="p">,</span><span class="mf">0.24442559480667114</span><span class="p">,</span><span class="mf">0.15896958112716675</span><span class="p">,</span><span class="mf">0.11037514358758926</span><span class="p">,</span><span class="mf">0.6563295722007751</span><span class="p">,</span><span class="mf">0.13818295300006866</span><span class="p">,</span><span class="mf">0.1965823620557785</span><span class="p">,</span><span class="mf">0.3687251806259155</span><span class="p">,</span><span class="mf">0.8209932446479797</span><span class="p">,</span><span class="mf">0.09710127860307693</span><span class="p">,</span><span class="mf">0.8379449248313904</span><span class="p">,</span><span class="mf">0.0960984081029892</span><span class="p">,</span><span class="mf">0.9764594435691833</span><span class="p">,</span><span class="mf">0.4686512053012848</span><span class="p">,</span><span class="mf">0.9767611026763916</span><span class="p">,</span><span class="mf">0.6048455238342285</span><span class="p">,</span><span class="mf">0.7392635941505432</span><span class="p">,</span><span class="mf">0.03918779268860817</span><span class="p">,</span><span class="mf">0.28280696272850037</span><span class="p">,</span><span class="mf">0.12019655853509903</span><span class="p">,</span><span class="mf">0.296140193939209</span><span class="p">,</span><span class="mf">0.11872772127389908</span><span class="p">,</span><span class="mf">0.3179831802845001</span><span class="p">,</span><span class="mf">0.414262980222702</span><span class="p">,</span><span class="mf">0.06414749473333359</span><span class="p">,</span><span class="mf">0.6924721002578735</span><span class="p">,</span><span class="mf">0.5666014552116394</span><span class="p">,</span><span class="mf">0.26538950204849243</span><span class="p">,</span><span class="mf">0.5232480764389038</span><span class="p">,</span><span class="mf">0.09394051134586334</span><span class="p">,</span><span class="mf">0.5759465098381042</span><span class="p">,</span><span class="mf">0.9292961955070496</span><span class="p">,</span><span class="mf">0.3185689449310303</span><span class="p">,</span><span class="mf">0.6674103736877441</span><span class="p">,</span><span class="mf">0.13179786503314972</span><span class="p">,</span><span class="mf">0.7163271903991699</span><span class="p">,</span><span class="mf">0.28940609097480774</span><span class="p">,</span><span class="mf">0.18319135904312134</span><span class="p">,</span><span class="mf">0.5865129232406616</span><span class="p">,</span><span class="mf">0.02010754682123661</span><span class="p">,</span><span class="mf">0.8289400339126587</span><span class="p">,</span><span class="mf">0.004695476032793522</span><span class="p">,</span><span class="mf">0.6778165102005005</span><span class="p">,</span><span class="mf">0.2700079679489136</span><span class="p">,</span><span class="mf">0.7351940274238586</span><span class="p">,</span><span class="mf">0.9621885418891907</span><span class="p">,</span><span class="mf">0.2487531453371048</span><span class="p">,</span><span class="mf">0.5761573314666748</span><span class="p">,</span><span class="mf">0.5920419096946716</span><span class="p">,</span><span class="mf">0.5722519159317017</span><span class="p">,</span><span class="mf">0.22308163344860077</span><span class="p">,</span><span class="mf">0.9527490139007568</span><span class="p">,</span><span class="mf">0.4471253752708435</span><span class="p">,</span><span class="mf">0.8464086651802063</span><span class="p">,</span><span class="mf">0.6994792819023132</span><span class="p">,</span><span class="mf">0.2974369525909424</span><span class="p">,</span><span class="mf">0.8137978315353394</span><span class="p">,</span><span class="mf">0.396505743265152</span><span class="p">,</span><span class="mf">0.8811032176017761</span><span class="p">,</span><span class="mf">0.5812729001045227</span><span class="p">,</span><span class="mf">0.8817353844642639</span><span class="p">,</span><span class="mf">0.6925315856933594</span><span class="p">,</span><span class="mf">0.7252542972564697</span><span class="p">,</span><span class="mf">0.5013243556022644</span><span class="p">,</span><span class="mf">0.9560836553573608</span><span class="p">,</span><span class="mf">0.6439902186393738</span><span class="p">,</span><span class="mf">0.4238550364971161</span><span class="p">,</span><span class="mf">0.6063932180404663</span><span class="p">,</span><span class="mf">0.019193198531866074</span><span class="p">,</span><span class="mf">0.30157482624053955</span><span class="p">,</span><span class="mf">0.6601735353469849</span><span class="p">,</span><span class="mf">0.2900775969028473</span><span class="p">,</span><span class="mf">0.6180154085159302</span><span class="p">,</span><span class="mf">0.42876869440078735</span><span class="p">,</span><span class="mf">0.1354740709066391</span><span class="p">,</span><span class="mf">0.29828232526779175</span><span class="p">,</span><span class="mf">0.5699648857116699</span><span class="p">,</span><span class="mf">0.5908727645874023</span><span class="p">,</span><span class="mf">0.5743252635002136</span><span class="p">,</span><span class="mf">0.6532008051872253</span><span class="p">,</span><span class="mf">0.6521032452583313</span><span class="p">,</span><span class="mf">0.43141844868659973</span><span class="p">,</span><span class="mf">0.8965466022491455</span><span class="p">,</span><span class="mf">0.36756187677383423</span><span class="p">,</span><span class="mf">0.4358649253845215</span><span class="p">,</span><span class="mf">0.8919233679771423</span><span class="p">,</span><span class="mf">0.806194007396698</span><span class="p">,</span><span class="mf">0.7038885951042175</span><span class="p">,</span><span class="mf">0.10022688657045364</span><span class="p">,</span><span class="mf">0.9194825887680054</span><span class="p">,</span><span class="mf">0.7142413258552551</span><span class="p">,</span><span class="mf">0.9988470077514648</span><span class="p">,</span><span class="mf">0.14944830536842346</span><span class="p">,</span><span class="mf">0.8681260347366333</span><span class="p">,</span><span class="mf">0.16249293088912964</span><span class="p">,</span><span class="mf">0.6155595779418945</span><span class="p">,</span><span class="mf">0.1238199844956398</span><span class="p">,</span><span class="mf">0.8480082154273987</span><span class="p">,</span><span class="mf">0.8073189854621887</span><span class="p">,</span><span class="mf">0.5691007375717163</span><span class="p">,</span><span class="mf">0.40718328952789307</span><span class="p">,</span><span class="mf">0.06916699558496475</span><span class="p">,</span><span class="mf">0.6974287629127502</span><span class="p">,</span><span class="mf">0.45354267954826355</span><span class="p">,</span><span class="mf">0.7220556139945984</span><span class="p">,</span><span class="mf">0.8663823008537292</span><span class="p">,</span><span class="mf">0.9755215048789978</span><span class="p">,</span><span class="mf">0.855803370475769</span><span class="p">,</span><span class="mf">0.011714084073901176</span><span class="p">,</span><span class="mf">0.359978049993515</span><span class="p">,</span><span class="mf">0.729990541934967</span><span class="p">,</span><span class="mf">0.17162968218326569</span><span class="p">,</span><span class="mf">0.5210366249084473</span><span class="p">,</span><span class="mf">0.054337989538908005</span><span class="p">,</span><span class="mf">0.19999653100967407</span><span class="p">,</span><span class="mf">0.01852179504930973</span><span class="p">,</span><span class="mf">0.793697714805603</span><span class="p">,</span><span class="mf">0.2239246815443039</span><span class="p">,</span><span class="mf">0.3453516662120819</span><span class="p">,</span><span class="mf">0.9280812740325928</span><span class="p">,</span><span class="mf">0.704414427280426</span><span class="p">,</span><span class="mf">0.031838931143283844</span><span class="p">,</span><span class="mf">0.1646941602230072</span><span class="p">,</span><span class="mf">0.6214783787727356</span><span class="p">,</span><span class="mf">0.5772286057472229</span><span class="p">,</span><span class="mf">0.23789282143115997</span><span class="p">,</span><span class="mf">0.9342139959335327</span><span class="p">,</span><span class="mf">0.6139659285545349</span><span class="p">,</span><span class="mf">0.5356327891349792</span><span class="p">,</span><span class="mf">0.5899099707603455</span><span class="p">,</span><span class="mf">0.7301220297813416</span><span class="p">,</span><span class="mf">0.31194499135017395</span><span class="p">,</span><span class="mf">0.39822107553482056</span><span class="p">,</span><span class="mf">0.20984375476837158</span><span class="p">,</span><span class="mf">0.18619300425052643</span><span class="p">,</span><span class="mf">0.9443724155426025</span><span class="p">,</span><span class="mf">0.739550769329071</span><span class="p">,</span><span class="mf">0.49045881628990173</span><span class="p">,</span><span class="mf">0.22741462290287018</span><span class="p">,</span><span class="mf">0.2543564736843109</span><span class="p">,</span><span class="mf">0.058029159903526306</span><span class="p">,</span><span class="mf">0.43441662192344666</span><span class="p">,</span><span class="mf">0.3117958903312683</span><span class="p">,</span><span class="mf">0.6963434815406799</span><span class="p">,</span><span class="mf">0.37775182723999023</span><span class="p">,</span><span class="mf">0.1796036809682846</span><span class="p">,</span><span class="mf">0.024678727611899376</span><span class="p">,</span><span class="mf">0.06724963337182999</span><span class="p">,</span><span class="mf">0.6793927550315857</span><span class="p">,</span><span class="mf">0.4536968469619751</span><span class="p">,</span><span class="mf">0.5365791916847229</span><span class="p">,</span><span class="mf">0.8966712951660156</span><span class="p">,</span><span class="mf">0.990338921546936</span><span class="p">,</span><span class="mf">0.21689698100090027</span><span class="p">,</span><span class="mf">0.6630781888961792</span><span class="p">,</span><span class="mf">0.2633223831653595</span><span class="p">,</span><span class="mf">0.02065099962055683</span><span class="p">,</span><span class="mf">0.7583786249160767</span><span class="p">,</span><span class="mf">0.32001715898513794</span><span class="p">,</span><span class="mf">0.38346388936042786</span><span class="p">,</span><span class="mf">0.5883170962333679</span><span class="p">,</span><span class="mf">0.8310484290122986</span><span class="p">,</span><span class="mf">0.6289818286895752</span><span class="p">,</span><span class="mf">0.872650682926178</span><span class="p">,</span><span class="mf">0.27354204654693604</span><span class="p">,</span><span class="mf">0.7980468273162842</span><span class="p">,</span><span class="mf">0.18563593924045563</span><span class="p">,</span><span class="mf">0.9527916312217712</span><span class="p">,</span><span class="mf">0.6874882578849792</span><span class="p">,</span><span class="mf">0.21550767123699188</span><span class="p">,</span><span class="mf">0.9473705887794495</span><span class="p">,</span><span class="mf">0.7308558225631714</span><span class="p">,</span><span class="mf">0.2539416551589966</span><span class="p">,</span><span class="mf">0.21331197023391724</span><span class="p">,</span><span class="mf">0.518200695514679</span><span class="p">,</span><span class="mf">0.02566271834075451</span><span class="p">,</span><span class="mf">0.20747007429599762</span><span class="p">,</span><span class="mf">0.4246854782104492</span><span class="p">,</span><span class="mf">0.3741699755191803</span><span class="p">,</span><span class="mf">0.46357542276382446</span><span class="p">,</span><span class="mf">0.27762871980667114</span><span class="p">,</span><span class="mf">0.5867843627929688</span><span class="p">,</span><span class="mf">0.8638556003570557</span><span class="p">,</span><span class="mf">0.11753185838460922</span><span class="p">,</span><span class="mf">0.517379105091095</span><span class="p">,</span><span class="mf">0.13206811249256134</span><span class="p">,</span><span class="mf">0.7168596982955933</span><span class="p">,</span><span class="mf">0.39605969190597534</span><span class="p">,</span><span class="mf">0.5654212832450867</span><span class="p">,</span><span class="mf">0.1832798421382904</span><span class="p">,</span><span class="mf">0.14484776556491852</span><span class="p">,</span><span class="mf">0.4880562722682953</span><span class="p">,</span><span class="mf">0.35561272501945496</span><span class="p">,</span><span class="mf">0.9404319524765015</span><span class="p">,</span><span class="mf">0.7653252482414246</span><span class="p">,</span><span class="mf">0.748663604259491</span><span class="p">,</span><span class="mf">0.9037197232246399</span><span class="p">,</span><span class="mf">0.08342243731021881</span><span class="p">,</span><span class="mf">0.5521924495697021</span><span class="p">,</span><span class="mf">0.5844760537147522</span><span class="p">,</span><span class="mf">0.961936354637146</span><span class="p">,</span><span class="mf">0.29214751720428467</span><span class="p">,</span><span class="mf">0.24082878232002258</span><span class="p">,</span><span class="mf">0.10029394179582596</span><span class="p">,</span><span class="mf">0.016429629176855087</span><span class="p">,</span><span class="mf">0.9295293092727661</span><span class="p">,</span><span class="mf">0.669916570186615</span><span class="p">,</span><span class="mf">0.7851529121398926</span><span class="p">,</span><span class="mf">0.28173011541366577</span><span class="p">,</span><span class="mf">0.5864101648330688</span><span class="p">,</span><span class="mf">0.06395526975393295</span><span class="p">,</span><span class="mf">0.48562759160995483</span><span class="p">,</span><span class="mf">0.9774951338768005</span><span class="p">,</span><span class="mf">0.8765052556991577</span><span class="p">,</span><span class="mf">0.3381589651107788</span><span class="p">,</span><span class="mf">0.961570143699646</span><span class="p">,</span><span class="mf">0.23170162737369537</span><span class="p">,</span><span class="mf">0.9493188261985779</span><span class="p">,</span><span class="mf">0.9413776993751526</span><span class="p">,</span><span class="mf">0.799202561378479</span><span class="p">,</span><span class="mf">0.6304479241371155</span><span class="p">,</span><span class="mf">0.8742879629135132</span><span class="p">,</span><span class="mf">0.2930202782154083</span><span class="p">,</span><span class="mf">0.8489435315132141</span><span class="p">,</span><span class="mf">0.6178767085075378</span><span class="p">,</span><span class="mf">0.013236857950687408</span><span class="p">,</span><span class="mf">0.34723350405693054</span><span class="p">,</span><span class="mf">0.14814086258411407</span><span class="p">,</span><span class="mf">0.9818294048309326</span><span class="p">,</span><span class="mf">0.4783703088760376</span><span class="p">,</span><span class="mf">0.49739137291908264</span><span class="p">,</span><span class="mf">0.6394725441932678</span><span class="p">,</span><span class="mf">0.36858460307121277</span><span class="p">,</span><span class="mf">0.13690027594566345</span><span class="p">,</span><span class="mf">0.8221177458763123</span><span class="p">,</span><span class="mf">0.1898479163646698</span><span class="p">,</span><span class="mf">0.5113189816474915</span><span class="p">,</span><span class="mf">0.2243170291185379</span><span class="p">,</span><span class="mf">0.09784448146820068</span><span class="p">,</span><span class="mf">0.8621914982795715</span><span class="p">,</span><span class="mf">0.9729194641113281</span><span class="p">,</span><span class="mf">0.9608346819877625</span><span class="p">,</span><span class="mf">0.9065554738044739</span><span class="p">,</span><span class="mf">0.774047315120697</span><span class="p">,</span><span class="mf">0.3331451416015625</span><span class="p">,</span><span class="mf">0.08110138773918152</span><span class="p">,</span><span class="mf">0.40724116563796997</span><span class="p">,</span><span class="mf">0.2322341352701187</span><span class="p">,</span><span class="mf">0.13248763978481293</span><span class="p">,</span><span class="mf">0.053427182137966156</span><span class="p">,</span><span class="mf">0.7255943417549133</span><span class="p">,</span><span class="mf">0.011427458375692368</span><span class="p">,</span><span class="mf">0.7705807685852051</span><span class="p">,</span><span class="mf">0.14694663882255554</span><span class="p">,</span><span class="mf">0.07952208071947098</span><span class="p">,</span><span class="mf">0.08960303664207458</span><span class="p">,</span><span class="mf">0.6720477938652039</span><span class="p">,</span><span class="mf">0.24536721408367157</span><span class="p">,</span><span class="mf">0.4205394685268402</span><span class="p">,</span><span class="mf">0.557368814945221</span><span class="p">,</span><span class="mf">0.8605511784553528</span><span class="p">,</span><span class="mf">0.7270442843437195</span><span class="p">,</span><span class="mf">0.2703278958797455</span><span class="p">,</span><span class="mf">0.131482794880867</span><span class="p">,</span><span class="mf">0.05537432059645653</span><span class="p">,</span><span class="mf">0.3015986382961273</span><span class="p">,</span><span class="mf">0.2621181607246399</span><span class="p">,</span><span class="mf">0.45614057779312134</span><span class="p">,</span><span class="mf">0.6832813620567322</span><span class="p">,</span><span class="mf">0.6956254243850708</span><span class="p">,</span><span class="mf">0.28351885080337524</span><span class="p">,</span><span class="mf">0.3799269497394562</span><span class="p">,</span><span class="mf">0.18115095794200897</span><span class="p">,</span><span class="mf">0.7885454893112183</span><span class="p">,</span><span class="mf">0.05684807524085045</span><span class="p">,</span><span class="mf">0.6969972252845764</span><span class="p">,</span><span class="mf">0.7786954045295715</span><span class="p">,</span><span class="mf">0.7774075865745544</span><span class="p">,</span><span class="mf">0.25942257046699524</span><span class="p">,</span><span class="mf">0.3738131523132324</span><span class="p">,</span><span class="mf">0.5875996351242065</span><span class="p">,</span><span class="mf">0.27282190322875977</span><span class="p">,</span><span class="mf">0.3708527982234955</span><span class="p">,</span><span class="mf">0.19705428183078766</span><span class="p">,</span><span class="mf">0.4598558843135834</span><span class="p">,</span><span class="mf">0.044612299650907516</span><span class="p">,</span><span class="mf">0.7997958660125732</span><span class="p">,</span><span class="mf">0.07695644348859787</span><span class="p">,</span><span class="mf">0.5188351273536682</span><span class="p">,</span><span class="mf">0.3068101108074188</span><span class="p">,</span><span class="mf">0.5775429606437683</span><span class="p">,</span><span class="mf">0.9594333171844482</span><span class="p">,</span><span class="mf">0.6455702185630798</span><span class="p">,</span><span class="mf">0.03536243736743927</span><span class="p">,</span><span class="mf">0.4304024279117584</span><span class="p">,</span><span class="mf">0.5100168585777283</span><span class="p">,</span><span class="mf">0.5361775159835815</span><span class="p">,</span><span class="mf">0.6813924908638</span><span class="p">,</span><span class="mf">0.2775960862636566</span><span class="p">,</span><span class="mf">0.12886056303977966</span><span class="p">,</span><span class="mf">0.3926756680011749</span><span class="p">,</span><span class="mf">0.9564056992530823</span><span class="p">,</span><span class="mf">0.1871308982372284</span><span class="p">,</span><span class="mf">0.9039839506149292</span><span class="p">,</span><span class="mf">0.5438059568405151</span><span class="p">,</span><span class="mf">0.4569114148616791</span><span class="p">,</span><span class="mf">0.8820413947105408</span><span class="p">,</span><span class="mf">0.45860394835472107</span><span class="p">,</span><span class="mf">0.7241676449775696</span><span class="p">,</span><span class="mf">0.3990253210067749</span><span class="p">,</span><span class="mf">0.9040443897247314</span><span class="p">,</span><span class="mf">0.6900250315666199</span><span class="p">,</span><span class="mf">0.6996220350265503</span><span class="p">,</span><span class="mf">0.32772040367126465</span><span class="p">,</span><span class="mf">0.7567786574363708</span><span class="p">,</span><span class="mf">0.6360610723495483</span><span class="p">,</span><span class="mf">0.2400202751159668</span><span class="p">,</span><span class="mf">0.16053882241249084</span><span class="p">,</span><span class="mf">0.796391487121582</span><span class="p">,</span><span class="mf">0.9591665863990784</span><span class="p">,</span><span class="mf">0.4581388235092163</span><span class="p">,</span><span class="mf">0.5909841656684875</span><span class="p">,</span><span class="mf">0.8577226400375366</span><span class="p">,</span><span class="mf">0.45722344517707825</span><span class="p">,</span><span class="mf">0.9518744945526123</span><span class="p">,</span><span class="mf">0.5757511854171753</span><span class="p">,</span><span class="mf">0.8207671046257019</span><span class="p">,</span><span class="mf">0.9088436961174011</span><span class="p">,</span><span class="mf">0.8155238032341003</span><span class="p">,</span><span class="mf">0.15941447019577026</span><span class="p">,</span><span class="mf">0.6288984417915344</span><span class="p">,</span><span class="mf">0.39843425154685974</span><span class="p">,</span><span class="mf">0.06271295249462128</span><span class="p">,</span><span class="mf">0.4240322411060333</span><span class="p">,</span><span class="mf">0.25868406891822815</span><span class="p">,</span><span class="mf">0.849038302898407</span><span class="p">,</span><span class="mf">0.03330462798476219</span><span class="p">,</span><span class="mf">0.9589827060699463</span><span class="p">,</span><span class="mf">0.35536885261535645</span><span class="p">,</span><span class="mf">0.3567068874835968</span><span class="p">,</span><span class="mf">0.01632850244641304</span><span class="p">,</span><span class="mf">0.18523232638835907</span><span class="p">,</span><span class="mf">0.40125951170921326</span><span class="p">,</span><span class="mf">0.9292914271354675</span><span class="p">,</span><span class="mf">0.0996149331331253</span><span class="p">,</span><span class="mf">0.9453015327453613</span><span class="p">,</span><span class="mf">0.869488537311554</span><span class="p">,</span><span class="mf">0.4541623890399933</span><span class="p">,</span><span class="mf">0.326700896024704</span><span class="p">,</span><span class="mf">0.23274412751197815</span><span class="p">,</span><span class="mf">0.6144647002220154</span><span class="p">,</span><span class="mf">0.03307459130883217</span><span class="p">,</span><span class="mf">0.015606064349412918</span><span class="p">,</span><span class="mf">0.428795725107193</span><span class="p">,</span><span class="mf">0.06807407736778259</span><span class="p">,</span><span class="mf">0.2519409954547882</span><span class="p">,</span><span class="mf">0.2211609184741974</span><span class="p">,</span><span class="mf">0.253191202878952</span><span class="p">,</span><span class="mf">0.13105523586273193</span><span class="p">,</span><span class="mf">0.012036222964525223</span><span class="p">,</span><span class="mf">0.11548429727554321</span><span class="p">,</span><span class="mf">0.6184802651405334</span><span class="p">,</span><span class="mf">0.9742562174797058</span><span class="p">,</span><span class="mf">0.9903450012207031</span><span class="p">,</span><span class="mf">0.40905410051345825</span><span class="p">,</span><span class="mf">0.1629544198513031</span><span class="p">,</span><span class="mf">0.6387617588043213</span><span class="p">,</span><span class="mf">0.4903053343296051</span><span class="p">,</span><span class="mf">0.9894098043441772</span><span class="p">,</span><span class="mf">0.06530420482158661</span><span class="p">,</span><span class="mf">0.7832344174385071</span><span class="p">,</span><span class="mf">0.28839850425720215</span><span class="p">,</span><span class="mf">0.24141861498355865</span><span class="p">,</span><span class="mf">0.6625045537948608</span><span class="p">,</span><span class="mf">0.24606318771839142</span><span class="p">,</span><span class="mf">0.6658591032028198</span><span class="p">,</span><span class="mf">0.5173085331916809</span><span class="p">,</span><span class="mf">0.4240889847278595</span><span class="p">,</span><span class="mf">0.5546877980232239</span><span class="p">,</span><span class="mf">0.2870515286922455</span><span class="p">,</span><span class="mf">0.7065746784210205</span><span class="p">,</span><span class="mf">0.414856880903244</span><span class="p">,</span><span class="mf">0.3605455458164215</span><span class="p">,</span><span class="mf">0.8286569118499756</span><span class="p">,</span><span class="mf">0.9249669313430786</span><span class="p">,</span><span class="mf">0.04600730910897255</span><span class="p">,</span><span class="mf">0.2326269894838333</span><span class="p">,</span><span class="mf">0.34851935505867004</span><span class="p">,</span><span class="mf">0.8149664998054504</span><span class="p">,</span><span class="mf">0.9854914546012878</span><span class="p">,</span><span class="mf">0.9689717292785645</span><span class="p">,</span><span class="mf">0.904948353767395</span><span class="p">,</span><span class="mf">0.2965562641620636</span><span class="p">,</span><span class="mf">0.9920112490653992</span><span class="p">,</span><span class="mf">0.24942004680633545</span><span class="p">,</span><span class="mf">0.10590615123510361</span><span class="p">,</span><span class="mf">0.9509525895118713</span><span class="p">,</span><span class="mf">0.2334202527999878</span><span class="p">,</span><span class="mf">0.6897682547569275</span><span class="p">,</span><span class="mf">0.05835635960102081</span><span class="p">,</span><span class="mf">0.7307090759277344</span><span class="p">,</span><span class="mf">0.8817201852798462</span><span class="p">,</span><span class="mf">0.27243688702583313</span><span class="p">,</span><span class="mf">0.3790569007396698</span><span class="p">,</span><span class="mf">0.3742961883544922</span><span class="p">,</span><span class="mf">0.7487882375717163</span><span class="p">,</span><span class="mf">0.2378072440624237</span><span class="p">,</span><span class="mf">0.17185309529304504</span><span class="p">,</span><span class="mf">0.4492916464805603</span><span class="p">,</span><span class="mf">0.30446839332580566</span><span class="p">,</span><span class="mf">0.8391891121864319</span><span class="p">,</span><span class="mf">0.23774182796478271</span><span class="p">,</span><span class="mf">0.5023894309997559</span><span class="p">,</span><span class="mf">0.9425836205482483</span><span class="p">,</span><span class="mf">0.6339976787567139</span><span class="p">,</span><span class="mf">0.8672894239425659</span><span class="p">,</span><span class="mf">0.940209686756134</span><span class="p">,</span><span class="mf">0.7507648468017578</span><span class="p">,</span><span class="mf">0.6995750665664673</span><span class="p">,</span><span class="mf">0.9679655432701111</span><span class="p">,</span><span class="mf">0.9944007992744446</span><span class="p">,</span><span class="mf">0.4518216848373413</span><span class="p">,</span><span class="mf">0.07086978107690811</span><span class="p">,</span><span class="mf">0.29279401898384094</span><span class="p">,</span><span class="mf">0.15235470235347748</span><span class="p">,</span><span class="mf">0.41748636960983276</span><span class="p">,</span><span class="mf">0.13128933310508728</span><span class="p">,</span><span class="mf">0.6041178107261658</span><span class="p">,</span><span class="mf">0.38280805945396423</span><span class="p">,</span><span class="mf">0.8953858613967896</span><span class="p">,</span><span class="mf">0.96779465675354</span><span class="p">,</span><span class="mf">0.5468848943710327</span><span class="p">,</span><span class="mf">0.2748235762119293</span><span class="p">,</span><span class="mf">0.5922304391860962</span><span class="p">,</span><span class="mf">0.8967611789703369</span><span class="p">,</span><span class="mf">0.40673333406448364</span><span class="p">,</span><span class="mf">0.5520782470703125</span><span class="p">,</span><span class="mf">0.2716527581214905</span><span class="p">,</span><span class="mf">0.4554441571235657</span><span class="p">,</span><span class="mf">0.4017135500907898</span><span class="p">,</span><span class="mf">0.24841345846652985</span><span class="p">,</span><span class="mf">0.5058664083480835</span><span class="p">,</span><span class="mf">0.31038081645965576</span><span class="p">,</span><span class="mf">0.37303486466407776</span><span class="p">,</span><span class="mf">0.5249704718589783</span><span class="p">,</span><span class="mf">0.7505950331687927</span><span class="p">,</span><span class="mf">0.3335074782371521</span><span class="p">,</span><span class="mf">0.9241587519645691</span><span class="p">,</span><span class="mf">0.8623185753822327</span><span class="p">,</span><span class="mf">0.048690296709537506</span><span class="p">,</span><span class="mf">0.2536425292491913</span><span class="p">,</span><span class="mf">0.4461355209350586</span><span class="p">,</span><span class="mf">0.10462789237499237</span><span class="p">,</span><span class="mf">0.34847599267959595</span><span class="p">,</span><span class="mf">0.7400975227355957</span><span class="p">,</span><span class="mf">0.6805144548416138</span><span class="p">,</span><span class="mf">0.6223844289779663</span><span class="p">,</span><span class="mf">0.7105283737182617</span><span class="p">,</span><span class="mf">0.20492368936538696</span><span class="p">,</span><span class="mf">0.3416981101036072</span><span class="p">,</span><span class="mf">0.676242470741272</span><span class="p">,</span><span class="mf">0.879234790802002</span><span class="p">,</span><span class="mf">0.5436780452728271</span><span class="p">,</span><span class="mf">0.2826996445655823</span><span class="p">,</span><span class="mf">0.030235258862376213</span><span class="p">,</span><span class="mf">0.7103368043899536</span><span class="p">,</span><span class="mf">0.007884103804826736</span><span class="p">,</span><span class="mf">0.37267908453941345</span><span class="p">,</span><span class="mf">0.5305371880531311</span><span class="p">,</span><span class="mf">0.922111451625824</span><span class="p">,</span><span class="mf">0.08949454873800278</span><span class="p">,</span><span class="mf">0.40594232082366943</span><span class="p">,</span><span class="mf">0.024313200265169144</span><span class="p">,</span><span class="mf">0.3426109850406647</span><span class="p">,</span><span class="mf">0.6222310662269592</span><span class="p">,</span><span class="mf">0.2790679335594177</span><span class="p">,</span><span class="mf">0.2097499519586563</span><span class="p">,</span><span class="mf">0.11570323258638382</span><span class="p">,</span><span class="mf">0.5771402716636658</span><span class="p">,</span><span class="mf">0.6952700018882751</span><span class="p">,</span><span class="mf">0.6719571352005005</span><span class="p">,</span><span class="mf">0.9488610029220581</span><span class="p">,</span><span class="mf">0.002703213831409812</span><span class="p">,</span><span class="mf">0.6471966505050659</span><span class="p">,</span><span class="mf">0.60039222240448</span><span class="p">,</span><span class="mf">0.5887396335601807</span><span class="p">,</span><span class="mf">0.9627703428268433</span><span class="p">,</span><span class="mf">0.016871673986315727</span><span class="p">,</span><span class="mf">0.6964824199676514</span><span class="p">,</span><span class="mf">0.8136786222457886</span><span class="p">,</span><span class="mf">0.5098071694374084</span><span class="p">,</span><span class="mf">0.33396488428115845</span><span class="p">,</span><span class="mf">0.7908401489257812</span><span class="p">,</span><span class="mf">0.09724292904138565</span><span class="p">,</span><span class="mf">0.44203564524650574</span><span class="p">,</span><span class="mf">0.5199523568153381</span><span class="p">,</span><span class="mf">0.6939564347267151</span><span class="p">,</span><span class="mf">0.09088572859764099</span><span class="p">,</span><span class="mf">0.2277594953775406</span><span class="p">,</span><span class="mf">0.4103015661239624</span><span class="p">,</span><span class="mf">0.6232946515083313</span><span class="p">,</span><span class="mf">0.8869608044624329</span><span class="p">,</span><span class="mf">0.618826150894165</span><span class="p">,</span><span class="mf">0.13346147537231445</span><span class="p">,</span><span class="mf">0.9805801510810852</span><span class="p">,</span><span class="mf">0.8717857599258423</span><span class="p">,</span><span class="mf">0.5027207732200623</span><span class="p">,</span><span class="mf">0.9223479628562927</span><span class="p">,</span><span class="mf">0.5413808226585388</span><span class="p">,</span><span class="mf">0.9233060479164124</span><span class="p">,</span><span class="mf">0.8298973441123962</span><span class="p">,</span><span class="mf">0.968286395072937</span><span class="p">,</span><span class="mf">0.919782817363739</span><span class="p">,</span><span class="mf">0.03603381663560867</span><span class="p">,</span><span class="mf">0.1747720092535019</span><span class="p">,</span><span class="mf">0.3891346752643585</span><span class="p">,</span><span class="mf">0.9521427154541016</span><span class="p">,</span><span class="mf">0.300028920173645</span><span class="p">,</span><span class="mf">0.16046763956546783</span><span class="p">,</span><span class="mf">0.8863046765327454</span><span class="p">,</span><span class="mf">0.4463944137096405</span><span class="p">,</span><span class="mf">0.9078755974769592</span><span class="p">,</span><span class="mf">0.16023047268390656</span><span class="p">,</span><span class="mf">0.6611174941062927</span><span class="p">,</span><span class="mf">0.4402637481689453</span><span class="p">,</span><span class="mf">0.07648676633834839</span><span class="p">,</span><span class="mf">0.6964631676673889</span><span class="p">,</span><span class="mf">0.2473987489938736</span><span class="p">,</span><span class="mf">0.03961552307009697</span><span class="p">,</span><span class="mf">0.05994429811835289</span><span class="p">,</span><span class="mf">0.06107853725552559</span><span class="p">,</span><span class="mf">0.9077329635620117</span><span class="p">,</span><span class="mf">0.7398838996887207</span><span class="p">,</span><span class="mf">0.8980623483657837</span><span class="p">,</span><span class="mf">0.6725823283195496</span><span class="p">,</span><span class="mf">0.5289399027824402</span><span class="p">,</span><span class="mf">0.30444636940956116</span><span class="p">,</span><span class="mf">0.997962236404419</span><span class="p">,</span><span class="mf">0.36218905448913574</span><span class="p">,</span><span class="mf">0.47064894437789917</span><span class="p">,</span><span class="mf">0.37824517488479614</span><span class="p">,</span><span class="mf">0.979526937007904</span><span class="p">,</span><span class="mf">0.1746583878993988</span><span class="p">,</span><span class="mf">0.32798799872398376</span><span class="p">,</span><span class="mf">0.6803486943244934</span><span class="p">,</span><span class="mf">0.06320761889219284</span><span class="p">,</span><span class="mf">0.60724937915802</span><span class="p">,</span><span class="mf">0.47764649987220764</span><span class="p">,</span><span class="mf">0.2839999794960022</span><span class="p">,</span><span class="mf">0.2384132742881775</span><span class="p">,</span><span class="mf">0.5145127177238464</span><span class="p">,</span><span class="mf">0.36792758107185364</span><span class="p">,</span><span class="mf">0.4565199017524719</span><span class="p">,</span><span class="mf">0.3374773859977722</span><span class="p">,</span><span class="mf">0.9704936742782593</span><span class="p">,</span><span class="mf">0.13343943655490875</span><span class="p">,</span><span class="mf">0.09680395573377609</span><span class="p">,</span><span class="mf">0.3433917164802551</span><span class="p">,</span><span class="mf">0.5910269021987915</span><span class="p">,</span><span class="mf">0.6591764688491821</span><span class="p">,</span><span class="mf">0.3972567617893219</span><span class="p">,</span><span class="mf">0.9992780089378357</span><span class="p">,</span><span class="mf">0.35189300775527954</span><span class="p">,</span><span class="mf">0.7214066386222839</span><span class="p">,</span><span class="mf">0.6375827193260193</span><span class="p">,</span><span class="mf">0.8130538463592529</span><span class="p">,</span><span class="mf">0.9762256741523743</span><span class="p">,</span><span class="mf">0.8897936344146729</span><span class="p">,</span><span class="mf">0.7645619511604309</span><span class="p">,</span><span class="mf">0.6982485055923462</span><span class="p">,</span><span class="mf">0.335498183965683</span><span class="p">,</span><span class="mf">0.14768557250499725</span><span class="p">,</span><span class="mf">0.06263600289821625</span><span class="p">,</span><span class="mf">0.2419017106294632</span><span class="p">,</span><span class="mf">0.432281494140625</span><span class="p">,</span><span class="mf">0.521996259689331</span><span class="p">,</span><span class="mf">0.7730835676193237</span><span class="p">,</span><span class="mf">0.9587409496307373</span><span class="p">,</span><span class="mf">0.1173204779624939</span><span class="p">,</span><span class="mf">0.10700414329767227</span><span class="p">,</span><span class="mf">0.5896947383880615</span><span class="p">,</span><span class="mf">0.7453980445861816</span><span class="p">,</span><span class="mf">0.848150372505188</span><span class="p">,</span><span class="mf">0.9358320832252502</span><span class="p">,</span><span class="mf">0.9834262132644653</span><span class="p">,</span><span class="mf">0.39980170130729675</span><span class="p">,</span><span class="mf">0.3803351819515228</span><span class="p">,</span><span class="mf">0.14780867099761963</span><span class="p">,</span><span class="mf">0.6849344372749329</span><span class="p">,</span><span class="mf">0.6567619442939758</span><span class="p">,</span><span class="mf">0.8620625734329224</span><span class="p">,</span><span class="mf">0.09725799411535263</span><span class="p">,</span><span class="mf">0.49777689576148987</span><span class="p">,</span><span class="mf">0.5810819268226624</span><span class="p">,</span><span class="mf">0.2415570467710495</span><span class="p">,</span><span class="mf">0.16902540624141693</span><span class="p">,</span><span class="mf">0.8595808148384094</span><span class="p">,</span><span class="mf">0.05853492394089699</span><span class="p">,</span><span class="mf">0.47062090039253235</span><span class="p">,</span><span class="mf">0.11583399772644043</span><span class="p">,</span><span class="mf">0.45705875754356384</span><span class="p">,</span><span class="mf">0.9799623489379883</span><span class="p">,</span><span class="mf">0.4237063527107239</span><span class="p">,</span><span class="mf">0.857124924659729</span><span class="p">,</span><span class="mf">0.11731556057929993</span><span class="p">,</span><span class="mf">0.2712520658969879</span><span class="p">,</span><span class="mf">0.40379273891448975</span><span class="p">,</span><span class="mf">0.39981213212013245</span><span class="p">,</span><span class="mf">0.6713835000991821</span><span class="p">,</span><span class="mf">0.3447181284427643</span><span class="p">,</span><span class="mf">0.713766872882843</span><span class="p">,</span><span class="mf">0.6391869187355042</span><span class="p">,</span><span class="mf">0.399161159992218</span><span class="p">,</span><span class="mf">0.43176013231277466</span><span class="p">,</span><span class="mf">0.614527702331543</span><span class="p">,</span><span class="mf">0.0700421929359436</span><span class="p">,</span><span class="mf">0.8224067091941833</span><span class="p">,</span><span class="mf">0.65342116355896</span><span class="p">,</span><span class="mf">0.7263424396514893</span><span class="p">,</span><span class="mf">0.5369229912757874</span><span class="p">,</span><span class="mf">0.11047711223363876</span><span class="p">,</span><span class="mf">0.4050356149673462</span><span class="p">,</span><span class="mf">0.40537357330322266</span><span class="p">,</span><span class="mf">0.3210429847240448</span><span class="p">,</span><span class="mf">0.029950324445962906</span><span class="p">,</span><span class="mf">0.73725426197052</span><span class="p">,</span><span class="mf">0.10978446155786514</span><span class="p">,</span><span class="mf">0.6063081622123718</span><span class="p">,</span><span class="mf">0.7032175064086914</span><span class="p">,</span><span class="mf">0.6347863078117371</span><span class="p">,</span><span class="mf">0.95914226770401</span><span class="p">,</span><span class="mf">0.10329815745353699</span><span class="p">,</span><span class="mf">0.8671671748161316</span><span class="p">,</span><span class="mf">0.02919023483991623</span><span class="p">,</span><span class="mf">0.534916877746582</span><span class="p">,</span><span class="mf">0.4042436182498932</span><span class="p">,</span><span class="mf">0.5241838693618774</span><span class="p">,</span><span class="mf">0.36509987711906433</span><span class="p">,</span><span class="mf">0.19056691229343414</span><span class="p">,</span><span class="mf">0.01912289671599865</span><span class="p">,</span><span class="mf">0.5181497931480408</span><span class="p">,</span><span class="mf">0.8427768349647522</span><span class="p">,</span><span class="mf">0.3732159435749054</span><span class="p">,</span><span class="mf">0.2228638231754303</span><span class="p">,</span><span class="mf">0.080532006919384</span><span class="p">,</span><span class="mf">0.0853109210729599</span><span class="p">,</span><span class="mf">0.22139644622802734</span><span class="p">,</span><span class="mf">0.10001406073570251</span><span class="p">,</span><span class="mf">0.26503971219062805</span><span class="p">,</span><span class="mf">0.06614946573972702</span><span class="p">,</span><span class="mf">0.06560486555099487</span><span class="p">,</span><span class="mf">0.8562761545181274</span><span class="p">,</span><span class="mf">0.1621202677488327</span><span class="p">,</span><span class="mf">0.5596824288368225</span><span class="p">,</span><span class="mf">0.7734555602073669</span><span class="p">,</span><span class="mf">0.4564095735549927</span><span class="p">,</span><span class="mf">0.15336887538433075</span><span class="p">,</span><span class="mf">0.19959613680839539</span><span class="p">,</span><span class="mf">0.43298420310020447</span><span class="p">,</span><span class="mf">0.52823406457901</span><span class="p">,</span><span class="mf">0.3494403064250946</span><span class="p">,</span><span class="mf">0.7814795970916748</span><span class="p">,</span><span class="mf">0.7510216236114502</span><span class="p">,</span><span class="mf">0.9272118210792542</span><span class="p">,</span><span class="mf">0.028952548280358315</span><span class="p">,</span><span class="mf">0.8956912755966187</span><span class="p">,</span><span class="mf">0.39256879687309265</span><span class="p">,</span><span class="mf">0.8783724904060364</span><span class="p">,</span><span class="mf">0.690784752368927</span><span class="p">,</span><span class="mf">0.987348735332489</span><span class="p">,</span><span class="mf">0.7592824697494507</span><span class="p">,</span><span class="mf">0.3645446300506592</span><span class="p">,</span><span class="mf">0.5010631680488586</span><span class="p">,</span><span class="mf">0.37638914585113525</span><span class="p">,</span><span class="mf">0.364911824464798</span><span class="p">,</span><span class="mf">0.2609044909477234</span><span class="p">,</span><span class="mf">0.49597030878067017</span><span class="p">,</span><span class="mf">0.6817399263381958</span><span class="p">,</span><span class="mf">0.27734026312828064</span><span class="p">,</span><span class="mf">0.5243797898292542</span><span class="p">,</span><span class="mf">0.117380291223526</span><span class="p">,</span><span class="mf">0.1598452925682068</span><span class="p">,</span><span class="mf">0.04680635407567024</span><span class="p">,</span><span class="mf">0.9707314372062683</span><span class="p">,</span><span class="mf">0.0038603513967245817</span><span class="p">,</span><span class="mf">0.17857997119426727</span><span class="p">,</span><span class="mf">0.6128667593002319</span><span class="p">,</span><span class="mf">0.08136960119009018</span><span class="p">,</span><span class="mf">0.8818964958190918</span><span class="p">,</span><span class="mf">0.7196201682090759</span><span class="p">,</span><span class="mf">0.9663899540901184</span><span class="p">,</span><span class="mf">0.5076355338096619</span><span class="p">,</span><span class="mf">0.3004036843776703</span><span class="p">,</span><span class="mf">0.549500584602356</span><span class="p">,</span><span class="mf">0.9308187365531921</span><span class="p">,</span><span class="mf">0.5207614302635193</span><span class="p">,</span><span class="mf">0.2672070264816284</span><span class="p">,</span><span class="mf">0.8773987889289856</span><span class="p">,</span><span class="mf">0.3719187378883362</span><span class="p">,</span><span class="mf">0.0013833499979227781</span><span class="p">,</span><span class="mf">0.2476850152015686</span><span class="p">,</span><span class="mf">0.31823351979255676</span><span class="p">,</span><span class="mf">0.8587774634361267</span><span class="p">,</span><span class="mf">0.4585031569004059</span><span class="p">,</span><span class="mf">0.4445872902870178</span><span class="p">,</span><span class="mf">0.33610227704048157</span><span class="p">,</span><span class="mf">0.880678117275238</span><span class="p">,</span><span class="mf">0.9450267553329468</span><span class="p">,</span><span class="mf">0.9918903112411499</span><span class="p">,</span><span class="mf">0.3767412602901459</span><span class="p">,</span><span class="mf">0.9661474227905273</span><span class="p">,</span><span class="mf">0.7918795943260193</span><span class="p">,</span><span class="mf">0.675689160823822</span><span class="p">,</span><span class="mf">0.24488948285579681</span><span class="p">,</span><span class="mf">0.21645726263523102</span><span class="p">,</span><span class="mf">0.1660478264093399</span><span class="p">,</span><span class="mf">0.9227566123008728</span><span class="p">,</span><span class="mf">0.2940766513347626</span><span class="p">,</span><span class="mf">0.4530942440032959</span><span class="p">,</span><span class="mf">0.49395784735679626</span><span class="p">,</span><span class="mf">0.7781715989112854</span><span class="p">,</span><span class="mf">0.8442349433898926</span><span class="p">,</span><span class="mf">0.1390727013349533</span><span class="p">,</span><span class="mf">0.4269043505191803</span><span class="p">,</span><span class="mf">0.842854917049408</span><span class="p">,</span><span class="mf">0.8180332779884338</span><span class="p">};</span>
<span class="kt">float</span><span class="w"> </span><span class="n">calib_output0_data</span><span class="p">[</span><span class="n">NET_OUTPUT0_SIZE</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">3.5647096e-05</span><span class="p">,</span><span class="mf">6.824297e-08</span><span class="p">,</span><span class="mf">0.009327697</span><span class="p">,</span><span class="mf">3.2340475e-05</span><span class="p">,</span><span class="mf">1.1117579e-05</span><span class="p">,</span><span class="mf">1.5117058e-06</span><span class="p">,</span><span class="mf">4.6314454e-07</span><span class="p">,</span><span class="mf">5.161628e-11</span><span class="p">,</span><span class="mf">0.9905911</span><span class="p">,</span><span class="mf">3.8835238e-10</span><span class="p">};</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="compiling-and-simulation-run">
<h4>Compiling and Simulation Run<a class="headerlink" href="#compiling-and-simulation-run" title="Permalink to this headline"></a></h4>
<p>In this example, software simulation is used to view and analyze the inference result.
On the <code class="docutils literal notranslate"><span class="pre">Workspace</span></code> page, right-click the project and choose <code class="docutils literal notranslate"><span class="pre">Options</span></code> from the shortcut menu. Select the <code class="docutils literal notranslate"><span class="pre">Debugger</span></code> option on the left of the Project Options window. In the <code class="docutils literal notranslate"><span class="pre">Setup</span></code> on the right, set <code class="docutils literal notranslate"><span class="pre">Driver</span></code> as <code class="docutils literal notranslate"><span class="pre">Simulator</span></code> to enable software simulation.</p>
<p>Close the project option window and choose <code class="docutils literal notranslate"><span class="pre">Project</span> <span class="pre">&gt;</span> <span class="pre">Download</span> <span class="pre">and</span> <span class="pre">Debug</span></code> on the menu bar to compile and simulate the project. By adding a breakpoint at the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code>, you can observe the inference result of the simulation run and the return value of the benchmark() function.</p>
</section>
</section>
<section id="code-integration-and-compilation-deployment-on-linux-code-integration-via-makefile">
<h3>Code Integration and Compilation Deployment on Linux: Code Integration via MakeFile<a class="headerlink" href="#code-integration-and-compilation-deployment-on-linux-code-integration-via-makefile" title="Permalink to this headline"></a></h3>
<p>This chapter describes how to integrate and develop the MCU inference code on the Linux platform, by taking the generated model code integration and developing through MakeFile on the Linux platform as an example.
The main steps are as follows:</p>
<ul class="simple">
<li><p>Download the required software and prepare the cross compilation and burning environment</p></li>
<li><p>Generate the required MCU startup code and demonstration project using the <code class="docutils literal notranslate"><span class="pre">STM32CubeMX</span></code> software</p></li>
<li><p>Modify the inference code and <code class="docutils literal notranslate"><span class="pre">Micro</span></code> lib of the <code class="docutils literal notranslate"><span class="pre">MakeFile</span></code> integrated model</p></li>
<li><p>Compiling and burning the project</p></li>
<li><p>Read and verify running result of the board</p></li>
</ul>
<p>For the complete demo code built in this example, click <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/micro/test_stmf767.tar.gz">Download here</a>.</p>
<section id="environment-preparation-1">
<h4>Environment Preparation<a class="headerlink" href="#environment-preparation-1" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://developer.arm.com/downloads/-/gnu-rm">GNU Arm Embedded Toolchain</a>  &gt;= 10-2020-q4-major-x86_64-linux</p>
<ul>
<li><p>This tool is a cross compilation tool for Cortex-M Linux.</p></li>
<li><p>Download the <code class="docutils literal notranslate"><span class="pre">gcc-arm-none-eabi</span></code> package of the x86_64-Linux version, decompress the package, and add the bin path in the directory to the PATH environment variable: <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">PATH=gcc-arm-none-eabi</span> <span class="pre">path/bin:$PATH</span></code>.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.st.com/en/development-tools/stm32cubemx.html">STM32CubeMX-Lin</a> &gt;= 6.5.0</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">STM32CubeMX</span></code> is a graphical configuration tool for STM32 chips provided by <code class="docutils literal notranslate"><span class="pre">STM</span></code>, which is used to generate the startup code and project of STM chips.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://www.st.com/en/development-tools/stm32cubeprog.html">STM32CubePrg-Lin</a> &gt;= 6.5.0</p>
<ul>
<li><p>This tool is a burning tool provided by <code class="docutils literal notranslate"><span class="pre">STM</span></code> and can be used for program burning and data reading.</p></li>
</ul>
</li>
</ul>
</section>
<section id="obtaining-the-mcu-startup-code-and-project-1">
<h4>Obtaining the MCU Startup Code and Project<a class="headerlink" href="#obtaining-the-mcu-startup-code-and-project-1" title="Permalink to this headline"></a></h4>
<p>If you have an MCU project, skip this chapter.
This chapter uses the STM32F767 startup project as an example to describe how to generate an MCU project for an STM32 chip via <code class="docutils literal notranslate"><span class="pre">STM32CubeMX</span></code>.</p>
<ul class="simple">
<li><p>Start <code class="docutils literal notranslate"><span class="pre">STM32CubeMX</span></code> and select <code class="docutils literal notranslate"><span class="pre">New</span> <span class="pre">Project</span></code> from <code class="docutils literal notranslate"><span class="pre">File</span></code> to create a project</p></li>
<li><p>In the <code class="docutils literal notranslate"><span class="pre">MCU/MPU</span> <span class="pre">Selector</span></code> window, search for and select <code class="docutils literal notranslate"><span class="pre">STM32F767IGT6</span></code>, and click <code class="docutils literal notranslate"><span class="pre">Start</span> <span class="pre">Project</span></code> to create a project for the chip</p></li>
<li><p>On the <code class="docutils literal notranslate"><span class="pre">Project</span> <span class="pre">Manager</span></code> page, configure the project name and the path of the generated project, and select <code class="docutils literal notranslate"><span class="pre">EWARM</span></code> in <code class="docutils literal notranslate"><span class="pre">Toolchain</span> <span class="pre">/</span> <span class="pre">IDE</span></code> to generate the IAR project</p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">GENERATE</span> <span class="pre">CODE</span></code> above to generate code</p></li>
<li><p>On the PC where the <code class="docutils literal notranslate"><span class="pre">IAR</span></code> is installed, double-click <code class="docutils literal notranslate"><span class="pre">Project.eww</span></code> in the <code class="docutils literal notranslate"><span class="pre">EWARM</span></code> directory of the generated project to open the IAR project</p></li>
</ul>
</section>
<section id="integrating-model-inference-code-and-micro-lib-1">
<h4>Integrating Model Inference Code and <code class="docutils literal notranslate"><span class="pre">Micro</span></code> Lib<a class="headerlink" href="#integrating-model-inference-code-and-micro-lib-1" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Copy the generated inference code to the project, decompress the package obtained in <span class="xref myst">Downloading <code class="docutils literal notranslate"><span class="pre">Micro</span></code> Lib of Cortex-M Architecture</span>, and place it in the generated inference code directory, as shown in the following figure:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>stm32f767                                       # MCU project directory
├── Core
│   ├── Inc
│   └── Src
│       ├── main.c
│       └── ...
├── Drivers
├── mnist                                        # Generate Code Root Directory
│   ├── benchmark                                # Benchmark routines for integrated calls to model inference code
│   │   ├── benchmark.c
│   │   ├── data.c
│   │   ├── data.h
│   │   └── ...
│   │── mindspore-lite-1.8.0-none-cortex-m7      # Downloaded Cortex-M7 Architecture `Micro` Lib
│   ├── src                                      # Model inference code directory
│   └── ...
├── Makefile
├── startup_stm32f767xx.s
└── STM32F767IGTx_FLASH.ld
</pre></div>
</div>
</li>
<li><p>Modify <code class="docutils literal notranslate"><span class="pre">MakeFile</span></code> and add the model inference code and dependency library to the project</p>
<p>In this example, the source code to be added to the project includes the model inference code in the <code class="docutils literal notranslate"><span class="pre">src</span></code> directory and the example code called by the model inference in the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> directory.
Modify the definition of the <code class="docutils literal notranslate"><span class="pre">C_SOURCES</span></code> variable in <code class="docutils literal notranslate"><span class="pre">MakeFile</span></code> and add the source file path.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">C_SOURCES</span><span class="w"> </span><span class="o">=</span><span class="w">  </span><span class="se">\</span>
mnist/src/context.c<span class="w"> </span><span class="se">\</span>
mnist/src/model.c<span class="w"> </span><span class="se">\</span>
mnist/src/net.c<span class="w"> </span><span class="se">\</span>
mnist/src/tensor.c<span class="w"> </span><span class="se">\</span>
mnist/src/weight.c<span class="w"> </span><span class="se">\</span>
mnist/benchmark/benchmark.c<span class="w"> </span><span class="se">\</span>
mnist/benchmark/calib_output.c<span class="w"> </span><span class="se">\</span>
mnist/benchmark/load_input.c<span class="w"> </span><span class="se">\</span>
mnist/benchmark/data.c<span class="w"> </span><span class="se">\</span>
...
</pre></div>
</div>
<p>Add the path of the dependent header file: Modify the definition of the <code class="docutils literal notranslate"><span class="pre">C_INCLUDES</span></code> variable in <code class="docutils literal notranslate"><span class="pre">MakeFile</span></code> and add the following path:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LITE_PACK = mindspore-lite-1.8.0-none-cortex-m7

C_INCLUDES =  \
-Imnist/$(LITE_PACK)/runtime \
-Imnist/$(LITE_PACK)/runtime/include \
-Imnist/$(LITE_PACK)/tools/codegen/include \
-Imnist/$(LITE_PACK)/tools/codegen/third_party/include/CMSIS/Core \
-Imnist/$(LITE_PACK)/tools/codegen/third_party/include/CMSIS/DSP \
-Imnist/$(LITE_PACK)/tools/codegen/third_party/include/CMSIS/NN \
-Imnist \
...
</pre></div>
</div>
<p>Add the dependent operator library (<code class="docutils literal notranslate"><span class="pre">-lnnacl</span> <span class="pre">-lwrapper</span> <span class="pre">-lcmsis_nn</span></code>), declare the path of the operator library file, and add the compilation option (<code class="docutils literal notranslate"><span class="pre">-specs=nosys.specs</span></code>).
In this example, the modified variables are defined as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>LIBS = -lc -lm -lnosys -lnnacl -lwrapper -lcmsis_nn
LIBDIR = -Lmnist/$(LITE_PACK)/tools/codegen/lib -Lmnist/$(LITE_PACK)/tools/codegen/third_party/lib
LDFLAGS = $(MCU) -specs=nosys.specs -specs=nano.specs -T$(LDSCRIPT) $(LIBDIR) $(LIBS) -Wl,-Map=$(BUILD_DIR)/$(TARGET).map,--cref -Wl,--gc-sections
</pre></div>
</div>
</li>
<li><p>Modify the main.c file and invoke the benchmark function</p>
<p>Invoke the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> function in <code class="docutils literal notranslate"><span class="pre">benchmark.c</span></code> in the main function. The program in the benchmark folder is a sample program that invokes the inference code in the generated <code class="docutils literal notranslate"><span class="pre">src</span></code> and compares the output, which can be modified freely. In this example, we call the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> function directly and assign the <code class="docutils literal notranslate"><span class="pre">run_dnn_flag</span></code> variable based on the returned result.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">run_dnn_flag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">&#39;0&#39;</span><span class="p">;</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">benchmark</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">run success.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">run_dnn_flag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">&#39;1&#39;</span><span class="p">;</span>
<span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">run failed.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">run_dnn_flag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="sc">&#39;2&#39;</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Add the header file reference and the definition of the <code class="docutils literal notranslate"><span class="pre">run_dnn_flag</span></code> variable to the beginning of <code class="docutils literal notranslate"><span class="pre">main.c</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;benchmark/benchmark.h&quot;</span>

<span class="kt">char</span><span class="w"> </span><span class="n">run_dnn_flag</span><span class="w"> </span><span class="n">__attribute__</span><span class="p">((</span><span class="n">section</span><span class="p">(</span><span class="s">&quot;.myram&quot;</span><span class="p">)))</span><span class="w"> </span><span class="p">;</span><span class="c1">// Array for testing</span>
</pre></div>
</div>
<p>In this example, to facilitate reading the inference result by using the burner, variables are defined in a customized section (<code class="docutils literal notranslate"><span class="pre">myram</span></code>). You can set the customized section in the following way or ignore the declaration: obtaining the inference result through serial ports or other interactive modes.</p>
<p>To set a customized section, perform the following steps:
Modify the <code class="docutils literal notranslate"><span class="pre">MEMORY</span></code> section in the <code class="docutils literal notranslate"><span class="pre">STM32F767IGTx_FLASH.ld</span></code> file, and add a customized memory segment <code class="docutils literal notranslate"><span class="pre">MYRAM</span></code>. (In this example, add 4 to the <code class="docutils literal notranslate"><span class="pre">RAM</span></code> memory start address to free up memory for <code class="docutils literal notranslate"><span class="pre">MYRAM</span></code>). Then add a customized <code class="docutils literal notranslate"><span class="pre">myram</span></code> segment declaration to the <code class="docutils literal notranslate"><span class="pre">SectionS</span></code> segment.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>MEMORY
{
MYRAM (xrw)     : ORIGIN = 0x20000000, LENGTH = 1
RAM (xrw)      : ORIGIN = 0x20000004, LENGTH = 524284
...
}
...
SECTIONS
{
  ...
  .myram (NOLOAD):
  {
    . = ALIGN(4);
    _smyram = .;        /* create a global symbol at data start */
    *(.sram)           /* .data sections */
    *(.sram*)          /* .data* sections */

    . = ALIGN(4);
    _emyram = .;        /* define a global symbol at data end */
  } &gt;MYRAM AT&gt; FLASH
}
</pre></div>
</div>
</li>
<li><p>Modify the <code class="docutils literal notranslate"><span class="pre">mnist/benchmark/data.c</span></code> file to store benchmark input and output data in the program for comparison.</p>
<p>In the benchmark routine, the input data of the model is set and the inference result is compared with the expected result to obtain the error offset.
In this example, modify the <code class="docutils literal notranslate"><span class="pre">calib_input0_data</span></code> array of <code class="docutils literal notranslate"><span class="pre">data.c</span></code> to set the input data of the model, and modify the <code class="docutils literal notranslate"><span class="pre">calib_output0_data</span></code> to set the expected result.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">float</span><span class="w"> </span><span class="n">calib_input0_data</span><span class="p">[</span><span class="n">NET_INPUT0_SIZE</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">0.54881352186203</span><span class="p">,</span><span class="mf">0.7151893377304077</span><span class="p">,</span><span class="mf">0.6027633547782898</span><span class="p">,</span><span class="mf">0.5448831915855408</span><span class="p">,</span><span class="mf">0.42365479469299316</span><span class="p">,</span><span class="mf">0.6458941102027893</span><span class="p">,</span><span class="mf">0.4375872015953064</span><span class="p">,</span><span class="mf">0.891772985458374</span><span class="p">,</span><span class="mf">0.9636627435684204</span><span class="p">,</span><span class="mf">0.3834415078163147</span><span class="p">,</span><span class="mf">0.7917250394821167</span><span class="p">,</span><span class="mf">0.5288949012756348</span><span class="p">,</span><span class="mf">0.5680445432662964</span><span class="p">,</span><span class="mf">0.9255966544151306</span><span class="p">,</span><span class="mf">0.07103605568408966</span><span class="p">,</span><span class="mf">0.08712930232286453</span><span class="p">,</span><span class="mf">0.020218396559357643</span><span class="p">,</span><span class="mf">0.832619845867157</span><span class="p">,</span><span class="mf">0.7781567573547363</span><span class="p">,</span><span class="mf">0.8700121641159058</span><span class="p">,</span><span class="mf">0.978618323802948</span><span class="p">,</span><span class="mf">0.7991585731506348</span><span class="p">,</span><span class="mf">0.4614793658256531</span><span class="p">,</span><span class="mf">0.7805292010307312</span><span class="p">,</span><span class="mf">0.11827442795038223</span><span class="p">,</span><span class="mf">0.6399210095405579</span><span class="p">,</span><span class="mf">0.14335328340530396</span><span class="p">,</span><span class="mf">0.9446688890457153</span><span class="p">,</span><span class="mf">0.5218483209609985</span><span class="p">,</span><span class="mf">0.4146619439125061</span><span class="p">,</span><span class="mf">0.26455560326576233</span><span class="p">,</span><span class="mf">0.7742336988449097</span><span class="p">,</span><span class="mf">0.4561503231525421</span><span class="p">,</span><span class="mf">0.568433940410614</span><span class="p">,</span><span class="mf">0.018789799883961678</span><span class="p">,</span><span class="mf">0.6176354885101318</span><span class="p">,</span><span class="mf">0.6120957136154175</span><span class="p">,</span><span class="mf">0.6169340014457703</span><span class="p">,</span><span class="mf">0.9437480568885803</span><span class="p">,</span><span class="mf">0.681820273399353</span><span class="p">,</span><span class="mf">0.35950788855552673</span><span class="p">,</span><span class="mf">0.43703195452690125</span><span class="p">,</span><span class="mf">0.6976311802864075</span><span class="p">,</span><span class="mf">0.0602254718542099</span><span class="p">,</span><span class="mf">0.6667667031288147</span><span class="p">,</span><span class="mf">0.670637845993042</span><span class="p">,</span><span class="mf">0.21038256585597992</span><span class="p">,</span><span class="mf">0.12892629206180573</span><span class="p">,</span><span class="mf">0.31542834639549255</span><span class="p">,</span><span class="mf">0.36371076107025146</span><span class="p">,</span><span class="mf">0.5701967477798462</span><span class="p">,</span><span class="mf">0.4386015236377716</span><span class="p">,</span><span class="mf">0.9883738160133362</span><span class="p">,</span><span class="mf">0.10204481333494186</span><span class="p">,</span><span class="mf">0.20887675881385803</span><span class="p">,</span><span class="mf">0.16130951046943665</span><span class="p">,</span><span class="mf">0.6531082987785339</span><span class="p">,</span><span class="mf">0.25329160690307617</span><span class="p">,</span><span class="mf">0.4663107693195343</span><span class="p">,</span><span class="mf">0.24442559480667114</span><span class="p">,</span><span class="mf">0.15896958112716675</span><span class="p">,</span><span class="mf">0.11037514358758926</span><span class="p">,</span><span class="mf">0.6563295722007751</span><span class="p">,</span><span class="mf">0.13818295300006866</span><span class="p">,</span><span class="mf">0.1965823620557785</span><span class="p">,</span><span class="mf">0.3687251806259155</span><span class="p">,</span><span class="mf">0.8209932446479797</span><span class="p">,</span><span class="mf">0.09710127860307693</span><span class="p">,</span><span class="mf">0.8379449248313904</span><span class="p">,</span><span class="mf">0.0960984081029892</span><span class="p">,</span><span class="mf">0.9764594435691833</span><span class="p">,</span><span class="mf">0.4686512053012848</span><span class="p">,</span><span class="mf">0.9767611026763916</span><span class="p">,</span><span class="mf">0.6048455238342285</span><span class="p">,</span><span class="mf">0.7392635941505432</span><span class="p">,</span><span class="mf">0.03918779268860817</span><span class="p">,</span><span class="mf">0.28280696272850037</span><span class="p">,</span><span class="mf">0.12019655853509903</span><span class="p">,</span><span class="mf">0.296140193939209</span><span class="p">,</span><span class="mf">0.11872772127389908</span><span class="p">,</span><span class="mf">0.3179831802845001</span><span class="p">,</span><span class="mf">0.414262980222702</span><span class="p">,</span><span class="mf">0.06414749473333359</span><span class="p">,</span><span class="mf">0.6924721002578735</span><span class="p">,</span><span class="mf">0.5666014552116394</span><span class="p">,</span><span class="mf">0.26538950204849243</span><span class="p">,</span><span class="mf">0.5232480764389038</span><span class="p">,</span><span class="mf">0.09394051134586334</span><span class="p">,</span><span class="mf">0.5759465098381042</span><span class="p">,</span><span class="mf">0.9292961955070496</span><span class="p">,</span><span class="mf">0.3185689449310303</span><span class="p">,</span><span class="mf">0.6674103736877441</span><span class="p">,</span><span class="mf">0.13179786503314972</span><span class="p">,</span><span class="mf">0.7163271903991699</span><span class="p">,</span><span class="mf">0.28940609097480774</span><span class="p">,</span><span class="mf">0.18319135904312134</span><span class="p">,</span><span class="mf">0.5865129232406616</span><span class="p">,</span><span class="mf">0.02010754682123661</span><span class="p">,</span><span class="mf">0.8289400339126587</span><span class="p">,</span><span class="mf">0.004695476032793522</span><span class="p">,</span><span class="mf">0.6778165102005005</span><span class="p">,</span><span class="mf">0.2700079679489136</span><span class="p">,</span><span class="mf">0.7351940274238586</span><span class="p">,</span><span class="mf">0.9621885418891907</span><span class="p">,</span><span class="mf">0.2487531453371048</span><span class="p">,</span><span class="mf">0.5761573314666748</span><span class="p">,</span><span class="mf">0.5920419096946716</span><span class="p">,</span><span class="mf">0.5722519159317017</span><span class="p">,</span><span class="mf">0.22308163344860077</span><span class="p">,</span><span class="mf">0.9527490139007568</span><span class="p">,</span><span class="mf">0.4471253752708435</span><span class="p">,</span><span class="mf">0.8464086651802063</span><span class="p">,</span><span class="mf">0.6994792819023132</span><span class="p">,</span><span class="mf">0.2974369525909424</span><span class="p">,</span><span class="mf">0.8137978315353394</span><span class="p">,</span><span class="mf">0.396505743265152</span><span class="p">,</span><span class="mf">0.8811032176017761</span><span class="p">,</span><span class="mf">0.5812729001045227</span><span class="p">,</span><span class="mf">0.8817353844642639</span><span class="p">,</span><span class="mf">0.6925315856933594</span><span class="p">,</span><span class="mf">0.7252542972564697</span><span class="p">,</span><span class="mf">0.5013243556022644</span><span class="p">,</span><span class="mf">0.9560836553573608</span><span class="p">,</span><span class="mf">0.6439902186393738</span><span class="p">,</span><span class="mf">0.4238550364971161</span><span class="p">,</span><span class="mf">0.6063932180404663</span><span class="p">,</span><span class="mf">0.019193198531866074</span><span class="p">,</span><span class="mf">0.30157482624053955</span><span class="p">,</span><span class="mf">0.6601735353469849</span><span class="p">,</span><span class="mf">0.2900775969028473</span><span class="p">,</span><span class="mf">0.6180154085159302</span><span class="p">,</span><span class="mf">0.42876869440078735</span><span class="p">,</span><span class="mf">0.1354740709066391</span><span class="p">,</span><span class="mf">0.29828232526779175</span><span class="p">,</span><span class="mf">0.5699648857116699</span><span class="p">,</span><span class="mf">0.5908727645874023</span><span class="p">,</span><span class="mf">0.5743252635002136</span><span class="p">,</span><span class="mf">0.6532008051872253</span><span class="p">,</span><span class="mf">0.6521032452583313</span><span class="p">,</span><span class="mf">0.43141844868659973</span><span class="p">,</span><span class="mf">0.8965466022491455</span><span class="p">,</span><span class="mf">0.36756187677383423</span><span class="p">,</span><span class="mf">0.4358649253845215</span><span class="p">,</span><span class="mf">0.8919233679771423</span><span class="p">,</span><span class="mf">0.806194007396698</span><span class="p">,</span><span class="mf">0.7038885951042175</span><span class="p">,</span><span class="mf">0.10022688657045364</span><span class="p">,</span><span class="mf">0.9194825887680054</span><span class="p">,</span><span class="mf">0.7142413258552551</span><span class="p">,</span><span class="mf">0.9988470077514648</span><span class="p">,</span><span class="mf">0.14944830536842346</span><span class="p">,</span><span class="mf">0.8681260347366333</span><span class="p">,</span><span class="mf">0.16249293088912964</span><span class="p">,</span><span class="mf">0.6155595779418945</span><span class="p">,</span><span class="mf">0.1238199844956398</span><span class="p">,</span><span class="mf">0.8480082154273987</span><span class="p">,</span><span class="mf">0.8073189854621887</span><span class="p">,</span><span class="mf">0.5691007375717163</span><span class="p">,</span><span class="mf">0.40718328952789307</span><span class="p">,</span><span class="mf">0.06916699558496475</span><span class="p">,</span><span class="mf">0.6974287629127502</span><span class="p">,</span><span class="mf">0.45354267954826355</span><span class="p">,</span><span class="mf">0.7220556139945984</span><span class="p">,</span><span class="mf">0.8663823008537292</span><span class="p">,</span><span class="mf">0.9755215048789978</span><span class="p">,</span><span class="mf">0.855803370475769</span><span class="p">,</span><span class="mf">0.011714084073901176</span><span class="p">,</span><span class="mf">0.359978049993515</span><span class="p">,</span><span class="mf">0.729990541934967</span><span class="p">,</span><span class="mf">0.17162968218326569</span><span class="p">,</span><span class="mf">0.5210366249084473</span><span class="p">,</span><span class="mf">0.054337989538908005</span><span class="p">,</span><span class="mf">0.19999653100967407</span><span class="p">,</span><span class="mf">0.01852179504930973</span><span class="p">,</span><span class="mf">0.793697714805603</span><span class="p">,</span><span class="mf">0.2239246815443039</span><span class="p">,</span><span class="mf">0.3453516662120819</span><span class="p">,</span><span class="mf">0.9280812740325928</span><span class="p">,</span><span class="mf">0.704414427280426</span><span class="p">,</span><span class="mf">0.031838931143283844</span><span class="p">,</span><span class="mf">0.1646941602230072</span><span class="p">,</span><span class="mf">0.6214783787727356</span><span class="p">,</span><span class="mf">0.5772286057472229</span><span class="p">,</span><span class="mf">0.23789282143115997</span><span class="p">,</span><span class="mf">0.9342139959335327</span><span class="p">,</span><span class="mf">0.6139659285545349</span><span class="p">,</span><span class="mf">0.5356327891349792</span><span class="p">,</span><span class="mf">0.5899099707603455</span><span class="p">,</span><span class="mf">0.7301220297813416</span><span class="p">,</span><span class="mf">0.31194499135017395</span><span class="p">,</span><span class="mf">0.39822107553482056</span><span class="p">,</span><span class="mf">0.20984375476837158</span><span class="p">,</span><span class="mf">0.18619300425052643</span><span class="p">,</span><span class="mf">0.9443724155426025</span><span class="p">,</span><span class="mf">0.739550769329071</span><span class="p">,</span><span class="mf">0.49045881628990173</span><span class="p">,</span><span class="mf">0.22741462290287018</span><span class="p">,</span><span class="mf">0.2543564736843109</span><span class="p">,</span><span class="mf">0.058029159903526306</span><span class="p">,</span><span class="mf">0.43441662192344666</span><span class="p">,</span><span class="mf">0.3117958903312683</span><span class="p">,</span><span class="mf">0.6963434815406799</span><span class="p">,</span><span class="mf">0.37775182723999023</span><span class="p">,</span><span class="mf">0.1796036809682846</span><span class="p">,</span><span class="mf">0.024678727611899376</span><span class="p">,</span><span class="mf">0.06724963337182999</span><span class="p">,</span><span class="mf">0.6793927550315857</span><span class="p">,</span><span class="mf">0.4536968469619751</span><span class="p">,</span><span class="mf">0.5365791916847229</span><span class="p">,</span><span class="mf">0.8966712951660156</span><span class="p">,</span><span class="mf">0.990338921546936</span><span class="p">,</span><span class="mf">0.21689698100090027</span><span class="p">,</span><span class="mf">0.6630781888961792</span><span class="p">,</span><span class="mf">0.2633223831653595</span><span class="p">,</span><span class="mf">0.02065099962055683</span><span class="p">,</span><span class="mf">0.7583786249160767</span><span class="p">,</span><span class="mf">0.32001715898513794</span><span class="p">,</span><span class="mf">0.38346388936042786</span><span class="p">,</span><span class="mf">0.5883170962333679</span><span class="p">,</span><span class="mf">0.8310484290122986</span><span class="p">,</span><span class="mf">0.6289818286895752</span><span class="p">,</span><span class="mf">0.872650682926178</span><span class="p">,</span><span class="mf">0.27354204654693604</span><span class="p">,</span><span class="mf">0.7980468273162842</span><span class="p">,</span><span class="mf">0.18563593924045563</span><span class="p">,</span><span class="mf">0.9527916312217712</span><span class="p">,</span><span class="mf">0.6874882578849792</span><span class="p">,</span><span class="mf">0.21550767123699188</span><span class="p">,</span><span class="mf">0.9473705887794495</span><span class="p">,</span><span class="mf">0.7308558225631714</span><span class="p">,</span><span class="mf">0.2539416551589966</span><span class="p">,</span><span class="mf">0.21331197023391724</span><span class="p">,</span><span class="mf">0.518200695514679</span><span class="p">,</span><span class="mf">0.02566271834075451</span><span class="p">,</span><span class="mf">0.20747007429599762</span><span class="p">,</span><span class="mf">0.4246854782104492</span><span class="p">,</span><span class="mf">0.3741699755191803</span><span class="p">,</span><span class="mf">0.46357542276382446</span><span class="p">,</span><span class="mf">0.27762871980667114</span><span class="p">,</span><span class="mf">0.5867843627929688</span><span class="p">,</span><span class="mf">0.8638556003570557</span><span class="p">,</span><span class="mf">0.11753185838460922</span><span class="p">,</span><span class="mf">0.517379105091095</span><span class="p">,</span><span class="mf">0.13206811249256134</span><span class="p">,</span><span class="mf">0.7168596982955933</span><span class="p">,</span><span class="mf">0.39605969190597534</span><span class="p">,</span><span class="mf">0.5654212832450867</span><span class="p">,</span><span class="mf">0.1832798421382904</span><span class="p">,</span><span class="mf">0.14484776556491852</span><span class="p">,</span><span class="mf">0.4880562722682953</span><span class="p">,</span><span class="mf">0.35561272501945496</span><span class="p">,</span><span class="mf">0.9404319524765015</span><span class="p">,</span><span class="mf">0.7653252482414246</span><span class="p">,</span><span class="mf">0.748663604259491</span><span class="p">,</span><span class="mf">0.9037197232246399</span><span class="p">,</span><span class="mf">0.08342243731021881</span><span class="p">,</span><span class="mf">0.5521924495697021</span><span class="p">,</span><span class="mf">0.5844760537147522</span><span class="p">,</span><span class="mf">0.961936354637146</span><span class="p">,</span><span class="mf">0.29214751720428467</span><span class="p">,</span><span class="mf">0.24082878232002258</span><span class="p">,</span><span class="mf">0.10029394179582596</span><span class="p">,</span><span class="mf">0.016429629176855087</span><span class="p">,</span><span class="mf">0.9295293092727661</span><span class="p">,</span><span class="mf">0.669916570186615</span><span class="p">,</span><span class="mf">0.7851529121398926</span><span class="p">,</span><span class="mf">0.28173011541366577</span><span class="p">,</span><span class="mf">0.5864101648330688</span><span class="p">,</span><span class="mf">0.06395526975393295</span><span class="p">,</span><span class="mf">0.48562759160995483</span><span class="p">,</span><span class="mf">0.9774951338768005</span><span class="p">,</span><span class="mf">0.8765052556991577</span><span class="p">,</span><span class="mf">0.3381589651107788</span><span class="p">,</span><span class="mf">0.961570143699646</span><span class="p">,</span><span class="mf">0.23170162737369537</span><span class="p">,</span><span class="mf">0.9493188261985779</span><span class="p">,</span><span class="mf">0.9413776993751526</span><span class="p">,</span><span class="mf">0.799202561378479</span><span class="p">,</span><span class="mf">0.6304479241371155</span><span class="p">,</span><span class="mf">0.8742879629135132</span><span class="p">,</span><span class="mf">0.2930202782154083</span><span class="p">,</span><span class="mf">0.8489435315132141</span><span class="p">,</span><span class="mf">0.6178767085075378</span><span class="p">,</span><span class="mf">0.013236857950687408</span><span class="p">,</span><span class="mf">0.34723350405693054</span><span class="p">,</span><span class="mf">0.14814086258411407</span><span class="p">,</span><span class="mf">0.9818294048309326</span><span class="p">,</span><span class="mf">0.4783703088760376</span><span class="p">,</span><span class="mf">0.49739137291908264</span><span class="p">,</span><span class="mf">0.6394725441932678</span><span class="p">,</span><span class="mf">0.36858460307121277</span><span class="p">,</span><span class="mf">0.13690027594566345</span><span class="p">,</span><span class="mf">0.8221177458763123</span><span class="p">,</span><span class="mf">0.1898479163646698</span><span class="p">,</span><span class="mf">0.5113189816474915</span><span class="p">,</span><span class="mf">0.2243170291185379</span><span class="p">,</span><span class="mf">0.09784448146820068</span><span class="p">,</span><span class="mf">0.8621914982795715</span><span class="p">,</span><span class="mf">0.9729194641113281</span><span class="p">,</span><span class="mf">0.9608346819877625</span><span class="p">,</span><span class="mf">0.9065554738044739</span><span class="p">,</span><span class="mf">0.774047315120697</span><span class="p">,</span><span class="mf">0.3331451416015625</span><span class="p">,</span><span class="mf">0.08110138773918152</span><span class="p">,</span><span class="mf">0.40724116563796997</span><span class="p">,</span><span class="mf">0.2322341352701187</span><span class="p">,</span><span class="mf">0.13248763978481293</span><span class="p">,</span><span class="mf">0.053427182137966156</span><span class="p">,</span><span class="mf">0.7255943417549133</span><span class="p">,</span><span class="mf">0.011427458375692368</span><span class="p">,</span><span class="mf">0.7705807685852051</span><span class="p">,</span><span class="mf">0.14694663882255554</span><span class="p">,</span><span class="mf">0.07952208071947098</span><span class="p">,</span><span class="mf">0.08960303664207458</span><span class="p">,</span><span class="mf">0.6720477938652039</span><span class="p">,</span><span class="mf">0.24536721408367157</span><span class="p">,</span><span class="mf">0.4205394685268402</span><span class="p">,</span><span class="mf">0.557368814945221</span><span class="p">,</span><span class="mf">0.8605511784553528</span><span class="p">,</span><span class="mf">0.7270442843437195</span><span class="p">,</span><span class="mf">0.2703278958797455</span><span class="p">,</span><span class="mf">0.131482794880867</span><span class="p">,</span><span class="mf">0.05537432059645653</span><span class="p">,</span><span class="mf">0.3015986382961273</span><span class="p">,</span><span class="mf">0.2621181607246399</span><span class="p">,</span><span class="mf">0.45614057779312134</span><span class="p">,</span><span class="mf">0.6832813620567322</span><span class="p">,</span><span class="mf">0.6956254243850708</span><span class="p">,</span><span class="mf">0.28351885080337524</span><span class="p">,</span><span class="mf">0.3799269497394562</span><span class="p">,</span><span class="mf">0.18115095794200897</span><span class="p">,</span><span class="mf">0.7885454893112183</span><span class="p">,</span><span class="mf">0.05684807524085045</span><span class="p">,</span><span class="mf">0.6969972252845764</span><span class="p">,</span><span class="mf">0.7786954045295715</span><span class="p">,</span><span class="mf">0.7774075865745544</span><span class="p">,</span><span class="mf">0.25942257046699524</span><span class="p">,</span><span class="mf">0.3738131523132324</span><span class="p">,</span><span class="mf">0.5875996351242065</span><span class="p">,</span><span class="mf">0.27282190322875977</span><span class="p">,</span><span class="mf">0.3708527982234955</span><span class="p">,</span><span class="mf">0.19705428183078766</span><span class="p">,</span><span class="mf">0.4598558843135834</span><span class="p">,</span><span class="mf">0.044612299650907516</span><span class="p">,</span><span class="mf">0.7997958660125732</span><span class="p">,</span><span class="mf">0.07695644348859787</span><span class="p">,</span><span class="mf">0.5188351273536682</span><span class="p">,</span><span class="mf">0.3068101108074188</span><span class="p">,</span><span class="mf">0.5775429606437683</span><span class="p">,</span><span class="mf">0.9594333171844482</span><span class="p">,</span><span class="mf">0.6455702185630798</span><span class="p">,</span><span class="mf">0.03536243736743927</span><span class="p">,</span><span class="mf">0.4304024279117584</span><span class="p">,</span><span class="mf">0.5100168585777283</span><span class="p">,</span><span class="mf">0.5361775159835815</span><span class="p">,</span><span class="mf">0.6813924908638</span><span class="p">,</span><span class="mf">0.2775960862636566</span><span class="p">,</span><span class="mf">0.12886056303977966</span><span class="p">,</span><span class="mf">0.3926756680011749</span><span class="p">,</span><span class="mf">0.9564056992530823</span><span class="p">,</span><span class="mf">0.1871308982372284</span><span class="p">,</span><span class="mf">0.9039839506149292</span><span class="p">,</span><span class="mf">0.5438059568405151</span><span class="p">,</span><span class="mf">0.4569114148616791</span><span class="p">,</span><span class="mf">0.8820413947105408</span><span class="p">,</span><span class="mf">0.45860394835472107</span><span class="p">,</span><span class="mf">0.7241676449775696</span><span class="p">,</span><span class="mf">0.3990253210067749</span><span class="p">,</span><span class="mf">0.9040443897247314</span><span class="p">,</span><span class="mf">0.6900250315666199</span><span class="p">,</span><span class="mf">0.6996220350265503</span><span class="p">,</span><span class="mf">0.32772040367126465</span><span class="p">,</span><span class="mf">0.7567786574363708</span><span class="p">,</span><span class="mf">0.6360610723495483</span><span class="p">,</span><span class="mf">0.2400202751159668</span><span class="p">,</span><span class="mf">0.16053882241249084</span><span class="p">,</span><span class="mf">0.796391487121582</span><span class="p">,</span><span class="mf">0.9591665863990784</span><span class="p">,</span><span class="mf">0.4581388235092163</span><span class="p">,</span><span class="mf">0.5909841656684875</span><span class="p">,</span><span class="mf">0.8577226400375366</span><span class="p">,</span><span class="mf">0.45722344517707825</span><span class="p">,</span><span class="mf">0.9518744945526123</span><span class="p">,</span><span class="mf">0.5757511854171753</span><span class="p">,</span><span class="mf">0.8207671046257019</span><span class="p">,</span><span class="mf">0.9088436961174011</span><span class="p">,</span><span class="mf">0.8155238032341003</span><span class="p">,</span><span class="mf">0.15941447019577026</span><span class="p">,</span><span class="mf">0.6288984417915344</span><span class="p">,</span><span class="mf">0.39843425154685974</span><span class="p">,</span><span class="mf">0.06271295249462128</span><span class="p">,</span><span class="mf">0.4240322411060333</span><span class="p">,</span><span class="mf">0.25868406891822815</span><span class="p">,</span><span class="mf">0.849038302898407</span><span class="p">,</span><span class="mf">0.03330462798476219</span><span class="p">,</span><span class="mf">0.9589827060699463</span><span class="p">,</span><span class="mf">0.35536885261535645</span><span class="p">,</span><span class="mf">0.3567068874835968</span><span class="p">,</span><span class="mf">0.01632850244641304</span><span class="p">,</span><span class="mf">0.18523232638835907</span><span class="p">,</span><span class="mf">0.40125951170921326</span><span class="p">,</span><span class="mf">0.9292914271354675</span><span class="p">,</span><span class="mf">0.0996149331331253</span><span class="p">,</span><span class="mf">0.9453015327453613</span><span class="p">,</span><span class="mf">0.869488537311554</span><span class="p">,</span><span class="mf">0.4541623890399933</span><span class="p">,</span><span class="mf">0.326700896024704</span><span class="p">,</span><span class="mf">0.23274412751197815</span><span class="p">,</span><span class="mf">0.6144647002220154</span><span class="p">,</span><span class="mf">0.03307459130883217</span><span class="p">,</span><span class="mf">0.015606064349412918</span><span class="p">,</span><span class="mf">0.428795725107193</span><span class="p">,</span><span class="mf">0.06807407736778259</span><span class="p">,</span><span class="mf">0.2519409954547882</span><span class="p">,</span><span class="mf">0.2211609184741974</span><span class="p">,</span><span class="mf">0.253191202878952</span><span class="p">,</span><span class="mf">0.13105523586273193</span><span class="p">,</span><span class="mf">0.012036222964525223</span><span class="p">,</span><span class="mf">0.11548429727554321</span><span class="p">,</span><span class="mf">0.6184802651405334</span><span class="p">,</span><span class="mf">0.9742562174797058</span><span class="p">,</span><span class="mf">0.9903450012207031</span><span class="p">,</span><span class="mf">0.40905410051345825</span><span class="p">,</span><span class="mf">0.1629544198513031</span><span class="p">,</span><span class="mf">0.6387617588043213</span><span class="p">,</span><span class="mf">0.4903053343296051</span><span class="p">,</span><span class="mf">0.9894098043441772</span><span class="p">,</span><span class="mf">0.06530420482158661</span><span class="p">,</span><span class="mf">0.7832344174385071</span><span class="p">,</span><span class="mf">0.28839850425720215</span><span class="p">,</span><span class="mf">0.24141861498355865</span><span class="p">,</span><span class="mf">0.6625045537948608</span><span class="p">,</span><span class="mf">0.24606318771839142</span><span class="p">,</span><span class="mf">0.6658591032028198</span><span class="p">,</span><span class="mf">0.5173085331916809</span><span class="p">,</span><span class="mf">0.4240889847278595</span><span class="p">,</span><span class="mf">0.5546877980232239</span><span class="p">,</span><span class="mf">0.2870515286922455</span><span class="p">,</span><span class="mf">0.7065746784210205</span><span class="p">,</span><span class="mf">0.414856880903244</span><span class="p">,</span><span class="mf">0.3605455458164215</span><span class="p">,</span><span class="mf">0.8286569118499756</span><span class="p">,</span><span class="mf">0.9249669313430786</span><span class="p">,</span><span class="mf">0.04600730910897255</span><span class="p">,</span><span class="mf">0.2326269894838333</span><span class="p">,</span><span class="mf">0.34851935505867004</span><span class="p">,</span><span class="mf">0.8149664998054504</span><span class="p">,</span><span class="mf">0.9854914546012878</span><span class="p">,</span><span class="mf">0.9689717292785645</span><span class="p">,</span><span class="mf">0.904948353767395</span><span class="p">,</span><span class="mf">0.2965562641620636</span><span class="p">,</span><span class="mf">0.9920112490653992</span><span class="p">,</span><span class="mf">0.24942004680633545</span><span class="p">,</span><span class="mf">0.10590615123510361</span><span class="p">,</span><span class="mf">0.9509525895118713</span><span class="p">,</span><span class="mf">0.2334202527999878</span><span class="p">,</span><span class="mf">0.6897682547569275</span><span class="p">,</span><span class="mf">0.05835635960102081</span><span class="p">,</span><span class="mf">0.7307090759277344</span><span class="p">,</span><span class="mf">0.8817201852798462</span><span class="p">,</span><span class="mf">0.27243688702583313</span><span class="p">,</span><span class="mf">0.3790569007396698</span><span class="p">,</span><span class="mf">0.3742961883544922</span><span class="p">,</span><span class="mf">0.7487882375717163</span><span class="p">,</span><span class="mf">0.2378072440624237</span><span class="p">,</span><span class="mf">0.17185309529304504</span><span class="p">,</span><span class="mf">0.4492916464805603</span><span class="p">,</span><span class="mf">0.30446839332580566</span><span class="p">,</span><span class="mf">0.8391891121864319</span><span class="p">,</span><span class="mf">0.23774182796478271</span><span class="p">,</span><span class="mf">0.5023894309997559</span><span class="p">,</span><span class="mf">0.9425836205482483</span><span class="p">,</span><span class="mf">0.6339976787567139</span><span class="p">,</span><span class="mf">0.8672894239425659</span><span class="p">,</span><span class="mf">0.940209686756134</span><span class="p">,</span><span class="mf">0.7507648468017578</span><span class="p">,</span><span class="mf">0.6995750665664673</span><span class="p">,</span><span class="mf">0.9679655432701111</span><span class="p">,</span><span class="mf">0.9944007992744446</span><span class="p">,</span><span class="mf">0.4518216848373413</span><span class="p">,</span><span class="mf">0.07086978107690811</span><span class="p">,</span><span class="mf">0.29279401898384094</span><span class="p">,</span><span class="mf">0.15235470235347748</span><span class="p">,</span><span class="mf">0.41748636960983276</span><span class="p">,</span><span class="mf">0.13128933310508728</span><span class="p">,</span><span class="mf">0.6041178107261658</span><span class="p">,</span><span class="mf">0.38280805945396423</span><span class="p">,</span><span class="mf">0.8953858613967896</span><span class="p">,</span><span class="mf">0.96779465675354</span><span class="p">,</span><span class="mf">0.5468848943710327</span><span class="p">,</span><span class="mf">0.2748235762119293</span><span class="p">,</span><span class="mf">0.5922304391860962</span><span class="p">,</span><span class="mf">0.8967611789703369</span><span class="p">,</span><span class="mf">0.40673333406448364</span><span class="p">,</span><span class="mf">0.5520782470703125</span><span class="p">,</span><span class="mf">0.2716527581214905</span><span class="p">,</span><span class="mf">0.4554441571235657</span><span class="p">,</span><span class="mf">0.4017135500907898</span><span class="p">,</span><span class="mf">0.24841345846652985</span><span class="p">,</span><span class="mf">0.5058664083480835</span><span class="p">,</span><span class="mf">0.31038081645965576</span><span class="p">,</span><span class="mf">0.37303486466407776</span><span class="p">,</span><span class="mf">0.5249704718589783</span><span class="p">,</span><span class="mf">0.7505950331687927</span><span class="p">,</span><span class="mf">0.3335074782371521</span><span class="p">,</span><span class="mf">0.9241587519645691</span><span class="p">,</span><span class="mf">0.8623185753822327</span><span class="p">,</span><span class="mf">0.048690296709537506</span><span class="p">,</span><span class="mf">0.2536425292491913</span><span class="p">,</span><span class="mf">0.4461355209350586</span><span class="p">,</span><span class="mf">0.10462789237499237</span><span class="p">,</span><span class="mf">0.34847599267959595</span><span class="p">,</span><span class="mf">0.7400975227355957</span><span class="p">,</span><span class="mf">0.6805144548416138</span><span class="p">,</span><span class="mf">0.6223844289779663</span><span class="p">,</span><span class="mf">0.7105283737182617</span><span class="p">,</span><span class="mf">0.20492368936538696</span><span class="p">,</span><span class="mf">0.3416981101036072</span><span class="p">,</span><span class="mf">0.676242470741272</span><span class="p">,</span><span class="mf">0.879234790802002</span><span class="p">,</span><span class="mf">0.5436780452728271</span><span class="p">,</span><span class="mf">0.2826996445655823</span><span class="p">,</span><span class="mf">0.030235258862376213</span><span class="p">,</span><span class="mf">0.7103368043899536</span><span class="p">,</span><span class="mf">0.007884103804826736</span><span class="p">,</span><span class="mf">0.37267908453941345</span><span class="p">,</span><span class="mf">0.5305371880531311</span><span class="p">,</span><span class="mf">0.922111451625824</span><span class="p">,</span><span class="mf">0.08949454873800278</span><span class="p">,</span><span class="mf">0.40594232082366943</span><span class="p">,</span><span class="mf">0.024313200265169144</span><span class="p">,</span><span class="mf">0.3426109850406647</span><span class="p">,</span><span class="mf">0.6222310662269592</span><span class="p">,</span><span class="mf">0.2790679335594177</span><span class="p">,</span><span class="mf">0.2097499519586563</span><span class="p">,</span><span class="mf">0.11570323258638382</span><span class="p">,</span><span class="mf">0.5771402716636658</span><span class="p">,</span><span class="mf">0.6952700018882751</span><span class="p">,</span><span class="mf">0.6719571352005005</span><span class="p">,</span><span class="mf">0.9488610029220581</span><span class="p">,</span><span class="mf">0.002703213831409812</span><span class="p">,</span><span class="mf">0.6471966505050659</span><span class="p">,</span><span class="mf">0.60039222240448</span><span class="p">,</span><span class="mf">0.5887396335601807</span><span class="p">,</span><span class="mf">0.9627703428268433</span><span class="p">,</span><span class="mf">0.016871673986315727</span><span class="p">,</span><span class="mf">0.6964824199676514</span><span class="p">,</span><span class="mf">0.8136786222457886</span><span class="p">,</span><span class="mf">0.5098071694374084</span><span class="p">,</span><span class="mf">0.33396488428115845</span><span class="p">,</span><span class="mf">0.7908401489257812</span><span class="p">,</span><span class="mf">0.09724292904138565</span><span class="p">,</span><span class="mf">0.44203564524650574</span><span class="p">,</span><span class="mf">0.5199523568153381</span><span class="p">,</span><span class="mf">0.6939564347267151</span><span class="p">,</span><span class="mf">0.09088572859764099</span><span class="p">,</span><span class="mf">0.2277594953775406</span><span class="p">,</span><span class="mf">0.4103015661239624</span><span class="p">,</span><span class="mf">0.6232946515083313</span><span class="p">,</span><span class="mf">0.8869608044624329</span><span class="p">,</span><span class="mf">0.618826150894165</span><span class="p">,</span><span class="mf">0.13346147537231445</span><span class="p">,</span><span class="mf">0.9805801510810852</span><span class="p">,</span><span class="mf">0.8717857599258423</span><span class="p">,</span><span class="mf">0.5027207732200623</span><span class="p">,</span><span class="mf">0.9223479628562927</span><span class="p">,</span><span class="mf">0.5413808226585388</span><span class="p">,</span><span class="mf">0.9233060479164124</span><span class="p">,</span><span class="mf">0.8298973441123962</span><span class="p">,</span><span class="mf">0.968286395072937</span><span class="p">,</span><span class="mf">0.919782817363739</span><span class="p">,</span><span class="mf">0.03603381663560867</span><span class="p">,</span><span class="mf">0.1747720092535019</span><span class="p">,</span><span class="mf">0.3891346752643585</span><span class="p">,</span><span class="mf">0.9521427154541016</span><span class="p">,</span><span class="mf">0.300028920173645</span><span class="p">,</span><span class="mf">0.16046763956546783</span><span class="p">,</span><span class="mf">0.8863046765327454</span><span class="p">,</span><span class="mf">0.4463944137096405</span><span class="p">,</span><span class="mf">0.9078755974769592</span><span class="p">,</span><span class="mf">0.16023047268390656</span><span class="p">,</span><span class="mf">0.6611174941062927</span><span class="p">,</span><span class="mf">0.4402637481689453</span><span class="p">,</span><span class="mf">0.07648676633834839</span><span class="p">,</span><span class="mf">0.6964631676673889</span><span class="p">,</span><span class="mf">0.2473987489938736</span><span class="p">,</span><span class="mf">0.03961552307009697</span><span class="p">,</span><span class="mf">0.05994429811835289</span><span class="p">,</span><span class="mf">0.06107853725552559</span><span class="p">,</span><span class="mf">0.9077329635620117</span><span class="p">,</span><span class="mf">0.7398838996887207</span><span class="p">,</span><span class="mf">0.8980623483657837</span><span class="p">,</span><span class="mf">0.6725823283195496</span><span class="p">,</span><span class="mf">0.5289399027824402</span><span class="p">,</span><span class="mf">0.30444636940956116</span><span class="p">,</span><span class="mf">0.997962236404419</span><span class="p">,</span><span class="mf">0.36218905448913574</span><span class="p">,</span><span class="mf">0.47064894437789917</span><span class="p">,</span><span class="mf">0.37824517488479614</span><span class="p">,</span><span class="mf">0.979526937007904</span><span class="p">,</span><span class="mf">0.1746583878993988</span><span class="p">,</span><span class="mf">0.32798799872398376</span><span class="p">,</span><span class="mf">0.6803486943244934</span><span class="p">,</span><span class="mf">0.06320761889219284</span><span class="p">,</span><span class="mf">0.60724937915802</span><span class="p">,</span><span class="mf">0.47764649987220764</span><span class="p">,</span><span class="mf">0.2839999794960022</span><span class="p">,</span><span class="mf">0.2384132742881775</span><span class="p">,</span><span class="mf">0.5145127177238464</span><span class="p">,</span><span class="mf">0.36792758107185364</span><span class="p">,</span><span class="mf">0.4565199017524719</span><span class="p">,</span><span class="mf">0.3374773859977722</span><span class="p">,</span><span class="mf">0.9704936742782593</span><span class="p">,</span><span class="mf">0.13343943655490875</span><span class="p">,</span><span class="mf">0.09680395573377609</span><span class="p">,</span><span class="mf">0.3433917164802551</span><span class="p">,</span><span class="mf">0.5910269021987915</span><span class="p">,</span><span class="mf">0.6591764688491821</span><span class="p">,</span><span class="mf">0.3972567617893219</span><span class="p">,</span><span class="mf">0.9992780089378357</span><span class="p">,</span><span class="mf">0.35189300775527954</span><span class="p">,</span><span class="mf">0.7214066386222839</span><span class="p">,</span><span class="mf">0.6375827193260193</span><span class="p">,</span><span class="mf">0.8130538463592529</span><span class="p">,</span><span class="mf">0.9762256741523743</span><span class="p">,</span><span class="mf">0.8897936344146729</span><span class="p">,</span><span class="mf">0.7645619511604309</span><span class="p">,</span><span class="mf">0.6982485055923462</span><span class="p">,</span><span class="mf">0.335498183965683</span><span class="p">,</span><span class="mf">0.14768557250499725</span><span class="p">,</span><span class="mf">0.06263600289821625</span><span class="p">,</span><span class="mf">0.2419017106294632</span><span class="p">,</span><span class="mf">0.432281494140625</span><span class="p">,</span><span class="mf">0.521996259689331</span><span class="p">,</span><span class="mf">0.7730835676193237</span><span class="p">,</span><span class="mf">0.9587409496307373</span><span class="p">,</span><span class="mf">0.1173204779624939</span><span class="p">,</span><span class="mf">0.10700414329767227</span><span class="p">,</span><span class="mf">0.5896947383880615</span><span class="p">,</span><span class="mf">0.7453980445861816</span><span class="p">,</span><span class="mf">0.848150372505188</span><span class="p">,</span><span class="mf">0.9358320832252502</span><span class="p">,</span><span class="mf">0.9834262132644653</span><span class="p">,</span><span class="mf">0.39980170130729675</span><span class="p">,</span><span class="mf">0.3803351819515228</span><span class="p">,</span><span class="mf">0.14780867099761963</span><span class="p">,</span><span class="mf">0.6849344372749329</span><span class="p">,</span><span class="mf">0.6567619442939758</span><span class="p">,</span><span class="mf">0.8620625734329224</span><span class="p">,</span><span class="mf">0.09725799411535263</span><span class="p">,</span><span class="mf">0.49777689576148987</span><span class="p">,</span><span class="mf">0.5810819268226624</span><span class="p">,</span><span class="mf">0.2415570467710495</span><span class="p">,</span><span class="mf">0.16902540624141693</span><span class="p">,</span><span class="mf">0.8595808148384094</span><span class="p">,</span><span class="mf">0.05853492394089699</span><span class="p">,</span><span class="mf">0.47062090039253235</span><span class="p">,</span><span class="mf">0.11583399772644043</span><span class="p">,</span><span class="mf">0.45705875754356384</span><span class="p">,</span><span class="mf">0.9799623489379883</span><span class="p">,</span><span class="mf">0.4237063527107239</span><span class="p">,</span><span class="mf">0.857124924659729</span><span class="p">,</span><span class="mf">0.11731556057929993</span><span class="p">,</span><span class="mf">0.2712520658969879</span><span class="p">,</span><span class="mf">0.40379273891448975</span><span class="p">,</span><span class="mf">0.39981213212013245</span><span class="p">,</span><span class="mf">0.6713835000991821</span><span class="p">,</span><span class="mf">0.3447181284427643</span><span class="p">,</span><span class="mf">0.713766872882843</span><span class="p">,</span><span class="mf">0.6391869187355042</span><span class="p">,</span><span class="mf">0.399161159992218</span><span class="p">,</span><span class="mf">0.43176013231277466</span><span class="p">,</span><span class="mf">0.614527702331543</span><span class="p">,</span><span class="mf">0.0700421929359436</span><span class="p">,</span><span class="mf">0.8224067091941833</span><span class="p">,</span><span class="mf">0.65342116355896</span><span class="p">,</span><span class="mf">0.7263424396514893</span><span class="p">,</span><span class="mf">0.5369229912757874</span><span class="p">,</span><span class="mf">0.11047711223363876</span><span class="p">,</span><span class="mf">0.4050356149673462</span><span class="p">,</span><span class="mf">0.40537357330322266</span><span class="p">,</span><span class="mf">0.3210429847240448</span><span class="p">,</span><span class="mf">0.029950324445962906</span><span class="p">,</span><span class="mf">0.73725426197052</span><span class="p">,</span><span class="mf">0.10978446155786514</span><span class="p">,</span><span class="mf">0.6063081622123718</span><span class="p">,</span><span class="mf">0.7032175064086914</span><span class="p">,</span><span class="mf">0.6347863078117371</span><span class="p">,</span><span class="mf">0.95914226770401</span><span class="p">,</span><span class="mf">0.10329815745353699</span><span class="p">,</span><span class="mf">0.8671671748161316</span><span class="p">,</span><span class="mf">0.02919023483991623</span><span class="p">,</span><span class="mf">0.534916877746582</span><span class="p">,</span><span class="mf">0.4042436182498932</span><span class="p">,</span><span class="mf">0.5241838693618774</span><span class="p">,</span><span class="mf">0.36509987711906433</span><span class="p">,</span><span class="mf">0.19056691229343414</span><span class="p">,</span><span class="mf">0.01912289671599865</span><span class="p">,</span><span class="mf">0.5181497931480408</span><span class="p">,</span><span class="mf">0.8427768349647522</span><span class="p">,</span><span class="mf">0.3732159435749054</span><span class="p">,</span><span class="mf">0.2228638231754303</span><span class="p">,</span><span class="mf">0.080532006919384</span><span class="p">,</span><span class="mf">0.0853109210729599</span><span class="p">,</span><span class="mf">0.22139644622802734</span><span class="p">,</span><span class="mf">0.10001406073570251</span><span class="p">,</span><span class="mf">0.26503971219062805</span><span class="p">,</span><span class="mf">0.06614946573972702</span><span class="p">,</span><span class="mf">0.06560486555099487</span><span class="p">,</span><span class="mf">0.8562761545181274</span><span class="p">,</span><span class="mf">0.1621202677488327</span><span class="p">,</span><span class="mf">0.5596824288368225</span><span class="p">,</span><span class="mf">0.7734555602073669</span><span class="p">,</span><span class="mf">0.4564095735549927</span><span class="p">,</span><span class="mf">0.15336887538433075</span><span class="p">,</span><span class="mf">0.19959613680839539</span><span class="p">,</span><span class="mf">0.43298420310020447</span><span class="p">,</span><span class="mf">0.52823406457901</span><span class="p">,</span><span class="mf">0.3494403064250946</span><span class="p">,</span><span class="mf">0.7814795970916748</span><span class="p">,</span><span class="mf">0.7510216236114502</span><span class="p">,</span><span class="mf">0.9272118210792542</span><span class="p">,</span><span class="mf">0.028952548280358315</span><span class="p">,</span><span class="mf">0.8956912755966187</span><span class="p">,</span><span class="mf">0.39256879687309265</span><span class="p">,</span><span class="mf">0.8783724904060364</span><span class="p">,</span><span class="mf">0.690784752368927</span><span class="p">,</span><span class="mf">0.987348735332489</span><span class="p">,</span><span class="mf">0.7592824697494507</span><span class="p">,</span><span class="mf">0.3645446300506592</span><span class="p">,</span><span class="mf">0.5010631680488586</span><span class="p">,</span><span class="mf">0.37638914585113525</span><span class="p">,</span><span class="mf">0.364911824464798</span><span class="p">,</span><span class="mf">0.2609044909477234</span><span class="p">,</span><span class="mf">0.49597030878067017</span><span class="p">,</span><span class="mf">0.6817399263381958</span><span class="p">,</span><span class="mf">0.27734026312828064</span><span class="p">,</span><span class="mf">0.5243797898292542</span><span class="p">,</span><span class="mf">0.117380291223526</span><span class="p">,</span><span class="mf">0.1598452925682068</span><span class="p">,</span><span class="mf">0.04680635407567024</span><span class="p">,</span><span class="mf">0.9707314372062683</span><span class="p">,</span><span class="mf">0.0038603513967245817</span><span class="p">,</span><span class="mf">0.17857997119426727</span><span class="p">,</span><span class="mf">0.6128667593002319</span><span class="p">,</span><span class="mf">0.08136960119009018</span><span class="p">,</span><span class="mf">0.8818964958190918</span><span class="p">,</span><span class="mf">0.7196201682090759</span><span class="p">,</span><span class="mf">0.9663899540901184</span><span class="p">,</span><span class="mf">0.5076355338096619</span><span class="p">,</span><span class="mf">0.3004036843776703</span><span class="p">,</span><span class="mf">0.549500584602356</span><span class="p">,</span><span class="mf">0.9308187365531921</span><span class="p">,</span><span class="mf">0.5207614302635193</span><span class="p">,</span><span class="mf">0.2672070264816284</span><span class="p">,</span><span class="mf">0.8773987889289856</span><span class="p">,</span><span class="mf">0.3719187378883362</span><span class="p">,</span><span class="mf">0.0013833499979227781</span><span class="p">,</span><span class="mf">0.2476850152015686</span><span class="p">,</span><span class="mf">0.31823351979255676</span><span class="p">,</span><span class="mf">0.8587774634361267</span><span class="p">,</span><span class="mf">0.4585031569004059</span><span class="p">,</span><span class="mf">0.4445872902870178</span><span class="p">,</span><span class="mf">0.33610227704048157</span><span class="p">,</span><span class="mf">0.880678117275238</span><span class="p">,</span><span class="mf">0.9450267553329468</span><span class="p">,</span><span class="mf">0.9918903112411499</span><span class="p">,</span><span class="mf">0.3767412602901459</span><span class="p">,</span><span class="mf">0.9661474227905273</span><span class="p">,</span><span class="mf">0.7918795943260193</span><span class="p">,</span><span class="mf">0.675689160823822</span><span class="p">,</span><span class="mf">0.24488948285579681</span><span class="p">,</span><span class="mf">0.21645726263523102</span><span class="p">,</span><span class="mf">0.1660478264093399</span><span class="p">,</span><span class="mf">0.9227566123008728</span><span class="p">,</span><span class="mf">0.2940766513347626</span><span class="p">,</span><span class="mf">0.4530942440032959</span><span class="p">,</span><span class="mf">0.49395784735679626</span><span class="p">,</span><span class="mf">0.7781715989112854</span><span class="p">,</span><span class="mf">0.8442349433898926</span><span class="p">,</span><span class="mf">0.1390727013349533</span><span class="p">,</span><span class="mf">0.4269043505191803</span><span class="p">,</span><span class="mf">0.842854917049408</span><span class="p">,</span><span class="mf">0.8180332779884338</span><span class="p">};</span>
<span class="kt">float</span><span class="w"> </span><span class="n">calib_output0_data</span><span class="p">[</span><span class="n">NET_OUTPUT0_SIZE</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">3.5647096e-05</span><span class="p">,</span><span class="mf">6.824297e-08</span><span class="p">,</span><span class="mf">0.009327697</span><span class="p">,</span><span class="mf">3.2340475e-05</span><span class="p">,</span><span class="mf">1.1117579e-05</span><span class="p">,</span><span class="mf">1.5117058e-06</span><span class="p">,</span><span class="mf">4.6314454e-07</span><span class="p">,</span><span class="mf">5.161628e-11</span><span class="p">,</span><span class="mf">0.9905911</span><span class="p">,</span><span class="mf">3.8835238e-10</span><span class="p">};</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="compiling-and-burning-the-project">
<h4>Compiling and Burning the Project<a class="headerlink" href="#compiling-and-burning-the-project" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Compile</p>
<p>In the MCU project directory, run the <code class="docutils literal notranslate"><span class="pre">make</span></code> command to compile the MCU project. After the compilation is successful, the following information is displayed, in which test_stm767 is the MCU project name in this example.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>arm-none-eabi-size build/test_stm767.elf
text      data    bss    dec       hex      filename
120316    3620    87885  211821    33b6d    build/test_stm767.elf
arm-none-eabi-objcopy -O ihex build/test_stm767.elf build/test_stm767.hex
arm-none-eabi-objcopy -O binary -S build/test_stm767.elf build/test_stm767.bin
</pre></div>
</div>
</li>
<li><p>Burn and run</p>
<p>The <code class="docutils literal notranslate"><span class="pre">STMSTM32CubePrg</span></code> tool can be used to burn and run the code. On the PC, use <code class="docutils literal notranslate"><span class="pre">STLink</span></code> to connect to a development board that can be burnt. Then, run the following commands in the current MCU project directory to burn and run the program:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span><span class="si">${</span><span class="nv">STMSTM32CubePrg_PATH</span><span class="si">}</span>/bin/STM32_Programmer.sh<span class="w"> </span>-c<span class="w"> </span><span class="nv">port</span><span class="o">=</span>SWD<span class="w"> </span>-w<span class="w"> </span>build/test_stm767.bin<span class="w"> </span>0x08000000<span class="w"> </span>-s<span class="w"> </span>0x08000000
</pre></div>
</div>
<p>${STMSTM32CubePrg_PATH is}: installation path of <code class="docutils literal notranslate"><span class="pre">STMSTM32CubePrg</span></code>. For details about the parameters in the command, see the <code class="docutils literal notranslate"><span class="pre">STMSTM32CubePrg</span></code> user manual.</p>
</li>
</ul>
</section>
<section id="inference-result-verification">
<h4>Inference Result Verification<a class="headerlink" href="#inference-result-verification" title="Permalink to this headline"></a></h4>
<p>In this example, the benchmark running result flag is stored in the memory segment whose start address is <code class="docutils literal notranslate"><span class="pre">0x20000000</span></code> and whose size is 1 byte. Therefore, you can directly obtain the data at this address by using the burner to obtain the result returned by the program.
On the PC, use <code class="docutils literal notranslate"><span class="pre">STLink</span></code> to connect to a development board where programs have been burnt. Run the following command to read the memory data:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span><span class="si">${</span><span class="nv">STMSTM32CubePrg_PATH</span><span class="p"> is </span><span class="si">}</span>/bin/STM32_Programmer.sh<span class="w"> </span>-c<span class="w"> </span><span class="nv">port</span><span class="o">=</span>SWD<span class="w"> </span><span class="nv">model</span><span class="o">=</span>HOTPLUG<span class="w"> </span>--upload<span class="w"> </span>0x20000000<span class="w"> </span>0x1<span class="w"> </span>ret.bin
</pre></div>
</div>
<p>${STMSTM32CubePrg_PATH is}: installation path of <code class="docutils literal notranslate"><span class="pre">STMSTM32CubePrg</span></code>. For details about the parameters in the command, see the <code class="docutils literal notranslate"><span class="pre">STMSTM32CubePrg</span></code> user manual.</p>
<p>The read data is saved in the <code class="docutils literal notranslate"><span class="pre">ret.bin</span></code> file and run <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">ret.bin</span></code>. If the board inference is successful and <code class="docutils literal notranslate"><span class="pre">ret.bin</span></code> stores <code class="docutils literal notranslate"><span class="pre">1</span></code>, the following information is displayed:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</section>
</section>
</section>
<section id="executing-inference-on-light-harmony-devices">
<h2>Executing Inference on Light Harmony Devices<a class="headerlink" href="#executing-inference-on-light-harmony-devices" title="Permalink to this headline"></a></h2>
<section id="preparing-the-light-harmony-compilation-environment">
<h3>Preparing the Light Harmony Compilation Environment<a class="headerlink" href="#preparing-the-light-harmony-compilation-environment" title="Permalink to this headline"></a></h3>
<p>You can learn how to compile and burn in the light Harmony environment at the <a class="reference external" href="https://www.openharmony.cn">OpenHarmony official website</a>.
This tutorial uses the Hi3516 development board as an example to demonstrate how to use Micro to deploy the inference model in the light Harmony environment.</p>
</section>
<section id="compiling-models">
<h3>Compiling Models<a class="headerlink" href="#compiling-models" title="Permalink to this headline"></a></h3>
<p>Use converter_lite to compile the <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/micro/mnist.tar.gz">lenet model</a> and generate the inference code corresponding to the light Harmony platform. The command is as follows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">SOURCE_CODE_DIR</span><span class="si">}</span><span class="w"> </span>--configFile<span class="o">=</span><span class="si">${</span><span class="nv">COFIG_FILE</span><span class="si">}</span>
</pre></div>
</div>
<p>Set target to ARM32 in the config configuration file.</p>
</section>
<section id="compiling-the-build-script">
<h3>Compiling the Build Script<a class="headerlink" href="#compiling-the-build-script" title="Permalink to this headline"></a></h3>
<p>For details about how to develop light Harmony applications, see <a class="reference external" href="https://device.harmonyos.com/cn/docs/start/introduce/quickstart-lite-steps-board3516-running-0000001151888681">Running Hello OHOS</a>. Copy the mnist directory generated in the previous step to any Harmony source code path, such as, applications/sample/, and create the Build.gn file.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&lt;harmony-source-path&gt;/applications/sample/mnist
├── benchmark
├── CMakeLists.txt
├── BUILD.gn
└── src  
</pre></div>
</div>
<p>Download the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/downloads.html">precompiled inference runtime package</a> for OpenHarmony and decompress it to any Harmony source code path. Compile Build.gn file:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>import(&quot;//build/lite/config/component/lite_component.gni&quot;)
import(&quot;//build/lite/ndk/ndk.gni&quot;)

lite_component(&quot;mnist_benchmark&quot;) {
    target_type = &quot;executable&quot;
    sources = [
        &quot;benchmark/benchmark.cc&quot;,
        &quot;benchmark/calib_output.cc&quot;,
        &quot;benchmark/load_input.c&quot;,
        &quot;src/net.c&quot;,
        &quot;src/weight.c&quot;,
        &quot;src/session.cc&quot;,
        &quot;src/tensor.cc&quot;,
    ]
    features = []
    include_dirs = [
        &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/runtime&quot;,
        &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/tools/codegen/include&quot;,
        &quot;//applications/sample/mnist/benchmark&quot;,
        &quot;//applications/sample/mnist/src&quot;,
    ]
    ldflags = [
        &quot;-fno-strict-aliasing&quot;,
        &quot;-Wall&quot;,
        &quot;-pedantic&quot;,
        &quot;-std=gnu99&quot;,
    ]
    libs = [
        &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/runtime/lib/libmindspore-lite.a&quot;,
        &quot;&lt;YOUR MINDSPORE LITE RUNTIME PATH&gt;/tools/codegen/lib/libwrapper.a&quot;,
    ]
    defines = [
        &quot;NOT_USE_STL&quot;,
        &quot;ENABLE_NEON&quot;,
        &quot;ENABLE_ARM&quot;,
        &quot;ENABLE_ARM32&quot;
    ]
    cflags = [
        &quot;-fno-strict-aliasing&quot;,
        &quot;-Wall&quot;,
        &quot;-pedantic&quot;,
        &quot;-std=gnu99&quot;,
    ]
    cflags_cc = [
        &quot;-fno-strict-aliasing&quot;,
        &quot;-Wall&quot;,
        &quot;-pedantic&quot;,
        &quot;-std=c++17&quot;,
    ]
}
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">&lt;YOUR</span> <span class="pre">MINDSPORE</span> <span class="pre">LITE</span> <span class="pre">RUNTIME</span> <span class="pre">PATH&gt;</span></code> is the path of the decompressed inference runtime package, such as //applications/sample/mnist/mindspore-lite-1.3.0-ohos-aarch32.
Modify the build/lite/components/applications.json file and add the mnist_benchmark configuration:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{
    &quot;component&quot;: &quot;mnist_benchmark&quot;,
    &quot;description&quot;: &quot;Communication related samples.&quot;,
    &quot;optional&quot;: &quot;true&quot;,
    &quot;dirs&quot;: [
    &quot;applications/sample/mnist&quot;
    ],
    &quot;targets&quot;: [
    &quot;//applications/sample/mnist:mnist_benchmark&quot;
    ],
    &quot;rom&quot;: &quot;&quot;,
    &quot;ram&quot;: &quot;&quot;,
    &quot;output&quot;: [],
    &quot;adapted_kernel&quot;: [ &quot;liteos_a&quot; ],
    &quot;features&quot;: [],
    &quot;deps&quot;: {
    &quot;components&quot;: [],
    &quot;third_party&quot;: []
    }
},
</pre></div>
</div>
<p>Modify vendor/hisilicon/hispark_taurus/config.json file and add mnist_benchmark component.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>{ &quot;component&quot;: &quot;mnist_benchmark&quot;, &quot;features&quot;:[] }
</pre></div>
</div>
</section>
<section id="compiling-benchmark">
<h3>Compiling benchmark<a class="headerlink" href="#compiling-benchmark" title="Permalink to this headline"></a></h3>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cd &lt;openharmony-source-path&gt;
hb set(Set the compilation path)
.(Select Current Path)
Select ipcamera_hispark_taurus@hisilicon and press Enter.
hb build mnist_benchmark (Perform compilation)
</pre></div>
</div>
<p>Generate the result file out/hispark_taurus/ipcamera_hispark_taurus/bin/mnist_benchmark.</p>
</section>
<section id="performing-benchmark">
<h3>Performing benchmark<a class="headerlink" href="#performing-benchmark" title="Permalink to this headline"></a></h3>
<p>Decompress mnist_benchmark, weight file (mnist/src/net.bin), and <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/micro/mnist.tar.gz">input file</a>, copy them to the development board, and run the following commands:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>OHOS # ./mnist_benchmark mnist_input.bin net.bin 1
OHOS # =======run benchmark======
input 0: mnist_input.bin

loop count: 1
total time: 10.11800ms, per time: 10.11800ms

outputs:
name: int8toft32_Softmax-7_post0/output-0, DataType: 43, Elements: 10, Shape: [1 10 ], Data:
0.000000, 0.000000, 0.003906, 0.000000, 0.000000, 0.992188, 0.000000, 0.000000, 0.000000, 0.000000,
========run success=======
</pre></div>
</div>
</section>
</section>
<section id="custom-kernel">
<h2>Custom Kernel<a class="headerlink" href="#custom-kernel" title="Permalink to this headline"></a></h2>
<p>Please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/register.html">Custom Kernel</a> to understand the basic concepts before using.
Micro currently only supports the registration and implementation of custom operators of custom type, and does not support the registration and custom implementation of built-in operators (such as conv2d and fc).
We use Hi3516D board as an example to show you how to use kernel register in Micro.</p>
<p>Please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/nnie.html">Usage Description of the Integrated NNIE</a> for the specific steps of using the conversion tool to generate custom operators for NNIE.</p>
<p>The manner that the model generates code is consistent with that of the non-custom operator model.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>mnist.tflite<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">SOURCE_CODE_DIR</span><span class="si">}</span><span class="w"> </span>--configFile<span class="o">=</span><span class="si">${</span><span class="nv">COFIG_FILE</span><span class="si">}</span>
</pre></div>
</div>
<p>where target sets to be ARM32.</p>
<section id="implementing-custom-kernel-by-users">
<h3>Implementing custom kernel by users<a class="headerlink" href="#implementing-custom-kernel-by-users" title="Permalink to this headline"></a></h3>
<p>The previous step generates the source code directory under the specified path with a header file called <code class="docutils literal notranslate"><span class="pre">src/registered_kernel.h</span></code> that specifies the function declarations for the custom operator.</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">CustomKernel</span><span class="p">(</span><span class="n">TensorC</span><span class="w"> </span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">input_num</span><span class="p">,</span><span class="w"> </span><span class="n">TensorC</span><span class="w"> </span><span class="o">*</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">output_num</span><span class="p">,</span><span class="w"> </span><span class="n">CustomParameter</span><span class="w"> </span><span class="o">*</span><span class="n">param</span><span class="p">);</span>
</pre></div>
</div>
<p>Users need to implement this function and add their source files to the cmake project. For example, we provide the custom kernel example dynamic library libmicro_nnie.so that supports NNIE from Hysis, which is included in the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/downloads.html">official download page</a> “NNIE inference runtime lib, benchmark tool” component. Users need to modify the CMakeLists.txt of the generated code, add the name and path of the linked library.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>link_directories<span class="o">(</span>&lt;YOUR_PATH&gt;/mindspore-lite-1.8.1-linux-aarch32/providers/Hi3516D<span class="o">)</span>

link_directories<span class="o">(</span>&lt;HI3516D_SDK_PATH&gt;<span class="o">)</span>

target_link_libraries<span class="o">(</span>benchmark<span class="w"> </span>net<span class="w"> </span>micro_nnie<span class="w"> </span>nnie<span class="w"> </span>mpi<span class="w"> </span>VoiceEngine<span class="w"> </span>upvqe<span class="w"> </span>dnvqe<span class="w"> </span>securec<span class="w"> </span>-lm<span class="w"> </span>-pthread<span class="o">)</span>
</pre></div>
</div>
<p>In the generated <code class="docutils literal notranslate"><span class="pre">benchmark/benchmark.c</span></code> file, add the <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/test/config_level0/micro/svp_sys_init.c">NNIE device related initialization code</a> before and after calling the main function.
Finally, we compile the source code:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>buid<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build

cmake<span class="w"> </span>-DCMAKE_TOOLCHAIN_FILE<span class="o">=</span>&lt;MS_SRC_PATH&gt;/mindspore/lite/cmake/himix200.toolchain.cmake<span class="w"> </span>-DPLATFORM_ARM32<span class="o">=</span>ON<span class="w"> </span>-DPKG_PATH<span class="o">=</span>&lt;RUNTIME_PKG_PATH&gt;<span class="w"> </span>..

make
</pre></div>
</div>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_java.html" class="btn btn-neutral float-left" title="Using Java Interface to Perform Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="asic.html" class="btn btn-neutral float-right" title="Application Specific Integrated Circuit Integration Instructions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>