

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Using C++ Interface to Perform Inference &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Using Java Interface to Perform Inference" href="runtime_java.html" />
    <link rel="prev" title="Executing Model Inference" href="runtime.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Getting Started in One Hour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_cpp.html">Experience C++ Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_cpp.html">Experience C++ Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_java.html">Experience Java Simple Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_java.html">Experience Java Minimalist Concurrent Reasoning Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_python.html">Experiencing the Python Simplified Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_server_inference_python.html">Experiencing the Python Simplified Concurrent Inference Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/quick_start_c.html">Expriencing Simpcified Inference Demo with C-language</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet.html">Implement Device Training Based On C++ Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/train_lenet_java.html">Implement Device Training Based On Java Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Inference on Devices</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime.html">Executing Model Inference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using C++ Interface to Perform Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-reading">Model Reading</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-and-configuring-context">Creating and Configuring Context</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-number-of-threads">Configuring the Number of Threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-thread-affinity">Configuring the Thread Affinity</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-parallelization">Configuring the Parallelization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-gpu-backend">Configuring the GPU Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-npu-backend">Configuring the NPU Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-nnie-backend">Configuring the NNIE Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-ascend-backend">Configuring the ASCEND Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-the-coreml-backend">Configuring the CoreML Backend</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-creating-loading-and-building">Model Creating Loading and Building</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inputting-data">Inputting Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#executing-inference">Executing Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#obtaining-output">Obtaining Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="#releasing-memory">Releasing Memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-usage">Advanced Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#resizing-the-input-dimension">Resizing the Input Dimension</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mixed-precision-inference">Mixed Precision Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multiple-heterogeneous-devices-inference">Multiple Heterogeneous Devices Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#opengl-texture-data-input">OpenGL Texture Data Input</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sharing-a-memory-pool">Sharing a Memory Pool</a></li>
<li class="toctree-l4"><a class="reference internal" href="#calling-back-a-model-during-the-running-process">Calling Back a Model During the Running Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="#separating-graph-loading-and-model-build">Separating Graph Loading and Model Build</a></li>
<li class="toctree-l4"><a class="reference internal" href="#viewing-logs">Viewing Logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-the-version-number">Obtaining the Version Number</a></li>
<li class="toctree-l4"><a class="reference internal" href="#extension-usage">Extension Usage</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="runtime_java.html">Using Java Interface to Perform Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training on Devices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Server Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime_server_inference.html">Executing Server Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime.html">Executing Model Inference</a> &raquo;</li>
      <li>Using C++ Interface to Perform Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/use/runtime_cpp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section class="tex2jax_ignore mathjax_ignore" id="using-c++-interface-to-perform-inference">
<h1>Using C++ Interface to Perform Inference<a class="headerlink" href="#using-c++-interface-to-perform-inference" title="Permalink to this headline"></a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0.0-alpha/docs/lite/docs/source_en/use/runtime_cpp.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0.0-alpha/resource/_static/logo_source_en.png"></a></p>
<blockquote>
<div><p>MindSpore has unified the inference API. If you want to continue to use the MindSpore Lite independent API for inference, you can refer to the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r1.3/use/runtime_cpp.html">document</a>.</p>
</div></blockquote>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>After the model is converted into a <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model by using the MindSpore Lite model conversion tool, the inference process can be performed in Runtime. For details, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/converter_tool.html">Converting Models for Inference</a>. This tutorial describes how to use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/index.html">C++ API</a> to perform inference.</p>
<p>To use the MindSpore Lite inference framework, perform the following steps:</p>
<ol class="arabic simple">
<li><p>Read the model: Read the <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model file converted by the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/converter_tool.html">model conversion tool</a> from the file system.</p></li>
<li><p>Create and configure context: Create and configure <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html#class-context">Context</a> to save some basic configuration parameters required to build and execute the model.</p></li>
<li><p>Create, load and build a model: Use <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#build">Build</a> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a> to create and build the model, and configure the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html#class-context">Context</a> obtained in the previous step. In the model loading phase, the file cache is parsed into a runtime model. In the model building phase, subgraph partition, operator selection and scheduling are performed, which will take a long time. Therefore, it is recommended that the model should be created once, built once, and performed for multiple times.</p></li>
<li><p>Input data: Before the model is executed, data needs to be filled in the <code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">Tensor</span></code>.</p></li>
<li><p>Perform inference: Use <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#predict">Predict</a> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a> to perform model inference.</p></li>
<li><p>Obtain the output: After the model execution is complete, you can obtain the inference result by <code class="docutils literal notranslate"><span class="pre">Output</span> <span class="pre">Tensor</span></code>.</p></li>
<li><p>Release the memory: If the MindSpore Lite inference framework is not required, release the created <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a>.</p></li>
</ol>
<p><img alt="img" src="../_images/lite_runtime.png" /></p>
<blockquote>
<div><p>For details about the calling process of MindSpore Lite inference, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/quick_start/quick_start_cpp.html">Simplified MindSpore Lite C++ Demo</a>.</p>
</div></blockquote>
</section>
<section id="model-reading">
<h2>Model Reading<a class="headerlink" href="#model-reading" title="Permalink to this headline"></a></h2>
<p>When MindSpore Lite is used for model inference, read the <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model file converted by using the model conversion tool from the file system and store it in the memory buffer. For details, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/converter_tool.html">Converting Models for Inference</a>.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L332">main.cc</a> demonstrates how to load a MindSpore Lite model from the file system:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Read model file.</span>
<span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_buf</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Read model file failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="creating-and-configuring-context">
<h2>Creating and Configuring Context<a class="headerlink" href="#creating-and-configuring-context" title="Permalink to this headline"></a></h2>
<p>The context saves some basic configuration parameters required to build and execute the model. If you use <code class="docutils literal notranslate"><span class="pre">new</span></code> to create a <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html#class-context">Context</a> and do not need it any more, use <code class="docutils literal notranslate"><span class="pre">delete</span></code> to release it. Generally, the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html#class-context">Context</a> is released after the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a> is created and built.</p>
<p>The default backend of MindSpore Lite is CPU. After Context is created, call <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#mutabledeviceinfo">MutableDeviceInfo</a> to return list of backend device information. Add the default <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_CPUDeviceInfo.html#class-cpudeviceinfo">CPUDeviceInfo</a> to the list.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L250">main.cc</a> demonstrates how to create a context, configure the default CPU backend, and enable CPU float16 inference.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">MutableDeviceInfo</span></code> supports multiple DeviceInfos, including <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_CPUDeviceInfo.html#class-cpudeviceinfo">CPUDeviceInfo</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_GPUDeviceInfo.html#class-gpudeviceinfo">GPUDeviceInfo</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_KirinNPUDeviceInfo.html#class-kirinnpudeviceinfo">KirinNPUDeviceInfo</a>. The device number limit is 3. During the inference, the operator will choose device in order.</p>
<p>Float16 takes effect only when the CPU is under the ARM v8.2 architecture. Other models and x86 platforms that do not supported Float16 will be automatically rolled back to Float32.</p>
<p>For the iOS platform, only the CPU backend is supported, and Float16 is temporarily not supported.</p>
</div></blockquote>
<p>The advanced interfaces contained in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html#class-context">Context</a> are defined as follows:</p>
<section id="configuring-the-number-of-threads">
<h3>Configuring the Number of Threads<a class="headerlink" href="#configuring-the-number-of-threads" title="Permalink to this headline"></a></h3>
<p>Use <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#setthreadnum">SetThreadNum</a> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html#class-context">Context</a> to configure the number of threads:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the number of worker threads in the thread pool to 2, including the main thread.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadNum</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="configuring-the-thread-affinity">
<h3>Configuring the Thread Affinity<a class="headerlink" href="#configuring-the-thread-affinity" title="Permalink to this headline"></a></h3>
<p>Use <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#setthreadaffinity">SetThreadAffinity</a> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html#class-context">Context</a> to configure the thread affinity. If the parameter is <code class="docutils literal notranslate"><span class="pre">int</span> <span class="pre">mode</span></code>, configure the binding strategy. The effective value is 0-2, 0 means no core binding by default, 1 means preferential binding to large cores, and 2 means preferential binding to small cores. If the parameter is <code class="docutils literal notranslate"><span class="pre">const</span> <span class="pre">std::vector&lt;int&gt;</span> <span class="pre">&amp;core_list</span></code>, configure the binding core list. When configuring at the same time, the core_list is effective, but the mode is not effective.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the thread to be bound to the big core first.</span>
<span class="c1">// Valid value: 0: no affinities, 1: big cores first, 2: little cores first</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadAffinity</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="configuring-the-parallelization">
<h3>Configuring the Parallelization<a class="headerlink" href="#configuring-the-parallelization" title="Permalink to this headline"></a></h3>
<p>Use <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#setenableparallel">SetEnableParallel</a> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html#class-context">Context</a> to configure whether to support parallelism when executing inference:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the inference supports parallel.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetEnableParallel</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="configuring-the-gpu-backend">
<h3>Configuring the GPU Backend<a class="headerlink" href="#configuring-the-gpu-backend" title="Permalink to this headline"></a></h3>
<p>If the backend to be executed is GPUs, you need to set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_GPUDeviceInfo.html#class-gpudeviceinfo">GPUDeviceInfo</a> as the first choice. It is suggested to set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_CPUDeviceInfo.html#class-cpudeviceinfo">CPUDeviceInfo</a> as the second choice, to ensure model inference. Use <code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code> to enable GPU Float16 inference.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L114">main.cc</a> demonstrates how to create the CPU and GPU heterogeneous inference backend and how to enable Float16 inference for the GPU.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set GPU device first, make GPU preferred backend.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New GPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// GPU use float16 operator as priority.</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="c1">// Set VNIDIA device id, only valid when GPU backend is TensorRT.</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// The GPU device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">gpu_device_info</span><span class="p">);</span>

<span class="c1">// Set CPU device after GPU as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>The current GPU backend distinguishes <code class="docutils literal notranslate"><span class="pre">arm64</span></code>and <code class="docutils literal notranslate"><span class="pre">x86_64</span></code>platforms.</p>
<ul class="simple">
<li><p>On <code class="docutils literal notranslate"><span class="pre">arm64</span></code>, the backend of GPU is based on OpenCL. GPUs of Mali and Adreno are supported. The OpenCL version is 2.0.</p></li>
</ul>
<p>The configuration is as follows:</p>
<p>CL_TARGET_OPENCL_VERSION=200</p>
<p>CL_HPP_TARGET_OPENCL_VERSION=120</p>
<p>CL_HPP_MINIMUM_OPENCL_VERSION=120</p>
<ul class="simple">
<li><p>On <code class="docutils literal notranslate"><span class="pre">x86_64</span></code>, the backend of GPU is based on TensorRT. The TensorRT version is 6.0.1.5.</p></li>
</ul>
<p>Whether the attribute <code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code> can be set successfully depends on the <a class="reference external" href="https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix">CUDA computer capability</a> of the current device.</p>
<p>The attribute <code class="docutils literal notranslate"><span class="pre">SetDeviceID</span></code> only valid for TensorRT, used to specify the NVIDIA device ID.</p>
</div></blockquote>
</section>
<section id="configuring-the-npu-backend">
<h3>Configuring the NPU Backend<a class="headerlink" href="#configuring-the-npu-backend" title="Permalink to this headline"></a></h3>
<p>If the backend to be executed is NPUs, you need to set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_KirinNPUDeviceInfo.html#class-kirinnpudeviceinfo">KirinNPUDeviceInfo</a> as the first choice. It is suggested to set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_CPUDeviceInfo.html#class-cpudeviceinfo">CPUDeviceInfo</a> as the second choice, to ensure model inference. Use <code class="docutils literal notranslate"><span class="pre">SetFrequency</span></code> to set npu frequency.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L127">main.cc</a> shows how to create the CPU and NPU heterogeneous inference backend and set the NPU frequency to 3. It can be set to 1 (low power consumption), 2 (balanced), 3 (high performance), 4 (extreme performance).</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set NPU device first, make NPU preferred backend.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">npu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">KirinNPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">npu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New KirinNPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// NPU set frequency to be 3.</span>
<span class="n">npu_device_info</span><span class="o">-&gt;</span><span class="n">SetFrequency</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="c1">// The NPU device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">npu_device_info</span><span class="p">);</span>

<span class="c1">// Set CPU device after NPU as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="configuring-the-nnie-backend">
<h3>Configuring the NNIE Backend<a class="headerlink" href="#configuring-the-nnie-backend" title="Permalink to this headline"></a></h3>
<p>When the backend that needs to be executed is the heterogeneous inference based on CPU and NNIE, you only need to create the Context according to the configuration method of <span class="xref myst">CPU Backend</span> without specifying a provider.</p>
</section>
<section id="configuring-the-ascend-backend">
<h3>Configuring the ASCEND Backend<a class="headerlink" href="#configuring-the-ascend-backend" title="Permalink to this headline"></a></h3>
<p>If the backend to be executed is Ascend(only support ASCEND310), you need to set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_AscendDeviceInfo.html#class-documentation">Ascend310DeviceInfo</a> as the first choice. It is suggested to set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_CPUDeviceInfo.html#class-cpudeviceinfo">CPUDeviceInfo</a> as the second choice, to ensure model inference. Use <code class="docutils literal notranslate"><span class="pre">SetDeviceID</span></code> to set ascend device id.</p>
<p>The following sample code shows how to create the CPU and ASCEND heterogeneous inference backend and set ascend device id to 0.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set Ascend310 device first, make Ascend310 preferred backend.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ascend_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Ascend310DeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ascend_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Ascend310DeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Ascend310 set device id to be 0.</span>
<span class="n">ascend_device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceId</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// The ascend310 device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">ascend_device_info</span><span class="p">);</span>

<span class="c1">// Set CPU device after Ascend310 as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="configuring-the-coreml-backend">
<h3>Configuring the CoreML Backend<a class="headerlink" href="#configuring-the-coreml-backend" title="Permalink to this headline"></a></h3>
<p>If the backend to be executed is CoreML, you need to instantiate the <a class="reference external" href="https://mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_CoreMLDelegate.html">CoreMLDelegate</a> class，and use <a class="reference external" href="https://mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html?highlight=SetDelegate">SetDelegate</a> to pass the instance object into the context object. It is slightly different from the configuring steps of backends defined by hardware such as NPU and GPU.</p>
<p>The following sample code shows how to create the CPU and CoreML heterogeneous inference backend：</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// Set CPU device after NPU as second choice.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">coreml_delegate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">CoreMLDelegate</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">coreml_delegate</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CoreMLDelegate failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetDelegate</span><span class="p">(</span><span class="n">coreml_delegate</span><span class="p">);</span>
</pre></div>
</div>
<blockquote>
<div><p>The CoreML backend is only supported on devices whose operating system version is not lower than iOS 11 for now.</p>
</div></blockquote>
</section>
</section>
<section id="model-creating-loading-and-building">
<h2>Model Creating Loading and Building<a class="headerlink" href="#model-creating-loading-and-building" title="Permalink to this headline"></a></h2>
<p>When MindSpore Lite is used for inference, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a> is the main entry for inference. You can use <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a> to load, build and execute model. Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html#class-context">Context</a> created in the previous step to call the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#build">Build</a> of Model to load and build the runtime model.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L265">main.cc</a> demonstrates how to create, load and build a model:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Build model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="c1">// After the model is built, the Context can be released.</span>
<span class="p">...</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>After the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a> is loaded and built, the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html#class-context">Context</a> created in the previous step can be released.</p>
</div></blockquote>
</section>
<section id="inputting-data">
<h2>Inputting Data<a class="headerlink" href="#inputting-data" title="Permalink to this headline"></a></h2>
<p>Before executing a model, obtain the input <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_MSTensor.html">MSTensor</a> of the model and copy the input data to the input Tensor using <code class="docutils literal notranslate"><span class="pre">memcpy</span></code>. In addition, you can use the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#datasize">DataSize</a> method to obtain the size of the data to be filled in to the tensor, use the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#datatype">DataType</a> method to obtain the data type of the tensor, and use the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#mutabledata">MutableData</a> method to obtain the writable data pointer.</p>
<p>MindSpore Lite provides two methods to obtain the input tensor of a model.</p>
<ol class="arabic">
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#getinputbytensorname">GetInputByTensorName</a> method to obtain the input tensor based on the name. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L154">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">GetInputByTensorName</span></code> to obtain the input tensor and fill in data.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Pre-processing of input data, convert input data format to NHWC.</span>
<span class="p">...</span>
<span class="c1">// Assume that the model has only one input tensor named graph_input-173.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">in_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputByTensorName</span><span class="p">(</span><span class="s">&quot;graph_input-173&quot;</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_tensor</span><span class="p">.</span><span class="n">impl</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in_tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;MallocData for inTensor failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span><span class="w"> </span><span class="n">input_buf</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">);</span>
<span class="c1">// Users need to free input_buf.</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#getinputs">GetInputs</a> method to directly obtain the vectors of all model input tensors. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L137">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">GetInputs</span></code> to obtain the input tensor and fill in data.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Pre-processing of input data, convert input data format to NHWC.</span>
<span class="p">...</span>
<span class="c1">// Assume we have created a Model instance named model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="c1">// Assume that the model has only one input tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">in_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">*</span><span class="n">in_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">in_tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">in_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data of in_tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">memcpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">,</span><span class="w"> </span><span class="n">input_buf</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">);</span>
<span class="c1">// Users need to free input_buf.</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>The data layout in the input tensor of the MindSpore Lite model must be <code class="docutils literal notranslate"><span class="pre">NHWC</span></code>.</p>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#getinputs">GetInputs</a> and <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#getinputbytensorname">GetInputByTensorName</a> methods return data that do not need to be released by users.</p>
</div></blockquote>
</section>
<section id="executing-inference">
<h2>Executing Inference<a class="headerlink" href="#executing-inference" title="Permalink to this headline"></a></h2>
<p>Call the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#predict">Predict</a> function of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a> for model inference.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L355">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">Predict</span></code> to perform inference.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="obtaining-output">
<h2>Obtaining Output<a class="headerlink" href="#obtaining-output" title="Permalink to this headline"></a></h2>
<p>After performing inference, MindSpore Lite can obtain the inference result of the model. MindSpore Lite provides three methods to obtain the output <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_MSTensor.html">MSTensor</a> of a model.</p>
<ol class="arabic">
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#getoutputsbynodename">GetOutputsByNodeName</a> method to obtain the vector of the tensor connected to the model output tensor based on the name of the model output node. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L170">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">GetOutputsByNodeName</span></code> to obtain the output tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model before.</span>
<span class="c1">// Assume that model has a output node named Softmax-65.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">output_vec</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputsByNodeName</span><span class="p">(</span><span class="s">&quot;Softmax-65&quot;</span><span class="p">);</span>
<span class="c1">// Assume that output node named Default/Sigmoid-op204 has only one output tensor.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_vec</span><span class="p">.</span><span class="n">front</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Post-processing your result data.</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#getoutputbytensorname">GetOutputByTensorName</a> method to obtain the corresponding model output tensor based on the name of the model output tensor. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L200">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">GetOutputsByTensorName</span></code> to obtain the output tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model.</span>
<span class="c1">// We can use GetOutputTensorNames method to get all name of output tensor of model which is in order.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tensor_names</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputTensorNames</span><span class="p">();</span>
<span class="c1">// Assume we have created a Model instance named model before.</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor_name</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">tensor_names</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputByTensorName</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">out_tensor</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output tensor is nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// Post-processing the result data.</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#getoutputs">GetOutputs</a> method to directly obtain the names of all model output tensors vector. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L226">main.cc</a> demonstrates how to call <code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code> to obtain the output tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">out_tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">out_tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">out_tensors</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Post-processing the result data.</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><p>The data returned by the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#getoutputsbynodename">GetOutputsByNodeName</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#getoutputbytensorname">GetOutputByTensorName</a>, and <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#getoutputs">GetOutputs</a> methods does not need to be released by the user.</p>
</div></blockquote>
</section>
<section id="releasing-memory">
<h2>Releasing Memory<a class="headerlink" href="#releasing-memory" title="Permalink to this headline"></a></h2>
<p>If the MindSpore Lite inference framework is not required, you need to release the created Model. The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L370">main.cc</a> demonstrates how to release the memory before the program ends.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Delete model.</span>
<span class="c1">// Assume that the variable of Model * is named model.</span>
<span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline"></a></h2>
<section id="resizing-the-input-dimension">
<h3>Resizing the Input Dimension<a class="headerlink" href="#resizing-the-input-dimension" title="Permalink to this headline"></a></h3>
<p>When MindSpore Lite is used for inference, if the input shape needs to be resized, you can call the <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#resize">Resize</a> API of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a> to resize the shape of the input tensor after a model is created and built.</p>
<blockquote>
<div><p>Some networks do not support variable dimensions. As a result, an error message is displayed and the model exits unexpectedly. For example, the model contains the MatMul operator, one input tensor of the MatMul operator is the weight, and the other input tensor is the input. If a variable dimension API is called, the input tensor does not match the shape of the weight tensor. As a result, the inference fails.</p>
<p>When the GPU backend is TensorRT, Resize only valid at dims NHW for NHWC format inputs, resize shape value should not be larger than the model inputs.</p>
</div></blockquote>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L321">main.cc</a> demonstrates how to perform Resize on the input tensor of MindSpore Lite:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Assume we have created a Model instance named model.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">resize_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">};</span>
<span class="c1">// Assume the model has only one input,resize input shape to [1, 128, 128, 3]</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">;</span>
<span class="n">new_shapes</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">resize_shape</span><span class="p">);</span>
<span class="k">return</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Resize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="mixed-precision-inference">
<h3>Mixed Precision Inference<a class="headerlink" href="#mixed-precision-inference" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite supports mixed precision inference.
Users can set mixed precision information by calling the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html">LoadConfig</a> API of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a> after a model is created and before built.
The example of the config file is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[execution_plan]
op_name1=data_type:float16
op_name2=data_type:float32
</pre></div>
</div>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L470">main.cc</a> demonstrates how to infer model in the mixed precision:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Status</span><span class="w"> </span><span class="n">load_config_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">LoadConfig</span><span class="p">(</span><span class="n">config_file_path</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">load_config_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model load config error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">load_config_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">Status</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">graph_cell</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model build error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="n">Status</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="multiple-heterogeneous-devices-inference">
<h3>Multiple Heterogeneous Devices Inference<a class="headerlink" href="#multiple-heterogeneous-devices-inference" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite supports multiple heterogeneous devices inference.
Users can set multiple heterogeneous devices inference information by set multiple <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_DeviceInfoContext.html">DeviceInfoContext</a> in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Context.html">Context</a>.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L546">main.cc</a> demonstrates how to infer the model in multiple heterogeneous devices:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="n">context</span><span class="p">;</span>
<span class="c1">// enable NPU CPU GPU in inference. NPU is preferentially used, then the CPU, and GPU get the lowest priority.</span>
<span class="n">context</span><span class="p">.</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">KirinNPUDeviceInfo</span><span class="o">&gt;</span><span class="p">());</span>
<span class="n">context</span><span class="p">.</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">());</span>
<span class="n">context</span><span class="p">.</span><span class="n">MutableDeviceInfo</span><span class="p">().</span><span class="n">push_back</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">());</span>

<span class="n">Status</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">graph_cell</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model build error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="n">Status</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Model predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="opengl-texture-data-input">
<h3>OpenGL Texture Data Input<a class="headerlink" href="#opengl-texture-data-input" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite supports OpenGL texture input, performs end-to-end GPU isomorphic inference, and the inference result is returned as OpenGL texture data. This function needs to be configured in the Context during use, and OpenGL texture data is bound to it during inference. These two processes.</p>
<ol class="arabic">
<li><p>Configured Context</p>
<p>The user needs to set the SetEnableGLTexture property in dev.gpu_device_info_ in Context to true, and configure the user’s current OpenGL EGLContext and EGLDisplay through the SetGLContext interface and SetGLDisplay interface respectively.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="w"> </span><span class="n">context</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// 1. Set EnableGLTexture true</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableGLTexture</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>

<span class="c1">// 2. Set GLContext</span>
<span class="k">auto</span><span class="w"> </span><span class="n">gl_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eglGetCurrentContext</span><span class="p">();</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetGLContext</span><span class="p">(</span><span class="n">gl_context</span><span class="p">);</span>

<span class="c1">// 3. Set GLDisplay</span>
<span class="k">auto</span><span class="w"> </span><span class="n">gl_display</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eglGetCurrentDisplay</span><span class="p">();</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetGLDisplay</span><span class="p">(</span><span class="n">gl_display</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Bind OpenGL Texture Data</p>
<p>After the model is compiled and before the model runs, the user needs to call BindGLTexture2DMemory(const std::map&lt;std::string, GLuint&gt; &amp;inputGlTexture, std::map&lt;std::string, GLuint&gt; *outputGLTexture;) function to bind the input Output texture, instead of the original input data step. Because MindSpore Lite itself does not allocate OpenGL memory, the user is required to create the input and output texture memory in advance according to the tensor size of the model input and output, and the texture memory corresponding to the texture ID Bind to the input and output of the model, the sample code is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">input_gl_texture</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">map</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_gl_texture</span><span class="p">;</span>

<span class="p">...</span><span class="w"> </span><span class="c1">// Write OpenGL Texture data(GLuint) into input_gl_texture and output_gl_texture</span>

<span class="c1">// Bind texture data with input and output tensors</span>
<span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms_model_</span><span class="p">.</span><span class="n">BindGLTexture2DMemory</span><span class="p">(</span><span class="n">input_gl_texture</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">output_gl_texture</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;BindGLTexture2DMemory failed&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteError</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
</pre></div>
</div>
<p>In std::map&lt;std::string, GLuint&gt; input_gl_texture, the key is the model input tensor name, and the value is the corresponding GLuint texture; std::map&lt;std::string, GLuint&gt; the key in the output_gl_texture variable is the model output tensor name, Value is the corresponding GLuint texture. The model input and output tensor name can be obtained through the tensor.Name() interface. The sample code is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">;</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inTextureIDs</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">inputGlTexture</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">inTextureIDs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">GLuint</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outTextureIDs</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">i</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">outputGlTexture</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">pair</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="p">,</span><span class="w"> </span><span class="n">GLuint</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">Name</span><span class="p">(),</span><span class="w"> </span><span class="n">outTextureIDs</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">i</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Predict</p>
<p>After the binding is completed, you can directly call the Predict interface of ms_model_ for inference. The model output will be copied to the memory corresponding to the bound output texture ID, and the user can obtain the inference result from the outputs.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ms_model_</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">ms_inputs_for_api_</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">ms_before_call_back_</span><span class="p">,</span><span class="w"> </span><span class="n">ms_after_call_back_</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">MS_LOG</span><span class="p">(</span><span class="n">ERROR</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inference error &quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inference error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteError</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="sharing-a-memory-pool">
<h3>Sharing a Memory Pool<a class="headerlink" href="#sharing-a-memory-pool" title="Permalink to this headline"></a></h3>
<p>If there are multiple <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a>, you can configure the same <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Allocator.html#class-allocator">Allocator</a> in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_DeviceInfoContext.html#class-deviceinfocontext">DeviceInfoContext</a> to share the memory pool and reduce the memory size during running. The maximum memory size of the memory pool is <code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">GB</span></code>, and the maximum memory size allocated each time is <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">GB</span></code>.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L546">main.cc</a> demonstrates how to share the memory pool between two models:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context1</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">device_list1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info1</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">model1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model1</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context1</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">auto</span><span class="w"> </span><span class="n">context2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context2</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Use the same allocator to share the memory pool.</span>
<span class="n">device_info2</span><span class="o">-&gt;</span><span class="n">SetAllocator</span><span class="p">(</span><span class="n">device_info1</span><span class="o">-&gt;</span><span class="n">GetAllocator</span><span class="p">());</span>
<span class="n">device_list2</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info2</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">model2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model2</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model2</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context2</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="calling-back-a-model-during-the-running-process">
<h3>Calling Back a Model During the Running Process<a class="headerlink" href="#calling-back-a-model-during-the-running-process" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite can pass two <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/typedef_mindspore_MSKernelCallBack-1.html">MSKernelCallBack</a> function pointers to <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#predict">Predict</a> to call back a model for inference. Compared with common graph execution, callback execution can obtain additional information during the running process to help developers analyze performance and debug bugs. Additional information includes:</p>
<ul class="simple">
<li><p>Name of the running node</p></li>
<li><p>Input and output tensors before the current node is inferred</p></li>
<li><p>Input and output tensors after the current node is inferred</p></li>
</ul>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L672">main.cc</a> demonstrates how to define two callback functions as the pre-callback pointer and post-callback pointer and pass them to the Predict API for callback inference.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Definition of callback function before forwarding operator.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">before_call_back</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">before_inputs</span><span class="p">,</span>
<span class="w">                           </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">before_outputs</span><span class="p">,</span>
<span class="w">                           </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSCallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Before forwarding &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_name_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_type_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>
<span class="c1">// Definition of callback function after forwarding operator.</span>
<span class="k">auto</span><span class="w"> </span><span class="n">after_call_back</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after_inputs</span><span class="p">,</span>
<span class="w">                          </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">after_outputs</span><span class="p">,</span>
<span class="w">                          </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSCallBackParam</span><span class="w"> </span><span class="o">&amp;</span><span class="n">call_param</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;After forwarding &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_name_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">call_param</span><span class="p">.</span><span class="n">node_type_</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="p">};</span>

<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">before_call_back</span><span class="p">,</span><span class="w"> </span><span class="n">after_call_back</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="separating-graph-loading-and-model-build">
<h3>Separating Graph Loading and Model Build<a class="headerlink" href="#separating-graph-loading-and-model-build" title="Permalink to this headline"></a></h3>
<p>Use <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#load">Load</a> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Serialization.html#class-serialization">Serialization</a> to load <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Graph.html#class-graph">Graph</a> and use <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore.html#build">Build</a> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_Model.html#class-model">Model</a> to build the model.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L282">main.cc</a> demonstrates how to load graph and build model separately.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>

<span class="c1">// Load graph</span>
<span class="n">mindspore</span><span class="o">::</span><span class="n">Graph</span><span class="w"> </span><span class="n">graph</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">load_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Serialization</span><span class="o">::</span><span class="n">Load</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">graph</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">load_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Load graph failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Create model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Build model</span>
<span class="n">mindspore</span><span class="o">::</span><span class="n">GraphCell</span><span class="w"> </span><span class="n">graph_cell</span><span class="p">(</span><span class="n">graph</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">graph_cell</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="viewing-logs">
<h3>Viewing Logs<a class="headerlink" href="#viewing-logs" title="Permalink to this headline"></a></h3>
<p>If an exception occurs during inference, you can view logs to locate the fault. For the Android platform, use the <code class="docutils literal notranslate"><span class="pre">Logcat</span></code> command line to view the MindSpore Lite inference log information and use <code class="docutils literal notranslate"><span class="pre">MS_LITE</span></code> to filter the log information.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>logcat<span class="w"> </span>-s<span class="w"> </span><span class="s2">&quot;MS_LITE&quot;</span>
</pre></div>
</div>
<blockquote>
<div><p>For the iOS platform, does not support viewing logs temporarily.</p>
</div></blockquote>
</section>
<section id="obtaining-the-version-number">
<h3>Obtaining the Version Number<a class="headerlink" href="#obtaining-the-version-number" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite provides the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/function_mindspore_Version-1.html#function-documentation">Version</a> method to obtain the version number, which is included in the <code class="docutils literal notranslate"><span class="pre">include/api/types.h</span></code> header file. You can call this method to obtain the version number of MindSpore Lite.</p>
<p>The following sample code from <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_cpp/main.cc#L717">main.cc</a> demonstrates how to obtain the version number of MindSpore Lite:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/types.h&quot;</span>
<span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Version</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="extension-usage">
<h3>Extension Usage<a class="headerlink" href="#extension-usage" title="Permalink to this headline"></a></h3>
<p>In this chapter, we will show the users an example of extending MindSpore Lite inference, covering the whole process of creation and registration of custom operator. The example will help the users understand the extension usage as soon as possible. The chapter takes a simple model that consists of a single operator <code class="docutils literal notranslate"><span class="pre">Add</span></code> as an example. The code related to the example can be obtained from the directory <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.0.0-alpha/mindspore/lite/examples/runtime_extend">mindspore/lite/examples/runtime_extend</a>.</p>
<p>The chapter only provides instruction in the Linux System.</p>
<section id="operator-infershape-extension">
<h4>Operator InferShape Extension<a class="headerlink" href="#operator-infershape-extension" title="Permalink to this headline"></a></h4>
<p>The users need to inherit the basic class <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0.0-alpha/generate/classmindspore_kernel_KernelInterface.html">KernelInterface</a>, and override the interface function Infer.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Status</span><span class="w"> </span><span class="nf">CheckInputs</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">         </span><span class="c1">// check function when compiling, to judge the shape of input tensor is valid or not</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">.</span><span class="n">Shape</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">find</span><span class="p">(</span><span class="n">input_shape</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">input_shape</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span><span class="w"> </span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">input_shape</span><span class="p">.</span><span class="n">end</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteInferInvalid</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">class</span><span class="w"> </span><span class="nc">CustomAddInfer</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">kernel</span><span class="o">::</span><span class="n">KernelInterface</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">CustomAddInfer</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="o">~</span><span class="n">CustomAddInfer</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>

<span class="w">  </span><span class="n">Status</span><span class="w"> </span><span class="nf">Infer</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">               </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w">        </span><span class="c1">// override interface</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetFormat</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">format</span><span class="p">());</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetDataType</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">DataType</span><span class="p">());</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CheckInputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">kLiteInferInvalid</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetShape</span><span class="p">({</span><span class="mi">-1</span><span class="p">});</span><span class="w">        </span><span class="c1">// set the shape as {-1}，which represents the inferring process will be called again when running</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteInferInvalid</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteError</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="p">(</span><span class="o">*</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">SetShape</span><span class="p">((</span><span class="o">*</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">Shape</span><span class="p">());</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">kernel</span><span class="o">::</span><span class="n">KernelInterface</span><span class="o">&gt;</span><span class="w"> </span><span class="n">CustomAddInferCreator</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">CustomAddInfer</span><span class="o">&gt;</span><span class="p">();</span><span class="w"> </span><span class="p">}</span>
<span class="n">REGISTER_CUSTOM_KERNEL_INTERFACE</span><span class="p">(</span><span class="n">CustomOpTutorial</span><span class="p">,</span><span class="w"> </span><span class="n">Custom_Add</span><span class="p">,</span><span class="w"> </span><span class="n">CustomAddInferCreator</span><span class="p">)</span><span class="w">       </span><span class="c1">// call the registration interface</span>
</pre></div>
</div>
<blockquote>
<div><p>The process of inferring shape is composed of two periods, one is static inference when compiling graph, and the other is dynamic inference when running.</p>
<p>Static inference:</p>
<ol class="arabic simple">
<li><p>If the called function <code class="docutils literal notranslate"><span class="pre">CheckInputs</span></code> returns false or the current node needs to be inferred in the period of running, the shape of output tensor should be set as {-1}, which will be viewed as an identification to infer again when running. In such situation, the return code needs to be set to <code class="docutils literal notranslate"><span class="pre">kLiteInferInvalid</span></code>.</p></li>
<li><p>In other situation, please return other code. If the code is not <code class="docutils literal notranslate"><span class="pre">kSuccess</span></code>, the program will be aborted and please check the program accordingly.</p></li>
</ol>
<p>Dynamic inference</p>
<p>In this period, whether the dynamic inference is needed is up to the shape of output tensor of current node. Please refer to the <code class="docutils literal notranslate"><span class="pre">Operator</span> <span class="pre">Extension</span></code> as follows.</p>
</div></blockquote>
</section>
<section id="operator-extension">
<h4>Operator Extension<a class="headerlink" href="#operator-extension" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>The users need to inherit the basic class <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore_kernel.html#kernel">Kernel</a>, and override the related interface.</p>
<ul>
<li><p>Prepare: The interface will be called during graph compilation. Users can make preparations or necessary verifications for the current node before running.</p></li>
<li><p>Execute：The interface is running interface. Users can call <strong>dynamic inference</strong> <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_extend/src/custom_add_kernel.cc">PreProcess</a> in this interface.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Status</span><span class="w"> </span><span class="nf">CheckOutputs</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">           </span><span class="c1">// Check function when running, to judge whether the shape inference is needed</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">output</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">output_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output</span><span class="p">.</span><span class="n">Shape</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">find</span><span class="p">(</span><span class="n">output_shape</span><span class="p">.</span><span class="n">begin</span><span class="p">(),</span><span class="w"> </span><span class="n">output_shape</span><span class="p">.</span><span class="n">end</span><span class="p">(),</span><span class="w"> </span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">output_shape</span><span class="p">.</span><span class="n">end</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">kLiteInferInvalid</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">kSuccess</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>ReSize: The interface is used to handle the changeable information of the current node due to the shape change of graph inputs.</p></li>
<li><p>Attribute Parsing: The users need to provide their own parsing of custom operator, which can refer to <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_extend/src/custom_add_kernel.cc">ParseAttrData</a>.</p></li>
</ul>
</li>
<li><p>Operator registration. The users can refer to the interface <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.0.0-alpha/api_cpp/mindspore_registry.html#register-custom-kernel">REGISTER_CUSTOM_KERNEL</a>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">kFloat32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">kNumberTypeFloat32</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Kernel</span><span class="o">&gt;</span><span class="w"> </span><span class="n">CustomAddCreator</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span>
<span class="w">                                         </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span>
<span class="w">                                         </span><span class="k">const</span><span class="w"> </span><span class="n">schema</span><span class="o">::</span><span class="n">Primitive</span><span class="w"> </span><span class="o">*</span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="w"> </span><span class="o">*</span><span class="n">ctx</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">CustomAddKernel</span><span class="o">&gt;</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="n">primitive</span><span class="p">,</span><span class="w"> </span><span class="n">ctx</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">REGISTER_CUSTOM_KERNEL</span><span class="p">(</span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">CustomOpTutorial</span><span class="p">,</span><span class="w"> </span><span class="n">kFloat32</span><span class="p">,</span><span class="w"> </span><span class="n">Custom_Add</span><span class="p">,</span><span class="w"> </span><span class="n">CustomAddCreator</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Compile</p>
<ul>
<li><p>Environment Requirements</p>
<ul class="simple">
<li><p>System environment: Linux x86_64; Recommend Ubuntu 18.04.02LTS</p></li>
<li><p>compilation dependencies:</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Compilation and Build</p>
<p>Execute the script <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.0.0-alpha/mindspore/lite/examples/runtime_extend/build.sh">build.sh</a> in the directory of <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend</span></code>, And then, the released package of MindSpore Lite will be downloaded and the demo will be compiled automatically.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>build.sh
</pre></div>
</div>
<blockquote>
<div><p>If the automatic download is failed, users can download the specified package manually. The hardware platform is CPU and the system is Ubuntu-x64 <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0.0-alpha/use/downloads.html">mindspore-lite-{version}-linux-x64.tar.gz</a>, After unzipping, please copy the dynamic library <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> in the directory of <code class="docutils literal notranslate"><span class="pre">runtime/lib</span></code> to the directory of <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/lib</span></code> and copy the directory of <code class="docutils literal notranslate"><span class="pre">runtime/include</span></code> to the directory of <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend</span></code>.</p>
<p>If the model <code class="docutils literal notranslate"><span class="pre">add_extend.ms</span></code> is failed to download, please download <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/add_extend.ms">add_extend.ms</a> manually, and copy to the directory of <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/model</span></code>.</p>
<p>After manually downloading and storing the specified file, users need to execute the <code class="docutils literal notranslate"><span class="pre">build.sh</span></code> script to complete the compilation and build process.</p>
</div></blockquote>
</li>
<li><p>Compilation Result</p>
<p>The executable program <code class="docutils literal notranslate"><span class="pre">runtime_extend_tutorial</span></code> will be generated in the directory of <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/build</span></code>.</p>
</li>
</ul>
</li>
<li><p>Execute Program</p>
<p>After compiling and building, please enter the directory of <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/runtime_extend/build</span></code>, and then execute the following command to experience the extension usaged.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./runtime_extend_tutorial<span class="w"> </span>../model/add_extend.ms
</pre></div>
</div>
<p>After the execution, the following information is displayed, including the tensor name, tensor size, number of output tensors, and the first 20 pieces of data.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tensor name is:add-0 tensor size is:400 tensor elements num is:100
output data is:2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime.html" class="btn btn-neutral float-left" title="Executing Model Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="runtime_java.html" class="btn btn-neutral float-right" title="Using Java Interface to Perform Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>