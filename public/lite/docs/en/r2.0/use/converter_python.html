

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Using Python Interface for Model Conversion &mdash; MindSpore Lite master documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" />
   
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  
  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/js/lite.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Benchmark Tool" href="benchmark.html" />
    <link rel="prev" title="Converting MindSpore Lite Models" href="converter_train.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> MindSpore Lite
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption"><span class="caption-text">Device-side Tools</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="converter.html">Converting Models for Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="converter_tool.html">Offline Converting Models for Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="converter_train.html">Converting MindSpore Lite Models</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Python Interface for Model Conversion</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linux-environment-usage-descriptions">Linux Environment Usage Descriptions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#environment-preparation">Environment Preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#directory-structure">Directory Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parameter-description">Parameter Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#usage-examples">Usage Examples</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/runtime_parallel.html">Performing Concurrent Inference</a></li>
</ul>
<p class="caption"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="cloud_infer/benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="converter.html">Converting Models for Inference</a> &raquo;</li>
        
      <li>Using Python Interface for Model Conversion</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/use/converter_python.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="using-python-interface-for-model-conversion">
<h1>Using Python Interface for Model Conversion<a class="headerlink" href="#using-python-interface-for-model-conversion" title="Permalink to this headline">¶</a></h1>
<p><a href="https://gitee.com/mindspore/docs/blob/r2.0/docs/lite/docs/source_en/use/converter_python.md" target="_blank"><img src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.0/resource/_static/logo_source_en.png"></a></p>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>MindSpore Lite supports model conversion via Python interface, supporting multiple types of model conversion, and the converted models can be used for inference. The interface contains a variety of personalized parameters to provide a convenient conversion path for users. This tutorial describes how to use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.Converter.html">Python interface</a> for model conversion.</p>
<p>The currently supported input model types are MindSpore, TensorFlow Lite, Caffe, TensorFlow, ONNX, and PyTorch.</p>
<p>Convert to MindSpore Lite or MindSpore model by conversion tool when the input model type is not MindSpore. In addition, support conversion of MindSpore model to MindSpore Lite models. For inference on the generated models, the required version of Runtime inference framework is the version that comes with the conversion tool and higher version.</p>
</div>
<div class="section" id="linux-environment-usage-descriptions">
<h2>Linux Environment Usage Descriptions<a class="headerlink" href="#linux-environment-usage-descriptions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="environment-preparation">
<h3>Environment Preparation<a class="headerlink" href="#environment-preparation" title="Permalink to this headline">¶</a></h3>
<p>To use Python interface of MindSpore Lite for model conversion, the following environment preparation is required.</p>
<ul>
<li><p><a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0/use/build.html">Compile</a> or <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0/use/downloads.html">download</a> Whl installation package of MindSpore Lite with the Converter component.</p>
<blockquote>
<div><p>Currently, the installation package corresponding to Python 3.7 is available for download. If you need other Python versions, please use the compilation function to generate the installation package.</p>
</div></blockquote>
</li>
<li><p>Then use the <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> command to install. After installation, you can use the following command to check if the installation is successful. If no error is reported, the installation is successful.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import mindspore_lite&quot;</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="directory-structure">
<h3>Directory Structure<a class="headerlink" href="#directory-structure" title="Permalink to this headline">¶</a></h3>
<p>After successful installation, you can use the <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">show</span> <span class="pre">mindspore_lite</span></code> command to see where the Python modules of MindSpore Lite are installed.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore_lite
├── __pycache__
├── include
├── lib
│   ├── _c_lite_wrapper.cpython-37m-x86_64-linux-gnu.so         # MindSpore Lite Python module encapsulates the dynamic library of the C++ interface framework
│   ├── libmindspore_converter.so                               # Dynamic library for MindSpore Lite conversion framework
│   ├── libmindspore_core.so                                    # Dynamic library for the MindSpore Lite core framework
│   ├── libmindspore_glog.so.0                                  # Dynamic library of Glog
│   ├── libmindspore-lite.so                                    # Dynamic library for MindSpore Lite reasoning framework
│   ├── libmindspore-lite-train.so                              # Dynamic library for MindSpore Lite training framework
│   ├── libmslite_converter_plugin.so                           # Registering dynamic library for plugins
│   ├── libopencv_core.so.4.5                                   # Dynamic library of OpenCV
│   ├── libopencv_imgcodecs.so.4.5                              # Dynamic library of OpenCV
│   └── libopencv_imgproc.so.4.5                                # Dynamic library of OpenCV
├── __init__.py        # Initialization package
├── context.py         # Code related to context interface
├── converter.py       # Code related to converter interface, conversion portal
├── model.py           # Code related to model interface, inference portal
├── tensor.py          # Code related to tensor interface
└── version.py         # MindSpore Lite version number
</pre></div>
</div>
</div>
<div class="section" id="parameter-description">
<h3>Parameter Description<a class="headerlink" href="#parameter-description" title="Permalink to this headline">¶</a></h3>
<p>Python interface model conversion of MindSpore Lite provides a variety of parameter settings that users can choose to use according to their needs.</p>
<p>Usage Scenarios: 1. Converting third-party models to generate MindSpore models or MindSpore Lite models, 2. Convert MindSpore models to generate MindSpore Lite models.</p>
<p>Detailed descriptions of the parameters and their correspondence to the parameters in <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0/use/converter_tool.html">Inference Model Offline Conversion</a> are provided below.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Python interface model conversion parameters</th>
<th>Parameter types</th>
<th>Parameters corresponding to the offline conversion of the model</th>
<th>Required or not</th>
<th>Parameters descriptions</th>
<th>Range of values</th>
<th>Default Values</th>
</tr>
</thead>
<tbody>
<tr>
<td>fmk_type</td>
<td>FmkType</td>
<td><code>--fmk=&lt;FMK&gt;</code></td>
<td>Required</td>
<td>The input model frame type.</td>
<td>FmkType.TF, FmkType.CAFFE, FmkType.ONNX, FmkType.MINDIR, FmkType.TFLITE, FmkType.PYTORCH</td>
<td>-</td>
</tr>
<tr>
<td>model_file</td>
<td>str</td>
<td><code>--modelFile=&lt;MODELFILE&gt;</code></td>
<td>Required</td>
<td>The path of the input model file for the conversion.</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>output_file</td>
<td>str</td>
<td><code>--outputFile=&lt;OUTPUTFILE&gt;</code></td>
<td>Required</td>
<td>The path to the output model when conversion can be automatically generated with a <code>.ms</code> suffix.</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>weight_file</td>
<td>str</td>
<td><code>--weightFile=&lt;WEIGHTFILE&gt;</code></td>
<td>Required when converting Caffe models</td>
<td>The path to the input model weights file.</td>
<td>-</td>
<td>""</td>
</tr>
<tr>
<td>config_file</td>
<td>str</td>
<td><code>--configFile=&lt;CONFIGFILE&gt;</code></td>
<td>Not required</td>
<td>Converter profile path to configure post-training quantization or offline splitting of parallel operators, or to disable the operator fusion function and set the plug-in to the so path.</td>
<td>-</td>
<td>""</td>
</tr>
<tr>
<td>weight_fp16</td>
<td>bool</td>
<td><code>--fp16=&lt;FP16&gt;</code></td>
<td>Not required</td>
<td>Set whether the weights in Float32 data format need to be stored in Float16 data format during model serialization.</td>
<td>True, False</td>
<td>False</td>
</tr>
<tr>
<td>input_shape</td>
<td>dict{string:list[int]}</td>
<td><code>--inputShape=&lt;INPUTSHAPE&gt;</code></td>
<td>Not required</td>
<td>Set the dimensions of the model input, and keep the order of the input dimensions the same as the original model. For example {"inTensor1": [1, 32, 32, 32], "inTensor2": [1, 1, 32, 32]}</td>
<td>-</td>
<td>None, None is equal to {}</td>
</tr>
<tr>
<td>input_format</td>
<td>Format</td>
<td><code>--inputDataFormat=&lt;INPUTDATAFORMAT&gt;</code></td>
<td>Not required</td>
<td>Set the input format of the exported model, valid only for 4-dimensional inputs.</td>
<td>Format.NCHW, Format.NHWC</td>
<td>Format.NHWC</td>
</tr>
<tr>
<td>input_data_type</td>
<td>DataType</td>
<td><code>--inputDataType=&lt;INPUTDATATYPE&gt;</code></td>
<td>Not required</td>
<td>Set the data type of the quantized model input Tensor. Only valid if the quantization parameters (<code>scale</code> and <code>zero point</code>) of the model input Tensor are available. The default is to keep the same data type as the original model input Tensor.</td>
<td>DataType.FLOAT32, DataType.INT8, DataType.UINT8, DataType.UNKNOWN</td>
<td>DataType.FLOAT32</td>
</tr>
<tr>
<td>output_data_type</td>
<td>DataType</td>
<td><code>--outputDataType=&lt;OUTPUTDATATYPE&gt;</code></td>
<td>Not required</td>
<td>Set the data type of the output Tensor of the quantized model, only if the quantization parameters (<code>scale</code> and <code>zero point</code>) of the output Tensor of the model are available. The default is the same as the data type of the original model output Tensor.</td>
<td>DataType.FLOAT32, DataType.INT8, DataType.UINT8, DataType.UNKNOWN</td>
<td>DataType.FLOAT32</td>
</tr>
<tr>
<td>export_mindir</td>
<td>ModelType</td>
<td><code>--exportMindIR=&lt;EXPORTMINDIR&gt;</code></td>
<td>Not required</td>
<td>Set the type of the exported model file.</td>
<td>ModelType.MINDIR, ModelType.MINDIR_LITE</td>
<td>ModelType.MINDIR_LITE</td>
</tr>
<tr>
<td>decrypt_key</td>
<td>str</td>
<td><code>--decryptKey=&lt;DECRYPTKEY&gt;</code></td>
<td>Not required</td>
<td>Set the key used to load the cipher text MindIR. The key is expressed in hexadecimal and is only valid when <code>fmk_type</code> is MINDIR.</td>
<td>-</td>
<td>""</td>
</tr>
<tr>
<td>decrypt_mode</td>
<td>str</td>
<td><code>--decryptMode=&lt;DECRYPTMODE&gt;</code></td>
<td>Not required</td>
<td>Set the mode to load cipher MindIR, only valid when <code>decrypt_key</code> is specified.</td>
<td>"AES-GCM", "AES-CBC"</td>
<td>"AES-GCM"</td>
</tr>
<tr>
<td>enable_encryption</td>
<td>bool</td>
<td><code>--encryption=&lt;ENCRYPTION&gt;</code></td>
<td>Not required</td>
<td>When exporting, whether the model is encrypted. Exporting encryption protects model integrity, but increases runtime initialization time.</td>
<td>True, False</td>
<td>False</td>
</tr>
<tr>
<td>encrypt_key</td>
<td>str</td>
<td><code>--encryptKey=&lt;ENCRYPTKEY&gt;</code></td>
<td>Not required</td>
<td>Set the key used to encrypt the file, expressed in hexadecimal characters. Only supported when <code>decrypt_mode</code> is "AES-GCM" and the key length is 16.</td>
<td>-</td>
<td>""</td>
</tr>
<tr>
<td>infer</td>
<td>bool</td>
<td><code>--infer=&lt;INFER&gt;</code></td>
<td>Not required</td>
<td>Whether to perform pre-inference at the completion of the conversion.</td>
<td>True, False</td>
<td>False</td>
</tr>
<tr>
<td>train_model</td>
<td>bool</td>
<td><code>--trainModel=&lt;TRAINMODEL&gt;</code></td>
<td>Not required</td>
<td>Whether the model will be trained on the device.</td>
<td>True, False</td>
<td>False</td>
</tr>
<tr>
<td>no_fusion</td>
<td>bool</td>
<td><code>--NoFusion=&lt;NOFUSION&gt;</code></td>
<td>Not required</td>
<td>Whether to avoid fusion optimization, the default allows fusion optimization.</td>
<td>True, False</td>
<td>False</td>
</tr>
</tbody>
</table>
<blockquote>
<div><p>For more information about the <code class="docutils literal notranslate"><span class="pre">fmk_type</span></code> parameter, see <a class="reference external" href="https://mindspore.cn/lite/api/en/r2.0/mindspore_lite/mindspore_lite.FmkType.html">FmkType</a>.</p>
<p>The download installeration package does not support converting PyTorch models because the compilation option that supports converting PyTorch models is turned off by default. You need to turn on the specified compilation options locally to compile the installation package that supports converting PyTorch models. Converting the PyTorch model has the following prerequisites: before compiling, export MSLITE_ENABLE_CONVERT_PYTORCH_MODEL=on is needed, and add libtorch environment variable: export LD_LIBRARY_PATH=”/home/user/libtorch/lib:${LD_LIBRARY_PATH}” &amp;&amp; export LIB_TORCH_PATH=”/home/user/libtorch” before conversion. Users can download the <a class="reference external" href="https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.12.1%2Bcpu.zip">CPU version libtorch</a> and extract it to the /home/user/libtorch path.</p>
<p>Example of <code class="docutils literal notranslate"><span class="pre">model_file</span></code>: “/home/user/model.prototxt”. Examples of different types of model suffixes: TF: “model.pb” | CAFFE: “model.prototxt” | ONNX: “model.onnx” | MINDIR: “model.mindir” | TFLITE: “model.tflite” | PYTORCH: “model.pt or model.pth”.</p>
<p><code class="docutils literal notranslate"><span class="pre">output_file</span></code> parameter descriptions: If <code class="docutils literal notranslate"><span class="pre">export_mindir</span></code> is set to <code class="docutils literal notranslate"><span class="pre">ModelType.MINDIR</span></code>, a MindSpore model will be generated, which uses <code class="docutils literal notranslate"><span class="pre">.mindir</span></code> as a suffix. If <code class="docutils literal notranslate"><span class="pre">export_mindir</span></code> is set to <code class="docutils literal notranslate"><span class="pre">ModelType.MINDIR_LITE</span></code>, a MindSpore Lite model will be generated, which uses <code class="docutils literal notranslate"><span class="pre">.ms</span></code> as a suffix. For example, input model is “/home/user/model.prototxt”, and export_mindir uses default value, it will generate model named model.prototxt.ms in /home/user/ path.</p>
<p>Caffe models are generally divided into two files: <code class="docutils literal notranslate"><span class="pre">*.prototxt</span></code> is the model structure, corresponding to the <code class="docutils literal notranslate"><span class="pre">model_file</span></code> parameter, and <code class="docutils literal notranslate"><span class="pre">model.caffemodel</span></code> is the model weights, corresponding to the <code class="docutils literal notranslate"><span class="pre">weight_file</span></code> parameter.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">config_file</span></code> configuration file uses <code class="docutils literal notranslate"><span class="pre">key</span> <span class="pre">=</span> <span class="pre">value</span></code> to define the relevant parameters. The quantization-related configuration parameters are detailed in <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0/use/post_training_quantization.html">quantization after training</a>. The configuration parameters related to the extended functions are detailed in <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0/use/nnie.html#extension-configuration">Extended Configuration</a>.</p>
<p>The priority of <code class="docutils literal notranslate"><span class="pre">weight_fp16</span></code> is very low, for example if quantization is turned on, <code class="docutils literal notranslate"><span class="pre">weight_fp16</span></code> will not take effect again for weights that have already been quantized. In summary, this parameter will only take effect on serialization for the weights of Float32 in the model.</p>
<p><code class="docutils literal notranslate"><span class="pre">input_shape</span></code> is a parameter that the user may need to set in the following scenarios:</p>
<ul class="simple">
<li><p>Usage 1: The input of the model to be transformed is dynamic shape, and the fixed-shape inference is to be used, then set this parameter to fixed-shape. After setting, when inference about the model after the Converter, the default input shape is the same as this parameter setting, and no resize operation is needed.</p></li>
<li><p>Usage 2: Regardless of whether the original input of the model to be transformed is a dynamic shape or not, use fixed-shape inference and make the performance of the model to be optimized as much as possible, set this parameter to fixed-shape. After setting, the model structure will be further optimized, but the transformed model may lose the characteristics of the dynamic shape (some operators strongly related to the shape will be fused).</p></li>
<li><p>Usage 3: When using the Converter function to generate code for Micro inference execution, it is recommended to configure this parameter to reduce the probability of errors during deployment. When the model contains a Shape operator or the model input to be transformed is a dynamic shape, this parameter must be configured to set a fixed shape, to support the associated shape optimization and code generation.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">input_format</span></code>: Generally in three-way hardware scenarios with integrated NCHW specifications (e.g., <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0/use/nnie.html#usage-description-of-the-integrated-nnie">Usage Description of the Integrated NNIE</a>), setting to NCHW will result in more significant performance improvement than setting to NHWC. In other scenarios, users can also set up on-demand.</p>
<p>The encryption and decryption function is only effective when set to <code class="docutils literal notranslate"><span class="pre">MSLITE_ENABLE_MODEL_ENCRYPTION=on</span></code> at <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.0/use/build.html">compilation</a>, and is only supported on Linux x86 platform, where the key is a hexadecimal representation of the string, such as the key is defined as <code class="docutils literal notranslate"><span class="pre">b'0123456789ABCDEF'</span></code> corresponding to the hexadecimal representation of <code class="docutils literal notranslate"><span class="pre">30313233343536373839414243444546</span></code>, and Linux platform users can use the <code class="docutils literal notranslate"><span class="pre">xxd</span></code> tool to convert the byte representation of the key to hexadecimal expression.
Note that the encryption and decryption algorithms were updated in version 1.7, resulting in the new version of the Python interface not supporting the conversion of models exported from MindSpore encryption in version 1.6 and earlier.</p>
</div></blockquote>
</div>
<div class="section" id="usage-examples">
<h3>Usage Examples<a class="headerlink" href="#usage-examples" title="Permalink to this headline">¶</a></h3>
<p>The following is a selection of common examples to illustrate the use of the conversion command.</p>
<ul>
<li><p>Take the Caffe model LeNet as an example.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">CAFFE</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;lenet.prototxt&quot;</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;lenet&quot;</span><span class="p">,</span> <span class="n">weight_file</span><span class="o">=</span><span class="s2">&quot;lenet.caffemodel&quot;</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">converter</span><span class="p">()</span>
</pre></div>
</div>
<p>In this example, because the Caffe model is used, two input files, model structure and model weights, are required. Together with the other two required parameters, fmk type and output path, it can be executed successfully.</p>
<p>The result is shown as:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERT RESULT SUCCESS:0
</pre></div>
</div>
<p>This means that the Caffe model has been successfully transformed into a MindSpore Lite model, obtaining the new file <code class="docutils literal notranslate"><span class="pre">lenet.ms</span></code>.</p>
</li>
<li><p>Take MindSpore, TensorFlow Lite, TensorFlow and ONNX models as examples and execute the conversion command.</p>
<ul>
<li><p>MindSpore model <code class="docutils literal notranslate"><span class="pre">model.mindir</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;model.mindir&quot;</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">converter</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>TensorFlow Lite model <code class="docutils literal notranslate"><span class="pre">model.tflite</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">TFLITE</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;model.tflite&quot;</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">converter</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>TensorFlow model <code class="docutils literal notranslate"><span class="pre">model.pb</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">TF</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;model.pb&quot;</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">converter</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>ONNX model <code class="docutils literal notranslate"><span class="pre">model.onnx</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">ONNX</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;model.onnx&quot;</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">converter</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>PyTorch model <code class="docutils literal notranslate"><span class="pre">model.pt</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">PYTORCH</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;model.pt&quot;</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">converter</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>PyTorch model <code class="docutils literal notranslate"><span class="pre">model.pth</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Converter</span><span class="p">(</span><span class="n">fmk_type</span><span class="o">=</span><span class="n">mslite</span><span class="o">.</span><span class="n">FmkType</span><span class="o">.</span><span class="n">PYTORCH</span><span class="p">,</span> <span class="n">model_file</span><span class="o">=</span><span class="s2">&quot;model.pth&quot;</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;model&quot;</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">converter</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ul>
<blockquote>
<div><p>Converting the PyTorch model has the following prerequisites: before compiling, export MSLITE_ENABLE_CONVERT_PYTORCH_MODEL=on is needed, and add libtorch environment variable: export LD_LIBRARY_PATH=”/home/user/libtorch/lib:${LD_LIBRARY_PATH}” &amp;&amp; export LIB_TORCH_PATH=”/home/user/libtorch” before conversion. Users can download the <a class="reference external" href="https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.12.1%2Bcpu.zip">CPU version libtorch</a> and extract it to the /home/user/libtorch path.</p>
</div></blockquote>
<p>In all of the above cases, the following conversion success message is displayed and the <code class="docutils literal notranslate"><span class="pre">model.ms</span></code> target file is obtained at the same time.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERTER RESULT SUCCESS:0
</pre></div>
</div>
</li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="benchmark.html" class="btn btn-neutral float-right" title="Benchmark Tool" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="converter_train.html" class="btn btn-neutral float-left" title="Converting MindSpore Lite Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, MindSpore Lite.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   
</body>
</html>