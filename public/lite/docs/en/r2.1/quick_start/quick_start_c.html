<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Experiencing C-language Simplified Inference Demo &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Android Application Development Based on JNI Interface" href="quick_start.html" />
    <link rel="prev" title="Experiencing Java Simplified Inference Demo" href="quick_start_java.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../device_infer_example.html">Device-side Inference Sample</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="quick_start_cpp.html">Experiencing C++ Simplified Inference Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="quick_start_java.html">Experiencing Java Simplified Inference Demo</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Experiencing C-language Simplified Inference Demo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-and-running-the-demo">Building and Running the Demo</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#linux-x86">Linux X86</a></li>
<li class="toctree-l4"><a class="reference internal" href="#windows">Windows</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#configuring-cmake">Configuring CMake</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-configuration-context">Creating Configuration Context</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating,-loading-and-compiling-model">Creating, Loading and Compiling Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-inference">Model Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#freeing-memory">Freeing memory</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="quick_start.html">Android Application Development Based on JNI Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="image_segmentation.html">Android Application Development Based on Java Interface</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../use/post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../device_infer_example.html">Device-side Inference Sample</a> &raquo;</li>
      <li>Experiencing C-language Simplified Inference Demo</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quick_start/quick_start_c.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="experiencing-c-language-simplified-inference-demo">
<h1>Experiencing C-language Simplified Inference Demo<a class="headerlink" href="#experiencing-c-language-simplified-inference-demo" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.1/docs/lite/docs/source_en/quick_start/quick_start_c.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial provides a sample program for MindSpore Lite to perform inference, which demonstrates the basic process of end-side inference with C-language by randomly typing, performing inference, and printing inference results, so that users can quickly understand the use of MindSpore Lite to perform inference-related APIs. This tutorial performs the inference of MobileNetV2 model by taking randomly generated data as input data and prints obtained output data. The related code is in the <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.1/mindspore/lite/examples/quick_start_c">mindspore/lite/examples/quick_start_c</a> directory.</p>
<p>Performing inference with MindSpore Lite consists of the following main steps:</p>
<ol class="arabic simple">
<li><p>Read model: Read the <code class="docutils literal notranslate"><span class="pre">.ms</span></code> model converted by <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.1/use/converter_tool.html">Model Conversion Tool</a> from the file system.</p></li>
<li><p>Create configuration context: Create a Configuration <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.1/api_c/context_c.html">Context</a> that holds some basic configuration parameters needed, to guide model compilation and model execution.</p></li>
<li><p>Create, load and compile Model: Before executing inference, you need to call <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.1/api_c/model_c.html#msmodelbuildfromfile">MSModelBuildFromFile</a> interface of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.1/api_c/model_c.html">Model</a> for model loading and compilation, and configure the Context obtained in the previous step into the Model. The model loading phase parses the file cache into a runtime model. The model compilation phase mainly carries out the process of operator selection scheduling, subgraph slicing, etc, which will consume more time, so it is recommended that the Model be created once, compiled once, and reasoned several times.</p></li>
<li><p>Input data: The data needs to be padded in the <code class="docutils literal notranslate"><span class="pre">Input</span> <span class="pre">Tensor</span></code> before model execution.</p></li>
<li><p>Execute inference: Use <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.1/generate/function_model_c.h_MSModelPredict-1.html">MSModelPredict</a> inferene of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.1/api_c/model_c.html">Model</a> for model inference.</p></li>
<li><p>Obtain output: After the model execution, the inference result can be obtained by <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">Tensor</span></code>.</p></li>
<li><p>Free memory: When do not need to use MindSpore Lite inference framework, you need to free the created <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.1/api_c/model_c.html">Model</a>.</p></li>
</ol>
<p><img alt="img" src="../_images/lite_runtime.png" /></p>
</section>
<section id="building-and-running-the-demo">
<h2>Building and Running the Demo<a class="headerlink" href="#building-and-running-the-demo" title="Permalink to this headline"></a></h2>
<section id="linux-x86">
<h3>Linux X86<a class="headerlink" href="#linux-x86" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Environment requirements</p>
<ul class="simple">
<li><p>System environment: Linux x86_64, Ubuntu 18.04.02LTS recommended</p></li>
<li><p>Compilation dependencies:</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Compiling and building</p>
<p>Execute the <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.1/mindspore/lite/examples/quick_start_c/build.sh">build script</a> in <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_c</span></code> directory, which will automatically download the MindSpore Lite inference framework library and the model file and compile the Demo.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>build.sh
</pre></div>
</div>
<blockquote>
<div><p>If the build script fails to download the MindSpore Lite inference framework, please manually download the MindSpore Lite model inference framework <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.1/use/downloads.html">mindspore-lite-{version}-linux-x64.tar.gz</a> for the CPU hardware platform and Ubuntu-x64 operating system, after decompression copy the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> file from the unpacked <code class="docutils literal notranslate"><span class="pre">runtime/lib</span></code> directory to <code class="docutils literal notranslate"><span class="pre">mindspore/</span> <span class="pre">lite/examples/quick_start_c/lib</span></code> directory, and the files in <code class="docutils literal notranslate"><span class="pre">runtime/include</span></code> directory to <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_c/include</span></code> directory, and copy the <code class="docutils literal notranslate"><span class="pre">libmindspore_glog.so.0</span></code> file from the <code class="docutils literal notranslate"><span class="pre">runtime/third_party/glog</span></code> directory to the <code class="docutils literal notranslate"><span class="pre">libmindspore_glog.so</span></code> file in <code class="docutils literal notranslate"><span class="pre">mindspore/</span> <span class="pre">lite/examples/quick_start_c/lib</span></code> directory.</p>
<p>If the build script fails to download the MobileNetV2 model, please manually download the relevant model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.ms">mobilenetv2.ms</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_c/model</span></code> directory.</p>
<p>After manually downloading and placing the files in the specified location, the build.sh script needs to be executed again to complete the compiling and building.</p>
</div></blockquote>
</li>
<li><p>Executing inference</p>
<p>After compiling and building, go to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/quick_start_c/build</span></code> directory and execute the following command to experience the MobileNetV2 model inference by MindSpore Lite.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./mindspore_quick_start_c<span class="w"> </span>../model/mobilenetv2.ms
</pre></div>
</div>
<p>When the execution is completed, the following results will be obtained. Print the name, the size and the number of the output Tensor and the first 50 data:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Tensor name: Softmax-65, tensor size is 4004 ,elements num: 1001.
output data is:
0.000011 0.000015 0.000015 0.000079 0.000070 0.000702 0.000120 0.000590 0.000009 0.000004 0.000004 0.000002 0.000002 0.000002 0.000010 0.000055 0.000006 0.000010 0.000003 0.000010 0.000002 0.000005 0.000001 0.000002 0.000004 0.000006 0.000008 0.000003 0.000015 0.000005 0.000011 0.000020 0.000006 0.000002 0.000011 0.000170 0.000005 0.000009 0.000006 0.000002 0.000003 0.000009 0.000005 0.000006 0.000003 0.000011 0.000005 0.000027 0.000003 0.000050 0.000016
</pre></div>
</div>
</li>
</ul>
</section>
<section id="windows">
<h3>Windows<a class="headerlink" href="#windows" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Environment requirements</p>
<ul class="simple">
<li><p>System environment: Windows 7, Windows 10; 64-bit</p></li>
<li><p>Compilation dependencies:</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p><a class="reference external" href="https://sourceforge.net/projects/mingw-w64/files/ToolchainstargettingWin64/PersonalBuilds/mingw-builds/7.3.0/threads-posix/seh/x86_64-7.3.0-release-posix-seh-rt_v5-rev0.7z/download">MinGW GCC</a> = 7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Compiling and building</p>
<ul class="simple">
<li><p>Library downloading: Please manually download the MindSpore Lite model inference framework <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.1/use/downloads.html">mindspore-lite-{version}-win-x64.zip</a> with CPU as the hardware platform and Windows-x64 as the operating system, after decompression copy all files in the <code class="docutils literal notranslate"><span class="pre">runtime\lib</span></code> directory to the <code class="docutils literal notranslate"><span class="pre">mindspore\lite\examples\quick_start_clib\</span></code> project directory, and the files in the <code class="docutils literal notranslate"><span class="pre">runtime\include</span></code> directory to the <code class="docutils literal notranslate"><span class="pre">mindspore\lite\examples\quick_start_c\include</span></code> project directory. (Note: the <code class="docutils literal notranslate"><span class="pre">lib</span></code> and <code class="docutils literal notranslate"><span class="pre">include</span></code> directories under the project need to be created manually)</p></li>
<li><p>Model downloading: Please manually download the relevant model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.ms">mobilenetv2.ms</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore\</span> <span class="pre">lite\examples\quick_start_c\model</span></code> directory.</p></li>
<li><p>Compiling: Execute the <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.1/mindspore/lite/examples/quick_start_c/build.bat">build script</a> in the <code class="docutils literal notranslate"><span class="pre">mindspore\lite\examples\quick_start_c</span></code> directory, which can automatically download the relevant files and compile the Demo.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>build.bat
</pre></div>
</div>
</li>
<li><p>Executing inference</p>
<p>After compiling and building, go to the <code class="docutils literal notranslate"><span class="pre">mindspore\lite\examples\quick_start_c\build</span></code> directory and execute the following command to experience the MobileNetV2 model inference by MindSpore Lite.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">set</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>..<span class="se">\l</span>ib<span class="p">;</span>%PATH%
call<span class="w"> </span>mindspore_quick_start_c.exe<span class="w"> </span>..<span class="se">\m</span>odel<span class="se">\m</span>obilenetv2.ms
</pre></div>
</div>
<p>When the execution is completed, the following results will be obtained. Print the name, the size and the number of the output Tensor and the first 50 data:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Tensor name: Softmax-65, tensor size is 4004 ,elements num: 1001.
output data is:
0.000011 0.000015 0.000015 0.000079 0.000070 0.000702 0.000120 0.000590 0.000009 0.000004 0.000004 0.000002 0.000002 0.000002 0.000010 0.000055 0.000006 0.000010 0.000003 0.000010 0.000002 0.000005 0.000001 0.000002 0.000004 0.000006 0.000008 0.000003 0.000015 0.000005 0.000011 0.000020 0.000006 0.000002 0.000011 0.000170 0.000005 0.000009 0.000006 0.000002 0.000003 0.000009 0.000005 0.000006 0.000003 0.000011 0.000005 0.000027 0.000003 0.000050 0.000016
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="configuring-cmake">
<h2>Configuring CMake<a class="headerlink" href="#configuring-cmake" title="Permalink to this headline"></a></h2>
<p>The following is sample code when the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> static library is integrated via CMake.</p>
<blockquote>
<div><p>The <code class="docutils literal notranslate"><span class="pre">-wl,--whole-archive</span></code> option needs to be passed to the linker when the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> static library is integrated.</p>
<p>Since the <code class="docutils literal notranslate"><span class="pre">-fstack-protector-strong</span></code> stack-protected compiling option was added when compiling MindSpore Lite, it is also necessary to link the <code class="docutils literal notranslate"><span class="pre">ssp</span></code> library in MinGW on the Windows platform.</p>
<p>Since support for so library file handling was added when compiling MindSpore Lite, it is also necessary to link the <code class="docutils literal notranslate"><span class="pre">dl</span></code> library on the Linux platform.</p>
</div></blockquote>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.18.3</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">QuickStartC</span><span class="p">)</span>

<span class="nb">if</span><span class="p">(</span><span class="s">CMAKE_CXX_COMPILER_ID</span><span class="w"> </span><span class="s">STREQUAL</span><span class="w"> </span><span class="s2">&quot;GNU&quot;</span><span class="w"> </span><span class="s">AND</span><span class="w"> </span><span class="s">CMAKE_CXX_COMPILER_VERSION</span><span class="w"> </span><span class="s">VERSION_LESS</span><span class="w"> </span><span class="s">7.3.0</span><span class="p">)</span>
<span class="w">    </span><span class="nb">message</span><span class="p">(</span><span class="s">FATAL_ERROR</span><span class="w"> </span><span class="s2">&quot;GCC version ${CMAKE_CXX_COMPILER_VERSION} must not be less than 7.3.0&quot;</span><span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span>

<span class="c"># Add directory to include search path</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="p">)</span>

<span class="c"># Add directory to linker search path</span>
<span class="nb">link_directories</span><span class="p">(</span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="s">/lib</span><span class="p">)</span>

<span class="nb">file</span><span class="p">(</span><span class="s">GLOB_RECURSE</span><span class="w"> </span><span class="s">QUICK_START_CXX</span><span class="w"> </span><span class="o">${</span><span class="nv">CMAKE_CURRENT_SOURCE_DIR</span><span class="o">}</span><span class="s">/*.cc</span><span class="p">)</span>
<span class="nb">add_executable</span><span class="p">(</span><span class="s">mindspore_quick_start_c</span><span class="w"> </span><span class="o">${</span><span class="nv">QUICK_START_CXX</span><span class="o">}</span><span class="p">)</span>

<span class="nb">target_link_libraries</span><span class="p">(</span>
<span class="w">        </span><span class="s">mindspore_quick_start_c</span>
<span class="w">        </span><span class="s">-Wl,--whole-archive</span><span class="w"> </span><span class="s">libmindspore-lite</span><span class="w"> </span><span class="s">-Wl,--no-whole-archive</span>
<span class="w">        </span><span class="s">pthread</span>
<span class="p">)</span>

<span class="c"># Due to the increased compilation options for stack protection,</span>
<span class="c"># it is necessary to target link ssp library when Use the static library in Windows.</span>
<span class="nb">if</span><span class="p">(</span><span class="s">WIN32</span><span class="p">)</span>
<span class="w">    </span><span class="nb">target_link_libraries</span><span class="p">(</span>
<span class="w">            </span><span class="s">mindspore_quick_start_c</span>
<span class="w">            </span><span class="s">ssp</span>
<span class="w">    </span><span class="p">)</span>
<span class="nb">else</span><span class="p">()</span>
<span class="w">    </span><span class="nb">target_link_libraries</span><span class="p">(</span>
<span class="w">            </span><span class="s">mindspore_quick_start_c</span>
<span class="w">            </span><span class="s">dl</span>
<span class="w">    </span><span class="p">)</span>
<span class="nb">endif</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="creating-configuration-context">
<h2>Creating Configuration Context<a class="headerlink" href="#creating-configuration-context" title="Permalink to this headline"></a></h2>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Create and init context, add CPU device info</span>
<span class="w">  </span><span class="n">MSContextHandle</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSContextCreate</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">NULL</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSContextCreate failed.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kMSStatusLiteError</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">thread_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">  </span><span class="n">MSContextSetThreadNum</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="n">thread_num</span><span class="p">);</span>
<span class="w">  </span><span class="n">MSContextSetThreadAffinityMode</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="n">MSDeviceInfoHandle</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSDeviceInfoCreate</span><span class="p">(</span><span class="n">kMSDeviceTypeCPU</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">NULL</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSDeviceInfoCreate failed.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSContextDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">context</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kMSStatusLiteError</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">MSDeviceInfoSetEnableFP16</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="n">MSContextAddDeviceInfo</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="creating,-loading-and-compiling-model">
<h2>Creating, Loading and Compiling Model<a class="headerlink" href="#creating,-loading-and-compiling-model" title="Permalink to this headline"></a></h2>
<p>Model loading and compilation can be done by calling <a class="reference external" href="https://www.mindspore.cn/lite/api/zh-CN/r2.1/api_c/model_c.html#msmodelbuildfromfile">MSModelBuildFromFile</a>  interface of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.1/api_c/model_c.html">Model</a> to load and compile from the file path to get the runtime model. In this case, <code class="docutils literal notranslate"><span class="pre">argv[1]</span></code> corresponds to the model file path inputted from the console.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Create model</span>
<span class="w">  </span><span class="n">MSModelHandle</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelCreate</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">NULL</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSModelCreate failed.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSContextDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">context</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kMSStatusLiteError</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelBuildFromFile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">kMSModelTypeMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kMSStatusSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSModelBuildFromFile failed, ret: %d.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ret</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="model-inference">
<h2>Model Inference<a class="headerlink" href="#model-inference" title="Permalink to this headline"></a></h2>
<p>The model inference mainly includes the steps of input data, inference execution, and obtaining output, where the input data in this example is generated by random data, and finally the output result after execution of inference is printed.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Get Inputs</span>
<span class="w">  </span><span class="n">MSTensorHandleArray</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelGetInputs</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">handle_list</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nb">NULL</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSModelGetInputs failed, ret: %d.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ret</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Generate random data as input data.</span>
<span class="w">  </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kMSStatusSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;GenerateInputDataWithRandom failed, ret: %d.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ret</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Model Predict</span>
<span class="w">  </span><span class="n">MSTensorHandleArray</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSModelPredict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">kMSStatusSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;MSModelPredict failed, ret: %d.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">ret</span><span class="p">);</span>
<span class="w">    </span><span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ret</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Print Output Tensor Data.</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">.</span><span class="n">handle_num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">MSTensorHandle</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">outputs</span><span class="p">.</span><span class="n">handle_list</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">element_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MSTensorGetElementNum</span><span class="p">(</span><span class="n">tensor</span><span class="p">);</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Tensor name: %s, tensor size is %ld ,elements num: %ld.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">MSTensorGetName</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span>
<span class="w">           </span><span class="n">MSTensorGetDataSize</span><span class="p">(</span><span class="n">tensor</span><span class="p">),</span><span class="w"> </span><span class="n">element_num</span><span class="p">);</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="p">)</span><span class="n">MSTensorGetData</span><span class="p">(</span><span class="n">tensor</span><span class="p">);</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;output data is:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">max_print_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">50</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">element_num</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">max_print_num</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;%f &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">]);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="freeing-memory">
<h2>Freeing memory<a class="headerlink" href="#freeing-memory" title="Permalink to this headline"></a></h2>
<p>When do not need to use MindSpore Lite inference framework, you need to free the <code class="docutils literal notranslate"><span class="pre">Model</span></code> that has been created.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="c1">// Delete model.</span>
<span class="n">MSModelDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">model</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="quick_start_java.html" class="btn btn-neutral float-left" title="Experiencing Java Simplified Inference Demo" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="quick_start.html" class="btn btn-neutral float-right" title="Android Application Development Based on JNI Interface" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>