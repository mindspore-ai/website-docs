<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Ascend Conversion Tool Description &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Graph Kernel Fusion Configuration Instructions (Beta Feature)" href="converter_tool_graph_kernel.html" />
    <link rel="prev" title="Using Python Interface to Perform Model Conversions" href="converter_python.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="converter.html">Model Converter</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="converter_tool.html">Offline Conversion of Inference Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="converter_python.html">Using Python Interface to Perform Model Conversions</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Ascend Conversion Tool Description</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuration-file">Configuration File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dynamic-shape-configuration">Dynamic Shape Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dynamic-batch-size">Dynamic Batch Size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dynamic-resolution">Dynamic Resolution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dynamic-dimension">Dynamic dimension</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#aoe-auto-tuning">AOE Auto-tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#aoe-tool-tuning">AOE Tool Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#aoe-api-tuning">AOE API Tuning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#deploying-ascend-custom-operators">Deploying Ascend Custom Operators</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="converter_tool_graph_kernel.html">Graph Kernel Fusion Configuration Instructions (Beta Feature)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="converter.html">Model Converter</a> &raquo;</li>
      <li>Ascend Conversion Tool Description</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/use/cloud_infer/converter_tool_ascend.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ascend-conversion-tool-description">
<h1>Ascend Conversion Tool Description<a class="headerlink" href="#ascend-conversion-tool-description" title="Permalink to this headline">ÔÉÅ</a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.1/docs/lite/docs/source_en/use/cloud_infer/converter_tool_ascend.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.1/resource/_static/logo_source_en.svg" /></a></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>This article introduces the related features of the cloud-side inference model conversion tool in Ascend back-end, such as profile options, dynamic shape, AOE, custom operators.</p>
</section>
<section id="configuration-file">
<h2>Configuration File<a class="headerlink" href="#configuration-file" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Table 1: Configure [ascend_context] parameter</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Attributes</p></th>
<th class="head"><p>Functions Description</p></th>
<th class="head"><p>Types</p></th>
<th class="head"><p>Values Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">input_format</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specify the model input format.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;NCHW&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;NHWC&quot;</span></code>, and <code class="docutils literal notranslate"><span class="pre">&quot;ND&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">input_shape</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specify the model input Shape. input_name must be the input name in the network model before conversion, in the order of the inputs, separated by <code class="docutils literal notranslate"><span class="pre">;</span></code>.</p></td>
<td><p>String</p></td>
<td><p>Such as <code class="docutils literal notranslate"><span class="pre">&quot;input1:[1,64,64,3];input2:[1,256,256,3]&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dynamic_dims</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specify the dynamic BatchSize and dynamic resolution parameters.</p></td>
<td><p>String</p></td>
<td><p>ËßÅ<a class="reference internal" href="#dynamic-shape-configuration"><span class="std std-doc">Dynamic shape configuration</span></a></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">precision_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure the model accuracy mode.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp32&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;preferred_fp32&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp16&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;enforce_origin&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;preferred_optimal&quot;</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">&quot;enforce_fp16&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">op_select_impl_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure the operator selection mode.</p></td>
<td><p>String</p></td>
<td><p>Optioans: <code class="docutils literal notranslate"><span class="pre">&quot;high_performance&quot;</span></code>, and <code class="docutils literal notranslate"><span class="pre">&quot;high_precision&quot;</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">&quot;high_performance&quot;</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">output_type</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specify the data type of network output</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;FP16&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;FP32&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;UINT8&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">fusion_switch_config_file_path</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure the <a class="reference external" href="https://www.hiascend.com/document/detail/en/canncommercial/601/inferapplicationdev/atctool/atctool_0078.html">Fusion Switch Configuration File</a> file path and file name.</p></td>
<td><p>String</p></td>
<td><p>Specify the configuration file for the fusion switch</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">insert_op_config_file_path</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Model insertion <a class="reference external" href="https://www.hiascend.com/document/detail/en/canncommercial/601/inferapplicationdev/atctool/atctool_0018.html">AIPP</a> operator</p></td>
<td><p>String</p></td>
<td><p>Path of <a class="reference external" href="https://www.hiascend.com/document/detail/en/canncommercial/601/inferapplicationdev/atctool/atctool_0021.html">AIPP</a> configuration file</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">aoe_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p><a class="reference external" href="https://www.hiascend.com/document/detail/en/canncommercial/601/devtools/auxiliarydevtool/aoe_16_001.html">AOE</a> auto-tuning mode</p></td>
<td><p>String</p></td>
<td><p>Options: ‚Äúsubgraph tuning‚Äù, ‚Äúoperator tuning‚Äù or ‚Äúsubgraph tuning, operator tuning‚Äù. Default: Not enabled</p></td>
</tr>
</tbody>
</table>
<p>Table 2:  Configure [acl_init_options] parameter</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Attributes</p></th>
<th class="head"><p>Functions Description</p></th>
<th class="head"><p>Types</p></th>
<th class="head"><p>Values Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.engineType</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set the core type used by the network model.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;VectorCore&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;AiCore&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.socVersion</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Version of the Ascend AI processor.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;Ascend310&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;Ascend710&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;Ascend910&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.bufferOptimize</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Data cache optimization switch.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;l1_optimize&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;l2_optimize&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;off_optimize&quot;</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">&quot;l2_optimize&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.enableCompressWeight</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Data compression can be performed on Weight to improve performance.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;true&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;false&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">compress_weight_conf</span></code></p></td>
<td><p>Optional</p></td>
<td><p>The path of the configuration file for the node list to be compressed is mainly composed of the conv operator and the fc operator.</p></td>
<td><p>String</p></td>
<td><p>path of config file</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.exec.precision_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Select the operator precision mode.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;force_fp32&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;force_fp16&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;allow_fp32_to_fp16&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;must_keep_origin_dtype&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;allow_mix_precision&quot;</span></code>, Default: <code class="docutils literal notranslate"><span class="pre">&quot;force_fp16&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.exec.disableReuseMemory</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Memory reuse switch.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;Ôºë&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.enableSingleStream</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Whether to enable a model to use only one stream.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;true&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;false&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.aicoreNum</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set the number of AI cores used during compilation.</p></td>
<td><p>String</p></td>
<td><p>Default: <code class="docutils literal notranslate"><span class="pre">&quot;Ôºë0&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.fusionSwitchFile</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Fusion configuration file path.</p></td>
<td><p>String</p></td>
<td><p>path of config file</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.enableSmallChannel</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Whether to enable the optimization of small channel.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;1&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.opSelectImplmode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Select the operator implementation mode.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;high_precision&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;high_performance&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.optypelistForImplmode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>A list of operators that uses the mode specified by the <code class="docutils literal notranslate"><span class="pre">ge.opSelectImplmode</span></code> parameter.</p></td>
<td><p>String</p></td>
<td><p>Operator type</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.op_compiler_cache_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure operator to compile Disk buffer mode.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;enable&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;force&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;disable&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.op_compiler_cache_dir</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure operator mutation Disk buffer directory.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">$HOME/atc_data</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.debugDir</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure the path to save debugging related process files generated by operator compilation.</p></td>
<td><p>String</p></td>
<td><p>Default Generate Current Path</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.opDebugLevel</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Operator debug function switch.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;1&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.exec.modify_mixlist</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure a mixed precision list.</p></td>
<td><p>String</p></td>
<td><p>path of config file</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.enableSparseMatrixWeight</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Enable global sparsity characteristics.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;1&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.externalWeight</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Whether to save the weights of constant nodes separately in a file.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;Ôºë&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.deterministic</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Whether to enable deterministic calculation.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;Ôºë&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.host_env_os</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Support for inconsistency between the compilation environment operating system and the runtime environment.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;linux&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.host_env_cpu</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Support for inconsistency between the operating system architecture and the runtime environment in the compilation environment.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;aarch64&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;x86_64&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.virtual_type</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Whether the offline model is supported to run on virtual devices generated by the Ascend virtualization instance feature.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;1&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.compressionOptimizeConf</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Compression optimization feature configuration file path.</p></td>
<td><p>String</p></td>
<td><p>path of config file</p></td>
</tr>
</tbody>
</table>
<p>Table 3: Configure [acl_build_options] parameter</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Attributes</p></th>
<th class="head"><p>Functions Description</p></th>
<th class="head"><p>Types</p></th>
<th class="head"><p>Values Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">input_format</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specify the model input format.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;NCHW&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;NHWC&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;ND&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">input_shape</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specify the model input shape.</p></td>
<td><p>String</p></td>
<td><p>For example: <code class="docutils literal notranslate"><span class="pre">input1:1,3,512,512;input2:1,3,224,224</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">input_shape_rang</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specify the model shape rang.</p></td>
<td><p>String</p></td>
<td><p>For example: <code class="docutils literal notranslate"><span class="pre">input1:[1-10,3,512,512];input2:[1-10,3,224,224]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">op_name_map</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Extension operator mapping configuration file path.</p></td>
<td><p>String</p></td>
<td><p>path of config file</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.dynamicBatchSize</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set dynamic batch gear parameters.</p></td>
<td><p>String</p></td>
<td><p>This parameter needs to be used in conjunction with the <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> parameter</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.dynamicImageSize</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set dynamic resolution parameters for input images.</p></td>
<td><p>String</p></td>
<td><p>This parameter needs to be used in conjunction with the <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> parameter</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.dynamicDims</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set the gear of dynamic dimensions in ND format.</p></td>
<td><p>String</p></td>
<td><p>This parameter needs to be used in conjunction with the <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> parameter</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.inserOpFile</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Enter the configuration file path for the preprocessing operator.</p></td>
<td><p>String</p></td>
<td><p>path of config file</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.exec.precision_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Enter the configuration file path for the preprocessing operator.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;force_fp32&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;force_fp16&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;allow_fp32_to_fp16&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;must_keep_origin_dtype&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;allow_mix_precision&quot;</span></code>ÔºåDefault: <code class="docutils literal notranslate"><span class="pre">&quot;force_fp16&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.exec.disableReuseMemory</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Memory reuse switch.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;Ôºë&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.outputDataType</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Network output data type.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;FP32&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;UINT8&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;FP16&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.outputNodeName</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specify output nodes.</p></td>
<td><p>String</p></td>
<td><p>For example: <code class="docutils literal notranslate"><span class="pre">&quot;node_name1:0;node_name1:1;node_name2:0&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.INPUT_NODES_SET_FP16</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Specify the input node name with input data type FP16.</p></td>
<td><p>String</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">&quot;node_name1;node_name2&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">log</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set Log Level.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;debug&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;info&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;warning&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;error&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.op_compiler_cache_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure operator to compile disk buffer mode.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;enable&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;force&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;disable&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.op_compiler_cache_dir</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure operator mutation disk buffer directory.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">$HOME/atc_data</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.debugDir</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure the path to save debugging related process files generated by operator compilation.</p></td>
<td><p>String</p></td>
<td><p>Default Generate Current Path</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.opDebugLevel</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Operator debug function switch.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;1&quot;</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.mdl_bank_path</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Path to custom knowledge base after loading model tuning.</p></td>
<td><p>String</p></td>
<td><p>This parameter needs to be used in conjunction with the <code class="docutils literal notranslate"><span class="pre">ge.bufferOptimize</span></code> parameter</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.op_bank_path</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Customizing the knowledge base path after loading operator tuning.</p></td>
<td><p>String</p></td>
<td><p>Knowledge Base Path</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.exec.modify_mixlist</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Configure mixed precision list.</p></td>
<td><p>String</p></td>
<td><p>path of config file</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.exec.op_precision_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set the precision mode of a specific operator and use this parameter to set the configuration file path.</p></td>
<td><p>String</p></td>
<td><p>path of config file</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.shape_generalized_build_mode</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Shape compilation method during image compilation.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;shape_generalized&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;shape_precise&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">op_debug_config</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Memory detection function switch.</p></td>
<td><p>String</p></td>
<td><p>path of config file</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ge.externalWeight</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Do you want to save the weights of constant nodes separately in a file.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;Ôºë&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;0&quot;</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">ge.exec.exclude_engines</span></code></p></td>
<td><p>Optional</p></td>
<td><p>Set the network model not to use one or some acceleration engines.</p></td>
<td><p>String</p></td>
<td><p>Options: <code class="docutils literal notranslate"><span class="pre">&quot;AiCore&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;AiVec&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;AiCpu&quot;</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="dynamic-shape-configuration">
<h2>Dynamic Shape Configuration<a class="headerlink" href="#dynamic-shape-configuration" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>In some inference scenarios, such as detecting a target and then executing the target recognition network, the number of targets is not fixed resulting in a variable input BatchSize for the target recognition network. If each inference is computed at the maximum BatchSize or maximum resolution, it will result in wasted computational resources. Therefore, it needs to support dynamic BatchSize and dynamic resolution scenarios during inference. Lite inference on Ascend supports dynamic BatchSize and dynamic resolution scenarios. The dynamic_dims dynamic parameter in [ascend_context] is configured via congFile in the convert phase, and the model <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.1/use/cloud_infer/runtime_cpp.html#dynamic-shape-input">Resize</a> is used during inference, to change the input shape.</p>
<section id="dynamic-batch-size">
<h3>Dynamic Batch Size<a class="headerlink" href="#dynamic-batch-size" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul>
<li><p>Parameter Name</p>
<p>dynamic_dims</p>
</li>
<li><p>Functions</p>
<p>Set the dynamic batch profile parameter for scenarios where the number of images processed at a time is not fixed during inference. This parameter needs to be used in conjunction with input_shape, and the position of -1 in input_shape is the dimension where the dynamic batch is located.</p>
</li>
<li><p>Value</p>
<p>Support up to 100 profiles configuration. Each profile is separated by English comma. The value limit of each profile: [1~2048]. For example, the parameters in the configuration file are configured as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ascend_context</span><span class="p">]</span>
<span class="n">input_shape</span><span class="o">=</span><span class="nb">input</span><span class="p">:[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">dynamic_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
<p>‚Äú-1‚Äù in input_shape means setting dynamic batch, and the profile can take the value of ‚Äú1,2‚Äù, that is, support profile 0: [1,64,64,3], profile 1: [2,64,64,3].</p>
<p>If more than one input exists, the profiles corresponding to the different inputs needs to be the same and separated by <code class="docutils literal notranslate"><span class="pre">;</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ascend_context</span><span class="p">]</span>
<span class="n">input_shape</span><span class="o">=</span><span class="n">input1</span><span class="p">:[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">,</span><span class="mi">3</span><span class="p">];</span><span class="n">input2</span><span class="p">:[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">256</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">dynamic_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">];[</span><span class="mi">1</span><span class="p">],[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</li>
<li><p>converter</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--configFile<span class="o">=</span>./config.txt<span class="w"> </span>--optimize<span class="o">=</span>ascend_oriented<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>Note: When enabling dynamic BatchSize, you do not need to specify the inputShape parameter, and only need to configure the [ascend_context] dynamic batch size through configFile, that is, the configuration content in the previous section.</p>
</li>
<li><p>Inference</p>
<p>Enable dynamic BatchSize. When the model inference is performed, the input shape can only choose the set value of the profile at the time of the converter. If you want to switch to the input shape corresponding to another profile, use the model <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.1/use/cloud_infer/runtime_cpp.html#dynamic-shape-input">Resize</a> function.</p>
</li>
<li><p>Precautions</p>
<ol class="arabic simple">
<li><p>If the user performs inference operations and the number of images processed is not fixed at a time, this parameter can be configured to dynamically allocate the number of images processed at a time. For example, if a user needs to process 2, 4, or 8 images each time to perform inference, it can be configured as 2, 4, and 8. Once the profile is requested, memory will be requested based on the actual profile during model inference.<br/></p></li>
<li><p>If the profile value set by the user is too large or the profiles are too many, it may cause model compilation failure, in which case the user is advised to reduce the profiles or turn down the profile value.<br/></p></li>
<li><p>If the profile value set by the user is too large or the profiles are too many, when performing inference in the runtime environment, it is recommended that the swapoff -a command be executed to turn off the swap interval as memory, to avoid that the swap space is continued to be called as memory, resulting in an unusually slow running environment due to the lack of memory.<br/></p></li>
</ol>
</li>
</ul>
</section>
<section id="dynamic-resolution">
<h3>Dynamic Resolution<a class="headerlink" href="#dynamic-resolution" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul>
<li><p>Parameter Name</p>
<p>dynamic_dims</p>
</li>
<li><p>Function</p>
<p>Set the dynamic resolution parameter of the input image. For scenarios where the width and height of the image are not fixed each time during inference. This parameter needs to be used in conjunction with input_shape, and the position of -1 in input_shape is the dimension where the dynamic resolution is located.</p>
</li>
<li><p>Value</p>
<p>Support up to 100 profiles configuration. Each profile is separated by English comma, such as ‚Äú[imagesize1_height,imagesize1_width],[imagesize2_height,imagesize2_width]‚Äù. For example, the parameters in the configuration file are configured as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">ascend_context</span><span class="p">]</span>
<span class="n">input_format</span><span class="o">=</span><span class="n">NHWC</span>
<span class="n">input_shape</span><span class="o">=</span><span class="nb">input</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">dynamic_dims</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">64</span><span class="p">],[</span><span class="mi">19200</span><span class="p">,</span><span class="mi">960</span><span class="p">]</span>
</pre></div>
</div>
<p>‚Äú-1‚Äù in input_shape means setting the dynamic resolution, i.e., it supports profile 0: [1,64,64,3] and profile 1: [1,19200,960,3].</p>
</li>
<li><p>converter</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--configFile<span class="o">=</span>./config.txt<span class="w"> </span>--optimize<span class="o">=</span>ascend_oriented<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>Note: When enabling dynamic BatchSize, you do not need to specify the inputShape parameter, and only need to configure the [ascend_context] dynamic resolution through configFile, that is, the configuration content in the previous section.</p>
</li>
<li><p>Inference</p>
<p>By enabling dynamic resolution, when model inference is performed, the input shape can only select the set profile value at the time of the converter. If you want to switch to the input shape corresponding to another profile, use the model <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.1/use/cloud_infer/runtime_cpp.html#dynamic-shape-input">Resize</a> function.</p>
</li>
<li><p>Precautions</p>
<ol class="arabic simple">
<li><p>If the resolution value set by the user is too large or the profiles are too many, it may cause model compilation failure, in which case the user is advised to reduce the profiles or turn down the profile value.<br/></p></li>
<li><p>If the user sets a dynamic resolution, the size of the dataset images used for the actual inference needs to match the specific resolution used.<br/></p></li>
<li><p>If the resolution value set by the user is too large or the profiles are too many, when performing inference in the runtime environment, it is recommended that the swapoff -a command be executed to turn off the swap interval as memory, to avoid that the swap space is continued to be called as memory, resulting in an unusually slow running environment due to the lack of memory.<br/></p></li>
</ol>
</li>
</ul>
</section>
<section id="dynamic-dimension">
<h3>Dynamic dimension<a class="headerlink" href="#dynamic-dimension" title="Permalink to this headline">ÔÉÅ</a></h3>
<ul>
<li><p>Parameter Name</p>
<p><code class="docutils literal notranslate"><span class="pre">ge.dynamicDims</span></code></p>
</li>
<li><p>Function</p>
<p>Set the gear of the dynamic dimension input in ND format. Applicable to scenarios where any dimension is processed each time reasoning is performed, This parameter needs to be used in conjunction with input_shape, and the position of -1 in input_shape is the dimension where the dynamic dim is located.</p>
</li>
<li><p>Value</p>
<p>Up to 100 configurations are supported, each separated by an English comma. For example, the parameters in the configuration file are configured as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">acl_build_options</span><span class="p">]</span>
<span class="n">input_format</span><span class="o">=</span><span class="s2">&quot;ND&quot;</span>
<span class="n">input_shape</span><span class="o">=</span><span class="s2">&quot;input1:1,-1,-1;input2:1,-1&quot;</span>
<span class="n">ge</span><span class="o">.</span><span class="n">dynamicDims</span><span class="o">=</span><span class="s2">&quot;32,32,24;64,64,36&quot;</span>
</pre></div>
</div>
<p>The ‚Äú-1‚Äù in the shape indicates the setting of dynamic dimensions, which supports gear 0: input1:1,32,32; input2:1,24, gear 1:1, 64,64; input2:1,36.</p>
</li>
<li><p>converter</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>.onnx<span class="w"> </span>--configFile<span class="o">=</span>./config.txt<span class="w"> </span>--optimize<span class="o">=</span>ascend_oriented<span class="w"> </span>--outputFile<span class="o">=</span><span class="si">${</span><span class="nv">model_name</span><span class="si">}</span>
</pre></div>
</div>
<p>Note: When enabling dynamic dimension, <code class="docutils literal notranslate"><span class="pre">input_format</span></code> must be set to <code class="docutils literal notranslate"><span class="pre">ND</span></code>.</p>
</li>
<li><p>Inference</p>
<p>By enabling dynamic dimension, when model inference is performed, the input shape can only select the set profile value at the time of the converter. If you want to switch to the input shape corresponding to another profile, use the model <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.1/use/cloud_infer/runtime_cpp.html#dynamic-shape-input">Resize</a> function.</p>
</li>
<li><p>Precautions</p>
<ol class="arabic simple">
<li><p>If the resolution value set by the user is too large or the profiles are too many, it may cause model compilation failure, in which case the user is advised to reduce the profiles or turn down the profile value.<br/></p></li>
<li><p>If the user sets a dynamic dimension, the dimension of the inputs for the actual inference needs to match the specific resolution used.<br/></p></li>
<li><p>If the resolution value set by the user is too large or the profiles are too many, when performing inference in the runtime environment, it is recommended that the swapoff -a command be executed to turn off the swap interval as memory, to avoid that the swap space is continued to be called as memory, resulting in an unusually slow running environment due to the lack of memory.<br/></p></li>
</ol>
</li>
</ul>
</section>
</section>
<section id="aoe-auto-tuning">
<h2>AOE Auto-tuning<a class="headerlink" href="#aoe-auto-tuning" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>AOE is a computational graph performance auto-tuning tool built specifically for the Davinci platform. Lite enables AOE ability to integrate the AOE offline executable in the converter phase, to perform performance tuning of the graph, generate a knowledge base, and save the offline model. This function supports subgraph tuning and operator tuning. The function supports subgraph tuning and operator tuning. The specific use process is as follows:</p>
<section id="aoe-tool-tuning">
<h3>AOE Tool Tuning<a class="headerlink" href="#aoe-tool-tuning" title="Permalink to this headline">ÔÉÅ</a></h3>
<ol class="arabic">
<li><p>Configure environment variables</p>
<p><code class="docutils literal notranslate"><span class="pre">${LOCAL_ASCEND}</span></code> is the path where the Ascend package is installed</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LOCAL_ASCEND</span><span class="o">=</span>/usr/local/Ascend
<span class="nb">source</span><span class="w"> </span><span class="si">${</span><span class="nv">LOCAL_ASCEND</span><span class="si">}</span>/latest/bin/setenv.bash
</pre></div>
</div>
<p>Confirm that the AOE executable program can be found and run in the environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>aoe<span class="w"> </span>-h
</pre></div>
</div>
</li>
<li><p>Specify the knowledge base path</p>
<p>AOE tuning generates an operator knowledge base. The default path:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/Ascend/latest/data/aoe/custom/graph<span class="o">(</span>op<span class="o">)</span>/<span class="si">${</span><span class="nv">soc_version</span><span class="si">}</span>
</pre></div>
</div>
<p>(Optional) You can also customize the knowledge base path with the <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">TUNE_BANK_PATH</span></code> environment variable.</p>
</li>
<li><p>Clear the cache</p>
<p>In order for the model compilation to get the knowledge base generated by AOE, it is best to delete the compilation cache before AOE is enabled to avoid cache reuse. Taking Ascend 310P environment with user as root for example, delete <code class="docutils literal notranslate"><span class="pre">/root/atc_data/kernel_cache/Ascend310P3</span></code> and <code class="docutils literal notranslate"><span class="pre">/root/atc_data/fuzzy_kernel_cache/Ascend310P3</span></code> directories.</p>
</li>
<li><p>Specified options of the configuration file</p>
<p>Specify the AOE tuning mode in the <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code> configuration file of the conversion tool config. In the following example, the subgraph tuning will be executed first, and then the operator tuning.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ascend_context<span class="o">]</span>
<span class="nv">aoe_mode</span><span class="o">=</span><span class="s2">&quot;subgraph tuning, operator tuning&quot;</span>
</pre></div>
</div>
</li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>The performance improvements will vary from environment to environment, and the actual latency reduction percentage is not exactly the same as the results shown in the tuning logs.</p></li>
<li><p>AOE tuning generates <code class="docutils literal notranslate"><span class="pre">aoe_workspace</span></code> directory in the current directory where the task is executed, which is used to save the models before and after tuning for performance improvement comparison, as well as the process data and result files necessary for tuning. This directory will occupy additional disk space, e.g., 2~10GB for a 500MB raw model, depending on the model size, operator type structure, input shape size and other factors. Therefore, it is recommended to reserve enough disk space, otherwise it may lead to tuning failure.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">aoe_workspace</span></code> directory needs to be deleted manually to free up disk space.</p></li>
</ul>
</div></blockquote>
</section>
<section id="aoe-api-tuning">
<h3>AOE API Tuning<a class="headerlink" href="#aoe-api-tuning" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>For Ascend inference, when the runtime specifies <code class="docutils literal notranslate"><span class="pre">provider</span></code> as <code class="docutils literal notranslate"><span class="pre">ge</span></code>, multiple models within one device can share weights, and some the weights in the model can be updated, that is, variables. Currently, only AOE API tuning supports variables exists in the model, and the default AOE tool tuning does not support that. The environment variables, setting and use of knowledge base paths, and AOE tuning cache are consistent with AOE tool tuning in the previous section. For details, please refer to <a class="reference external" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/63RC2alpha003/developmenttools/devtool/aoe_16_001.html">AOE tuning</a>.</p>
<p>AOE API tuning needs to be done through converter tool. When <code class="docutils literal notranslate"><span class="pre">optimize=ascend_oriented</span></code>, in the configuration file, there is <code class="docutils literal notranslate"><span class="pre">provider=ge</span></code> in <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code>, and there is a valid <code class="docutils literal notranslate"><span class="pre">aoe_mode</span></code> in <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code> or <code class="docutils literal notranslate"><span class="pre">acl_option_cfg_param]</span></code>, or there is a valid <code class="docutils literal notranslate"><span class="pre">job_type</span></code> in <code class="docutils literal notranslate"><span class="pre">[aoe_global_options]</span></code>, AOE API tuning will be performed. AOE API tuning only generates a knowledge base and does not generate an optimized model.</p>
<ol class="arabic">
<li><p>Specify <code class="docutils literal notranslate"><span class="pre">provider</span></code> as <code class="docutils literal notranslate"><span class="pre">ge</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ascend_context<span class="o">]</span>
<span class="nv">provider</span><span class="o">=</span>ge
</pre></div>
</div>
</li>
<li><p>AOE options</p>
<p>The options in <code class="docutils literal notranslate"><span class="pre">[aoe_global_options]</span></code> will be passed through to the <a class="reference external" href="https://gitee.com/link?target=https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/63RC2alpha003/developmenttools/devtool/aoe_16_070.html">global options</a> of the AOE API. The options in <code class="docutils literal notranslate"><span class="pre">[aoe_tuning_options]</span></code> will be passed through to the <a class="reference external" href="https://gitee.com/link?target=https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/63RC2alpha003/developmenttools/devtool/aoe_16_071.html">tuning options</a> of the AOE API.</p>
<p>We will extract the options in sections <code class="docutils literal notranslate"><span class="pre">[acl_option_cfg_param]</span></code>, <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code>, <code class="docutils literal notranslate"><span class="pre">[ge_session_options]</span></code> and <code class="docutils literal notranslate"><span class="pre">[ge_graph_options]</span></code> and convert them into AOE options to avoid the need for users to manually convert these options. The extracted options include <code class="docutils literal notranslate"><span class="pre">input_format</span></code>, <code class="docutils literal notranslate"><span class="pre">input_shape</span></code>, <code class="docutils literal notranslate"><span class="pre">dynamic_dims</span></code> and <code class="docutils literal notranslate"><span class="pre">precision_mode</span></code>. When the same option exists in multiple configuration sections at the same time, the priority ranges from low to high, with options in <code class="docutils literal notranslate"><span class="pre">[aoe_global_options]</span></code> and <code class="docutils literal notranslate"><span class="pre">[aoe_tuning_options]</span></code> having the highest priority. It is recommended to use <code class="docutils literal notranslate"><span class="pre">[ge_graph_options]</span></code> and <code class="docutils literal notranslate"><span class="pre">aoe_uning_options</span></code>.</p>
</li>
<li><p>AOE tuning mode</p>
<p>The <code class="docutils literal notranslate"><span class="pre">aoe_mode</span></code> is currently limited to <code class="docutils literal notranslate"><span class="pre">subgraph</span> <span class="pre">tuning</span></code> or <code class="docutils literal notranslate"><span class="pre">operator</span> <span class="pre">tuning</span></code>. Currently, <code class="docutils literal notranslate"><span class="pre">subgraph</span> <span class="pre">tuning,</span> <span class="pre">operator</span> <span class="pre">tuning</span></code> is not supported, which means that subgraph and operator tuning is not supported in the same tuning process. If necessary, subgraph and operator tuning can be performed separately.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">[aoe_global_options]</span></code>, when the value of <code class="docutils literal notranslate"><span class="pre">job_type</span></code> is <code class="docutils literal notranslate"><span class="pre">1</span></code>, it means subgraph tuning, and when the value is <code class="docutils literal notranslate"><span class="pre">2</span></code>, it means operator tuning.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ascend_context<span class="o">]</span>
<span class="nv">aoe_mode</span><span class="o">=</span><span class="s2">&quot;operator tuning&quot;</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>acl_option_cfg_param<span class="o">]</span>
<span class="nv">aoe_mode</span><span class="o">=</span><span class="s2">&quot;operator tuning&quot;</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>aoe_global_options<span class="o">]</span>
<span class="nv">job_type</span><span class="o">=</span><span class="m">2</span>
</pre></div>
</div>
</li>
<li><p>Dynamic dimension profiles</p>
<p>Dynamic dimension profiles can be set in <code class="docutils literal notranslate"><span class="pre">[acl_option_cfg_param]</span></code>, <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code>, <code class="docutils literal notranslate"><span class="pre">[ge_graph_options]</span></code>, <code class="docutils literal notranslate"><span class="pre">[aoe_tuning_options]</span></code>, with priority ranging from low to high. The following settings are equivalent. Setting the dynamic dimension profiles in <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code> can refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.1/use/cloud_infer/converter_tool_ascend.html#dynamic-shape-configuration">Dynamic Shape Configuration</a>. Setting the dynamic dimension profiles in <code class="docutils literal notranslate"><span class="pre">[acl_option_cfg_param]</span></code>, <code class="docutils literal notranslate"><span class="pre">[ge_graph_options]</span></code> and <code class="docutils literal notranslate"><span class="pre">[aoe_tuning_options]</span></code> can refer to <a class="reference external" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/63RC2alpha003/developmenttools/devtool/aoepar_16_015.html">dynamic_dims</a>, <a class="reference external" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/63RC2alpha003/developmenttools/devtool/aoepar_16_013.html">dynamic_batch_size</a>, <a class="reference external" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/63RC2alpha003/developmenttools/devtool/aoepar_16_014.html">dynamic_image_size</a>. Note that the <code class="docutils literal notranslate"><span class="pre">[ge_graph_options]</span></code> only supports the <code class="docutils literal notranslate"><span class="pre">ge.dynamicDims</span></code> and does not support the forms of <code class="docutils literal notranslate"><span class="pre">dynamic_batch_size</span></code> and <code class="docutils literal notranslate"><span class="pre">dynamic_image_size</span></code>. <code class="docutils literal notranslate"><span class="pre">input_format</span></code> is used to specify the input dimension layout for dynamic profiles. When using <code class="docutils literal notranslate"><span class="pre">dynamic_image_size</span></code>, it is necessary to specify <code class="docutils literal notranslate"><span class="pre">input_format</span></code> as <code class="docutils literal notranslate"><span class="pre">NCHW</span></code> or <code class="docutils literal notranslate"><span class="pre">NHWC</span></code> to indicate the location of the <code class="docutils literal notranslate"><span class="pre">H</span></code> and <code class="docutils literal notranslate"><span class="pre">W</span></code> dimensions.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ascend_context<span class="o">]</span>
<span class="nv">input_shape</span><span class="o">=</span>x1:<span class="o">[</span>-1,3,224,224<span class="o">]</span><span class="p">;</span>x2:<span class="o">[</span>-1,3,1024,1024<span class="o">]</span>
<span class="nv">dynamic_dims</span><span class="o">=[</span><span class="m">1</span><span class="o">]</span>,<span class="o">[</span><span class="m">2</span><span class="o">]</span>,<span class="o">[</span><span class="m">3</span><span class="o">]</span>,<span class="o">[</span><span class="m">4</span><span class="o">]</span><span class="p">;</span><span class="o">[</span><span class="m">1</span><span class="o">]</span>,<span class="o">[</span><span class="m">2</span><span class="o">]</span>,<span class="o">[</span><span class="m">3</span><span class="o">]</span>,<span class="o">[</span><span class="m">4</span><span class="o">]</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>acl_option_cfg_param<span class="o">]</span>
<span class="nv">input_shape</span><span class="o">=</span>x1:-1,3,224,224<span class="p">;</span>x2:-1,3,1024,1024
<span class="nv">dynamic_dims</span><span class="o">=</span><span class="m">1</span>,1<span class="p">;</span><span class="m">2</span>,2<span class="p">;</span><span class="m">3</span>,3<span class="p">;</span><span class="m">4</span>,4
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ge_graph_options<span class="o">]</span>
ge.inputShape<span class="o">=</span>x1:-1,3,224,224<span class="p">;</span>x2:-1,3,1024,1024
ge.dynamicDims<span class="o">=</span><span class="m">1</span>,1<span class="p">;</span><span class="m">2</span>,2<span class="p">;</span><span class="m">3</span>,3<span class="p">;</span><span class="m">4</span>,4
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>aoe_tuning_options<span class="o">]</span>
<span class="nv">input_shape</span><span class="o">=</span>x1:-1,3,224,224<span class="p">;</span>x2:-1,3,1024,1024
<span class="nv">dynamic_dims</span><span class="o">=</span><span class="m">1</span>,1<span class="p">;</span><span class="m">2</span>,2<span class="p">;</span><span class="m">3</span>,3<span class="p">;</span><span class="m">4</span>,4
</pre></div>
</div>
</li>
<li><p>Precision mode</p>
<p>Precision mode can be set in <code class="docutils literal notranslate"><span class="pre">[acl_option_cfg_param]</span></code>, <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code>, <code class="docutils literal notranslate"><span class="pre">[ge_graph_options]</span></code>, <code class="docutils literal notranslate"><span class="pre">[aoe_tuning_options]</span></code>, with priority ranging from low to high. The following settings are equivalent. Setting the precision mode in <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code> and <code class="docutils literal notranslate"><span class="pre">[acl_option_cfg_param]</span></code> can refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.1/use/cloud_infer/converter_tool_ascend.html#configuration-file">ascend_context - precision_mode</a>. Setting the precision mode in <code class="docutils literal notranslate"><span class="pre">[ge_graph_options]</span></code> and <code class="docutils literal notranslate"><span class="pre">[aoe_tuning_options]</span></code> can refer to <a class="reference external" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/63RC2alpha003/developmenttools/devtool/aoepar_16_046.html">precision_mode</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ascend_context<span class="o">]</span>
<span class="nv">precision_mode</span><span class="o">=</span>preferred_fp32
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>acl_option_cfg_param<span class="o">]</span>
<span class="nv">precision_mode</span><span class="o">=</span>preferred_fp32
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>ge_graph_options<span class="o">]</span>
<span class="nv">precision_mode</span><span class="o">=</span>allow_fp32_to_fp16
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>aoe_tuning_options<span class="o">]</span>
<span class="nv">precision_mode</span><span class="o">=</span>allow_fp32_to_fp16
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="deploying-ascend-custom-operators">
<h2>Deploying Ascend Custom Operators<a class="headerlink" href="#deploying-ascend-custom-operators" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>MindSpore Lite converter supports converting models with MindSpore Lite custom Ascend operators to MindSpore Lite models. Custom operators can be used to optimize model inference performance in special scenarios, such as using custom MatMul to achieve higher matrix multiplication, using the transformer fusion operators provided by MindSpore Lite to improve transformer model performance (to be launched) and using the AKG graph fusion operator to automatically fuse models to improve inference performance.</p>
<p>If MindSpore Lite converts Ascend models with custom operators, user needs to deploy the custom operators to the ACL operator library before calling the converter in order to complete the conversion properly. The following describes the key steps to deploy Ascend custom operators:</p>
<ol class="arabic">
<li><p>Configure environment variables</p>
<p><code class="docutils literal notranslate"><span class="pre">${ASCEND_OPP_PATH}</span></code> is the operator library path of Ascend software CANN package, usually under Ascend software installation path. The default is usually <code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend/latest/opp</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ASCEND_OPP_PATH</span><span class="o">=</span>/usr/local/Ascend/latest/opp
</pre></div>
</div>
</li>
<li><p>Obtain Ascend custom operator package</p>
<p>MindSpore Lite cloud-side inference package will contain Ascend custom operator package directory whose relative directory is <code class="docutils literal notranslate"><span class="pre">${LITE_PACKAGE_PATH}/tools/custom_kernels/ascend</span></code>. After unzip the Mindspore Lite cloud-side inference package, enter the corresponding directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar<span class="w"> </span>zxf<span class="w"> </span>mindspore-lite-<span class="o">{</span>version<span class="o">}</span>-linux-<span class="o">{</span>arch<span class="o">}</span>.tar.gz
<span class="nb">cd</span><span class="w"> </span>tools/custom_kernels/ascend
</pre></div>
</div>
</li>
<li><p>Run install.sh script to deploy custom operator</p>
<p>Run the installation script in the operator package directory to deploy the custom operator.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>install.sh
</pre></div>
</div>
</li>
<li><p>Check the Ascend library directory to see if the installation is successful</p>
<p>After deploying the custom operator, go to the Ascend operator library directory <code class="docutils literal notranslate"><span class="pre">/usr/local/Ascend/latest/opp/vendors/</span></code> and check whether there are corresponding custom operator files in the directory. At present, we mainly provide the basic operator sample and the AKG graph fusion operator implementation. The specific file structure is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>/usr/local/Ascend/latest/opp/vendors/
‚îú‚îÄ‚îÄ config.ini                                                     # Custom operator vendor configuration file, define the priority between different vendors, which needs to have vendor configuration of mslite
‚îî‚îÄ‚îÄ mslite                                                         # Custom operator directory provided by mslite
    ‚îú‚îÄ‚îÄ framework                                                  # Third-party framework adaptation configuration
    ‚îÇ    ‚îî‚îÄ‚îÄ tensorflow                                            # tensorflow adaptation configuration, not required
    ‚îÇ       ‚îî‚îÄ‚îÄ npu_supported_ops.json
    ‚îú‚îÄ‚îÄ op_impl                                                    # Custom operator implementation directory
    ‚îÇ   ‚îú‚îÄ‚îÄ ai_core                                                # Run operator implementation directory in ai_core
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tbe                                                # tbe operator implementation directory
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ config                                         # Operator configurations for different chips
    ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ascend310                                  # Operator configuration of 310 chip
    ‚îÇ   ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ aic_ascend310-ops-info.json
    ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ascend310p                                 # Operator configuration of 310p chip
    ‚îÇ   ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ aic_ascend310p-ops-info.json
    ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ascend910                                  # Operator configuration of 910 chip
    ‚îÇ   ‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ aic_ascend910-ops-info.json
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ mslite_impl                                    # Implementation logic directory of operators
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ add_dsl.py                                 # add sample logic implementation file based on dsl development
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ add_tik.py                                 # add sample logic implementation file based on tik development
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ compiler.py                                # Operator compilation logic file needed for akg graph
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ custom.py                                  # akg custom operator implementation file
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ matmul_tik.py                              # matmul sample logic implementation file based on tik development
    ‚îÇ   ‚îú‚îÄ‚îÄ cpu                                                    # aicpu custom operator subdirectory, not required
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aicpu_kernel
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ impl
    ‚îÇ   ‚îî‚îÄ‚îÄ vector_core                                            # Run operator implementation directory in vector_core
    ‚îÇ       ‚îî‚îÄ‚îÄ tbe                                                # tbe operator implementation directory
    ‚îÇ           ‚îî‚îÄ‚îÄ mslite_impl                                    # Implementation logic directory of operators
    ‚îÇ               ‚îú‚îÄ‚îÄ add_dsl.py                                 # add sample logic implementation file based on dsl development
    ‚îÇ               ‚îú‚îÄ‚îÄ add_tik.py                                 # add sample logic implementation file based on tik development
    ‚îÇ               ‚îî‚îÄ‚îÄ matmul_tik.py                              # matmul sample logic implementation file based on tik development
    ‚îî‚îÄ‚îÄ op_proto                                                   # Operator prototype definition package directory
        ‚îî‚îÄ‚îÄ libcust_op_proto.so                                    # operator prototype definition so file. akg custom operator is registered by default, and do not need this file
</pre></div>
</div>
</li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="converter_python.html" class="btn btn-neutral float-left" title="Using Python Interface to Perform Model Conversions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="converter_tool_graph_kernel.html" class="btn btn-neutral float-right" title="Graph Kernel Fusion Configuration Instructions (Beta Feature)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>