<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Performing Cloud-side Distributed Inference Using C++ Interface &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Performing Cloud-side Distributed Inference Using Python Interface" href="runtime_distributed_python.html" />
    <link rel="prev" title="Distributed Inference" href="runtime_distributed.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime_distributed.html">Distributed Inference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Performing Cloud-side Distributed Inference Using C++ Interface</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparation">Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-contextual-configuration">Creating Contextual Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuring-ascend-device-context">Configuring Ascend Device Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-gpu-device-context">Configuring GPU Device Context</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-creation,-loading-and-compilation">Model Creation, Loading and Compilation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-input-data-padding">Model Input Data Padding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distributed-inference-execution">Distributed Inference Execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-output-data-obtaining">Model Output Data Obtaining</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiling-and-performing-distributed-inference-example">Compiling and Performing Distributed Inference Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-models-sharing-weights">Multiple Models Sharing Weights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="runtime_distributed_python.html">Performing Cloud-side Distributed Inference Using Python Interface</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime_distributed.html">Distributed Inference</a> &raquo;</li>
      <li>Performing Cloud-side Distributed Inference Using C++ Interface</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/use/cloud_infer/runtime_distributed_cpp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="performing-cloud-side-distributed-inference-using-c++-interface">
<h1>Performing Cloud-side Distributed Inference Using C++ Interface<a class="headerlink" href="#performing-cloud-side-distributed-inference-using-c++-interface" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.2/docs/lite/docs/source_en/use/cloud_infer/runtime_distributed_cpp.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>For scenarios where large-scale neural network models have many parameters and cannot be fully loaded into a single device for inference, distributed inference can be performed using multiple devices. This tutorial describes how to perform MindSpore Lite cloud-side distributed inference using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/index.html">C++ interface</a>. Cloud-side distributed inference is roughly the same process as <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/runtime_cpp.html">Cloud-side single-card inference</a> and can be cross-referenced. For the related contents of distributed inference, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.2/parallel/model_loading.html#inference">MindSpore Distributed inference</a>, and MindSpore Lite cloud-side distributed inference has more optimization for performance aspects.</p>
<p>MindSpore Lite cloud-side distributed inference is only supported to run in Linux environment deployments with Ascend 910 and Nvidia GPU as the supported device types. As shown in the figure below, the distributed inference is currently initiated by a multi-process approach, where each process corresponds to a <code class="docutils literal notranslate"><span class="pre">Rank</span></code> in the communication set, loading, compiling and executing the respective sliced model, with the same input data for each process.</p>
<p><img alt="img" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/docs/lite/docs/source_zh_cn/use/cloud_infer/images/lite_runtime_distributed.png" /></p>
<p>Each process consists of the following main steps:</p>
<ol class="arabic simple">
<li><p>Model reading: Slice and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.2/parallel/model_loading.html#exporting-mindir-files-in-the-distributed-scenario">export the distributed MindIR model</a> via MindSpore. The number of MindIR models is the same as the number of devices for loading to each device for inference.</p></li>
<li><p>Context creation and configuration: Create and configure the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Context.html">Context</a>, and hold the distributed inference parameters to guide distributed model compilation and model execution.</p></li>
<li><p>Model loading and compilation: Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Model.html">Model::Build</a> interface for model loading and model compilation. The model loading phase parses the file cache into a runtime model. The model compilation phase optimizes the front-end computational graph into a high-performance back-end computational graph. The process is time-consuming and it is recommended to compile once and inference multiple times.</p></li>
<li><p>Model input data padding.</p></li>
<li><p>Distributed inference execution: use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Model.html">Model::Predict</a> interface for model distributed inference.</p></li>
<li><p>Model output data obtaining.</p></li>
<li><p>Compilation and multi-process execution of distributed inference programs.</p></li>
</ol>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>To download the cloud-side distributed inference C++ sample code, please select the device type: <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.2/mindspore/lite/examples/cloud_infer/ascend_ge_distributed_cpp">Ascend</a> or <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.2/mindspore/lite/examples/cloud_infer/gpu_trt_distributed_cpp">GPU</a>. The directory will be referred to later as the example code directory.</p></li>
<li><p>Slice and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.2/parallel/model_loading.html#exporting-mindir-files-in-the-distributed-scenario">export the distributed MindIR model</a> via MindSpore and store it to the sample code directory. For a quick experience, you can download the two sliced Matmul model files <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/Matmul0.mindir">Matmul0.mindir</a>, <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/Matmul1.mindir">Matmul1.mindir</a>.</p></li>
<li><p>For Ascend device type, generate the networking information file through hccl_tools.py as needed, store it in the sample code directory, and fill the path of the file into the configuration file <code class="docutils literal notranslate"><span class="pre">config_file.ini</span></code> in the sample code directory.</p></li>
<li><p>Download the MindSpore Lite cloud-side inference installation package <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/downloads.html">mindspore-lite-{version}-linux-{arch}.tar.gz</a> and store it to the sample code directory. Unzip this installation package and refer to the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/quick_start/one_hour_introduction_cloud.html#environment-variables">Environment Variables</a> section in the Quick Start to set environment variables.</p></li>
</ol>
<p>The main steps of MindSpore Lite cloud-side distributed inference will be described in the subsequent sections in conjunction with the code, and please refer to <code class="docutils literal notranslate"><span class="pre">main.cc</span></code> in the sample code directory for the complete code.</p>
</section>
<section id="creating-contextual-configuration">
<h2>Creating Contextual Configuration<a class="headerlink" href="#creating-contextual-configuration" title="Permalink to this headline"></a></h2>
<p>The contextual configuration holds the required basic configuration parameters and distributed inference parameters to guide model compilation and model distributed execution. The following sample code demonstrates how to create a context through <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Context.html">Context</a> and specify a running device through <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Context.html">Context:. MutableDeviceInfo</a> to specify the running device.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Create and init context, add Ascend device info</span>
<span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
</pre></div>
</div>
<p>Distributed inference scenarios support <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_AscendDeviceInfo.html">AscendDeviceInfo</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_GPUDeviceInfo.html">GPUDeviceInfo</a>, which are used to set Ascend and Nvidia GPU context information respectively.</p>
<section id="configuring-ascend-device-context">
<h3>Configuring Ascend Device Context<a class="headerlink" href="#configuring-ascend-device-context" title="Permalink to this headline"></a></h3>
<p>When the device type is Ascend (Ascend910 is currently supported by distributed inference), a new <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_AscendDeviceInfo.html">AscendDeviceInfo</a> is created, and set <code class="docutils literal notranslate"><span class="pre">DeviceID</span></code>, <code class="docutils literal notranslate"><span class="pre">RankID</span></code> respectively by <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_AscendDeviceInfo.html">AscendDeviceInfo::SetDeviceID</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_AscendDeviceInfo.html">AscendDeviceInfo::SetRankID</a>. Since Ascend provides multiple inference engine backends, currently only the <code class="docutils literal notranslate"><span class="pre">ge</span></code> backend supports distributed inference, and the Ascend inference engine backend is specified as <code class="docutils literal notranslate"><span class="pre">ge</span></code> by <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_DeviceInfoContext.html">DeviceInfoContext::SetProvider</a>. The sample code is as follows.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// for Ascend 910</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">AscendDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New AscendDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Set Ascend 910 device id， rank id and provider.</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetRankID</span><span class="p">(</span><span class="n">rank_id</span><span class="p">);</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetProvider</span><span class="p">(</span><span class="s">&quot;ge&quot;</span><span class="p">);</span>
<span class="c1">// Device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="configuring-gpu-device-context">
<h3>Configuring GPU Device Context<a class="headerlink" href="#configuring-gpu-device-context" title="Permalink to this headline"></a></h3>
<p>When the device type is GPU, new <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_GPUDeviceInfo.html">GPUDeviceInfo</a> is created. The distributed inference multi-process application for GPU devices is pulled up by mpi, which automatically sets the <code class="docutils literal notranslate"><span class="pre">RankID</span></code> of each process, and the user only needs to specify <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> in the environment variable, without specifying the group information file. Therefore, the <code class="docutils literal notranslate"><span class="pre">RankID</span></code> of each process can be used as <code class="docutils literal notranslate"><span class="pre">DeviceID</span></code>. In addition, GPU also provides multiple inference engine backends. Currently only <code class="docutils literal notranslate"><span class="pre">tensorrt</span></code> backend supports distributed inference, and the GPU inference engine backend is specified as <code class="docutils literal notranslate"><span class="pre">tensorrt</span></code> by <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_DeviceInfoContext.html">DeviceInfoContext::SetProvider</a>. The sample code is as follows.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// for GPU</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New GPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// set distributed info</span>
<span class="k">auto</span><span class="w"> </span><span class="n">rank_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">device_info</span><span class="o">-&gt;</span><span class="n">GetRankID</span><span class="p">();</span><span class="w">  </span><span class="c1">// rank id is returned from mpi</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="n">rank_id</span><span class="p">);</span><span class="w">  </span><span class="c1">// as we set visible device id in env, we use rank id as device id</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetProvider</span><span class="p">(</span><span class="s">&quot;tensorrt&quot;</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>
<section id="model-creation,-loading-and-compilation">
<h2>Model Creation, Loading and Compilation<a class="headerlink" href="#model-creation,-loading-and-compilation" title="Permalink to this headline"></a></h2>
<p>Consistent with <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/runtime_cpp.html">MindSpore Lite Cloud-side Single Card Inference</a>, the main entry point for distributed inference is the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Model.html">Model</a> interface for model loading, compilation and execution. For Ascend devices, use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Model.html">Model::LoadConfig</a> interface to load the configuration file <a class="reference external" href="https://gitee.com/mindspore/mindspore/blob/r2.2/mindspore/lite/examples/cloud_infer/ascend_ge_distributed_cpp/config_file.ini">config_file.ini</a>, which is not required for GPU devices. Finally, call the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Model.html">Model::Build</a> interface to implement model loading and model compilation, and the sample code is as follows.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="c1">// Load config file for Ascend910</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">config_path</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">LoadConfig</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load config file &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">config_path</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="c1">// Build model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="model-input-data-padding">
<h2>Model Input Data Padding<a class="headerlink" href="#model-input-data-padding" title="Permalink to this headline"></a></h2>
<p>First, use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Model.html">Model::GetInputs</a> method to get all the input <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, and fill in the Host data through the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_MSTensor.html">MSTensor</a>-related interface. The sample code is as follows.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// helper function</span>
<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">Distribution</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">GenerateRandomData</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Distribution</span><span class="w"> </span><span class="n">distribution</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">mt19937</span><span class="w"> </span><span class="n">random_engine</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">elements_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">);</span>
<span class="w">  </span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">generate_n</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">elements_num</span><span class="p">,</span>
<span class="w">                        </span><span class="p">[</span><span class="o">&amp;</span><span class="n">distribution</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">random_engine</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">distribution</span><span class="p">(</span><span class="n">random_engine</span><span class="p">));</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>

<span class="c1">// Get input tensor pointer and write random data</span>
<span class="kt">int</span><span class="w"> </span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;MallocData for inTensor failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">GenerateRandomData</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">(),</span><span class="w"> </span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">uniform_real_distribution</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">1.0f</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Get Input</span>
<span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="c1">// Generate random data as input data.</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">GenerateInputDataWithRandom</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Generate Random Input Data failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="distributed-inference-execution">
<h2>Distributed Inference Execution<a class="headerlink" href="#distributed-inference-execution" title="Permalink to this headline"></a></h2>
<p>Create a model output <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> of type <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_MSTensor.html">MSTensor</a>. Call the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Model.html">Model::Predict</a> interface to perform distributed inference, with the following sample code.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Model Predict</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="model-output-data-obtaining">
<h2>Model Output Data Obtaining<a class="headerlink" href="#model-output-data-obtaining" title="Permalink to this headline"></a></h2>
<p>The model output data is stored in the output <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> defined in the previous step, and the output data is accessible through the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_MSTensor.html">MSTensor</a>-related interface. The following example code shows how to access the output data and print it.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Print Output Tensor Data.</span>
<span class="k">constexpr</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">kNumPrintOfOutData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tensor name is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">Name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; tensor size is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span>
<span class="w">            </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; tensor elements num is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">Data</span><span class="p">().</span><span class="n">get</span><span class="p">());</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;output data is:&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">kNumPrintOfOutData</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="compiling-and-performing-distributed-inference-example">
<h2>Compiling and Performing Distributed Inference Example<a class="headerlink" href="#compiling-and-performing-distributed-inference-example" title="Permalink to this headline"></a></h2>
<p>In the sample code directory, compile the sample in the following way. Please refer to <code class="docutils literal notranslate"><span class="pre">build.sh</span></code> in the sample code directory for the complete command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>build
<span class="nb">cd</span><span class="w"> </span>build<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">exit</span>
cmake<span class="w"> </span>..
make
</pre></div>
</div>
<p>After successful compilation, the <code class="docutils literal notranslate"><span class="pre">{device_type}_{backend}_distributed_cpp</span></code> executable programs is obtained in the <code class="docutils literal notranslate"><span class="pre">build</span></code> directory, and the distributed inference is started in the following multi-process manner. Please refer to <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> in the sample code directory for the complete run command. When run successfully, the name, data size, number of elements and the first 10 elements of each output <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> will be printed.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># for Ascend, run the executable file for each rank using shell commands</span>
./build/ascend_ge_distributed<span class="w"> </span>/your/path/to/Matmul0.mindir<span class="w"> </span><span class="m">0</span><span class="w"> </span><span class="m">0</span><span class="w"> </span>./config_file.ini<span class="w"> </span><span class="p">&amp;</span>
./build/ascend_ge_distributed<span class="w"> </span>/your/path/to/Matmul1.mindir<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">1</span><span class="w"> </span>./config_file.ini

<span class="c1"># for GPU, run the executable file for each rank using mpi</span>
<span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">2</span>
mpirun<span class="w"> </span>-n<span class="w"> </span><span class="nv">$RANK_SIZE</span><span class="w"> </span>./build/gpu_trt_distributed<span class="w"> </span>/your/path/to/Matmul.mindir
</pre></div>
</div>
</section>
<section id="multiple-models-sharing-weights">
<h2>Multiple Models Sharing Weights<a class="headerlink" href="#multiple-models-sharing-weights" title="Permalink to this headline"></a></h2>
<p>In the Ascend device GE scenario, a single card can deploy multiple models, and models deployed to the same card can share weights. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/runtime_cpp.html#multiple-models-sharing-weights">Advanced Usage - Multiple Model Sharing Weights</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_distributed.html" class="btn btn-neutral float-left" title="Distributed Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="runtime_distributed_python.html" class="btn btn-neutral float-right" title="Performing Cloud-side Distributed Inference Using Python Interface" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>