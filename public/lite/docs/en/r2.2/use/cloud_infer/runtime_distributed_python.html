<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Performing Cloud-side Distributed Inference Using Python Interface &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Model Converter" href="converter.html" />
    <link rel="prev" title="Performing Cloud-side Distributed Inference Using C++ Interface" href="runtime_distributed_cpp.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="runtime_distributed.html">Distributed Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="runtime_distributed_cpp.html">Performing Cloud-side Distributed Inference Using C++ Interface</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Performing Cloud-side Distributed Inference Using Python Interface</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparation">Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-contextual-configuration">Creating Contextual Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuring-ascend-device-context">Configuring Ascend Device Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-gpu-device-context">Configuring GPU Device Context</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-creation,-loading-and-compilation">Model Creation, Loading and Compilation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-input-data-padding">Model Input Data Padding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distributed-inference-execution">Distributed Inference Execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-output-data-obtaining">Model Output Data Obtaining</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performing-distributed-inference-example">Performing Distributed Inference Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-models-sharing-weights">Multiple Models Sharing Weights</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime_distributed.html">Distributed Inference</a> &raquo;</li>
      <li>Performing Cloud-side Distributed Inference Using Python Interface</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/use/cloud_infer/runtime_distributed_python.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="performing-cloud-side-distributed-inference-using-python-interface">
<h1>Performing Cloud-side Distributed Inference Using Python Interface<a class="headerlink" href="#performing-cloud-side-distributed-inference-using-python-interface" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.2/docs/lite/docs/source_en/use/cloud_infer/runtime_distributed_python.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>For scenarios where large-scale neural network models have many parameters and cannot be fully loaded into a single device for inference, distributed inference can be performed using multiple devices. This tutorial describes how to perform MindSpore Lite cloud-side distributed inference using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite.html">Python interface</a>. Cloud-side distributed inference is roughly the same process as <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/runtime_python.html">Cloud-side single-card inference</a> and can be cross-referenced. For the related contents of distributed inference, please refer to <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.2/parallel/model_loading.html#inference">MindSpore Distributed inference</a>, and MindSpore Lite cloud-side distributed inference has more optimization for performance aspects.</p>
<p>MindSpore Lite cloud-side distributed inference is only supported to run in Linux environment deployments with Ascend 910 and Nvidia GPU as the supported device types. As shown in the figure below, the distributed inference is currently initiated by a multi-process approach, where each process corresponds to a <code class="docutils literal notranslate"><span class="pre">Rank</span></code> in the communication set, loading, compiling and executing the respective sliced model, with the same input data for each process.</p>
<p><img alt="img" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/docs/lite/docs/source_zh_cn/use/cloud_infer/images/lite_runtime_distributed.png" /></p>
<p>Each process consists of the following main steps:</p>
<ol class="arabic simple">
<li><p>Model reading: Slice and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.2/parallel/model_loading.html#exporting-mindir-files-in-the-distributed-scenario">export the distributed MindIR model</a> via MindSpore. The number of MindIR models is the same as the number of devices for loading to each device for inference.</p></li>
<li><p>Context creation and configuration: Create and configure the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context</a>, and hold the distributed inference parameters to guide distributed model compilation and model execution.</p></li>
<li><p>Model loading and compilation: Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.build_from_file">Model.build_from_file</a> interface for model loading and model compilation. The model loading phase parses the file cache into a runtime model. The model compilation phase optimizes the front-end computational graph into a high-performance back-end computational graph. The process is time-consuming and it is recommended to compile once and inference multiple times.</p></li>
<li><p>Model input data padding.</p></li>
<li><p>Distributed inference execution: use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.predict">Model.predict</a> interface for model distributed inference.</p></li>
<li><p>Model output data obtaining.</p></li>
<li><p>Multi-process execution of distributed inference programs.</p></li>
</ol>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>To download the cloud-side distributed inference python sample code, please select the device type: <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.2/mindspore/lite/examples/cloud_infer/ascend_ge_distributed_cpp">Ascend</a> or <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.2/mindspore/lite/examples/cloud_infer/gpu_trt_distributed_cpp">GPU</a>. The directory will be referred to later as the example code directory.</p></li>
<li><p>Slice and <a class="reference external" href="https://www.mindspore.cn/tutorials/experts/en/r2.2/parallel/model_loading.html#exporting-mindir-files-in-the-distributed-scenario">export the distributed MindIR model</a> via MindSpore and store it to the sample code directory. For a quick experience, you can download the two sliced Matmul model files <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/Matmul0.mindir">Matmul0.mindir</a>, <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/Matmul1.mindir">Matmul1.mindir</a>.</p></li>
<li><p>For Ascend device type, generate the networking information file through hccl_tools.py as needed, store it in the sample code directory, and fill the path of the file into the configuration file <code class="docutils literal notranslate"><span class="pre">config_file.ini</span></code> in the sample code directory.</p></li>
<li><p>Download the MindSpore Lite cloud-side inference installation package <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/downloads.html">mindspore-lite-{version}-linux-{arch}.whl</a>, store it to the sample code directory, and install it via <code class="docutils literal notranslate"><span class="pre">pip</span></code>.</p></li>
</ol>
<p>The main steps of MindSpore Lite cloud-side distributed inference will be described in the subsequent sections in conjunction with the code, and please refer to <code class="docutils literal notranslate"><span class="pre">main.py</span></code> in the sample code directory for the complete code.</p>
</section>
<section id="creating-contextual-configuration">
<h2>Creating Contextual Configuration<a class="headerlink" href="#creating-contextual-configuration" title="Permalink to this headline"></a></h2>
<p>The contextual configuration holds the required basic configuration parameters and distributed inference parameters to guide model compilation and model distributed execution. The following sample code demonstrates how to create a context through <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># init context</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
</pre></div>
</div>
<p>Ascend, Nvidia GPU are supported in distributed inference scenarios, and can be specified by <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.target</a> to specify the device to run.</p>
<section id="configuring-ascend-device-context">
<h3>Configuring Ascend Device Context<a class="headerlink" href="#configuring-ascend-device-context" title="Permalink to this headline"></a></h3>
<p>When the device type is Ascend (Ascend910 is currently supported by distributed inference), set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.target</a> to <code class="docutils literal notranslate"><span class="pre">Ascend</span></code> and set <code class="docutils literal notranslate"><span class="pre">DeviceID</span></code>, <code class="docutils literal notranslate"><span class="pre">RankID</span></code> by the following way. Since Ascend provides multiple inference engine backends, currently only the <code class="docutils literal notranslate"><span class="pre">ge</span></code> backend supports distributed inference, and the Ascend inference engine backend is specified as <code class="docutils literal notranslate"><span class="pre">ge</span></code> by via <code class="docutils literal notranslate"><span class="pre">ascend.provider</span></code>.The sample code is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># set Ascend target and distributed info</span>
<span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Ascend&quot;</span><span class="p">]</span>
<span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">device_id</span>
<span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">rank_id</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">rank_id</span>
<span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">provider</span> <span class="o">=</span> <span class="s2">&quot;ge&quot;</span>
</pre></div>
</div>
</section>
<section id="configuring-gpu-device-context">
<h3>Configuring GPU Device Context<a class="headerlink" href="#configuring-gpu-device-context" title="Permalink to this headline"></a></h3>
<p>When the device type is GPU, set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context.target">Context.target</a> as <code class="docutils literal notranslate"><span class="pre">gpu</span></code>. The distributed inference multi-process application for GPU devices is pulled up by mpi, which automatically sets the <code class="docutils literal notranslate"><span class="pre">RankID</span></code> of each process, and the user only needs to specify <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> in the environment variable, without specifying the group information file. Therefore, the <code class="docutils literal notranslate"><span class="pre">RankID</span></code> of each process can be used as <code class="docutils literal notranslate"><span class="pre">DeviceID</span></code>. In addition, GPU also provides multiple inference engine backends. Currently only <code class="docutils literal notranslate"><span class="pre">tensorrt</span></code> backend supports distributed inference, and the GPU inference engine backend is specified as <code class="docutils literal notranslate"><span class="pre">tensorrt</span></code> by <code class="docutils literal notranslate"><span class="pre">gpu.provider</span></code>. The sample code is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># set GPU target and distributed info</span>
<span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gpu&quot;</span><span class="p">]</span>
<span class="n">context</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">rank_id</span>
<span class="n">context</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">provider</span> <span class="o">=</span> <span class="s2">&quot;tensorrt&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="model-creation,-loading-and-compilation">
<h2>Model Creation, Loading and Compilation<a class="headerlink" href="#model-creation,-loading-and-compilation" title="Permalink to this headline"></a></h2>
<p>Consistent with <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/runtime_cpp.html">MindSpore Lite Cloud-side Single Card Inference</a>, the main entry point for distributed inference is the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Model.html">Model</a> interface for model loading, compilation and execution. Create <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model">Model</a> and call the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.build_from_file">Model.build_from_file</a> interface to implement the model Loading and model compilation, the sample code is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create Model and build Model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">config_file</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-input-data-padding">
<h2>Model Input Data Padding<a class="headerlink" href="#model-input-data-padding" title="Permalink to this headline"></a></h2>
<p>First, use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.get_inputs">Model.get_inputs</a> method to get all the input <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor">Tensor</a>, and fill in the Host data through the related interface. The sample code is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># set model input as ones</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()</span>
<span class="k">for</span> <span class="n">input_i</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
    <span class="n">input_i</span><span class="o">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">input_i</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
<p>MindSpore Lite input can also be constructed in the following way.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># np_inputs is a list or tuple of numpy array</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">mslite</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np_input</span><span class="p">)</span> <span class="k">for</span> <span class="n">np_input</span> <span class="ow">in</span> <span class="n">np_inputs</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="distributed-inference-execution">
<h2>Distributed Inference Execution<a class="headerlink" href="#distributed-inference-execution" title="Permalink to this headline"></a></h2>
<p>Call the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_Model.html">Model.predict</a> interface to perform distributed inference, with the following sample code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># execute inference</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-output-data-obtaining">
<h2>Model Output Data Obtaining<a class="headerlink" href="#model-output-data-obtaining" title="Permalink to this headline"></a></h2>
<p>The model output data is stored in the output <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Tensor.html#mindspore_lite.Tensor">Tensor</a> defined in the previous step, and the output data can be accessed through the relevant interface. The following example code shows how to access the output data and print it.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># get output and print</span>
<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>
    <span class="n">data_size</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">data_size</span>
    <span class="n">element_num</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">element_num</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor&#39;s name is:</span><span class="si">%s</span><span class="s2"> data size is:</span><span class="si">%s</span><span class="s2"> tensor elements num is:</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
          <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">data_size</span><span class="p">,</span> <span class="n">element_num</span><span class="p">))</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get_data_to_numpy</span><span class="p">()</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output data is:&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="performing-distributed-inference-example">
<h2>Performing Distributed Inference Example<a class="headerlink" href="#performing-distributed-inference-example" title="Permalink to this headline"></a></h2>
<p>Start the distributed inference in the following multi-process manner. Please refer to <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> in the sample code directory for the complete run command. When run successfully, the name, data size, number of elements and the first 10 elements of each output <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> will be printed.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># for Ascend, run the executable file for each rank using shell commands</span>
python3<span class="w"> </span>./ascend_ge_distributed.py<span class="w"> </span>--model_path<span class="o">=</span>/your/path/to/Matmul0.mindir<span class="w"> </span>--device_id<span class="o">=</span><span class="m">0</span><span class="w"> </span>--rank_id<span class="o">=</span><span class="m">0</span><span class="w"> </span>--config_file<span class="o">=</span>./config_file.ini<span class="w"> </span><span class="p">&amp;</span>
python3<span class="w"> </span>./ascend_ge_distributed.py<span class="w"> </span>--model_path<span class="o">=</span>/your/path/to/Matmul1.mindir<span class="w"> </span>--device_id<span class="o">=</span><span class="m">1</span><span class="w"> </span>--rank_id<span class="o">=</span><span class="m">1</span><span class="w"> </span>--config_file<span class="o">=</span>./config_file.ini

<span class="c1"># for GPU, run the executable file for each rank using mpi</span>
<span class="nv">RANK_SIZE</span><span class="o">=</span><span class="m">2</span>
mpirun<span class="w"> </span>-n<span class="w"> </span><span class="nv">$RANK_SIZE</span><span class="w"> </span>python3<span class="w"> </span>./main.py<span class="w"> </span>--model_path<span class="o">=</span>/your/path/to/Matmul.mindir
</pre></div>
</div>
</section>
<section id="multiple-models-sharing-weights">
<h2>Multiple Models Sharing Weights<a class="headerlink" href="#multiple-models-sharing-weights" title="Permalink to this headline"></a></h2>
<p>In the Ascend device GE scenario, a single card can deploy multiple models, and models deployed to the same card can share weights. For details, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/runtime_cpp.html#multiple-models-sharing-weights">Advanced Usage - Multiple Model Sharing Weights</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_distributed_cpp.html" class="btn btn-neutral float-left" title="Performing Cloud-side Distributed Inference Using C++ Interface" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="converter.html" class="btn btn-neutral float-right" title="Model Converter" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>