<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Java Interface to Perform Cloud-side Inference &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Using Python Interface to Perform Cloud-side Inference" href="runtime_python.html" />
    <link rel="prev" title="Using C++ Interface to Perform Cloud-side Inference" href="runtime_cpp.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="runtime.html">Performing Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="runtime_cpp.html">Using C++ Interface to Perform Cloud-side Inference</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Java Interface to Perform Cloud-side Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reference-to-mindspore-lite-java-library">Reference to MindSpore Lite Java Library</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#linux-project-references-to-jar-library">Linux Project References to JAR Library</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-path">Model Path</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-configuration-context">Creating Configuration Context</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuring-to-use-the-cpu-backend">Configuring to Use the CPU Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-to-use-the-gpu-backend">Configuring to Use the GPU Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-to-use-the-ascend-backend">Configuring to Use the Ascend Backend</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-creation,-loading-and-compilation">Model Creation, Loading and Compilation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inputting-the-data">Inputting the Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#executing-the-inference">Executing the Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#obtaining-the-output">Obtaining the Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="#releasing-the-memory">Releasing the Memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-usage">Advanced Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#input-dimension-resize">Input Dimension Resize</a></li>
<li class="toctree-l4"><a class="reference internal" href="#viewing-the-logs">Viewing the Logs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-the-version-number">Obtaining the Version Number</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="runtime_python.html">Using Python Interface to Perform Cloud-side Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">RELEASE NOTES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../RELEASE.html">Release Notes</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime.html">Performing Inference</a> &raquo;</li>
      <li>Using Java Interface to Perform Cloud-side Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/use/cloud_infer/runtime_java.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="using-java-interface-to-perform-cloud-side-inference">
<h1>Using Java Interface to Perform Cloud-side Inference<a class="headerlink" href="#using-java-interface-to-perform-cloud-side-inference" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.2/docs/lite/docs/source_en/use/cloud_infer/runtime_java.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>After converting the <code class="docutils literal notranslate"><span class="pre">.mindir</span></code> model by <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/converter_tool.html">MindSpore Lite model conversion tool</a>, you can execute the inference process of the model in Runtime. This tutorial describes how to perform cloud-side inference by using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/index.html">JAVA interface</a>.</p>
<p>Compared with C++ API, Java API can be called directly in Java Class, and users do not need to implement the code related to JNI layer, with better convenience. Running MindSpore Lite inference framework mainly consists of the following steps:</p>
<ol class="arabic simple">
<li><p>Model reading: Export MindIR model via MindSpore or get MindIR model by <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/converter_tool.html">model conversion tool</a>.</p></li>
<li><p>Create configuration context: Create a configuration context <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mscontext.html#mscontext">MSContext</a> and save some basic configuration parameters used to guide model compilation and model execution, including device type, number of threads, CPU pinning, and enabling fp16 mixed precision inference.</p></li>
<li><p>Model creation, loading and compilation: Before executing inference, you need to call <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#build">build</a> interface of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#model">Model</a> for model loading and model compilation. Both loading files and MappedByteBuffer are currently supported. The model loading phase parses the file or buffer into a runtime model.</p></li>
<li><p>Input data: The model needs to be padded with data from the input Tensor before execution.</p></li>
<li><p>Execute inference: Use <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#predict">predict</a> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#model">Model</a> method for model inference.</p></li>
<li><p>Obtain the output: After the graph execution, the inference result can be obtained by outputting the Tensor.</p></li>
<li><p>Release memory: When there is no need to use MindSpore Lite inference framework, you need to release the created <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#model">Model</a>.</p></li>
</ol>
<p><img alt="img" src="../../_images/lite_runtime.png" /></p>
</section>
<section id="reference-to-mindspore-lite-java-library">
<h2>Reference to MindSpore Lite Java Library<a class="headerlink" href="#reference-to-mindspore-lite-java-library" title="Permalink to this headline"></a></h2>
<section id="linux-project-references-to-jar-library">
<h3>Linux Project References to JAR Library<a class="headerlink" href="#linux-project-references-to-jar-library" title="Permalink to this headline"></a></h3>
<p>When using <code class="docutils literal notranslate"><span class="pre">Maven</span></code> as a build tool, you can copy <code class="docutils literal notranslate"><span class="pre">mindspore-lite-java.jar</span></code> to the <code class="docutils literal notranslate"><span class="pre">lib</span></code> directory in the root directory and add the jar package dependencies in <code class="docutils literal notranslate"><span class="pre">pom.xml</span></code>.</p>
<div class="highlight-xml notranslate"><div class="highlight"><pre><span></span><span class="nt">&lt;dependencies&gt;</span>
<span class="w">    </span><span class="nt">&lt;dependency&gt;</span>
<span class="w">        </span><span class="nt">&lt;groupId&gt;</span>com.mindspore.lite<span class="nt">&lt;/groupId&gt;</span>
<span class="w">        </span><span class="nt">&lt;artifactId&gt;</span>mindspore-lite-java<span class="nt">&lt;/artifactId&gt;</span>
<span class="w">        </span><span class="nt">&lt;version&gt;</span>1.0<span class="nt">&lt;/version&gt;</span>
<span class="w">        </span><span class="nt">&lt;scope&gt;</span>system<span class="nt">&lt;/scope&gt;</span>
<span class="w">        </span><span class="nt">&lt;systemPath&gt;</span>${project.basedir}/lib/mindspore-lite-java.jar<span class="nt">&lt;/systemPath&gt;</span>
<span class="w">    </span><span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;/dependencies&gt;</span>
</pre></div>
</div>
</section>
</section>
<section id="model-path">
<h2>Model Path<a class="headerlink" href="#model-path" title="Permalink to this headline"></a></h2>
<p>To perform model inference with MindSpore Lite, you need to get the path of the <code class="docutils literal notranslate"><span class="pre">.mindir</span></code> model file in the file system converted by <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/converter_tool.html">Model Conversion Tool</a>.</p>
</section>
<section id="creating-configuration-context">
<h2>Creating Configuration Context<a class="headerlink" href="#creating-configuration-context" title="Permalink to this headline"></a></h2>
<p>Create a configuration context <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mscontext.html#mscontext">MSContext</a> and save some basic configuration parameters required for the session, which is used to guide graph compilation and graph execution. Configure the number of threads, thread affinity and whether to enable heterogeneous parallel inference via the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mscontext.html#init">init</a> interface. MindSpore Lite has a built-in thread pool shared by processes. The maximum number of threads in the pool is specified by <code class="docutils literal notranslate"><span class="pre">threadNum</span></code> when inference, and the default is 2 threads.</p>
<p>The backend of MindSpore Lite inference can call <code class="docutils literal notranslate"><span class="pre">deviceType</span></code> in the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mscontext.html#adddeviceinfo">AddDeviceInfo</a> interface to specify, currently supporting CPU, GPU and Ascend. When graph compilation is performed, the operator selection is scheduled based on the main selection backend. If the backend supports float16, float16 operator can be used in preference by setting <code class="docutils literal notranslate"><span class="pre">isEnableFloat16</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>.</p>
<section id="configuring-to-use-the-cpu-backend">
<h3>Configuring to Use the CPU Backend<a class="headerlink" href="#configuring-to-use-the-cpu-backend" title="Permalink to this headline"></a></h3>
<p>When the backend to be executed is CPU, <code class="docutils literal notranslate"><span class="pre">MSContext</span></code> needs to be initialized in <code class="docutils literal notranslate"><span class="pre">DeviceType.DT_CPU</span></code> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mscontext.html#adddeviceinfo">addDeviceInfo</a>, while the CPU supports setting the CPU pinning mode and whether to use float16 operator in preference.</p>
<p>The following demonstrates how to create a CPU backend, set the number of threads to 2, set the CPU pinning mode to large core priority and enable float16 inference, and turn off parallelism:</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSContext</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MSContext</span><span class="p">();</span>
<span class="n">context</span><span class="p">.</span><span class="na">init</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">CpuBindMode</span><span class="p">.</span><span class="na">HIGHER_CPU</span><span class="p">);</span>
<span class="n">context</span><span class="p">.</span><span class="na">addDeviceInfo</span><span class="p">(</span><span class="n">DeviceType</span><span class="p">.</span><span class="na">DT_CPU</span><span class="p">,</span><span class="w"> </span><span class="kc">true</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="configuring-to-use-the-gpu-backend">
<h3>Configuring to Use the GPU Backend<a class="headerlink" href="#configuring-to-use-the-gpu-backend" title="Permalink to this headline"></a></h3>
<p>When the backend to be executed is GPU, after <code class="docutils literal notranslate"><span class="pre">MSContext</span></code> is created, you need to add <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/generate/classmindspore_GPUDeviceInfo.html#class-gpudeviceinfo">GPUDeviceInfo</a> in the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mscontext.html#adddeviceinfo">addDeviceInfo</a>. If float16 inference is enabled, the GPU will use the float16 operator in preference.</p>
<p>The following code demonstrates how to create a GPU inference backend:</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSContext</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MSContext</span><span class="p">();</span>
<span class="n">context</span><span class="p">.</span><span class="na">init</span><span class="p">();</span>
<span class="n">context</span><span class="p">.</span><span class="na">addDeviceInfo</span><span class="p">(</span><span class="n">DeviceType</span><span class="p">.</span><span class="na">DT_GPU</span><span class="p">,</span><span class="w"> </span><span class="kc">true</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="configuring-to-use-the-ascend-backend">
<h3>Configuring to Use the Ascend Backend<a class="headerlink" href="#configuring-to-use-the-ascend-backend" title="Permalink to this headline"></a></h3>
<p>When the backend to be executed is Ascend, <code class="docutils literal notranslate"><span class="pre">MSContext</span></code> needs to be initialized <code class="docutils literal notranslate"><span class="pre">DeviceType.DT_ASCEND</span></code> in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mscontext.html#adddeviceinfo">addDeviceInfo</a>.</p>
<p>The following demonstrates how to create an Ascend backend:</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSContext</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MSContext</span><span class="p">();</span>
<span class="n">context</span><span class="p">.</span><span class="na">init</span><span class="p">();</span>
<span class="n">context</span><span class="p">.</span><span class="na">addDeviceInfo</span><span class="p">(</span><span class="n">DeviceType</span><span class="p">.</span><span class="na">DT_ASCEND</span><span class="p">,</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
</section>
</section>
<section id="model-creation,-loading-and-compilation">
<h2>Model Creation, Loading and Compilation<a class="headerlink" href="#model-creation,-loading-and-compilation" title="Permalink to this headline"></a></h2>
<p>When using MindSpore Lite to perform inference, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#model">Model</a> is the main entry for inference. Model loading, compilation and execution are implemented through Model. Using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mscontext.html#init">MSContext</a> created in the previous step, call the compound <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#build">build</a> interface of Model to implement model loading and model compilation.</p>
<p>The following demonstrates the process of Model creation, loading and compilation:</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">Model</span><span class="p">();</span>
<span class="kt">boolean</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">build</span><span class="p">(</span><span class="n">filePath</span><span class="p">,</span><span class="w"> </span><span class="n">ModelType</span><span class="p">.</span><span class="na">MT_MINDIR</span><span class="p">,</span><span class="w"> </span><span class="n">msContext</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="inputting-the-data">
<h2>Inputting the Data<a class="headerlink" href="#inputting-the-data" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite Java interface provides <code class="docutils literal notranslate"><span class="pre">getInputsByTensorName</span></code> and <code class="docutils literal notranslate"><span class="pre">getInputs</span></code> methods to get the input Tensor, and supports <code class="docutils literal notranslate"><span class="pre">byte[]</span></code> or <code class="docutils literal notranslate"><span class="pre">ByteBuffer</span></code> types of data, set the input Tensor data by <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mstensor.html#setdata">setData</a>.</p>
<ol class="arabic">
<li><p>Use <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#getinputsbytensorname">getInputsByTensorName</a> method. Obtain the Tensor connected to the input node in the model input Tensor based on the name of the model input Tensor. The following demonstrates how to call <code class="docutils literal notranslate"><span class="pre">getInputsByTensorName</span></code> to get the input Tensor and pad the data.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSTensor</span><span class="w"> </span><span class="n">inputTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getInputsByTensorName</span><span class="p">(</span><span class="s">&quot;2031_2030_1_construct_wrapper:x&quot;</span><span class="p">);</span>
<span class="c1">// Set Input Data.</span>
<span class="n">inputTensor</span><span class="p">.</span><span class="na">setData</span><span class="p">(</span><span class="n">inputData</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Use <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#getinputs">getInputs</a> method and obtain all the model input Tensor vectors directly. The following demonstrates how to call <code class="docutils literal notranslate"><span class="pre">getInputs</span></code> to get the input Tensor and pad the data.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">List</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getInputs</span><span class="p">();</span>
<span class="n">MSTensor</span><span class="w"> </span><span class="n">inputTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="p">.</span><span class="na">get</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// Set Input Data.</span>
<span class="n">inputTensor</span><span class="p">.</span><span class="na">setData</span><span class="p">(</span><span class="n">inputData</span><span class="p">);</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="executing-the-inference">
<h2>Executing the Inference<a class="headerlink" href="#executing-the-inference" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite can call <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#predict">predict</a> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#model">Model</a> to execute model inference after the model is compiled.</p>
<p>The following sample code demonstrates calling <code class="docutils literal notranslate"><span class="pre">predict</span></code> to perform inference.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="c1">// Run graph to infer results.</span>
<span class="kt">boolean</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">predict</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="obtaining-the-output">
<h2>Obtaining the Output<a class="headerlink" href="#obtaining-the-output" title="Permalink to this headline"></a></h2>
<p>MindSpore Lite can get the inference result by outputting Tensor after performing inference. MindSpore Lite provides three methods to get the output of the model <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mstensor.html">MSTensor</a>, and also supports <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mstensor.html#getbytedata">getByteData</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mstensor.html#getfloatdata">getFloatData</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mstensor.html#getintdata">getIntData</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mstensor.html#getlongdata">getLongData</a> four methods to get the output data.</p>
<ol class="arabic">
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#getoutputs">getOutputs</a> method, get all the model to output list of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mstensor.html#mstensor">MSTensor</a>. The following demonstrates how to call <code class="docutils literal notranslate"><span class="pre">getOutputs</span></code> to get the list of output Tensor.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">List</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outTensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getOutputs</span><span class="p">();</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#getoutputsbynodename">getOutputsByNodeName</a> method and get the vector of the Tensor connected to that node in the model output <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mstensor.html#mstensor">MSTensor</a> according to the name of model output node. The following demonstrates how to call <code class="docutils literal notranslate"> <span class="pre">getOutputByTensorName</span></code> to get the output Tensor.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSTensor</span><span class="w"> </span><span class="n">outTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getOutputsByNodeName</span><span class="p">(</span><span class="s">&quot;Default/head-MobileNetV2Head/Softmax-op204&quot;</span><span class="p">);</span>
<span class="c1">// Apply infer results.</span>
<span class="p">...</span>
</pre></div>
</div>
</li>
<li><p>Use the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#getoutputbytensorname">getOutputByTensorName</a> method to get the corresponding model output <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/mstensor.html#mstensor">MSTensor</a> based on the name of the model output Tensor. The following demonstrates how to call <code class="docutils literal notranslate"><span class="pre">getOutputByTensorName</span></code> to get the output Tensor.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">MSTensor</span><span class="w"> </span><span class="n">outTensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getOutputByTensorName</span><span class="p">(</span><span class="s">&quot;Default/head-MobileNetV2Head/Softmax-op204&quot;</span><span class="p">);</span>
<span class="c1">// Apply infer results.</span>
<span class="p">...</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="releasing-the-memory">
<h2>Releasing the Memory<a class="headerlink" href="#releasing-the-memory" title="Permalink to this headline"></a></h2>
<p>When there is no need to use the MindSpore Lite inference framework, it is necessary to free the created models. The following demonstrates how to do the memory release before the end of the program.</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">.</span><span class="na">free</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline"></a></h2>
<section id="input-dimension-resize">
<h3>Input Dimension Resize<a class="headerlink" href="#input-dimension-resize" title="Permalink to this headline"></a></h3>
<p>When using MindSpore Lite for inference, if you need to Resize the input shape, you can call the<a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#resize">Resize</a> of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html">Model</a> to reset the shape of the input Tensor after the model compiles <code class="docutils literal notranslate"><span class="pre">build</span></code>.</p>
<blockquote>
<div><p>Some networks do not support variable dimensions and will exit abnormally after prompting an error message. For example, when there is a MatMul operator in the model and one input Tensor of MatMul is the weight and the other input Tensor is the input, calling the variable dimension interface will cause the Shape of the input Tensor and the weight Tensor to mismatch, which eventually fails the inference.</p>
</div></blockquote>
<p>The following demonstrates how to <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html#resize">Resize</a> the input Tensor of MindSpore Lite:</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="n">List</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">getInputs</span><span class="p">();</span>
<span class="kt">int</span><span class="o">[][]</span><span class="w"> </span><span class="n">dims</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">300</span><span class="p">,</span><span class="w"> </span><span class="mi">300</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">}};</span>
<span class="n">bool</span><span class="w"> </span><span class="n">ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="na">resize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">dims</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="viewing-the-logs">
<h3>Viewing the Logs<a class="headerlink" href="#viewing-the-logs" title="Permalink to this headline"></a></h3>
<p>When an exception occurs in inference, the problem can be located by viewing log information.</p>
</section>
<section id="obtaining-the-version-number">
<h3>Obtaining the Version Number<a class="headerlink" href="#obtaining-the-version-number" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite provides the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/api_java/model.html">Version</a> method to get the version number, which is included in the <code class="docutils literal notranslate"><span class="pre">com.mindspore.lite.config.Version</span></code> header file. This method is called to get the current version number of MindSpore Lite.</p>
<p>The following demonstrates how to get the version number of MindSpore Lite:</p>
<div class="highlight-java notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">com.mindspore.lite.config.Version</span><span class="p">;</span>
<span class="n">String</span><span class="w"> </span><span class="n">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Version</span><span class="p">.</span><span class="na">version</span><span class="p">();</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_cpp.html" class="btn btn-neutral float-left" title="Using C++ Interface to Perform Cloud-side Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="runtime_python.html" class="btn btn-neutral float-right" title="Using Python Interface to Perform Cloud-side Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>