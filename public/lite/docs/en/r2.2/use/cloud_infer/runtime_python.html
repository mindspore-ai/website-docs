<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Python Interface to Perform Cloud-side Inference &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script>
      <link rel="stylesheet" href="../../_static/css/bootstrap.min.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/lite.css" type="text/css" /><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Performing Concurrent Inference" href="runtime_parallel.html" />
    <link rel="prev" title="Using Java Interface to Perform Cloud-side Inference" href="runtime_java.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="runtime.html">Performing Inference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="runtime_cpp.html">Using C++ Interface to Perform Cloud-side Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="runtime_java.html">Using Java Interface to Perform Cloud-side Inference</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using Python Interface to Perform Cloud-side Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#one-click-installation">One-click Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#executing-demo">Executing Demo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#demo-content-description">Demo Content Description</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#creating-configuration-context">Creating Configuration Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-loading-and-compilation">Model Loading and Compilation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inputting-the-data">Inputting the Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#executing-inference">Executing Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#obtaining-the-output">Obtaining the Output</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime.html">Performing Inference</a> &raquo;</li>
      <li>Using Python Interface to Perform Cloud-side Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/use/cloud_infer/runtime_python.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-python-interface-to-perform-cloud-side-inference">
<h1>Using Python Interface to Perform Cloud-side Inference<a class="headerlink" href="#using-python-interface-to-perform-cloud-side-inference" title="Permalink to this headline">ÔÉÅ</a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.2/docs/lite/docs/source_en/use/cloud_infer/runtime_python.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.2/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>This tutorial provides a sample program for MindSpore Lite to perform cloud-side inference, demonstrating the <a class="reference external" href="https://mindspore.cn/lite/api/en/r2.2/mindspore_lite.html">Python interface</a> to perform the basic process of cloud-side inference through file input, inference execution, and inference result printing, and enables users to quickly understand the use of MindSpore Lite APIs related to cloud-side inference execution. The related files are put in the directory <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.2/mindspore/lite/examples/cloud_infer/quick_start_python">mindspore/lite/examples/cloud_infer/quick_start_python</a>.</p>
<p>MindSpore Lite cloud-side inference is supported to run in Linux environment deployment only. Ascend 310/310P/910, Nvidia GPU and CPU hardware backends are supported.</p>
<p>The following is an example of how to use the Python Cloud-side Inference Demo on a Linux X86 operating system and a CPU hardware platform, using Ubuntu 18.04 as an example:</p>
<ul class="simple">
<li><p>One-click installation of inference-related model files, MindSpore Lite and its required dependencies. See the <a class="reference internal" href="#one-click-installation"><span class="std std-doc">One-click installation</span></a> section for details.</p></li>
<li><p>Execute the Python Cloud-side Inference Demo. See the <a class="reference internal" href="#executing-demo"><span class="std std-doc">Execute Demo</span></a> section for details.</p></li>
<li><p>For a description of the Python Cloud-side Inference Demo content, see the <a class="reference internal" href="#demo-content-description"><span class="std std-doc">Demo Content Description</span></a> section for details.</p></li>
</ul>
</section>
<section id="one-click-installation">
<h2>One-click Installation<a class="headerlink" href="#one-click-installation" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>This session introduces the installation of MindSpore Lite for Python version 3.7 via pip on a Linux-x86_64 system with a CPU environment, taking the new Ubuntu 18.04 as an example.</p>
<p>Go to the <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.2/mindspore/lite/examples/cloud_infer/quick_start_python">mindspore/lite/examples/cloud_infer/quick_start_python</a> directory, and execute the <code class="docutils literal notranslate"><span class="pre">lite-cpu-pip.sh</span></code> script for a one-click installation, taking installation of MindSpore Lite version 2.0.0 as an example. Script installation needs to download the model required for inference and input data files, the dependencies required for MindSpore_Lite installation, and download and install MindSpore Lite.</p>
<p>Note: This command sets the installed version of MindSpore Lite. Since the cloud-side inference Python interface is supported from MindSpore Lite version 2.0.0, the version cannot be set lower than 2.0.0. See the version provided in <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/downloads.html">Download MindSpore Lite</a> for details on the versions that can be set.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MINDSPORE_LITE_VERSION</span><span class="o">=</span><span class="m">2</span>.0.0<span class="w"> </span>bash<span class="w"> </span>./lite-cpu-pip.sh
</pre></div>
</div>
<blockquote>
<div><p>If the MobileNetV2 model download fails, please manually download the relevant model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.mindir">mobilenetv2.mindir</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/quick_start_python/model</span></code> directory.</p>
<p>If the input.bin input data file download fails, please manually download the relevant input data file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/input.bin">input.bin</a> and copy it to the <code class="docutils literal notranslate"> <span class="pre">mindspore/lite/examples/cloud_infer/quick_start_python/model</span></code> directory.</p>
<p>If MindSpore Lite inference framework by using the script download fails, please manually download <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/downloads.html">MindSpore Lite model cloud-side inference framework</a> corresponding to the hardware platform of CPU and operating system of Linux-x86_64 or Linux-aarch64. Users can use the <code class="docutils literal notranslate"><span class="pre">uname</span> <span class="pre">-m</span></code> command to query the operating system in the terminal, and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/quick_start_python</span></code> directory.</p>
<p>If you need to use MindSpore Lite corresponding to Python 3.7 or above, please <a class="reference external" href="https://mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/build.html">compile</a> locally. Note that the Python API module compilation depends on Python &gt;= 3.7.0, NumPy &gt;= 1.17.0, wheel &gt;= 0.32.0. After successful compilation, copy the Whl installation package generated in the <code class="docutils literal notranslate"><span class="pre">output/</span></code> directory to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/quick_start_python</span></code> directory.</p>
<p>If the MindSpore Lite installation package does not exist in the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/quick_start_python</span></code> directory, the one-click installation script will uninstall the currently installed MindSpore Lite and then download and install MindSpore Lite from the Huawei image. Otherwise, if the MindSpore Lite installation package exists in the directory, it will be installed first.</p>
<p>After manually downloading and placing the files in the specified location, you need to execute the lite-cpu-pip.sh script again to complete the one-click installation.</p>
</div></blockquote>
<p>A successful execution will show the following results. The model files and input data files can be found in the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/quick_start_python/model</span></code> directory.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Successfully installed mindspore-lite-2.0.0
</pre></div>
</div>
</section>
<section id="executing-demo">
<h2>Executing Demo<a class="headerlink" href="#executing-demo" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>After one-click installation, go to the <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.2/mindspore/lite/examples/cloud_infer/quick_start_python">mindspore/lite/examples/cloud_infer/quick_start_python</a> directory and execute the following command to experience MindSpore Lite inference MobileNetV2 models.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>quick_start_cloud_infer_python.py
</pre></div>
</div>
<p>When the execution is completed, the following results will be obtained, printing the name of the output Tensor, the data size of the output Tensor, the number of elements of the output Tensor and the first 50 pieces of data.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tensor&#39;s name is:shape1 data size is:4000 tensor elements num is:1000
output data is: 5.3937547e-05 0.00037763786 0.00034193686 0.00037316754 0.00022436169 9.953917e-05 0.00025308868 0.00032044895 0.00025788433 0.00018915901 0.00079509866 0.003382262 0.0016214572 0.0010760546 0.0023826156 0.0011769629 0.00088481285 0.000534926 0.0006929171 0.0010826243 0.0005747609 0.0014443205 0.0010454883 0.0016276307 0.00034437355 0.0001039985 0.00022641376 0.00035307938 0.00014567627 0.00051178376 0.00016933997 0.00075814105 9.704676e-05 0.00066705025 0.00087511574 0.00034623547 0.00026317223 0.000319407 0.0015627446 0.0004044049 0.0008798965 0.0005202293 0.00044808138 0.0006453716 0.00044969268 0.0003431648 0.0009871059 0.00020436312 7.405098e-05 8.805057e-05
</pre></div>
</div>
</section>
<section id="demo-content-description">
<h2>Demo Content Description<a class="headerlink" href="#demo-content-description" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Running MindSpore Lite inference framework mainly consists of the following steps:</p>
<ol class="arabic simple">
<li><p>Model reading: Export MindIR model via MindSpore or get MindIR model by <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.2/use/cloud_infer/converter_tool.html">model conversion tool</a>.</p></li>
<li><p>Create configuration context: Create a configuration context <a class="reference external" href="https://mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Context.html#mindspore_lite.Context">Context</a> and save some basic configuration parameters used to guide model compilation and model execution.</p></li>
<li><p>Model creation and compilation: Before executing inference, you need to call <a class="reference external" href="https://mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.build_from_file">build_from_file</a> interface of <a class="reference external" href="https://mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model">Model</a> for model loading and model compilation. The model loading phase parses the file cache into a runtime model. The model compilation phase can take more time, so it is recommended that the model be created once, compiled once and performed inference about multiple times.</p></li>
<li><p>Input data: The input data needs to be padded before the model execution.</p></li>
<li><p>Execute inference: Use <a class="reference external" href="https://mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.predict">Predict</a> of <a class="reference external" href="https://mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model">Model</a> method for model inference.</p></li>
</ol>
<p>For more advanced usage and examples of Python interfaces, please refer to the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite.html">Python API</a>.</p>
<p><img alt="img" src="../../_images/lite_runtime.png" /></p>
<section id="creating-configuration-context">
<h3>Creating Configuration Context<a class="headerlink" href="#creating-configuration-context" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Create the configuration context <code class="docutils literal notranslate"><span class="pre">Context</span></code>. Since this tutorial demonstrates a scenario where inference is performed on a CPU device, they need to set Context‚Äôs target to cpu.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>

<span class="c1"># init context, and set target is cpu</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">]</span>
<span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_num</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_affinity_mode</span><span class="o">=</span><span class="mi">2</span>
</pre></div>
</div>
<p>If the user needs to run inference on Ascend device, they need to set Context‚Äôs target to ascend.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>

<span class="c1"># init context, and set target is ascend.</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ascend&quot;</span><span class="p">]</span>
<span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_num</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_affinity_mode</span><span class="o">=</span><span class="mi">2</span>
</pre></div>
</div>
<p>If the backend is Ascend deployed on the Elastic Cloud Server, set the <code class="docutils literal notranslate"><span class="pre">provider</span></code> to <code class="docutils literal notranslate"><span class="pre">ge</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">provider</span> <span class="o">=</span> <span class="s2">&quot;ge&quot;</span>
</pre></div>
</div>
<p>If the user needs to run inference on a GPU device, they need to set Context‚Äôs target to gpu.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>

<span class="c1"># init context, and set target is gpu.</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gpu&quot;</span><span class="p">]</span>
<span class="n">context</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_num</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">context</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">thread_affinity_mode</span><span class="o">=</span><span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="model-loading-and-compilation">
<h3>Model Loading and Compilation<a class="headerlink" href="#model-loading-and-compilation" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Model loading and compilation can be done by calling <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.build_from_file">build_from_file</a> interface of <code class="docutils literal notranslate"><span class="pre">Model</span></code> to load and compile the runtime model directly from the file cache.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># build model from file</span>
<span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="s2">&quot;./model/mobilenetv2.mindir&quot;</span>
<span class="n">IN_DATA_PATH</span> <span class="o">=</span> <span class="s2">&quot;./model/input.bin&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">MODEL_PATH</span><span class="p">,</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="inputting-the-data">
<h3>Inputting the Data<a class="headerlink" href="#inputting-the-data" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The way that this tutorial sets the input data is importing from a file. For other ways to set the input data, please refer to <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.predict">predict</a> interface of <code class="docutils literal notranslate"><span class="pre">Model</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># set model input</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()</span>
<span class="n">in_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromfile</span><span class="p">(</span><span class="n">IN_DATA_PATH</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">in_data</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="executing-inference">
<h3>Executing Inference<a class="headerlink" href="#executing-inference" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Call <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.2/mindspore_lite/mindspore_lite.Model.html#mindspore_lite.Model.predict">predict</a> interface of <code class="docutils literal notranslate"><span class="pre">Model</span></code> to perform inference, and the inference result is output to <code class="docutils literal notranslate"><span class="pre">output</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># execute inference</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="obtaining-the-output">
<h3>Obtaining the Output<a class="headerlink" href="#obtaining-the-output" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Print the output results after performing inference. Iterate through the <code class="docutils literal notranslate"><span class="pre">outputs</span></code> list and print the name, data size, number of elements, and the first 50 data for each output Tensor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># get output</span>
<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
  <span class="n">name</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span>
  <span class="n">data_size</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">data_size</span>
  <span class="n">element_num</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">element_num</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tensor&#39;s name is:</span><span class="si">%s</span><span class="s2"> data size is:</span><span class="si">%s</span><span class="s2"> tensor elements num is:</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">data_size</span><span class="p">,</span> <span class="n">element_num</span><span class="p">))</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">get_data_to_numpy</span><span class="p">()</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;output data is:&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime_java.html" class="btn btn-neutral float-left" title="Using Java Interface to Perform Cloud-side Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="runtime_parallel.html" class="btn btn-neutral float-right" title="Performing Concurrent Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, MindSpore Lite.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>