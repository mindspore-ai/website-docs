<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using C++ Interface to Perform Cloud-side Inference &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script><script src="../../_static/jquery.js"></script>
        <script src="../../_static/js/theme.js"></script><script src="../../_static/underscore.js"></script><script src="../../_static/doctools.js"></script><script src="../../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Using Java Interface to Perform Cloud-side Inference" href="runtime_java.html" />
    <link rel="prev" title="Performing Inference" href="runtime.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction.html">Quick Start to Device-side Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quick_start/one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="runtime.html">Performing Inference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Using C++ Interface to Perform Cloud-side Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#preparation">Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#creating-configuration-context">Creating Configuration Context</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#configuring-to-use-the-cpu-backend">Configuring to Use the CPU Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-using-gpu-backend">Configuring Using GPU Backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-using-ascend-backend">Configuring Using Ascend Backend</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-creation-loading-and-compilation">Model Creation Loading and Compilation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inputting-the-data">Inputting the Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#executing-inference">Executing Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compilation-and-execution">Compilation and Execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#advanced-usage">Advanced Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dynamic-shape-input">Dynamic Shape Input</a></li>
<li class="toctree-l4"><a class="reference internal" href="#specifying-input-and-output-host-memory">Specifying Input and Output Host Memory</a></li>
<li class="toctree-l4"><a class="reference internal" href="#specifying-the-memory-of-the-input-and-output-devices">Specifying the Memory of the Input and Output Devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ascend-backend-ge-inference">Ascend Backend GE Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#loading-models-through-multiple-threads">Loading Models through Multiple Threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multiple-models-sharing-weights">Multiple Models Sharing Weights</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#experimental-feature">Experimental feature</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multi-backend-runtime">multi-backend runtime</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="runtime_java.html">Using Java Interface to Perform Cloud-side Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="runtime_python.html">Using Python Interface to Perform Cloud-side Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../log.html">Log</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="runtime.html">Performing Inference</a> &raquo;</li>
      <li>Using C++ Interface to Perform Cloud-side Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/use/cloud_infer/runtime_cpp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="using-c++-interface-to-perform-cloud-side-inference">
<h1>Using C++ Interface to Perform Cloud-side Inference<a class="headerlink" href="#using-c++-interface-to-perform-cloud-side-inference" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3.q1/docs/lite/docs/source_en/use/cloud_infer/runtime_cpp.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3.q1/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This tutorial describes how to perform cloud-side inference with MindSpore Lite by using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/index.html">C++ interface</a>.</p>
<p>MindSpore Lite cloud-side inference is supported to run in Linux environment deployment only. Atlas 200/300/500 inference product, Atlas inference series (with Ascend 310P AI processor), Atlas training series, Nvidia GPU and CPU hardware backends are supported.</p>
<p>To experience the MindSpore Lite device-side inference process, please refer to the document <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3.0rc1/use/runtime_cpp.html">Using C++ Interface to Perform Cloud-side Inference</a>.</p>
<p>Using the MindSpore Lite inference framework consists of the following main steps:</p>
<ol class="arabic simple">
<li><p>Model reading: Export MindIR model via MindSpore or get MindIR model by <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3.0rc1/use/cloud_infer/converter_tool.html">model conversion tool</a>.</p></li>
<li><p>Create a Configuration Context: Create a configuration context <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Context.html">Context</a> and save some basic configuration parameters used to guide model compilation and model execution.</p></li>
<li><p>Model loading and compilation: Before executing inference, you need to call Build interface of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Model.html#class-model">Model</a> for model loading and model compilation. The model loading phase parses the file cache into a runtime model. The model compilation phase can take more time so it is recommended that the model be created once, compiled once and perform inference about multiple times.</p></li>
<li><p>Input data: The input data needs to be padded before the model can be executed.</p></li>
<li><p>Execute inference: Use Predict of <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Model.html#class-model">Model</a> for model inference.</p></li>
</ol>
<p><img alt="img" src="../../_images/lite_runtime.png" /></p>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>The following code samples are from <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.3.q1/mindspore/lite/examples/cloud_infer/runtime_cpp">using C++ interface to perform cloud-side inference sample code</a>.</p></li>
<li><p>Export the MindIR model via MindSpore, or get the MindIR model by converting it with <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3.0rc1/use/cloud_infer/converter_tool.html">model conversion tool</a> and copy it to the <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/runtime_cpp/model</span></code> directory. You can download the MobileNetV2 model file <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.mindir">mobilenetv2.mindir</a>.</p></li>
<li><p>Download the Ascend, Nvidia GPU, CPU triplet MindSpore Lite cloud-side inference package <code class="docutils literal notranslate"><span class="pre">mindspore-</span> <span class="pre">lite-{version}-linux-{arch}.tar.gz</span></code> in the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3.0rc1/use/downloads.html">official website</a> and save it to <code class="docutils literal notranslate"><span class="pre">mindspore/lite/examples/cloud_infer/runtime_cpp</span></code> directory.</p></li>
</ol>
</section>
<section id="creating-configuration-context">
<h2>Creating Configuration Context<a class="headerlink" href="#creating-configuration-context" title="Permalink to this headline"></a></h2>
<p>The context will save some basic configuration parameters used to guide model compilation and model execution.</p>
<p>The following sample code demonstrates how to create a Context.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
</pre></div>
</div>
<p>Return a reference to the list of backend information for specifying the running device via <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Context.html">MutableDeviceInfo</a>. User-set device information is supported in <code class="docutils literal notranslate"><span class="pre">MutableDeviceInfo</span></code>, including <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_CPUDeviceInfo.html">CPUDeviceInfo</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_GPUDeviceInfo.html">GPUDeviceInfo</a>, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_AscendDeviceInfo.html">AscendDeviceInfo</a>. The number of devices set can only be one of them currently.</p>
<section id="configuring-to-use-the-cpu-backend">
<h3>Configuring to Use the CPU Backend<a class="headerlink" href="#configuring-to-use-the-cpu-backend" title="Permalink to this headline"></a></h3>
<p>When the backend to be executed is CPU, you need to set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_CPUDeviceInfo.html">CPUDeviceInfo</a> as the inference backend. Enable float16 inference by <code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">cpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// CPU use float16 operator as priority.</span>
<span class="n">cpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">cpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<p>Optionally, you can additionally set the number of threads, thread affinity, parallelism strategy and other features.</p>
<ol class="arabic">
<li><p>Configure the number of threads</p>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Context.html">Context</a> configure the number of threads via <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Context.html">SetThreadNum</a>:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the number of worker threads in the thread pool to 2, including the main thread.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadNum</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Configure thread affinity</p>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Context.html">Context</a> configure threads and CPU binding via <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Context.html">SetThreadAffinity</a>.
Set the CPU binding list with the parameter <code class="docutils literal notranslate"><span class="pre">const</span> <span class="pre">std::vector&lt;int&gt;</span> <span class="pre">&amp;core_list</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the thread to be bound to the core list.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetThreadAffinity</span><span class="p">({</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">});</span>
</pre></div>
</div>
</li>
<li><p>Configure parallelism strategy</p>
<p><a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Context.html">Context</a> configure the number of operator parallel inference at runtime via <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Context.html">SetInterOpParallelNum</a>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Configure the inference supports parallel.</span>
<span class="n">context</span><span class="o">-&gt;</span><span class="n">SetInterOpParallelNum</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="configuring-using-gpu-backend">
<h3>Configuring Using GPU Backend<a class="headerlink" href="#configuring-using-gpu-backend" title="Permalink to this headline"></a></h3>
<p>When the backend to be executed is GPU, you need to set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_GPUDeviceInfo.html#class-gpudeviceinfo">GPUDeviceInfo</a> as the inference backend. GPUDeviceInfo sets the device ID by <code class="docutils literal notranslate"><span class="pre">SetDeviceID</span></code> and enables float16 inference by <code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code> or <code class="docutils literal notranslate"><span class="pre">SetPrecisionMode</span></code>.</p>
<p>The following sample code demonstrates how to create a GPU inference backend while the device ID is set to 0:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="k">auto</span><span class="w"> </span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">GPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">gpu_device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New GPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Set NVIDIA device id.</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// GPU use float16 operator as priority.</span>
<span class="n">gpu_device_info</span><span class="o">-&gt;</span><span class="n">SetEnableFP16</span><span class="p">(</span><span class="nb">true</span><span class="p">);</span>
<span class="c1">// The GPU device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">gpu_device_info</span><span class="p">);</span>
</pre></div>
</div>
<p>Whether the <code class="docutils literal notranslate"><span class="pre">SetEnableFP16</span></code> is set successfully depends on the [CUDA computing power] of the current device (https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix).</p>
<p><code class="docutils literal notranslate"><span class="pre">SetPrecisionMode()</span></code> has two parameters to control float16 inference, <code class="docutils literal notranslate"><span class="pre">SetPrecisionMode(&quot;preferred_fp16&quot;)</span></code> equals to <code class="docutils literal notranslate"><span class="pre">SetEnableFP16(true)</span></code>, vice versa.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>SetPrecisionMode()</p></th>
<th class="head"><p>SetEnableFP16()</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>enforce_fp32</p></td>
<td><p>false</p></td>
</tr>
<tr class="row-odd"><td><p>preferred_fp16</p></td>
<td><p>true</p></td>
</tr>
</tbody>
</table>
</section>
<section id="configuring-using-ascend-backend">
<h3>Configuring Using Ascend Backend<a class="headerlink" href="#configuring-using-ascend-backend" title="Permalink to this headline"></a></h3>
<p>When the backend to be executed is Ascend (Atlas 200/300/500 inference product, Atlas inference series (with Ascend 310P AI processor), or Atlas training series are currently supported), you need to set <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_AscendDeviceInfo.html#class-ascenddeviceinfo">AscendDeviceInfo</a> as the inference backend. AscendDeviceInfo sets the device ID by <code class="docutils literal notranslate"><span class="pre">SetDeviceID</span></code>. Ascend enables float16 precision by default, and the precision mode can be changed by <code class="docutils literal notranslate"><span class="pre">AscendDeviceInfo.SetPrecisionMode</span></code>.</p>
<p>The following sample code demonstrates how to create Ascend inference backend while the device ID is set to 0:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>

<span class="c1">// for Atlas 200/300/500 inference product, Atlas inference series (with Ascend 310P AI processor), and Atlas training series</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">AscendDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New AscendDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Set Atlas 200/300/500 inference product, Atlas inference series (with Ascend 310P AI processor), and Atlas training series device id.</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span>
<span class="c1">// The Ascend device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>
</pre></div>
</div>
<p>If the backend is Ascend deployed on the Elastic Cloud Server, use the <code class="docutils literal notranslate"><span class="pre">SetProvider</span></code> to set the provider to <code class="docutils literal notranslate"><span class="pre">ge</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Set the provider to ge.</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetProvider</span><span class="p">(</span><span class="s">&quot;ge&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p>The user can configure the precision mode by calling the <code class="docutils literal notranslate"><span class="pre">SetPrecisionMode()</span></code> interface, and the usage scenarios are shown in the following table:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>user configure precision mode param</p></th>
<th class="head"><p>ACL obtain precision mode param</p></th>
<th class="head"><p>ACL scenario description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>enforce_fp32</p></td>
<td><p>force_fp32</p></td>
<td><p>force to use fp32</p></td>
</tr>
<tr class="row-odd"><td><p>preferred_fp32</p></td>
<td><p>allow_fp32_to_fp16</p></td>
<td><p>prefer to use fp32</p></td>
</tr>
<tr class="row-even"><td><p>enforce_fp16</p></td>
<td><p>force_fp16</p></td>
<td><p>force to use fp16</p></td>
</tr>
<tr class="row-odd"><td><p>enforce_origin</p></td>
<td><p>must_keep_origin_dtype</p></td>
<td><p>force to use original type</p></td>
</tr>
<tr class="row-even"><td><p>preferred_optimal</p></td>
<td><p>allow_mix_precision</p></td>
<td><p>prefer to use fp16</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="model-creation-loading-and-compilation">
<h2>Model Creation Loading and Compilation<a class="headerlink" href="#model-creation-loading-and-compilation" title="Permalink to this headline"></a></h2>
<p>When using MindSpore Lite to perform inference, <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Model.html#class-model">Model</a> is the main entry point for inference. Model loading, model compilation and model execution is implemented through model. Using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Context.html#class-context">Context</a> created in the previous step, call the compound Build interface of Model to implement model loading and model compilation.</p>
<p>The following sample code demonstrates the process of model creation, loading and compilation:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span><span class="w"> </span><span class="n">BuildModel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_type</span><span class="p">,</span>
<span class="w">                                             </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">device_id</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Create and init context, add CPU device info</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">DeviceInfoContext</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;CPU&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateCPUDeviceInfo</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;GPU&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateGPUDeviceInfo</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_type</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;Ascend&quot;</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreateAscendDeviceInfo</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">device_type</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;DeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Create model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p>For large models, when using the model buffer to load and compile, you need to set the path of the weight file separately, sets the model path through <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Model.html">LoadConfig</a> or <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Model.html">UpdateConfig</a> interface, where <code class="docutils literal notranslate"><span class="pre">section</span></code> is <code class="docutils literal notranslate"><span class="pre">model_</span> <span class="pre">File</span></code> , <code class="docutils literal notranslate"><span class="pre">key</span></code> is <code class="docutils literal notranslate"><span class="pre">mindir_path</span></code>. When using the model path to load and compile, you do not need to set other parameters. The weight parameters will be automatically read.</p>
</div></blockquote>
</section>
<section id="inputting-the-data">
<h2>Inputting the Data<a class="headerlink" href="#inputting-the-data" title="Permalink to this headline"></a></h2>
<p>Before the model execution, the input data needs to be set, using the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Model.html">GetInputs</a> method, which directly gets all vectors of the model input Tensor. You can get the size of the data that the Tensor should fill in by the DataSize method of the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_MSTensor.html">MSTensor</a>. The data type of the Tensor can be obtained by the DataType. The input host data is set by SetData method.</p>
<p>There are currently two ways to specify input data:</p>
<ol class="arabic">
<li><p>By setting the input data via <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_MSTensor.html">SetData</a>, copying between hosts can be avoided and the input data will eventually be copied directly to the inference device.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">SetTensorHostData</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MemBuffer</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">tensors</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">!</span><span class="n">buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Argument tensors or buffers cannot be nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensors</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">buffers</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tensors size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; != &quot;</span>
<span class="w">              </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; buffers size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">buffers</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">)[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">buffers</span><span class="p">)[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">size</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Tensor data size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; != buffer size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// set tensor data, and the memory should be freed by user</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetData</span><span class="p">(</span><span class="n">buffer</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetDeviceData</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// Set the input data of the model, this inference input will be copied directly to the device.</span>
<span class="w">  </span><span class="n">SetTensorHostData</span><span class="p">(</span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input_buffer</span><span class="p">);</span>
</pre></div>
</div>
</li>
<li><p>Copy the input data to the Tensor cache returned by <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_MSTensor.html">MutableData</a>. It should be noted that if the data address has been set by <code class="docutils literal notranslate"><span class="pre">SetData</span></code>, <code class="docutils literal notranslate"><span class="pre">MutableData</span></code> will return the data address of <code class="docutils literal notranslate"><span class="pre">SetData</span></code>, and you need to call <code class="docutils literal notranslate"><span class="pre">SetData(nullptr)</span></code> first.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">CopyTensorHostData</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MemBuffer</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">tensors</span><span class="p">)[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="n">buffers</span><span class="p">)[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">size</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Tensor data size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; != buffer size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">dst_mem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">dst_mem</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Tensor MutableData return nullptr&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">memcpy</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">(),</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">buffer</span><span class="p">.</span><span class="n">size</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// Set the input data of the model, copy data to the tensor buffer of Model.GetInputs.</span>
<span class="w">  </span><span class="n">CopyTensorHostData</span><span class="p">(</span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input_buffer</span><span class="p">);</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="executing-inference">
<h2>Executing Inference<a class="headerlink" href="#executing-inference" title="Permalink to this headline"></a></h2>
<p>The Model.Predict interface is called to perform inference and subsequent processing of the returned output.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">SpecifyInputDataExample</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_type</span><span class="p">,</span><span class="w"> </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">device_id</span><span class="p">,</span>
<span class="w">                            </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">batch_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">BuildModel</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">device_type</span><span class="p">,</span><span class="w"> </span><span class="n">device_id</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Create and build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// InferenceApp is user-defined code. Users need to obtain inputs and process outputs based on</span>
<span class="w">  </span><span class="c1">// the actual situation.</span>
<span class="w">  </span><span class="n">InferenceApp</span><span class="w"> </span><span class="n">app</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// Obtain inputs. The input data for inference may come from the preprocessing result.</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input_buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">app</span><span class="p">.</span><span class="n">GetInferenceInputs</span><span class="p">(</span><span class="n">inputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_buffer</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Set the input data of the model, this inference input will be copied directly to the device.</span>
<span class="w">  </span><span class="n">SetTensorHostData</span><span class="p">(</span><span class="o">&amp;</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input_buffer</span><span class="p">);</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">predict_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Predict error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">predict_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Process outputs.</span>
<span class="w">  </span><span class="n">app</span><span class="p">.</span><span class="n">OnInferenceResult</span><span class="p">(</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="compilation-and-execution">
<h2>Compilation and Execution<a class="headerlink" href="#compilation-and-execution" title="Permalink to this headline"></a></h2>
<p>Set the environment variables as described in the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3.0rc1/quick_start/one_hour_introduction_cloud.html#environment-variables">Environment Variables section in Quilk Start</a>, and then compile the prograom as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>build<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>build
cmake<span class="w"> </span>../
make
</pre></div>
</div>
<p>After successful compilation, you can get the <code class="docutils literal notranslate"><span class="pre">runtime_cpp</span></code> executable in the <code class="docutils literal notranslate"><span class="pre">build</span></code> directory. Execute program <code class="docutils literal notranslate"><span class="pre">runtime_cpp</span></code> to run the sample:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./runtime_cpp<span class="w"> </span>--model_path<span class="o">=</span>../model/mobilenetv2.mindir<span class="w"> </span>--device_type<span class="o">=</span>CPU
</pre></div>
</div>
</section>
<section id="advanced-usage">
<h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline"></a></h2>
<section id="dynamic-shape-input">
<h3>Dynamic Shape Input<a class="headerlink" href="#dynamic-shape-input" title="Permalink to this headline"></a></h3>
<p>Lite cloud-side inference framework supports dynamic shape input for models. GPU and Ascend hardware backend needs to be configured with dynamic input information during model conversion and model inference.</p>
<p>The configuration of dynamic input information is related to offline and online scenarios. For offline scenarios, the model conversion tool parameter <code class="docutils literal notranslate"><span class="pre">--optimize=general</span></code>, <code class="docutils literal notranslate"><span class="pre">--optimize=gpu_oriented</span></code> or <code class="docutils literal notranslate"><span class="pre">--optimize=ascend_oriented</span></code>, i.e. experiencing the hardware-related fusion and optimization. The generated MindIR model can only run on the corresponding hardware backend. For example, in Atlas 200/300/500 inference product environment, if the model conversion tool specifies <code class="docutils literal notranslate"><span class="pre">--optimize=ascend_oriented</span></code>, the generated model will only support running on Atlas 200/300/500 inference product. If <code class="docutils literal notranslate"><span class="pre">--optimize=general</span></code> is specified, running on GPU and CPU is supported. For online scenarios, the loaded MindIR has not experienced hardware-related fusion and optimization, supports running on Ascend, GPU, and CPU. The model conversion tool parameter <code class="docutils literal notranslate"><span class="pre">--optimize=none</span></code>, or the MindSpore-exported MindIR model has not been processed by the conversion tool.</p>
<p>Ascend hardware backend offline scenarios require dynamic input information to be configured during the model conversion phase. Ascend hardware backend online scenarios, as well as GPU hardware backend offline and online scenarios, require dynamic input information to be configured during the model loading phase via the [LoadConfig](https://www.mindspore.cn/lite/api/en/r2.3.0rc1/api_cpp/mindspore.html# loadconfig) interface.</p>
<p>An example configuration file loaded via <code class="docutils literal notranslate"><span class="pre">LoadConfig</span></code> is shown below:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[ascend_context]</span>
<span class="na">input_shape</span><span class="o">=</span><span class="s">input_1:[-1,3,224,224]</span>
<span class="na">dynamic_dims</span><span class="o">=</span><span class="s">[1~4],[8],[16]</span>

<span class="k">[gpu_context]</span>
<span class="na">input_shape</span><span class="o">=</span><span class="s">input_1:[-1,3,224,224]</span>
<span class="na">dynamic_dims</span><span class="o">=</span><span class="s">[1~16]</span>
<span class="na">opt_dims</span><span class="o">=</span><span class="s">[1]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">[ascend_context]</span></code> and <code class="docutils literal notranslate"><span class="pre">[gpu_context]</span></code> act on the Ascend and GPU hardware backends, respectively.</p>
<ol class="arabic simple">
<li><p>Ascend and GPU hardware backends require dynamic input information for graph compilation and optimization, while CPU hardware backends do not require configuration of dynamic dimensional information.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_shape</span></code> is used to indicate the input shape information in the format <code class="docutils literal notranslate"><span class="pre">input_name1:[shape1];input_name2:[shape2]</span></code>. If there are dynamic inputs, the corresponding dimension needs to be set to -1. Multiple inputs are separated by the English semicolon <code class="docutils literal notranslate"><span class="pre">;</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dynamic_dims</span></code> is used to indicate the value range of the dynamic dimension, with multiple non-contiguous ranges of values separated by the comma <code class="docutils literal notranslate"><span class="pre">,</span></code>. In the above example, Ascend batch dimension values range in <code class="docutils literal notranslate"><span class="pre">1,2,3,4,8,16</span></code> and GPU batch dimension values range from 1 to 16. Ascend hardware backend with dynamic inputs are in multi-step mode, the larger the dynamic input range, the longer the model compilation time.</p></li>
<li><p>For the GPU hardware backend, additional configuration of <code class="docutils literal notranslate"><span class="pre">opt_dims</span></code> is required to indicate the optimal value in the <code class="docutils literal notranslate"><span class="pre">dynamic_dims</span></code> range.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> is configured as a static shape, <code class="docutils literal notranslate"><span class="pre">dynamic_dims</span></code> and <code class="docutils literal notranslate"><span class="pre">opt_dims</span></code> do not need to be configured.</p></li>
</ol>
<p>Load the configuration file information via <code class="docutils literal notranslate"><span class="pre">LoadConfig</span></code> before the model <code class="docutils literal notranslate"><span class="pre">Build</span></code>:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Create model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">config_file</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="o">-&gt;</span><span class="n">LoadConfig</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load config file &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">config_file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
<p>In model inference, if the input to the model is dynamic and the input and output shape returned via <code class="docutils literal notranslate"><span class="pre">GetInputs</span></code> and <code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code> may include -1, i.e., it is a dynamic shape,  the input shape needs to be specified via the <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_Model.html">Resize</a> interface. If the input Shape needs to change, for example, the <code class="docutils literal notranslate"><span class="pre">batch</span></code> dimension changes, the <code class="docutils literal notranslate"><span class="pre">Resize</span></code> interface needs to be called again to adjust the input Shape.</p>
<p>After calling the <code class="docutils literal notranslate"><span class="pre">Resize</span></code> interface, the shape of the Tensor in the called and subsequently called <code class="docutils literal notranslate"><span class="pre">GetInputs</span></code> and <code class="docutils literal notranslate"><span class="pre">GetOutputs</span></code> will be changed.</p>
<p>The following sample code demonstrates how to <code class="docutils literal notranslate"><span class="pre">Resize</span></code> the input Tensor of MindSpore Lite:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">ResizeModel</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">batch_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input</span><span class="p">.</span><span class="n">Shape</span><span class="p">();</span>
<span class="w">    </span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">batch_size</span><span class="p">;</span>
<span class="w">    </span><span class="n">new_shapes</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">shape</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Resize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">new_shapes</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to resize to batch size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="specifying-input-and-output-host-memory">
<h3>Specifying Input and Output Host Memory<a class="headerlink" href="#specifying-input-and-output-host-memory" title="Permalink to this headline"></a></h3>
<p>Specifies that device memory supports CPU, Ascend, and GPU hardware backends. The specified input host memory and the data in the cache will be copied directly to the device memory, and the data in the device memory will be copied directly to this cache for the specified output host memory. It avoids additional data copying between hosts and improves inference performance.</p>
<p>Input and output host memory can be specified separately or simultaneously by <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_MSTensor.html">SetData</a>. It is recommended that the parameter <code class="docutils literal notranslate"><span class="pre">own_data</span></code> be false. When <code class="docutils literal notranslate"><span class="pre">own_data</span></code> is false, the user needs to maintain the life cycle of host memory and is responsible for the request and release of host memory. When the parameter <code class="docutils literal notranslate"><span class="pre">own_data</span></code> is true, the specified memory is freed at the MSTensor destruct.</p>
<ol class="arabic">
<li><p>Specify input host memory</p>
<p>The values of input host memory are generally derived from the preprocessing results of C++ and Python on the host side.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">host_buffers</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// ... get host buffer from preprocessing etc.</span>
<span class="w">  </span><span class="c1">// Get Input</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">host_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">host_buffers</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetData</span><span class="p">(</span><span class="n">host_data</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetDeviceData</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Specify output host memory</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Get Output from model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">output_buffers</span><span class="p">;</span>
<span class="w">  </span><span class="n">ResourceGuard</span><span class="w"> </span><span class="nf">output_device_rel</span><span class="p">([</span><span class="o">&amp;</span><span class="n">output_buffers</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">item</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">output_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">free</span><span class="p">(</span><span class="n">item</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">});</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">buffer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">malloc</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">());</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetData</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetDeviceData</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
<span class="w">    </span><span class="n">output_buffers</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span><span class="w"> </span><span class="c1">// for free</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="specifying-the-memory-of-the-input-and-output-devices">
<h3>Specifying the Memory of the Input and Output Devices<a class="headerlink" href="#specifying-the-memory-of-the-input-and-output-devices" title="Permalink to this headline"></a></h3>
<p>Specifying device memory supports Ascend and GPU hardware backends. Specifying input and output device memory can avoid mutual copying from device to host memory, for example, the device memory input generated by chip dvpp preprocessing is directly used as input for model inference, avoiding preprocessing results copied from device memory to host memory and host results used as model inference input and re-copied to device before inference.</p>
<p>Sample memory for specified input and output devices can be found in <a class="reference external" href="https://gitee.com/mindspore/mindspore/tree/r2.3.q1/mindspore/lite/examples/cloud_infer/device_example_cpp">sample device memory</a>.</p>
<p>Input and output device memory can be specified separately or simultaneously by <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/generate/classmindspore_MSTensor.html">SetDeviceData</a>. The user needs to maintain the device memory lifecycle and is responsible for device memory requests and releases.</p>
<ol class="arabic">
<li><p>Specify the input device memory</p>
<p>In the sample, the value of the input device memory is copied from host, and the value of the general device memory comes from the preprocessing result of chip or the output of another model.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">SetDeviceData</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">uint8_t</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="n">host_data_buffer</span><span class="p">,</span>
<span class="w">                  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">device_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">host_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">host_data_buffer</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data_size</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data size cannot be 0, tensor shape: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ShapeToString</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">Shape</span><span class="p">())</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">device_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MallocDeviceMemory</span><span class="p">(</span><span class="n">data_size</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to alloc device data, data size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">device_buffers</span><span class="o">-&gt;</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_data</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">CopyMemoryHost2Device</span><span class="p">(</span><span class="n">device_data</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">,</span><span class="w"> </span><span class="n">host_data</span><span class="p">,</span><span class="w"> </span><span class="n">data_size</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to copy data to device, data size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetDeviceData</span><span class="p">(</span><span class="n">device_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetData</span><span class="p">(</span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Input</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">device_buffers</span><span class="p">;</span>
<span class="w">  </span><span class="n">ResourceGuard</span><span class="w"> </span><span class="nf">device_rel</span><span class="p">([</span><span class="o">&amp;</span><span class="n">device_buffers</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">item</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">device_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">FreeDeviceMemory</span><span class="p">(</span><span class="n">item</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">});</span>
<span class="w">  </span><span class="n">SetDeviceData</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">host_buffers</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_buffers</span><span class="p">);</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">Predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Specify the output device memory</p>
<p>In the sample, the output device memory is copied to the host and prints the output. Generally the output device memory can be used as input for other models.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">SetOutputDeviceData</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">device_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensors</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensors</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data_size</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Data size cannot be 0, tensor shape: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ShapeToString</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">Shape</span><span class="p">())</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">device_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">MallocDeviceMemory</span><span class="p">(</span><span class="n">data_size</span><span class="p">);</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to alloc device data, data size &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">data_size</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">device_buffers</span><span class="o">-&gt;</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetDeviceData</span><span class="p">(</span><span class="n">device_data</span><span class="p">);</span>
<span class="w">    </span><span class="n">tensor</span><span class="p">.</span><span class="n">SetData</span><span class="p">(</span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Output from model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">outputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetOutputs</span><span class="p">();</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="w"> </span><span class="o">*&gt;</span><span class="w"> </span><span class="n">output_device_buffers</span><span class="p">;</span>
<span class="w">  </span><span class="n">ResourceGuard</span><span class="w"> </span><span class="nf">output_device_rel</span><span class="p">([</span><span class="o">&amp;</span><span class="n">output_device_buffers</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">item</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">output_device_buffers</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">FreeDeviceMemory</span><span class="p">(</span><span class="n">item</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">});</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">SetOutputDeviceData</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">output_device_buffers</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to set output device data&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">Predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="ascend-backend-ge-inference">
<h3>Ascend Backend GE Inference<a class="headerlink" href="#ascend-backend-ge-inference" title="Permalink to this headline"></a></h3>
<p>Ascend inference currently has two methods.</p>
<p>The first method is the default ACL inference. The ACL interface only has global and model (graph) level option configurations. So multiple graphs cannot indicate association relationships, they are relatively independent and cannot share weights (including constants and variables). If there are variables, which can be changed in the model, variables need to be initialized first, so an additional initialization graph needs to be constructed and executed, and variables need to be shared with the calculation graph. Due to the relative independence of multiple graphs, the model cannot have variables when using default ACL inference.</p>
<p>The ACL interface supports building models in advance and loading them using already built models.</p>
<p>Another method is the GE inference. The GE interface has global, session and model (graph) level option configurations. Multiple graphs can be in the same session, and can share weights. In the same session, initialization graphs can be created for variables and shared with computational graphs. When using the default GE inference, the model can have variables.</p>
<p>The current GE interface does not support building models in advance, and models need to be built during loading.</p>
<p>GE can be enabled by specifying <code class="docutils literal notranslate"><span class="pre">provider</span></code> as <code class="docutils literal notranslate"><span class="pre">ge</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mindspore_lite</span> <span class="k">as</span> <span class="nn">mslite</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
<span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Ascend&quot;</span><span class="p">]</span>
<span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">rank_id</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">provider</span> <span class="o">=</span> <span class="s2">&quot;ge&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="s2">&quot;seq_1024.mindir&quot;</span><span class="p">,</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="s2">&quot;config.ini&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">AscendDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New AscendDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Set Atlas training series device id， rank id and provider.</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetRankID</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetProvider</span><span class="p">(</span><span class="s">&quot;ge&quot;</span><span class="p">);</span>
<span class="c1">// Device context needs to be push_back into device_list to work.</span>
<span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">LoadConfig</span><span class="p">(</span><span class="s">&quot;config.ini&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load config file &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;config.ini&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Build model</span>
<span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="s">&quot;seq_1024.mindir&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In the configuration file, the options from <code class="docutils literal notranslate"><span class="pre">[ge_global_options]</span></code>, <code class="docutils literal notranslate"><span class="pre">[ge_sesion_options]</span></code> and <code class="docutils literal notranslate"><span class="pre">[ge_graph_options]</span></code> will be used as global, session and model (graph) level options for the GE interface. For details, please refer to <a class="reference external" href="https://www.hiascend.com/document/detail/zh/canncommercial/700/inferapplicationdev/graphdevg/atlasgeapi_07_0119.html">GE Options</a>. For example:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[ge_global_options]</span>
<span class="na">ge.opSelectImplmode</span><span class="o">=</span><span class="s">high_precision</span>

<span class="k">[ge_session_options]</span>
<span class="na">ge.externalWeight</span><span class="o">=</span><span class="s">1</span>

<span class="k">[ge_graph_options]</span>
<span class="na">ge.exec.precision_mode</span><span class="o">=</span><span class="s">allow_fp32_to_fp16</span>
<span class="na">ge.inputShape</span><span class="o">=</span><span class="s">x1:-1,3,224,224</span><span class="c1">;x2:-1,3,1024,1024</span>
<span class="na">ge.dynamicDims</span><span class="o">=</span><span class="s">1,1</span><span class="c1">;2,2;3,3;4,4</span>
<span class="na">ge.dynamicNodeType</span><span class="o">=</span><span class="s">1</span>
</pre></div>
</div>
</section>
<section id="loading-models-through-multiple-threads">
<h3>Loading Models through Multiple Threads<a class="headerlink" href="#loading-models-through-multiple-threads" title="Permalink to this headline"></a></h3>
<p>When the backend is Ascend and the provider is the default, it supports loading multiple Ascend optimized models through multiple threads to improve model loading performance. Using the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3.0rc1/use/converter_tool.html">Model converting tool</a>, we can specify <code class="docutils literal notranslate"><span class="pre">--optimize=ascend_oriented</span></code> to convert <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> models exported from MindSpore, third-party framework models such as TensorFlow and ONNX into Ascend optimized models. The <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> models exported by MindSpore have not undergone Ascend optimization. For third-party framework models, the <code class="docutils literal notranslate"><span class="pre">MindIR</span></code> model generated by specifying <code class="docutils literal notranslate"><span class="pre">--optimize=none</span></code> in the converting tool has not undergone Ascend optimization.</p>
</section>
<section id="multiple-models-sharing-weights">
<h3>Multiple Models Sharing Weights<a class="headerlink" href="#multiple-models-sharing-weights" title="Permalink to this headline"></a></h3>
<p>In the Ascend device GE scenario, a single device can deploy multiple models, and models deployed in the same device can share weights, including constants and variables.</p>
<p>The same model script can export different models with the same weights for different conditional branches or input shapes. During the inference process, some weights can no longer be updated and are parsed as constants, where multiple models will have the same constant weights, while some weights may be updated and are parsed as variables. If one model updates one weight, the modified weight can be use and updated in the next inference or by other models.</p>
<p>The relationship between multiple models sharing weights can be indicated through interface <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3.0rc1/mindspore_lite/mindspore_lite.ModelGroup.html#mindspore_lite.ModelGroup">ModelGroup</a>.</p>
<p>Python implementation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="n">model_path0</span><span class="p">,</span> <span class="n">model_path1</span><span class="p">,</span> <span class="n">config_file_0</span><span class="p">,</span> <span class="n">config_file_1</span><span class="p">,</span> <span class="n">rank_id</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Context</span><span class="p">()</span>
    <span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="n">device_id</span>
    <span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">rank_id</span> <span class="o">=</span> <span class="n">rank_id</span>  <span class="c1"># for distributed model</span>
    <span class="n">context</span><span class="o">.</span><span class="n">ascend</span><span class="o">.</span><span class="n">provider</span> <span class="o">=</span> <span class="s2">&quot;ge&quot;</span>
    <span class="n">context</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Ascend&quot;</span><span class="p">]</span>
    <span class="n">model0</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>
    <span class="n">model1</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>

    <span class="n">model_group</span> <span class="o">=</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelGroup</span><span class="p">(</span><span class="n">mslite</span><span class="o">.</span><span class="n">ModelGroupFlag</span><span class="o">.</span><span class="n">SHARE_WEIGHT</span><span class="p">)</span>
    <span class="n">model_group</span><span class="o">.</span><span class="n">add_model</span><span class="p">([</span><span class="n">model0</span><span class="p">,</span> <span class="n">model1</span><span class="p">])</span>

    <span class="n">model0</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path0</span><span class="p">,</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">config_file_0</span><span class="p">)</span>
    <span class="n">model1</span><span class="o">.</span><span class="n">build_from_file</span><span class="p">(</span><span class="n">model_path1</span><span class="p">,</span> <span class="n">mslite</span><span class="o">.</span><span class="n">ModelType</span><span class="o">.</span><span class="n">MINDIR</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">config_file_1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model0</span><span class="p">,</span> <span class="n">model1</span>
</pre></div>
</div>
<p>C++ implementation:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Model</span><span class="o">&gt;</span><span class="w"> </span><span class="n">LoadModel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_path0</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">model_path1</span><span class="p">,</span>
<span class="w">                             </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">config_file_0</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="o">&amp;</span><span class="n">config_file_1</span><span class="p">,</span>
<span class="w">                             </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">rank_id</span><span class="p">,</span><span class="w"> </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">device_id</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">AscendDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New AscendDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetDeviceID</span><span class="p">(</span><span class="n">device_id</span><span class="p">);</span>
<span class="w">    </span><span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetRankID</span><span class="p">(</span><span class="n">rank_id</span><span class="p">);</span>
<span class="w">    </span><span class="n">device_info</span><span class="o">-&gt;</span><span class="n">SetProvider</span><span class="p">(</span><span class="s">&quot;ge&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="w">    </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model0</span><span class="p">;</span>
<span class="w">    </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="w"> </span><span class="n">model1</span><span class="p">;</span>
<span class="w">    </span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelGroup</span><span class="w"> </span><span class="nf">model_group</span><span class="p">(</span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelGroupFlag</span><span class="o">::</span><span class="n">kShareWeight</span><span class="p">);</span>
<span class="w">    </span><span class="n">model_group</span><span class="p">.</span><span class="n">AddModel</span><span class="p">({</span><span class="n">model0</span><span class="p">,</span><span class="w"> </span><span class="n">model1</span><span class="p">});</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">model0</span><span class="p">.</span><span class="n">LoadConfig</span><span class="p">(</span><span class="n">config_file_0</span><span class="p">).</span><span class="n">IsOk</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load config file &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">config_file_0</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">model0</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path0</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">).</span><span class="n">IsOk</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load model &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">model_path0</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">model1</span><span class="p">.</span><span class="n">LoadConfig</span><span class="p">(</span><span class="n">config_file_1</span><span class="p">).</span><span class="n">IsOk</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load config file &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">config_file_1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">model1</span><span class="p">.</span><span class="n">Build</span><span class="p">(</span><span class="n">model_path1</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">ModelType</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">).</span><span class="n">IsOk</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Failed to load model &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">model_path1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">{};</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">model0</span><span class="p">,</span><span class="w"> </span><span class="n">model1</span><span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
<p>By default, multiple models in the above configuration only share variables. When constants need to be shared, the weight externalization option needs to be configured in the configuration file. The configuration files are the <code class="docutils literal notranslate"><span class="pre">config_file_0</span></code> and <code class="docutils literal notranslate"><span class="pre">config_file_1</span></code> of the above examples.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[ge_session_options]</span>
<span class="na">ge.externalWeight</span><span class="o">=</span><span class="s">1</span>
</pre></div>
</div>
</section>
</section>
<section id="experimental-feature">
<h2>Experimental feature<a class="headerlink" href="#experimental-feature" title="Permalink to this headline"></a></h2>
<section id="multi-backend-runtime">
<h3>multi-backend runtime<a class="headerlink" href="#multi-backend-runtime" title="Permalink to this headline"></a></h3>
<p>MindSpore Lite cloud inference is supporting multi-backend heterogeneous inference, which can be enabled by specifying the environment variable ‘export ENABLE_MULTI_BACKEND_RUNTIME=on’ during runtime, and other interfaces are used in the same way as the original cloud inference. At present, this feature is an experimental feature, and the correctness, stability and subsequent compatibility of the feature are not guaranteed.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="runtime.html" class="btn btn-neutral float-left" title="Performing Inference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="runtime_java.html" class="btn btn-neutral float-right" title="Using Java Interface to Perform Cloud-side Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>