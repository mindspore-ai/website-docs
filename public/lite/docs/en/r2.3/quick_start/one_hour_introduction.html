<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quick Start to Device-side Inference &mdash; MindSpore Lite master documentation</title><script>;(()=>{const e=localStorage.getItem("ms-theme"),t=window.matchMedia("(prefers-color-scheme: dark)").matches;(e?"dark"===e:t)&&document.documentElement.setAttribute("data-o-theme","dark")})();</script><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script><script src="../_static/jquery.js"></script>
        <script src="../_static/js/theme.js"></script><script src="../_static/underscore.js"></script><script src="../_static/doctools.js"></script><script src="../_static/js/lite.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Quick Start to Cloud-side Inference" href="one_hour_introduction_cloud.html" />
    <link rel="prev" title="Building Cloud-side MindSpore Lite" href="../use/cloud_infer/build.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MindSpore Lite
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Obtain MindSpore Lite</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/downloads.html">Downloading MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/build.html">Building Device-side MindSpore Lite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/build.html">Building Cloud-side MindSpore Lite</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick Start to Device-side Inference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#for-linux">For Linux</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#converting-a-model">Converting a Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#downloading-the-release-package">Downloading the Release Package</a></li>
<li class="toctree-l4"><a class="reference internal" href="#converting-the-model">Converting the Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#netron-visualization">Netron Visualization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-inference">Model Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#downloading-the-release-package-1">Downloading the Release Package</a></li>
<li class="toctree-l4"><a class="reference internal" href="#benchmark-inference-test">Benchmark Inference Test</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integration-and-inference">Integration and Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#for-windows">For Windows</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#converting-a-model-1">Converting a Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#downloading-the-release-package-2">Downloading the Release Package</a></li>
<li class="toctree-l4"><a class="reference internal" href="#converting-the-model-1">Converting the Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#netron-visualization-1">Netron Visualization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-inference-1">Model Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#downloading-the-release-package-3">Downloading the Release Package</a></li>
<li class="toctree-l4"><a class="reference internal" href="#benchmark-inference-test-1">Benchmark Inference Test</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integration-and-inference-1">Integration and Inference</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="one_hour_introduction_cloud.html">Quick Start to Cloud-side Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_infer_example.html">Device-side Inference Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/post_training_quantization.html">Post Training Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/data_preprocessing.html">Data Preprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime.html">Executing Model Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/micro.html">Performing Inference or Training on MCU or Small Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/asic.html">Application Specific Integrated Circuit Integration Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../device_train_example.html">Device-side Training Sample</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/runtime_train.html">Executing Model Training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Third-party hardware docking</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/register.html">Custom Kernel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/delegate.html">Using Delegate to Support Third-party AI Framework</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Device-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/converter.html">Converting Models for Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/benchmark.html">Benchmark Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cropper_tool.html">Static Library Cropper Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/visual_tool.html">Visualization Tool</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/obfuscator_tool.html">Model Obfuscation Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Inference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime.html">Performing Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime_parallel.html">Performing Concurrent Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/runtime_distributed.html">Distributed Inference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cloud-side Tools</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/converter.html">Model Converter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../use/cloud_infer/benchmark.html">Benchmark Tool</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture_lite.html">Overall Architecture (Lite)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_lite.html">Lite Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operator_list_codegen.html">Codegen Operator List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_lite.html">Model List</a></li>
<li class="toctree-l1"><a class="reference internal" href="../troubleshooting_guide.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../log.html">Log</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MindSpore Lite</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Quick Start to Device-side Inference</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/quick_start/one_hour_introduction.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="quick-start-to-device-side-inference">
<h1>Quick Start to Device-side Inference<a class="headerlink" href="#quick-start-to-device-side-inference" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://gitee.com/mindspore/docs/blob/r2.3/docs/lite/docs/source_en/quick_start/one_hour_introduction.md"><img alt="View Source On Gitee" src="https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r2.3/resource/_static/logo_source_en.svg" /></a></p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This document uses a model inference example to describe how to use basic MindSpore Lite functions. Before using MindSpore Lite, you need to have a Linux (such as Ubuntu, CentOS, and EulerOS) or Windows environment for verification at any time.</p>
<p>To use MindSpore Lite to infer a model, perform the following steps:</p>
<ol class="arabic">
<li><p>Convert the model.</p>
<p>Before inferring a model, you need to convert the model into a MindSpore Lite model file.</p>
</li>
<li><p>Perform integration and inference.</p>
<p>Integrate the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> library in the release package, call related APIs, and transfer the pre-processed data to the framework to implement forward inference of the <code class="docutils literal notranslate"><span class="pre">ms</span></code> model.</p>
</li>
</ol>
</section>
<section id="for-linux">
<h2>For Linux<a class="headerlink" href="#for-linux" title="Permalink to this headline"></a></h2>
<p>This section describes how to convert a model and perform integration and inference on Linux.</p>
<section id="converting-a-model">
<h3>Converting a Model<a class="headerlink" href="#converting-a-model" title="Permalink to this headline"></a></h3>
<p>Convert a model to a MindSpore Lite model file. This operation includes the following steps:</p>
<ol class="arabic">
<li><p>Download a release package.</p>
<p>Both Windows and Linux release packages contain the converter. You can download any release package based on your platform.
Use the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> tool in the release package to convert a non-<code class="docutils literal notranslate"><span class="pre">ms</span></code> model into the <code class="docutils literal notranslate"><span class="pre">ms</span></code> model used by MindSpore Lite.
The same converted model file can be obtained regardless of the platform where the conversion is performed.</p>
</li>
<li><p>Convert the model.</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> tool in the release package to convert the model.</p>
</li>
</ol>
<section id="downloading-the-release-package">
<h4>Downloading the Release Package<a class="headerlink" href="#downloading-the-release-package" title="Permalink to this headline"></a></h4>
<p>You can download MindSpore Lite from the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/downloads.html">official website</a>.
In this example, we use MindSpore Lite 1.6.0 (download <a class="reference external" href="https://ms-release.obs.cn-north-4.myhuaweicloud.com/1.6.0/MindSpore/lite/release/linux/x86_64/mindspore-lite-1.6.0-linux-x64.tar.gz">here</a>) and a CPU release package with Linux OS and the x86_64 underlying architecture.
The structure of each release package varies. In this example, the structure of the Linux release package is as follows (files in the release package will be introduced later):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
├── runtime
│   ├── include                        # Header file of APIs integrated and developed by MindSpore Lite
│   ├── lib
│   │   ├── libminddata-lite.a         # Static library for image processing
│   │   ├── libminddata-lite.so        # Dynamic library for image processing
│   │   ├── libmindspore-lite.a        # Static library of the MindSpore Lite inference framework
│   │   ├── libmindspore-lite-jni.so   # JNI dynamic library of the MindSpore Lite inference framework
│   │   ├── libmindspore-lite.so       # Dynamic library of the MindSpore Lite inference framework
│   │   ├── libmindspore-lite-train.a  # Static library of the MindSpore Lite training framework
│   │   ├── libmindspore-lite-train.so # Dynamic library of the MindSpore Lite training framework
│   │   ├── libmsdeobfuscator-lite.so  # To load the dynamic library file for obfuscating models, you need to enable the `MSLITE_ENABLE_MODEL_OBF` option.
│   │   └── mindspore-lite-java.jar    # MindSpore Lite inference framework JAR package
│   └── third_party
│       └── libjpeg-turbo
└── tools
    ├── benchmark       # Directory of the benchmark test tool
    ├── benchmark_train # Directory of the benchmark test tool for training models
    ├── codegen         # Directory of the code generation tool
    ├── converter       # Directory of the converter
    ├── obfuscator      # Directory of the obfuscator
    └── cropper         # Directory of the library cropping tool
</pre></div>
</div>
</section>
<section id="converting-the-model">
<h4>Converting the Model<a class="headerlink" href="#converting-the-model" title="Permalink to this headline"></a></h4>
<p>Decompress the downloaded release package and find the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> tool in the <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-linux-x64/tools/converter/converter</span></code> directory.
The <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> provides the offline model conversion function for the MindSpore, CAFFE, TensorFlow Lite, TensorFlow and ONNX models.
The model conversion procedure is as follows:</p>
<ol class="arabic">
<li><p>Set up the environment.</p>
<p>Add the dynamic link library required by the converter to the environment variable LD_LIBRARY_PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/lib:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} indicates the path generated after the release package is decompressed.</p>
</li>
<li><p>Go to the directory where the converter is stored.</p>
<p>Run the following command to go to the directory where the converter is stored:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/converter/converter
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} indicates the path generated after the release package is decompressed.</p>
</li>
<li><p>Set conversion parameters.</p>
<p>When using converter_lite to perform conversion, you need to set related parameters. Table 1 describes the parameters used in this example.</p>
<p>The following uses the conversion commands for various models as examples to describe how to use the parameters.</p>
<ul>
<li><p>Command for converting the Caffe model <code class="docutils literal notranslate"><span class="pre">lenet.prototxt</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>CAFFE<span class="w"> </span>--modelFile<span class="o">=</span>lenet.prototxt<span class="w"> </span>--weightFile<span class="o">=</span>lenet.caffemodel<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
<p>When converting a Caffe model, set <code class="docutils literal notranslate"><span class="pre">fmk</span></code> to CAFFE (<code class="docutils literal notranslate"><span class="pre">--fmk=CAFFE</span></code>), and transfer the model structure file (lenet.prototxt) and model weight file (lenet.caffemodel) by using the modelFile and weightFile parameters, respectively.
In addition, use outputFile to specify the name of the output model after conversion. Because the path is not specified, the generated model is in the current path by default and has the suffix <code class="docutils literal notranslate"><span class="pre">.ms</span></code>, that is, <code class="docutils literal notranslate"><span class="pre">lenet.ms</span></code>.</p>
</li>
<li><p>Commands for converting the MindSpore, TensorFlow Lite, TensorFlow, and ONNX models</p>
<ul>
<li><p>MindSpore model <code class="docutils literal notranslate"><span class="pre">lenet.mindir</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>lenet.mindir<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
</li>
<li><p>TensorFlow Lite model <code class="docutils literal notranslate"><span class="pre">lenet.tflite</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>lenet.tflite<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
</li>
<li><p>TensorFlow model <code class="docutils literal notranslate"><span class="pre">lenet.pb</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>TF<span class="w"> </span>--modelFile<span class="o">=</span>lenet.pb<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
</li>
<li><p>ONNX model <code class="docutils literal notranslate"><span class="pre">lenet.onnx</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span>lenet.onnx<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
</li>
</ul>
<p>During model conversion, set <code class="docutils literal notranslate"><span class="pre">fmk</span></code> to a symbol corresponding to the model type and transfer the model file by using the modelFile parameter.
Use outputFile to specify the name of the output model after conversion. Because the path is not specified, the generated model is in the current path by default and has the suffix <code class="docutils literal notranslate"><span class="pre">.ms</span></code>, that is, <code class="docutils literal notranslate"><span class="pre">lenet.ms</span></code>.</p>
</li>
</ul>
</li>
<li><p>Execute the conversion.</p>
<p>You can use your own model or click <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.mindir">here</a> to download a MindSpore model for testing.
Take the downloaded model as an example. Copy the model <code class="docutils literal notranslate"><span class="pre">mobilenetv2.mindir</span></code> to the directory where the converter is located. The model conversion command is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>./converter_lite<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>mobilenetv2.mindir<span class="w"> </span>--outputFile<span class="o">=</span>mobilenetv2
</pre></div>
</div>
<p>If the conversion is successful, the following information is displayed, and a new model file named <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> is generated in the current directory:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERT RESULT SUCCESS:0
</pre></div>
</div>
</li>
<li><p>Perform advanced functions.</p>
<p>For details about the converter, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/converter_tool.html">Converting Models for Inference</a>.</p>
<p>For details about how to use the converter to implement post training quantization, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/post_training_quantization.html">Post Training Quantization</a>.</p>
<p>If you want to train a converted model, you need to convert a training model. For details, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/converter_train.html">Creating MindSpore Lite Models</a>.</p>
</li>
</ol>
<p>Table 1: converter_lite parameters</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Mandatory or Not</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Value Range</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--fmk=&lt;FMK&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Original format of the model to be converted.</p></td>
<td><p>MINDIR, CAFFE, TFLITE, TF, or ONNX</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--modelFile=&lt;MODELFILE&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Path of the model to be converted.</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--outputFile=&lt;OUTPUTFILE&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Path and name of the converted model. The suffix <code class="docutils literal notranslate"><span class="pre">.ms</span></code> is automatically generated.</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--weightFile=&lt;WEIGHTFILE&gt;</span></code></p></td>
<td><p>Yes for Caffe model conversion</p></td>
<td><p>Path of the input model weight file.</p></td>
<td><p>-</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>The parameter name and value are separated by an equal sign (=) and no space is allowed between them.</p></li>
<li><p>Generally, a Caffe model has two files: the model structure <code class="docutils literal notranslate"><span class="pre">*.prototxt</span></code>, which corresponds to the <code class="docutils literal notranslate"><span class="pre">--modelFile</span></code> parameter, and the model weight <code class="docutils literal notranslate"><span class="pre">*.caffemodel</span></code>, which corresponds to the <code class="docutils literal notranslate"><span class="pre">--weightFile</span></code> parameter.</p></li>
</ul>
</div></blockquote>
</section>
<section id="netron-visualization">
<h4>Netron Visualization<a class="headerlink" href="#netron-visualization" title="Permalink to this headline"></a></h4>
<p><a class="reference external" href="https://github.com/lutzroeder/netron">Netron</a> is a neural network model visualization tool developed based on the <a class="reference external" href="http://www.electronjs.org/">Electron</a> platform. It supports visualization of many mainstream AI framework models, including MindSpore Lite, and can be used online on multiple platforms (such as Mac, Windows, and Linux) and browsers.
After a MindSpore Lite model is loaded using <code class="docutils literal notranslate"><span class="pre">Netron</span></code>, the topology, graph, and node information of the model can be displayed.
In this example, we use <code class="docutils literal notranslate"><span class="pre">Netron</span></code> on a browser to visualize the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model converted in the previous section. You can also use <code class="docutils literal notranslate"><span class="pre">Netron</span></code> to visualize your own model.</p>
<ol class="arabic">
<li><p>Open a browser and enter <a class="reference external" href="https://netron.app/">https://netron.app/</a> in the address box.</p></li>
<li><p>Drag the model file <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> to the web page to load and open the model.</p></li>
<li><p>View the model.</p>
<p>The following figure shows the loaded model.</p>
<p><img alt="img" src="../_images/netron_model.png" /></p>
<p><em>Figure 1 Page displayed after the model is loaded</em></p>
<p>It can be observed that the model consists of a series of operator nodes connected in sequence. In this model, the <code class="docutils literal notranslate"><span class="pre">Conv2DFusion</span></code> operator appears most frequently.</p>
<ul>
<li><p>View the model input and output.</p>
<p>Click input node 0 or output node 0. The following figure is displayed.</p>
<p><img alt="img" src="../_images/netron_model_input.png" /></p>
<p><em>Figure 2 Model input and output nodes</em></p>
<p><code class="docutils literal notranslate"><span class="pre">MODEL</span> <span class="pre">PROPERTIES</span></code> in the upper column indicates the model attribute. The model format <code class="docutils literal notranslate"><span class="pre">MindSpore</span> <span class="pre">Lite</span> <span class="pre">v1.6.0</span></code> indicates that the model is converted by the converter 1.6.</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">INPUTS</span></code> field, there is an input node, indicating that the model has an input. The input node name is <code class="docutils literal notranslate"><span class="pre">x</span></code> and the data type is <code class="docutils literal notranslate"><span class="pre">float32[1,224,224,3]</span></code>, that is, the <code class="docutils literal notranslate"><span class="pre">1x224x224x3</span></code> tensor of <code class="docutils literal notranslate"><span class="pre">float32</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">0</span></code> before the input node is a serial number identifier, indicating that the node is the 0th node in the model input.</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">OUTPUTS</span></code> field, there is an output node, indicating that the model has an output. The output node name is <code class="docutils literal notranslate"><span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code> and the data type is <code class="docutils literal notranslate"><span class="pre">float32</span> <span class="pre">[1,1000]</span></code>, that is, the <code class="docutils literal notranslate"><span class="pre">1x1000</span></code> tensor of <code class="docutils literal notranslate"><span class="pre">float32</span></code>.</p>
</li>
<li><p>View an operator.</p>
<p>Click the <code class="docutils literal notranslate"><span class="pre">Conv2DFusion</span></code> operator. The following figure is displayed.</p>
<p><img alt="img" src="../_images/netron_model_op.png" /></p>
<p><em>Figure 3 <code class="docutils literal notranslate"><span class="pre">Conv2DFusion</span></code> operator</em></p>
<p>We can see:</p>
<ul>
<li><p>Node operator type</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">NODE</span> <span class="pre">PROPERTIES</span></code> field, you can see that the node type is <code class="docutils literal notranslate"><span class="pre">Conv2DFusion</span></code>, indicating that the node performs the <code class="docutils literal notranslate"><span class="pre">Conv2DFusion</span></code> operation, that is, the two-dimensional convolution operation.</p>
</li>
<li><p>Node name</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">NODE</span> <span class="pre">PROPERTIES</span></code> field, you can see that the node name is <code class="docutils literal notranslate"><span class="pre">Default/backbone-MobileNetV2Backbone/features-SequentialCell/0-ConvBNReLU/features-SequentialCell/0-Conv2d/Conv2D-op0</span></code>.</p>
</li>
<li><p>Operator attribute</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ATTRIBUTES</span></code> field lists the operator attributes, which store the parameter values required for operator computation.</p>
</li>
<li><p>Node inputs</p>
<p>The <code class="docutils literal notranslate"><span class="pre">INPUTS</span></code> field shows the operator inputs. You can see that the operator has three inputs: <code class="docutils literal notranslate"><span class="pre">input</span></code>, <code class="docutils literal notranslate"><span class="pre">weight</span></code>, and <code class="docutils literal notranslate"><span class="pre">bias</span></code>.
The name of the input tensor <code class="docutils literal notranslate"><span class="pre">input</span></code> is <code class="docutils literal notranslate"><span class="pre">x</span></code>, which is the model input.
The name of the input tensor <code class="docutils literal notranslate"><span class="pre">weight</span></code> is <code class="docutils literal notranslate"><span class="pre">Default/backbone-MobileNetV2Backbone/features-SequentialCell/0-ConvBNReLU/features-SequentialCell/0-Conv2d/Conv2D-op0features.0.features.0.weight</span></code>.
Click the plus sign on the right to view the value of the tensor.</p>
</li>
<li><p>Node outputs</p>
<p>The <code class="docutils literal notranslate"><span class="pre">OUTPUTS</span></code> field displays the operator output. You can see that the operator has an output tensor named <code class="docutils literal notranslate"><span class="pre">Default/backbone-MobileNetV2Backbone/features-SequentialCell/0-ConvBNReLU/features-SequentialCell/0-Conv2d/Conv2D-op0</span></code>.</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Understand the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model.</p>
<p>By viewing the model, you can know that the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model is computed as follows:
Continuously convolute the input tensor <code class="docutils literal notranslate"><span class="pre">x</span></code> in the format of <code class="docutils literal notranslate"><span class="pre">float32[1,224,224,3]</span></code>, perform the matrix multiplication operation on the <code class="docutils literal notranslate"><span class="pre">MatMulFusion</span></code> fully-connected layer, and perform the Softmax operation to obtain the <code class="docutils literal notranslate"><span class="pre">1x1000</span></code> output tensor. The output tensor name is <code class="docutils literal notranslate"><span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model provided in this example is an image classification model with 1000 classes. By viewing the model, you can know that the model does not contain pre-processing operations on images, receives the float32 value of 1 x 224 x 224 x 3 and outputs the float32 value of 1 x 1000.
Therefore, when using this model for inference, you need to encode and pre-process images, transfer the processed data to the inference framework for forward inference, and post-process the 1 x 1000 output.</p>
</li>
</ol>
</section>
</section>
<section id="model-inference">
<h3>Model Inference<a class="headerlink" href="#model-inference" title="Permalink to this headline"></a></h3>
<p>You need to integrate the <code class="docutils literal notranslate"><span class="pre">mindspore-lite</span></code> library file in the release package and use the APIs declared in the MindSpore Lite header file to perform model inference.
Before integration, you can also use the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool (stored in <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-linux-x64/tools/benchmark</span></code>) released with the release package to perform inference tests.
The <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool is an executable program that integrates the <code class="docutils literal notranslate"><span class="pre">mindspore-lite</span></code> library. It uses command parameters to implement multiple functions, including inference.</p>
<section id="downloading-the-release-package-1">
<h4>Downloading the Release Package<a class="headerlink" href="#downloading-the-release-package-1" title="Permalink to this headline"></a></h4>
<p>Download a release package based on the system environment used for model inference.
In this example, we use MindSpore Lite 1.6.0 (download <a class="reference external" href="https://ms-release.obs.cn-north-4.myhuaweicloud.com/1.6.0/MindSpore/lite/release/linux/x86_64/mindspore-lite-1.6.0-linux-x64.tar.gz">here</a>) and a CPU release package with Linux OS and the x86_64 underlying architecture.</p>
</section>
<section id="benchmark-inference-test">
<h4>Benchmark Inference Test<a class="headerlink" href="#benchmark-inference-test" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Set up the environment.</p>
<p>Add the dynamic link library required for <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> inference to the environment variable LD_LIBRARY_PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/runtime/lib:<span class="si">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="si">}</span>
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} indicates the path generated after the release package is decompressed.</p>
</li>
<li><p>Go to the directory where the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool is located.</p>
<p>Run the following command to go to the directory where the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool is located:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="si">${</span><span class="nv">PACKAGE_ROOT_PATH</span><span class="si">}</span>/tools/benchmark
</pre></div>
</div>
<p>${PACKAGE_ROOT_PATH} indicates the path generated after the release package is decompressed.</p>
</li>
<li><p>Set the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> parameters.</p>
<p>When using <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> to perform inference, you need to set related parameters. Table 2 describes the parameters used in this example.</p>
</li>
<li><p>Execute inference and analyze inference performance.</p>
<p>You can use the converted model <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> to perform the inference test. Copy the model to the directory where the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool is located and run the following command to perform inference:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./benchmark<span class="w"> </span>--modelFile<span class="o">=</span>mobilenetv2.ms
</pre></div>
</div>
<p>Use <code class="docutils literal notranslate"><span class="pre">modelFile</span></code> to specify the model to be inferred and use the default values for other parameters.
In this example, if no input data is specified, a random value is generated as the input.
After the command is executed, if the inference is successful, information similar to the following is displayed. The information shows performance indicators such as the number of concurrent threads during inference (<code class="docutils literal notranslate"><span class="pre">NumThreads</span> <span class="pre">=</span> <span class="pre">2</span></code>), the minimum duration of a single inference of the test model (<code class="docutils literal notranslate"><span class="pre">6.677000</span> <span class="pre">ms</span></code>), maximum duration of a single inference (<code class="docutils literal notranslate"><span class="pre">8.656000</span> <span class="pre">ms</span></code>), and average inference duration (<code class="docutils literal notranslate"><span class="pre">7.291000</span> <span class="pre">ms</span></code>). The performance value varies according to the environment.
Because the <code class="docutils literal notranslate"><span class="pre">numThreads</span></code> parameter is not specified, two threads are used for inference by default. You can set the number of threads to test the inference performance. (When the number of threads reaches a certain value, the inference time is prolonged due to the thread switchover overhead.)</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ModelPath = mobilenetv2.ms
ModelType = MindIR
InDataPath =
ConfigFilePath =
InDataType = bin
LoopCount = 10
DeviceType = CPU
AccuracyThreshold = 0.5
CosineDistanceThreshold = -1.1
WarmUpLoopCount = 3
NumThreads = 2
Fp16Priority = 0
EnableParallel = 0
calibDataPath =
cpuBindMode = HIGHER_CPU
CalibDataType = FLOAT
start unified benchmark run
PrepareTime = 30.013 ms
Running warm up loops...
Running benchmark loops...
Model = mobilenetv2.ms, NumThreads = 2, MinRunTime = 6.677000 ms, MaxRuntime = 8.656000 ms, AvgRunTime = 7.291000 ms
Run Benchmark mobilenetv2.ms Success.
</pre></div>
</div>
</li>
<li><p>Execute inference and analyze inference accuracy.</p>
<p>To use the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool to test the inference accuracy of MindSpore Lite, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./benchmark<span class="w"> </span>--modelFile<span class="o">=</span>mobilenetv2.ms<span class="w"> </span>--inDataFile<span class="o">=</span>input.bin<span class="w"> </span>--benchmarkDataFile<span class="o">=</span>output.txt
</pre></div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">modelFile</span></code> specifies the model to be inferred.</p>
<p><code class="docutils literal notranslate"><span class="pre">inDataFile</span></code> specifies the model input data file, which is set to <code class="docutils literal notranslate"><span class="pre">input.bin</span></code>.
The model is opened in the <code class="docutils literal notranslate"><span class="pre">Netron</span></code>, and we know that the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model receives the <code class="docutils literal notranslate"><span class="pre">1x224x224x3</span></code> tensor of <code class="docutils literal notranslate"><span class="pre">float32</span></code>.
The <code class="docutils literal notranslate"><span class="pre">inDataFile</span></code> option of <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> receives data files in binary format by default. The <code class="docutils literal notranslate"><span class="pre">input.bin</span></code> file saves 150528 <code class="docutils literal notranslate"><span class="pre">float32</span></code> binary values in sequence, which is the same as the <code class="docutils literal notranslate"><span class="pre">1x224x224x3</span></code> data volume required by the model, and the format is <code class="docutils literal notranslate"><span class="pre">float32</span></code>.</p>
<p>Generally, the input data file can be generated in the following ways:</p>
<ul class="simple">
<li><p>Save training data: Pre-process the data in the model training dataset and save the pre-processed data.</p></li>
<li><p>Random generation: Randomly generate data within a specified range.</p></li>
</ul>
<p>In this example, a randomly generated number is used as the input. You can run the following Python script or click <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/input.bin">here</a> to download the <code class="docutils literal notranslate"><span class="pre">input.bin</span></code> file and save it to the benchmark directory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">t</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="s2">&quot;input.bin&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>After the input data is provided, you need to provide the benchmark data for comparison with the inference result for accuracy error analysis.
In this example, <code class="docutils literal notranslate"><span class="pre">benchmarkDataFile</span></code> specifies the model output benchmark file, which is set to <code class="docutils literal notranslate"><span class="pre">output.txt</span></code>. The format of the benchmark file is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[Name of output node 1][Dimension length of shape of output node 1][Value of the first dimension of shape of output node 1]... [Value of the n dimension of shape of output node 1]
[Node 1 data 1]  [Node 1 data 2]...
[Name of output node 2][Dimension length of shape of output node 2][Value of the first dimension of shape of output node 2]... [Value of the n dimension of shape of output node 2]
[Node 2 data 1]  [Node 2 data 2]...
</pre></div>
</div>
<p>Generally, the benchmark file can be generated in the following ways:</p>
<ul class="simple">
<li><p>Comparison with other frameworks: Use another deep learning model inference framework with the same input, and save the inference result in the required format.</p></li>
<li><p>Comparison with model training: In the training framework, save the pre-processed data as the input data specified by <code class="docutils literal notranslate"><span class="pre">inDataFile</span></code>. After model inference, save the output data that has not been post-processed in the benchmark file format, and use it as the benchmark.</p></li>
<li><p>Comparison with different devices or data types: Use different data types (such as FP16) or devices (such as GPU/NPU) for inference to obtain the benchmark in the environment.</p></li>
<li><p>Comparison with theoretical values: For some simple models, manually construct output benchmarks based on your understanding of the models.</p></li>
</ul>
<p>The size of the provided benchmark data must be the same as that of the model output so that the benchmark data can be compared with the model output to obtain the inference accuracy error.
The output node name of the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model is <code class="docutils literal notranslate"><span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code>, and the output shape of the node is <code class="docutils literal notranslate"><span class="pre">1x1000</span></code> (as shown in Figure 2). Therefore, the dimension length of the node shape is 2. The first dimension value of the node shape is 1, and the second dimension value of the node shape is 1000.
In this example, the benchmark is generated by comparing with other frameworks. The previously obtained <code class="docutils literal notranslate"><span class="pre">input.bin</span></code> file uses another framework to generate the inference data and is saved in the benchmark file format.
The benchmark data is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Default/head-MobileNetV2Head/Softmax-op204 2 1 1000
4.75662418466527e-05 0.00044544308912009 ...
</pre></div>
</div>
<p>The second row of data in the benchmark indicates the inference output of another framework with the same input (<code class="docutils literal notranslate"><span class="pre">input.bin</span></code>). You can click <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/output.txt">here</a> to download the output.txt file in this example and save it to the benchmark directory.
After the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> command is executed, if the inference is successful, information similar to the following is displayed:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ModelPath = mobilenetv2.ms
ModelType = MindIR
InDataPath = input.bin
ConfigFilePath =
InDataType = bin
LoopCount = 10
DeviceType = CPU
AccuracyThreshold = 0.5
CosineDistanceThreshold = -1.1
WarmUpLoopCount = 3
NumThreads = 2
Fp16Priority = 0
EnableParallel = 0
calibDataPath = output.txt
cpuBindMode = HIGHER_CPU
CalibDataType = FLOAT
start unified benchmark run
PrepareTime = 31.709 ms
MarkAccuracy
InData 0: 0.417022 0.720325 0.000114375 0.302333 0.146756 0.0923386 0.18626 0.345561 0.396767 0.538817 0.419195 0.68522 0.204452 0.878117 0.0273876 0.670467 0.417305 0.55869 0.140387 0.198101
================ Comparing Output data ================
Data of node Default/head-MobileNetV2Head/Softmax-op204 : 4.75662e-05 0.000445443 0.000294212 0.000354572 0.000165406 8.36175e-05 0.000198424 0.000329004 0.000288576 0.000203605 0.000962143 0.00421465 0.0019162 0.00129701 0.00260928 0.0012302 0.000879829 0.000609378 0.000691054 0.00119472 0.000516733 0.00160048 0.000959531 0.00176164 0.000365934 0.00013575 0.000245539 0.000414651 0.000165337 0.000480154 0.000216396 0.00101303 0.000105544 0.000475172 0.000761407 0.000305815 0.000294882 0.000307003 0.00188077 0.000454868 0.000897518 0.00051352 0.000595383 0.000644214 0.000513376 0.000343709 0.00103984 0.000197185 7.54722e-05 8.89811e-05
Mean bias of node/tensor Default/head-MobileNetV2Head/Softmax-op204 : 0%
Mean bias of all nodes/tensors: 0%
=======================================================

Run Benchmark mobilenetv2.ms Success.
</pre></div>
</div>
<p>In the output information, the <code class="docutils literal notranslate"><span class="pre">InData</span> <span class="pre">0</span></code> line displays the input data (only the first 20 values are displayed) of the inference, and the <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">of</span> <span class="pre">node</span> <span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code> line displays the inference result (only the first 50 values are displayed) of the related output node (<code class="docutils literal notranslate"><span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code>). You can directly observe the differences between them and the benchmark file.
In line <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">bias</span> <span class="pre">of</span> <span class="pre">node/tensor</span> <span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code>, the average error between the <code class="docutils literal notranslate"><span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code> output tensor and the benchmark data is provided. The error is computed using the comparison algorithm provided by the benchmark tool.
<code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">bias</span> <span class="pre">of</span> <span class="pre">all</span> <span class="pre">nodes/tensors</span></code> provides the average error of all tensors compared with the benchmark. In this example, there is only one output tensor. Therefore, the total average error is the same as that of the <code class="docutils literal notranslate"><span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code> tensor. It can be observed that the total average error of inference is 0%.</p>
</li>
<li><p>Perform advanced functions.</p>
<p>For details about <code class="docutils literal notranslate"><span class="pre">benchmark</span></code>, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/benchmark_tool.html">benchmark</a>.</p>
</li>
</ol>
<p>Table 2 Definition of benchmark parameters</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Mandatory or Not</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--modelFile=&lt;MODELPATH&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Specifies the path of the MindSpore Lite model file for which the benchmark test is to be performed.</p></td>
<td><p>String</p></td>
<td><p>null</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--numThreads=&lt;NUMTHREADS&gt;</span></code></p></td>
<td><p>No</p></td>
<td><p>Specifies the number of threads for running the model inference program.</p></td>
<td><p>Integer</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--inDataFile=&lt;INDATAPATH&gt;</span></code></p></td>
<td><p>No</p></td>
<td><p>Specifies the file path of the input data of the test model. By default, data files in binary format are received. In the accuracy test, this input is used as the benchmark input. If this parameter is not set, a random value is used.</p></td>
<td><p>String</p></td>
<td><p>null</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--benchmarkDataFile=&lt;CALIBDATAPATH&gt;</span></code></p></td>
<td><p>No</p></td>
<td><p>Specifies the file path of the benchmark data (for accuracy comparison) to be compared and receives the character text in the specified format.</p></td>
<td><p>String</p></td>
<td><p>null</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>The parameter name and value are separated by an equal sign (=) and no space is allowed between them.</p></li>
</ul>
</div></blockquote>
</section>
<section id="integration-and-inference">
<h4>Integration and Inference<a class="headerlink" href="#integration-and-inference" title="Permalink to this headline"></a></h4>
<p>In the previous section, the official inference test tool is used to perform the model inference test. This section uses the C++ APIs of MindSpore Lite as an example to describe how to use the MindSpore Lite release package to perform integrated development and build your own inference program.</p>
<ol class="arabic">
<li><p>Environment Requirements</p>
<ul class="simple">
<li><p>System environment: Linux x86_64 (Ubuntu 18.04.02LTS is recommended.)</p></li>
<li><p>C++ build dependencies</p>
<ul>
<li><p><a class="reference external" href="https://gcc.gnu.org/releases.html">GCC</a> &gt;= 7.3.0</p></li>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.12</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Obtain the release package.</p>
<p>Click <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/downloads.html">here</a> to obtain a MindSpore Lite release package.
The release package for integration and development in this example is the same as that in the previous sections. You can click <a class="reference external" href="https://ms-release.obs.cn-north-4.myhuaweicloud.com/1.6.0/MindSpore/lite/release/linux/x86_64/mindspore-lite-1.6.0-linux-x64.tar.gz">here</a> to download the package.
The following content in the release package is required:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-linux-x64
└── runtime
    │── include                        # Header file of APIs integrated and developed by MindSpore Lite
    └── lib
        └── libmindspore-lite.so       # Dynamic library of the MindSpore Lite inference framework
</pre></div>
</div>
</li>
<li><p>Build a project directory.</p>
<p>In this example, the project directory is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>demo
├── CMakeLists.txt                  # CMake project management file
├── main.cc                         # User code
├── build                           # Build directory
├── model
│    └── mobilenetv2.ms             # Model file (the converted model)
└── runtime                         # Runtime directory of the release package
    ├── include                     # Header file of APIs integrated and developed by MindSpore Lite
    └── lib
        └── libmindspore-lite.so    # Dynamic library of the MindSpore Lite inference framework
</pre></div>
</div>
<p>Create a <code class="docutils literal notranslate"><span class="pre">demo</span></code> folder and create the <code class="docutils literal notranslate"><span class="pre">main.cc</span></code> and <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> files.
Create the <code class="docutils literal notranslate"><span class="pre">build</span></code> and <code class="docutils literal notranslate"><span class="pre">model</span></code> directories and place <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> in <code class="docutils literal notranslate"><span class="pre">model</span></code>.
Copy the <code class="docutils literal notranslate"><span class="pre">runtime</span></code> directory in the release package to <code class="docutils literal notranslate"><span class="pre">demo</span></code>. You can retain files in the <code class="docutils literal notranslate"><span class="pre">runtime</span></code> directory or delete library files except the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> and <code class="docutils literal notranslate"><span class="pre">include</span></code> folders.</p>
</li>
<li><p>Build a CMake project.</p>
<p>Open the created <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> file and paste the following content.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>cmake_minimum_required(VERSION 3.12)  # The CMake version must be 3.12 or later.
project(Demo)  # The project name is Demo.

# The GCC version must be 7.3.0 or later.
if(CMAKE_CXX_COMPILER_ID STREQUAL &quot;GNU&quot; AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.3.0)
    message(FATAL_ERROR &quot;GCC version ${CMAKE_CXX_COMPILER_VERSION} must not be less than 7.3.0&quot;)
endif()

include_directories(${CMAKE_CURRENT_SOURCE_DIR}/runtime/)  # Add the header file search path.

link_directories(${CMAKE_CURRENT_SOURCE_DIR}/runtime/lib)  # Add the library file search path.

add_executable(demo main.cc)  # Build and generate the demo execution program.

# Declare the library to be linked to the demo execution program. mindspore-lite is the dynamic library of the MindSpore Lite inference framework.
target_link_libraries(
        demo
        mindspore-lite
        pthread
        dl
)
</pre></div>
</div>
<blockquote>
<div><p>If you want to integrate the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.a</span></code> static library, replace <code class="docutils literal notranslate"><span class="pre">mindspore-lite</span></code> with the <code class="docutils literal notranslate"><span class="pre">-Wl,--whole-archive</span> <span class="pre">mindspore-lite</span> <span class="pre">-Wl,--no-whole-archive</span></code> option.</p>
</div></blockquote>
</li>
<li><p>Write code.</p>
<p>Open the created <code class="docutils literal notranslate"><span class="pre">main.cc</span></code> and paste the following content:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;fstream&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;random&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/model.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/context.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/status.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/types.h&quot;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="p">;</span>

<span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="nf">ReadFile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="o">*</span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">file</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;file is nullptr.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">ifstream</span><span class="w"> </span><span class="n">ifs</span><span class="p">(</span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ifstream</span><span class="o">::</span><span class="n">in</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ifstream</span><span class="o">::</span><span class="n">binary</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">ifs</span><span class="p">.</span><span class="n">good</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;file: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; is not exist.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">ifs</span><span class="p">.</span><span class="n">is_open</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;file: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; open failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">ifs</span><span class="p">.</span><span class="n">seekg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">end</span><span class="p">);</span>
<span class="w">  </span><span class="o">*</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ifs</span><span class="p">.</span><span class="n">tellg</span><span class="p">();</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="kt">char</span><span class="p">[]</span><span class="o">&gt;</span><span class="w"> </span><span class="n">buf</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="kt">char</span><span class="p">[</span><span class="o">*</span><span class="n">size</span><span class="p">]);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">buf</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;malloc buf failed, file: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="n">ifs</span><span class="p">.</span><span class="n">close</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">ifs</span><span class="p">.</span><span class="n">seekg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">beg</span><span class="p">);</span>
<span class="w">  </span><span class="n">ifs</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">buf</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="o">*</span><span class="n">size</span><span class="p">);</span>
<span class="w">  </span><span class="n">ifs</span><span class="p">.</span><span class="n">close</span><span class="p">();</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">buf</span><span class="p">.</span><span class="n">release</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">Distribution</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">GenerateRandomData</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Distribution</span><span class="w"> </span><span class="n">distribution</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">mt19937</span><span class="w"> </span><span class="n">random_engine</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">elements_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">);</span>
<span class="w">  </span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">generate_n</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">elements_num</span><span class="p">,</span>
<span class="w">                        </span><span class="p">[</span><span class="o">&amp;</span><span class="n">distribution</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">random_engine</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">distribution</span><span class="p">(</span><span class="n">random_engine</span><span class="p">));</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Read model file.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">model_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;../model/mobilenetv2.ms&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_buf</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Read model file failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Create and init context, add CPU device info</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Create model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Input</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">());</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;MallocData for inTensor failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">GenerateRandomData</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">(),</span><span class="w"> </span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">uniform_real_distribution</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">0.1f</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Predict</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inference error.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Output Tensor Data.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">------- print outputs ----------&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;out tensor name is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">Name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">out tensor size is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span>
<span class="w">              </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">out tensor elements num is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">());</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;output data is:&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;------- print end ----------</span><span class="se">\n</span><span class="s">&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Delete model.</span>
<span class="w">  </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The code function is parsed as follows:</p>
<p>(1) Read the model file to the buffer.</p>
<p>Call the <code class="docutils literal notranslate"><span class="pre">ReadFile</span></code> function to read the model file to the <code class="docutils literal notranslate"><span class="pre">model_buf</span></code> memory and use the <code class="docutils literal notranslate"><span class="pre">size</span></code> variable to save the model size.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
</pre></div>
</div>
<p>(2) Initialize the context configuration.</p>
<p>The context stores configurations required for model inference, including the operator preference, number of threads, automatic concurrency, and other configurations related to the inference processor.
For details about the context, see “Context” in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/generate/classmindspore_Context.html">API</a>.
When MindSpore Lite loads a model, an object of the <code class="docutils literal notranslate"><span class="pre">Context</span></code> class must be provided. In this example, the <code class="docutils literal notranslate"><span class="pre">context</span></code> object of the <code class="docutils literal notranslate"><span class="pre">Context</span></code> class is applied for.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
<p>Then, use the <code class="docutils literal notranslate"><span class="pre">Context::MutableDeviceInfo</span></code> interface to obtain the device management list of the <code class="docutils literal notranslate"><span class="pre">context</span></code> object.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
</pre></div>
</div>
<p>In this example, the CPU is used for inference. Therefore, you need to apply for the <code class="docutils literal notranslate"><span class="pre">device_info</span></code> object of the <code class="docutils literal notranslate"><span class="pre">CPUDeviceInfo</span></code> class.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
<p>Because the default CPU settings are used, you can directly add the <code class="docutils literal notranslate"><span class="pre">device_info</span></code> object to the <code class="docutils literal notranslate"><span class="pre">context</span></code> device management list without making any modification.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>
</pre></div>
</div>
<p>(3) Load the model.</p>
<p>Create a <code class="docutils literal notranslate"><span class="pre">Model</span></code> class object <code class="docutils literal notranslate"><span class="pre">model</span></code>. The <code class="docutils literal notranslate"><span class="pre">Model</span></code> class defines the model in MindSpore for computational graph management.
For details about the <code class="docutils literal notranslate"><span class="pre">Model</span></code> class, see <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/generate/classmindspore_Model.html">API</a>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
</pre></div>
</div>
<p>Call the <code class="docutils literal notranslate"><span class="pre">Build</span></code> API to transfer the model and build the model to a state that can run on the device.
After the model is loaded and built, the parsed model information is recorded in the <code class="docutils literal notranslate"><span class="pre">model</span></code> variable, and the original model file memory <code class="docutils literal notranslate"><span class="pre">model_buf</span></code> can be released.
Because <code class="docutils literal notranslate"><span class="pre">model_buf</span></code> is applied for in <code class="docutils literal notranslate"><span class="pre">char</span></code> array mode, <code class="docutils literal notranslate"><span class="pre">delete[]</span></code> is used to release the memory.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
</pre></div>
</div>
<p>(4) Input data.</p>
<p>Before performing model inference, you need to set the input data for inference.
In this example, the <code class="docutils literal notranslate"><span class="pre">Model.GetInputs</span></code> API is used to obtain all input tensors of a model. The format of a single tensor is <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.
For details about <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>, see <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/generate/classmindspore_MSTensor.html">API</a>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">MutableData</span></code> API of the tensor can be used to obtain the data memory pointer of the tensor.
In this example, the input to the model is in floating-point format, so the pointer is forcibly converted to a floating-point pointer. You can process the data based on the data format of your model or obtain the data type of the tensor by using the <code class="docutils literal notranslate"><span class="pre">DataType</span></code> API of the tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">());</span>
</pre></div>
</div>
<p>Then, the data to be inferred is transferred to the tensor through the data pointer.
In this example, the input is a randomly generated floating-point number ranging from 0.1 to 1, and the data is evenly distributed.
In actual inference, after reading actual data such as images or audio files, you need to perform algorithm-specific preprocessing and transfer the processed data to the model.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">Distribution</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">GenerateRandomData</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Distribution</span><span class="w"> </span><span class="n">distribution</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">mt19937</span><span class="w"> </span><span class="n">random_engine</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">elements_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">);</span>
<span class="w">  </span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">generate_n</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">elements_num</span><span class="p">,</span>
<span class="w">                       </span><span class="p">[</span><span class="o">&amp;</span><span class="n">distribution</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">random_engine</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">distribution</span><span class="p">(</span><span class="n">random_engine</span><span class="p">));</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>

<span class="p">...</span>

<span class="n">GenerateRandomData</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">(),</span><span class="w"> </span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">uniform_real_distribution</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">0.1f</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">));</span>
</pre></div>
</div>
<p>(5) Perform inference.</p>
<p>Apply for an array <code class="docutils literal notranslate"><span class="pre">outputs</span></code> for storing the model inference output tensor, call the model inference API <code class="docutils literal notranslate"><span class="pre">Predict</span></code>, and use the input and output tensors as parameters.
After the inference is successful, the output tensor is saved in <code class="docutils literal notranslate"><span class="pre">outputs</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
</pre></div>
</div>
<p>(6) Verify the inference result.</p>
<p>Obtain the data pointer of the output tensor by using <code class="docutils literal notranslate"><span class="pre">MutableData</span></code>.
In this example, it is forcibly converted to a floating-point pointer. You can convert the data type based on the data type of your model or obtain the data type by using the <code class="docutils literal notranslate"><span class="pre">DataType</span></code> API of the tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">());</span>
</pre></div>
</div>
<p>In this example, you can view the accuracy of the inference output in the printed result.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>(7) Release the model object.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>Perform build.</p>
<p>Go to the <code class="docutils literal notranslate"><span class="pre">build</span></code> directory, enter <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">..</span></code> to generate a makefile, and enter <code class="docutils literal notranslate"><span class="pre">make</span></code> to build the project. After the build is successful, you can obtain the <code class="docutils literal notranslate"><span class="pre">demo</span></code> executable program in the <code class="docutils literal notranslate"><span class="pre">build</span></code> directory.</p>
</li>
<li><p>Run the inference program.</p>
<p>Enter <code class="docutils literal notranslate"><span class="pre">./demo</span></code> to execute the <code class="docutils literal notranslate"><span class="pre">demo</span></code> program. According to the preceding description, the <code class="docutils literal notranslate"><span class="pre">demo</span></code> program loads the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model, transfers the randomly generated input tensor to the model for inference, and prints the value of the output tensor after inference.
If the inference is successful, the following output is displayed. The output tensor contains 1000 values, which is consistent with the understanding of the model obtained in <a class="reference internal" href="#netron-visualization"><span class="std std-doc">Netron Visualization</span></a>. In this example, the input data is evenly distributed from 0.1 to 1 (which can be considered as noise). Therefore, the output has no classification characteristics:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------- print outputs ----------
out tensor name is:Default/head-MobileNetV2Head/Softmax-op204
out tensor size is:4000
out tensor elements num is:1000
output data is:5.26822e-05 0.000497521 0.000296722 0.000377606 0.000177048 8.02106e-05 0.000212863 0.000422287 0.000273189 0.000234106 0.000998072 0.00423312 0.00204994 0.00124968 0.00294459 0.00139796 0.00111545 0.00065636 0.000809462 0.00153732 0.000621052 0.00224638 0.00127046 0.00187558 0.000420145 0.000150638 0.000266477 0.000438629 0.000187774 0.00054668 0.000212853 0.000921661 0.000127179 0.000565873 0.00100395 0.00030016 0.000282677 0.000358068 0.00215288 0.000477846 0.00107597 0.00065134 0.000722135 0.000807503 0.000631416 0.000432471 0.00125898 0.000255094 8.26058e-05 9.91919e-05 0.000794514 0.00031873 0.000525145 0.000564177 0.000390949 0.000239435 0.000769301 0.000522169 0.000489711 0.00106033 0.00188065 0.00162756 0.000154417 0.000423661 0.00114033 0.000521169 0.00104491 0.000394101 0.000574376 0.00103071 0.000328134 0.00220263 0.000588063 0.00114022 0.000639888 0.00160103 0.000883627 0.00168485 0.00749697 0.00378326 0.00049545 0.000697699 0.00094152 0.000694751 0.000361998 0.00249769 0.00224123 0.00144733 0.000867953 0.000409967 0.000414645 0.000921754 0.00362981 0.000598768 0.00939566 0.000354318 0.0011853 0.000582604 0.000977179 0.000363443 0.000252788 0.000161903 0.000498172 0.000835043 0.000125615 0.000150972 0.000271722 0.000391777 8.49806e-05 0.000175627 0.000255629 0.0104205 0.000473356 0.000470714 0.00154926 3.52034e-05 0.00017297 0.000381467 0.000286569 0.00022002 0.000270967 0.00012511 0.000102305 0.000113712 0.000152496 0.00216914 0.000232594 0.00118621 0.00120123 0.000756038 0.000361149 0.000279887 0.00072076 0.0030916 0.000839053 0.000305989 0.000185089 0.00106419 0.00141358 0.000819862 0.000874739 0.00194274 0.000707348 0.00158608 0.000395842 0.000749171 0.00119562 0.000445385 0.000481742 7.57984e-05 0.000101538 0.000709718 0.000151491 0.00051427 0.000212376 0.000216051 9.55411e-05 0.000147092 0.00030403 9.3476e-05 5.85228e-05 0.000247954 0.000708926 0.00022098 0.000342199 0.000117494 0.000191572 3.63169e-05 0.000411851 0.000342481 0.000239097 0.000764161 0.000259073 0.000524563 0.000426145 0.000111397 0.000177984 8.50417e-05 0.000275155 0.000141314 0.000509691 0.000179604 0.000770131 0.000168981 0.000312896 0.000104055 9.1071e-05 0.000408717 8.05139e-05 0.000312057 0.000296877 0.000172418 0.00024341 0.000300782 0.000146993 0.00109211 0.000191816 8.35939e-05 0.000299942 0.000315375 0.000193755 0.000319056 0.000516599 0.000504943 0.000136374 0.000324095 0.000102209 0.000352826 0.000103771 0.000373529 0.000360807 0.000296265 0.000313525 0.000118756 0.000198175 0.000219075 0.000174439 0.000216093 0.000438399 0.000296872 0.000128021 0.00017442 0.000189079 0.000399597 0.000100693 0.000123358 5.15012e-05 0.000218214 0.000222177 0.000299965 0.000147799 0.000234641 0.000149353 4.5897e-05 0.000133614 0.000225688 0.000322703 0.000510069 0.000426839 0.000150078 6.61004e-05 4.68818e-05 0.000280284 0.000124997 0.000113089 0.000687338 0.000183928 0.000232998 0.00018996 0.00016634 9.61161e-05 0.000261457 7.62777e-05 0.000892919 0.00027851 4.25679e-05 0.00012095 0.000143962 0.000543232 0.00019522 0.000152532 8.21291e-05 5.86343e-05 0.000454828 0.000232324 0.000326869 0.00050617 8.3308e-05 8.23556e-05 7.82488e-05 0.000349937 0.000162254 0.000584313 0.000380654 7.41325e-05 0.000328623 0.00052962 0.000750176 0.000374926 0.000511254 0.000546927 0.000420431 0.000673729 0.000211782 0.00163466 0.000524799 0.000383476 0.000244811 7.51562e-05 6.57744e-05 0.000155914 0.000270638 0.000106245 0.000186127 0.000346968 0.000485479 0.000271254 0.00036631 0.000252093 0.000184659 0.000340458 0.00393658 0.00120467 0.00258523 0.000523741 0.00142551 0.00168216 0.00274844 0.00230136 0.000254464 0.000689839 0.00200172 0.000789165 0.00147548 0.00497233 0.00245074 0.00351014 0.000964297 0.0116707 0.00263743 0.000911238 0.000140097 0.000427111 0.000229297 0.000354368 0.000327572 0.000399973 0.000969767 0.000753985 0.000151906 0.000319341 0.00177747 0.00014731 0.000247144 0.00028714 0.000162237 0.000406454 0.000167767 0.000141812 8.20427e-05 0.000140652 0.000154833 0.000414694 0.000191989 0.00028065 0.000298302 0.000326194 0.000358242 0.000218887 0.000214568 0.000456112 0.000153574 5.4711e-05 0.000176373 0.000716305 6.97331e-05 0.000924458 0.00036906 0.000147747 0.000464726 0.000195069 0.000472077 0.000196377 0.000422707 0.000132992 5.76273e-05 0.000180634 0.000355361 0.000247252 0.000157627 0.000537573 0.00020566 0.000577524 0.00019596 0.000227313 0.000237615 0.000251934 0.000581737 0.000156606 0.000377661 0.000534264 9.59369e-05 0.000165362 0.000174582 7.18626e-05 0.000134693 4.02814e-05 0.000179219 0.000100272 9.8463e-05 0.000262976 0.000178799 0.000224355 8.18936e-05 0.000143329 0.000117873 8.40231e-05 0.000588662 0.000158744 0.00069335 0.000287121 0.000151016 0.00152298 0.00024393 0.000737831 0.00115437 5.96499e-05 0.000118379 0.000228003 0.0041712 5.89845e-05 0.00273432 0.00321251 0.00269996 0.000762481 4.82307e-05 0.000160988 0.00115545 0.0155078 0.00138022 0.0025505 0.000223013 0.000251236 0.000123665 5.52253e-05 0.000267688 0.000453393 0.00029877 0.000429822 0.00099786 0.000183652 0.000397013 0.00108393 0.000333911 0.0008731 0.000275806 0.000101959 0.000920896 0.000532173 0.000526293 0.0006834 0.000935434 0.000351484 0.00198101 0.000158832 0.00025276 0.0309715 0.000236896 0.000507701 7.17417e-05 0.000136413 0.00511946 0.001006 0.00030655 0.000170018 0.00102066 0.000676819 0.00111926 0.00101352 0.00122263 0.000436026 0.000709552 0.00280173 0.000343102 0.000684757 0.00250305 8.5246e-05 8.35988e-05 8.50596e-05 0.000745612 0.000384923 0.000115974 0.000104449 0.00142474 0.000464432 0.00013609 4.29949e-05 0.000410546 0.000318726 8.40787e-05 0.00206693 0.00057538 0.000382494 0.000160234 0.000307552 0.000529971 0.000586405 0.00398225 0.00151492 0.00026454 0.000511842 9.7473e-05 0.000163672 0.000160056 0.000816508 3.00784e-05 0.00037759 0.00014328 8.48268e-05 0.00142338 6.22116e-05 0.000788073 0.00155491 0.00121945 0.000680781 0.000758789 0.000459647 0.00708145 0.00120801 7.03766e-05 0.000364867 0.000123017 0.00420891 0.000513928 0.00123761 0.000267312 0.000333363 0.00122328 0.000298812 0.000238888 0.000615765 8.10465e-05 0.000246716 0.00123949 0.000508113 7.77746e-05 0.000487965 0.000462255 0.000310659 0.000585418 0.00176246 0.000181668 0.000288837 0.000232032 0.00549264 0.000113551 0.000251434 0.000276892 0.000604927 0.00410441 0.000628254 0.000532845 0.00177639 0.000769542 0.000172925 0.00065605 0.0015078 4.19799e-05 0.000255064 0.00488681 0.000521465 0.000326431 0.00111252 0.00235686 0.000651842 8.37604e-05 0.00319951 0.000679279 0.00160411 0.000953606 0.00047153 8.01442e-05 0.00192255 0.0110213 0.000130118 0.00018916 0.00082058 0.000194114 0.000183411 0.000152358 0.000211961 5.22587e-05 0.00303399 0.000128953 0.00159357 0.000101542 5.38654e-05 0.000206161 0.000293241 0.000191215 7.02916e-05 0.000230206 0.000109719 0.000682147 0.000378998 0.000515589 0.000204293 0.00115819 0.00252224 0.00132761 4.51228e-05 0.00333054 0.000486169 0.000733327 0.000177619 9.41916e-05 0.00120342 0.00432701 0.000222835 0.000197637 0.00449768 0.00115172 0.000184445 0.000111001 0.00112382 0.0018688 0.00320062 0.000278918 0.000906152 0.000116432 0.00164653 0.000537722 0.000249092 0.00221725 0.000161599 0.000414339 0.00299422 0.000435541 0.00880695 0.00490311 0.00325869 6.05041e-05 0.00458625 0.00517385 0.00024982 0.000220774 0.0032148 0.000275533 0.00222638 0.00206151 0.000420763 0.00028658 0.0149218 0.000693565 6.89355e-05 0.000175107 0.000611934 0.000185402 0.00048781 0.00104491 0.000305031 0.000719747 0.000464874 0.000902618 0.00710998 0.00028243 0.000266798 0.000557195 0.00018203 0.000165886 0.00432344 0.0018616 0.00081676 0.000688068 0.000116212 0.00375912 0.00011202 0.0119426 0.000395667 0.00134768 0.000107723 8.29395e-05 0.00874447 0.000217795 0.00201653 0.000200428 0.000784866 0.000739253 0.000223862 0.000716373 9.37279e-05 1.64484e-05 0.000103597 0.00134084 0.00208305 6.15101e-05 0.000264137 0.00421874 0.000816694 0.019055 0.000882248 0.0265989 0.000885313 0.00189269 0.000819798 0.000479354 0.000194866 4.39721e-05 0.000374197 0.00102411 0.000391648 0.000144945 0.000320067 0.000943551 6.28455e-05 0.000563089 0.00319211 0.000219879 8.42234e-05 0.000555672 0.00231883 0.0037087 0.000302303 0.000149123 0.000789137 7.45903e-05 0.000133478 0.000470522 0.000542576 0.000413181 0.000967243 0.00134348 0.000439858 0.0010091 0.00714279 0.000202303 0.000809548 8.99185e-05 0.000199892 0.00059308 0.00129059 0.00162076 0.00793667 0.000529655 0.000417269 0.00100714 0.000160703 0.00097642 0.000691081 7.56624e-05 0.000217106 0.00290805 0.000661668 0.00104081 0.000133569 0.000945062 0.00132827 0.000932787 0.00482219 3.9986e-05 0.000903322 0.000455647 0.00143754 0.000103266 0.00367346 0.000897197 0.000118318 0.00149419 0.000865034 0.00126782 0.00090065 0.000132982 0.0039552 0.00210961 0.000428278 0.000123607 0.000284831 2.11637e-05 0.000587767 0.000752392 0.00159891 0.00253384 4.46648e-05 0.00597254 0.00373919 0.000849701 4.3499e-05 0.000935258 0.000311729 0.00719802 0.000368296 0.00284921 0.00317468 0.000813635 0.0011214 0.000610401 0.000484875 0.00417738 0.000496244 9.79432e-05 0.000734274 0.000259079 0.00247699 0.00460816 0.00708891 0.000724271 0.00048205 0.000174656 0.000596118 0.000401012 8.25042e-05 0.000161686 0.00197722 0.000806688 0.00684481 0.000596325 0.00131103 0.000204451 0.00100593 0.00151624 8.50725e-05 0.000122174 0.00021799 0.000259111 0.002961 0.000829398 0.000533044 5.0536e-05 0.000946751 6.78423e-05 0.000485367 0.00306399 0.00523905 0.00123471 0.000224707 0.000101096 0.0014873 0.000104553 0.00355624 0.000205465 0.000169472 5.07939e-05 0.000195914 0.000791247 0.000246651 0.000205654 0.000285258 0.000651622 0.00211643 6.79842e-05 0.000138115 0.00103942 0.000187132 0.000409764 0.00214586 0.000292729 0.00031472 0.000691548 0.000382784 0.000125186 0.00233764 0.000536727 0.000502022 4.95937e-05 0.0264263 0.00477407 0.00376776 0.00014371 0.00137865 0.00109858 0.000563498 0.00261839 0.00397829 0.000242258 0.000141749 0.00157776 0.00031561 0.000136863 0.000277254 0.000887197 5.00407e-05 0.0031923 0.000459011 9.37109e-05 0.000129428 9.72145e-05 0.000116087 5.26294e-05 0.000929531 0.00363911 0.000738978 0.000344878 0.00242673 0.000193775 4.87371e-05 0.0010458 0.00015866 0.000108444 7.05613e-05 0.000979656 0.000203967 0.000434424 0.00147155 0.00623083 0.000167943 0.00654287 0.000231375 0.000144977 7.44322e-05 0.000271412 0.000257479 0.000125951 0.0084965 0.00708424 0.000741149 0.000327848 0.00072125 0.00155309 0.000849641 0.000468936 0.000597561 0.000343363 0.0013401 0.000644772 0.00296955 0.00203899 0.000344333 0.000654109 0.000579819 0.000307663 0.00295462 0.00098848 0.000224191 0.000616008 0.000192669 0.000124413 0.000265971 6.58702e-05 0.00031542 0.000253495 0.000276654 0.00289865 0.000574721 0.000131497 0.000204278 0.000568842 9.16333e-05 0.000257384 0.000243244 0.000443263 0.000109194 0.000139178 0.000500078 0.000198004 0.000107967 0.000169954 0.000123934 0.000258871 9.86871e-05 0.000293493 0.000323952 0.000504283 0.00036182 0.000316952 0.000177675 0.00168864 8.59652e-05 7.83207e-05 0.000429965 0.000663861 0.000506927 0.000768278 0.000391109 0.000419152 0.000638448 0.000214176 0.00016114 0.000515638 0.000709203 0.000154853 0.000343536 0.000326896 0.000130624 7.31075e-05 6.40462e-05 0.000139256 0.000695747 0.000437555 0.00208446 0.000180979 0.000100821 0.000760209 0.000238104 9.52171e-05 0.000459536 0.000388741 0.000291907 0.000577998 0.000439995 9.34035e-05 0.000309451 0.000308385 0.00012725 6.17104e-05 0.000231779 0.000489432 0.00012118 0.000211306 0.000357186 0.000356726 0.000311104 0.000615516 0.000252385 0.000400029 0.000204223 0.000195018 7.65522e-05 0.00028094 0.000303784 0.00186481
------- print end ----------
</pre></div>
</div>
</li>
<li><p>See more detailed integration description.</p>
<p>For details about how to use the C++ API for integration and advanced usage, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_cpp.html">Using C++ Interface to Perform Inference</a>.</p>
<p>For details about how to use the Java API for integration and development, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_java.html">Using Java Interface to Perform Inference</a>.</p>
<p>For details about Android integration and development, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/quick_start/quick_start.html">Android Application Development Based on JNI Interface</a>, <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a>, and <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/model_lite.html">Model List</a>.</p>
</li>
</ol>
</section>
</section>
</section>
<section id="for-windows">
<h2>For Windows<a class="headerlink" href="#for-windows" title="Permalink to this headline"></a></h2>
<p>This section describes how to convert a model and perform integration and inference on Windows.</p>
<section id="converting-a-model-1">
<h3>Converting a Model<a class="headerlink" href="#converting-a-model-1" title="Permalink to this headline"></a></h3>
<p>Convert a model to a MindSpore Lite model file. This operation includes the following steps:</p>
<ol class="arabic">
<li><p>Download a release package.</p>
<p>Both Windows and Linux release packages contain the converter. You can download any release package based on your platform.
Use the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> tool in the release package to convert a non-<code class="docutils literal notranslate"><span class="pre">ms</span></code> model into the <code class="docutils literal notranslate"><span class="pre">ms</span></code> model used by MindSpore Lite.
The same converted model file can be obtained regardless of the platform where the conversion is performed.</p>
</li>
<li><p>Convert the model.</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">converter_lite</span></code> tool in the release package to convert the model.</p>
</li>
</ol>
<section id="downloading-the-release-package-2">
<h4>Downloading the Release Package<a class="headerlink" href="#downloading-the-release-package-2" title="Permalink to this headline"></a></h4>
<p>You can download MindSpore Lite from the <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/downloads.html">official website</a>.
In this example, we use MindSpore Lite 1.6.0 (download <a class="reference external" href="https://ms-release.obs.cn-north-4.myhuaweicloud.com/1.6.0/MindSpore/lite/release/windows/mindspore-lite-1.6.0-win-x64.zip">here</a>) and a CPU release package with Windows OS and the x86_64 underlying architecture.
The structure of each release package varies. In this example, the structure of the Windows release package is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-win-x64
├── runtime
│   ├── include
│   └── lib
│       ├── libgcc_s_seh-1.dll      # MinGW dynamic library
│       ├── libmindspore-lite.a     # Static library of the MindSpore Lite inference framework
│       ├── libmindspore-lite.dll   # Dynamic library of the MindSpore Lite inference framework
│       ├── libmindspore-lite.dll.a # Link file of the dynamic library of the MindSpore Lite inference framework
│       ├── libssp-0.dll            # MinGW dynamic library
│       ├── libstdc++-6.dll         # MinGW dynamic library
│       └── libwinpthread-1.dll     # MinGW dynamic library
└── tools
    ├── benchmark # Directory of the benchmark test tool
    └── converter # Directory of the converter
</pre></div>
</div>
</section>
<section id="converting-the-model-1">
<h4>Converting the Model<a class="headerlink" href="#converting-the-model-1" title="Permalink to this headline"></a></h4>
<p>Decompress the downloaded release package and find the <code class="docutils literal notranslate"><span class="pre">converter_lite.exe</span></code> tool in the <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-win-x64\tools\converter\converter</span></code> directory.
The <code class="docutils literal notranslate"><span class="pre">converter_lite.exe</span></code> provides the offline model conversion function for the MindSpore, CAFFE, TensorFlow Lite, TensorFlow and ONNX models.
The model conversion procedure is as follows:</p>
<ol class="arabic">
<li><p>Set up the environment.</p>
<p>Add the dynamic link library required by the converter to the environment variable PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">set</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>%PACKAGE_ROOT_PATH%<span class="se">\t</span>ools<span class="se">\c</span>onverter<span class="se">\l</span>ib<span class="p">;</span>%PATH%
</pre></div>
</div>
</li>
<li><p>Go to the directory where the converter is stored.</p>
<p>Run the following command to go to the directory where the converter is stored:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>%PACKAGE_ROOT_PATH%<span class="se">\t</span>ools<span class="se">\c</span>onverter<span class="se">\c</span>onverter
</pre></div>
</div>
</li>
<li><p>Set conversion parameters.</p>
<p>When using converter_lite.exe to perform conversion, you need to set related parameters. Table 3 describes the parameters used in this example.</p>
<p>The following uses the conversion commands for various models as examples to describe how to use the parameters.</p>
<ul>
<li><p>Command for converting the Caffe model <code class="docutils literal notranslate"><span class="pre">lenet.prototxt</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>converter_lite.exe<span class="w"> </span>--fmk<span class="o">=</span>CAFFE<span class="w"> </span>--modelFile<span class="o">=</span>lenet.prototxt<span class="w"> </span>--weightFile<span class="o">=</span>lenet.caffemodel<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
<p>When converting a Caffe model, set <code class="docutils literal notranslate"><span class="pre">fmk</span></code> to CAFFE (<code class="docutils literal notranslate"><span class="pre">--fmk=CAFFE</span></code>), and transfer the model structure file (lenet.prototxt) and model weight file (lenet.caffemodel) by using the modelFile and weightFile parameters, respectively.
In addition, use outputFile to specify the name of the output model after conversion. Because the path is not specified, the generated model is in the current path by default and has the suffix <code class="docutils literal notranslate"><span class="pre">.ms</span></code>, that is, <code class="docutils literal notranslate"><span class="pre">lenet.ms</span></code>.</p>
</li>
<li><p>Commands for converting the MindSpore, TensorFlow Lite, TensorFlow, and ONNX models</p>
<ul>
<li><p>MindSpore model <code class="docutils literal notranslate"><span class="pre">lenet.mindir</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>converter_lite.exe<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>lenet.mindir<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
</li>
<li><p>TensorFlow Lite model <code class="docutils literal notranslate"><span class="pre">lenet.tflite</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>converter_lite.exe<span class="w"> </span>--fmk<span class="o">=</span>TFLITE<span class="w"> </span>--modelFile<span class="o">=</span>lenet.tflite<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
</li>
<li><p>TensorFlow model <code class="docutils literal notranslate"><span class="pre">lenet.pb</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>converter_lite.exe<span class="w"> </span>--fmk<span class="o">=</span>TF<span class="w"> </span>--modelFile<span class="o">=</span>lenet.pb<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
</li>
<li><p>ONNX model <code class="docutils literal notranslate"><span class="pre">lenet.onnx</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>converter_lite.exe<span class="w"> </span>--fmk<span class="o">=</span>ONNX<span class="w"> </span>--modelFile<span class="o">=</span>lenet.onnx<span class="w"> </span>--outputFile<span class="o">=</span>lenet
</pre></div>
</div>
</li>
</ul>
<p>During model conversion, set <code class="docutils literal notranslate"><span class="pre">fmk</span></code> to a symbol corresponding to the model type and transfer the model file by using the modelFile parameter.
Use outputFile to specify the name of the output model after conversion. Because the path is not specified, the generated model is in the current path by default and has the suffix <code class="docutils literal notranslate"><span class="pre">.ms</span></code>, that is, <code class="docutils literal notranslate"><span class="pre">lenet.ms</span></code>.</p>
</li>
</ul>
</li>
<li><p>Execute the conversion.</p>
<p>You can use your own model or click <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/mobilenetv2.mindir">here</a> to download a MindSpore model for testing.
Take the downloaded model as an example. Copy the model <code class="docutils literal notranslate"><span class="pre">mobilenetv2.mindir</span></code> to the directory where the converter is located. The model conversion command is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>call<span class="w"> </span>converter_lite.exe<span class="w"> </span>--fmk<span class="o">=</span>MINDIR<span class="w"> </span>--modelFile<span class="o">=</span>mobilenetv2.mindir<span class="w"> </span>--outputFile<span class="o">=</span>mobilenetv2
</pre></div>
</div>
<p>If the conversion is successful, the following information is displayed, and a new model file named <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> is generated in the current directory:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONVERT RESULT SUCCESS:0
</pre></div>
</div>
</li>
<li><p>Perform advanced functions.</p>
<p>For details about the converter, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/converter_tool.html">Converting Models for Inference</a>.</p>
<p>For details about how to use the converter to implement post training quantization, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/post_training_quantization.html">Post Training Quantization</a>.</p>
<p>If you want to train a converted model, you need to convert a training model. For details, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/converter_train.html">Creating MindSpore Lite Models</a>.</p>
</li>
</ol>
<p>Table 3: converter_lite.exe parameters</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Mandatory or Not</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Value Range</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--fmk=&lt;FMK&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Original format of the model to be converted.</p></td>
<td><p>MINDIR, CAFFE, TFLITE, TF, or ONNX</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--modelFile=&lt;MODELFILE&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Path of the model to be converted.</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--outputFile=&lt;OUTPUTFILE&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Path and name of the converted model. The suffix .ms is automatically generated.</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--weightFile=&lt;WEIGHTFILE&gt;</span></code></p></td>
<td><p>Yes for Caffe model conversion</p></td>
<td><p>Path of the input model weight file.</p></td>
<td><p>-</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>The parameter name and value are separated by an equal sign (=) and no space is allowed between them.</p></li>
<li><p>Generally, a Caffe model has two files: the model structure <code class="docutils literal notranslate"><span class="pre">*.prototxt</span></code>, which corresponds to the <code class="docutils literal notranslate"><span class="pre">--modelFile</span></code> parameter, and the model weight <code class="docutils literal notranslate"><span class="pre">*.caffemodel</span></code>, which corresponds to the <code class="docutils literal notranslate"><span class="pre">--weightFile</span></code> parameter.</p></li>
</ul>
</div></blockquote>
</section>
<section id="netron-visualization-1">
<h4>Netron Visualization<a class="headerlink" href="#netron-visualization-1" title="Permalink to this headline"></a></h4>
<p>For details, see <a class="reference internal" href="#netron-visualization"><span class="std std-doc">Netron Visualization</span></a> in Linux.</p>
</section>
</section>
<section id="model-inference-1">
<h3>Model Inference<a class="headerlink" href="#model-inference-1" title="Permalink to this headline"></a></h3>
<p>You need to integrate the <code class="docutils literal notranslate"><span class="pre">mindspore-lite</span></code> library file in the release package and use the APIs declared in the MindSpore Lite header file to perform model inference.
Before integration, you can also use the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool (stored in <code class="docutils literal notranslate"><span class="pre">mindspore-lite-{version}-win-x64/tools/benchmark</span></code>) released with the release package to perform inference tests.
The <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool is an executable program that integrates the <code class="docutils literal notranslate"><span class="pre">mindspore-lite</span></code> library. It uses command parameters to implement multiple functions, including inference.</p>
<section id="downloading-the-release-package-3">
<h4>Downloading the Release Package<a class="headerlink" href="#downloading-the-release-package-3" title="Permalink to this headline"></a></h4>
<p>Download a release package based on the system environment used for model inference.
In this example, we use MindSpore Lite 1.6.0 (download <a class="reference external" href="https://ms-release.obs.cn-north-4.myhuaweicloud.com/1.6.0/MindSpore/lite/release/windows/mindspore-lite-1.6.0-win-x64.zip">here</a>) and a CPU release package with Windows OS and the x86_64 underlying architecture.</p>
</section>
<section id="benchmark-inference-test-1">
<h4>Benchmark Inference Test<a class="headerlink" href="#benchmark-inference-test-1" title="Permalink to this headline"></a></h4>
<ol class="arabic">
<li><p>Set up the environment.</p>
<p>Add the dynamic link library required for <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> inference to the environment variable PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">set</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>%PACKAGE_ROOT_PATH%<span class="se">\r</span>untime<span class="se">\l</span>ib<span class="p">;</span>%PATH%
</pre></div>
</div>
</li>
<li><p>Go to the directory where the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool is located.</p>
<p>Run the following command to go to the directory where the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool is located:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>%PACKAGE_ROOT_PATH%<span class="se">\t</span>ools<span class="se">\b</span>enchmark
</pre></div>
</div>
</li>
<li><p>Set the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> parameters.</p>
<p>When using <code class="docutils literal notranslate"><span class="pre">benchmark.exe</span></code> to perform inference, you need to set related parameters. Table 4 describes the parameters used in this example.</p>
</li>
<li><p>Execute inference and analyze inference performance.</p>
<p>You can use the converted model <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> to perform the inference test. Copy the model to the directory where the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> tool is located and run the following command to perform inference:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>benchmark.exe<span class="w"> </span>--modelFile<span class="o">=</span>mobilenetv2.ms
</pre></div>
</div>
<p>Use <code class="docutils literal notranslate"><span class="pre">modelFile</span></code> to specify the model to be inferred and use the default values for other parameters.
In this example, if no input data is specified, a random value is generated as the input.
After the command is executed, if the inference is successful, information similar to the following is displayed. The information shows performance indicators such as the number of concurrent threads during inference (<code class="docutils literal notranslate"><span class="pre">NumThreads</span> <span class="pre">=</span> <span class="pre">2</span></code>), the minimum duration of a single inference of the test model (<code class="docutils literal notranslate"><span class="pre">6.677000</span> <span class="pre">ms</span></code>), maximum duration of a single inference (<code class="docutils literal notranslate"><span class="pre">8.656000</span> <span class="pre">ms</span></code>), and average inference duration (<code class="docutils literal notranslate"><span class="pre">7.291000</span> <span class="pre">ms</span></code>). The performance value varies according to the environment.
Because the <code class="docutils literal notranslate"><span class="pre">numThreads</span></code> parameter is not specified, two threads are used for inference by default. You can set the number of threads to test the inference performance. (When the number of threads reaches a certain value, the inference time is prolonged due to the thread switchover overhead.)</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ModelPath = mobilenetv2.ms
ModelType = MindIR
InDataPath =
ConfigFilePath =
InDataType = bin
LoopCount = 10
DeviceType = CPU
AccuracyThreshold = 0.5
CosineDistanceThreshold = -1.1
WarmUpLoopCount = 3
NumThreads = 2
Fp16Priority = 0
EnableParallel = 0
calibDataPath =
cpuBindMode = HIGHER_CPU
CalibDataType = FLOAT
start unified benchmark run
PrepareTime = 30.013 ms
Running warm up loops...
Running benchmark loops...
Model = mobilenetv2.ms, NumThreads = 2, MinRunTime = 6.677000 ms, MaxRuntime = 8.656000 ms, AvgRunTime = 7.291000 ms
Run Benchmark mobilenetv2.ms Success.
</pre></div>
</div>
</li>
<li><p>Execute inference and analyze inference accuracy.</p>
<p>To use the <code class="docutils literal notranslate"><span class="pre">benchmark.exe</span></code> tool to test the inference accuracy of MindSpore Lite, run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>call<span class="w"> </span>benchmark.exe<span class="w"> </span>--modelFile<span class="o">=</span>mobilenetv2.ms<span class="w"> </span>--inDataFile<span class="o">=</span>input.bin<span class="w"> </span>--benchmarkDataFile<span class="o">=</span>output.txt
</pre></div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">modelFile</span></code> specifies the model to be inferred.</p>
<p><code class="docutils literal notranslate"><span class="pre">inDataFile</span></code> specifies the model input data file, which is set to <code class="docutils literal notranslate"><span class="pre">input.bin</span></code>.
The model is opened in the <code class="docutils literal notranslate"><span class="pre">Netron</span></code>, and we know that the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model receives the <code class="docutils literal notranslate"><span class="pre">1x224x224x3</span></code> tensor of <code class="docutils literal notranslate"><span class="pre">float32</span></code>.
The <code class="docutils literal notranslate"><span class="pre">inDataFile</span></code> option of <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> receives data files in binary format by default. The <code class="docutils literal notranslate"><span class="pre">input.bin</span></code> file saves 150528 <code class="docutils literal notranslate"><span class="pre">float32</span></code> binary values in sequence, which is the same as the <code class="docutils literal notranslate"><span class="pre">1x224x224x3</span></code> data volume required by the model, and the format is <code class="docutils literal notranslate"><span class="pre">float32</span></code>.</p>
<p>Generally, the input data file can be generated in the following ways:</p>
<ul class="simple">
<li><p>Pre-process the data in the model training dataset and save the pre-processed data.</p></li>
<li><p>Randomly generate data within a specified range.</p></li>
</ul>
<p>In this example, a randomly generated number is used as the input. You can run the following Python script or click <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/input.bin">here</a> to download the <code class="docutils literal notranslate"><span class="pre">input.bin</span></code> file and save it to the benchmark directory.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">t</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="s2">&quot;input.bin&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>After the input data is provided, you need to provide the benchmark data for comparison with the inference result for accuracy error analysis.
In this example, <code class="docutils literal notranslate"><span class="pre">benchmarkDataFile</span></code> specifies the model output benchmark file, which is set to <code class="docutils literal notranslate"><span class="pre">output.txt</span></code>. The format of the benchmark file is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[Name of output node 1][Dimension length of shape of output node 1][Value of the first dimension of shape of output node 1]... [Value of the n dimension of shape of output node 1]
[Node 1 data 1]  [Node 1 data 2]...
[Name of output node 2][Dimension length of shape of output node 2][Value of the first dimension of shape of output node 2]... [Value of the n dimension of shape of output node 2]
[Node 2 data 1]  [Node 2 data 2]...
</pre></div>
</div>
<p>Generally, the benchmark file can be generated in the following ways:</p>
<ul class="simple">
<li><p>Comparison with other frameworks: Use another deep learning model inference framework with the same input, and save the inference result in the required format.</p></li>
<li><p>Comparison with model training: In the training framework, save the pre-processed data as the input data specified by <code class="docutils literal notranslate"><span class="pre">inDataFile</span></code>. After model inference, save the output data that has not been post-processed in the benchmark file format, and use it as the benchmark.</p></li>
<li><p>Comparison with different devices or data types: Use different data types (such as FP16) or devices (such as GPU/NPU) for inference to obtain the benchmark in the environment.</p></li>
<li><p>Comparison with theoretical values: For some simple models, manually construct output benchmarks based on your understanding of the models.</p></li>
</ul>
<p>The size of the provided benchmark data must be the same as that of the model output so that the benchmark data can be compared with the model output to obtain the inference accuracy error.
The output node name of the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model is <code class="docutils literal notranslate"><span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code>, and the output shape of the node is <code class="docutils literal notranslate"><span class="pre">1x1000</span></code> (as shown in Figure 2). Therefore, the dimension length of the node shape is 2. The first dimension value of the node shape is 1, and the second dimension value of the node shape is 1000.
In this example, the benchmark is generated by comparing with other frameworks. The previously obtained <code class="docutils literal notranslate"><span class="pre">input.bin</span></code> file uses another framework to generate the inference data and is saved in the benchmark file format.
The benchmark data is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Default/head-MobileNetV2Head/Softmax-op204 2 1 1000
4.75662418466527e-05 0.00044544308912009 ...
</pre></div>
</div>
<p>The second row of data in the benchmark indicates the inference output of another framework with the same input (<code class="docutils literal notranslate"><span class="pre">input.bin</span></code>). You can click <a class="reference external" href="https://download.mindspore.cn/model_zoo/official/lite/quick_start/output.txt">here</a> to download the output.txt file in this example and save it to the benchmark directory.
After the <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> command is executed, if the inference is successful, information similar to the following is displayed:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ModelPath = mobilenetv2.ms
ModelType = MindIR
InDataPath = input.bin
ConfigFilePath =
InDataType = bin
LoopCount = 10
DeviceType = CPU
AccuracyThreshold = 0.5
CosineDistanceThreshold = -1.1
WarmUpLoopCount = 3
NumThreads = 2
Fp16Priority = 0
EnableParallel = 0
calibDataPath = output.txt
cpuBindMode = HIGHER_CPU
CalibDataType = FLOAT
start unified benchmark run
PrepareTime = 31.709 ms
MarkAccuracy
InData 0: 0.417022 0.720325 0.000114375 0.302333 0.146756 0.0923386 0.18626 0.345561 0.396767 0.538817 0.419195 0.68522 0.204452 0.878117 0.0273876 0.670467 0.417305 0.55869 0.140387 0.198101
================ Comparing Output data ================
Data of node Default/head-MobileNetV2Head/Softmax-op204 : 4.75662e-05 0.000445443 0.000294212 0.000354572 0.000165406 8.36175e-05 0.000198424 0.000329004 0.000288576 0.000203605 0.000962143 0.00421465 0.0019162 0.00129701 0.00260928 0.0012302 0.000879829 0.000609378 0.000691054 0.00119472 0.000516733 0.00160048 0.000959531 0.00176164 0.000365934 0.00013575 0.000245539 0.000414651 0.000165337 0.000480154 0.000216396 0.00101303 0.000105544 0.000475172 0.000761407 0.000305815 0.000294882 0.000307003 0.00188077 0.000454868 0.000897518 0.00051352 0.000595383 0.000644214 0.000513376 0.000343709 0.00103984 0.000197185 7.54722e-05 8.89811e-05
Mean bias of node/tensor Default/head-MobileNetV2Head/Softmax-op204 : 0%
Mean bias of all nodes/tensors: 0%
=======================================================

Run Benchmark mobilenetv2.ms Success.
</pre></div>
</div>
<p>In the output information, the <code class="docutils literal notranslate"><span class="pre">InData</span> <span class="pre">0</span></code> line displays the input data (only the first 20 values are displayed) of the inference, and the <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">of</span> <span class="pre">node</span> <span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code> line displays the inference result (only the first 50 values are displayed) of the related output node (<code class="docutils literal notranslate"><span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code>). You can directly observe the differences between them and the benchmark file.
In line <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">bias</span> <span class="pre">of</span> <span class="pre">node/tensor</span> <span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code>, the average error between the <code class="docutils literal notranslate"><span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code> output tensor and the benchmark data is provided. The error is computed using the comparison algorithm provided by the benchmark tool.
<code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">bias</span> <span class="pre">of</span> <span class="pre">all</span> <span class="pre">nodes/tensors</span></code> provides the average error of all tensors compared with the benchmark. In this example, there is only one output tensor. Therefore, the total average error is the same as that of the <code class="docutils literal notranslate"><span class="pre">Default/head-MobileNetV2Head/Softmax-op204</span></code> tensor. It can be observed that the total average error of inference is 0%.</p>
</li>
<li><p>Perform advanced functions.</p>
<p>For details about <code class="docutils literal notranslate"><span class="pre">benchmark</span></code>, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/benchmark_tool.html">benchmark</a>.</p>
</li>
</ol>
<p>Table 4 Definition of benchmark parameters</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Mandatory or Not</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--modelFile=&lt;MODELPATH&gt;</span></code></p></td>
<td><p>Yes</p></td>
<td><p>Specifies the path of the MindSpore Lite model file for which the benchmark test is to be performed.</p></td>
<td><p>String</p></td>
<td><p>null</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--numThreads=&lt;NUMTHREADS&gt;</span></code></p></td>
<td><p>No</p></td>
<td><p>Specifies the number of threads for running the model inference program.</p></td>
<td><p>Integer</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--inDataFile=&lt;INDATAPATH&gt;</span></code></p></td>
<td><p>No</p></td>
<td><p>Specifies the file path of the input data of the test model. By default, data files in binary format are received. If this parameter is not set, a random value is used.</p></td>
<td><p>String</p></td>
<td><p>null</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--benchmarkDataFile=&lt;CALIBDATAPATH&gt;</span></code></p></td>
<td><p>No</p></td>
<td><p>Specifies the file path of the benchmark data (for accuracy comparison) to be compared and receives the character text in the specified format.</p></td>
<td><p>String</p></td>
<td><p>null</p></td>
</tr>
</tbody>
</table>
<blockquote>
<div><ul class="simple">
<li><p>The parameter name and value are separated by an equal sign (=) and no space is allowed between them.</p></li>
</ul>
</div></blockquote>
</section>
<section id="integration-and-inference-1">
<h4>Integration and Inference<a class="headerlink" href="#integration-and-inference-1" title="Permalink to this headline"></a></h4>
<p>In the previous section, the official inference test tool is used to perform the model inference test. This section uses the C++ APIs of MindSpore Lite as an example to describe how to use the MindSpore Lite release package to perform integrated development and build your own inference program.</p>
<ol class="arabic">
<li><p>Environment Requirements</p>
<ul class="simple">
<li><p>System environment: 64-bit Windows 7 or 64-bit Windows 10</p></li>
<li><p>MinGW build dependencies</p>
<ul>
<li><p><a class="reference external" href="https://cmake.org/download/">CMake</a> &gt;= 3.18.3</p></li>
<li><p>64-bit build: <a class="reference external" href="https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win64/Personal%20Builds/mingw-builds/7.3.0/threads-posix/seh/x86_64-7.3.0-release-posix-seh-rt_v5-rev0.7z">MinGW-W64 x86_64</a> = GCC-7.3.0</p></li>
<li><p>32-bit build: <a class="reference external" href="https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/7.3.0/threads-posix/dwarf/i686-7.3.0-release-posix-dwarf-rt_v5-rev0.7z">MinGW-W64 i686</a> = GCC-7.3.0</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Obtain the release package.</p>
<p>Click <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/downloads.html">here</a> to obtain a MindSpore Lite release package.
The release package for integration and development in this example is the same as that in the previous sections. You can click <a class="reference external" href="https://ms-release.obs.cn-north-4.myhuaweicloud.com/1.6.0/MindSpore/lite/release/windows/mindspore-lite-1.6.0-win-x64.zip">here</a> to download the package.
The following content in the release package is required:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mindspore-lite-{version}-win-x64
└── runtime
    ├── include
    └── lib
        ├── libgcc_s_seh-1.dll      # MinGW dynamic library
        ├── libmindspore-lite.a     # Static library of the MindSpore Lite inference framework
        ├── libmindspore-lite.dll   # Dynamic library of the MindSpore Lite inference framework
        ├── libmindspore-lite.dll.a # Link file of the dynamic library of the MindSpore Lite inference framework
        ├── libssp-0.dll            # MinGW dynamic library
        ├── libstdc++-6.dll         # MinGW dynamic library
        └── libwinpthread-1.dll     # MinGW dynamic library
</pre></div>
</div>
</li>
<li><p>Build a project directory.</p>
<p>In this example, the project directory is as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>demo
├── CMakeLists.txt                  # CMake project management file
├── main.cc                         # User code
├── build                           # Build directory
├── model
│    └── mobilenetv2.ms             # Model file (the converted model)
└── runtime                         # Runtime directory of the release package
    ├── include                     # Header file of APIs integrated and developed by MindSpore Lite
    └── lib
        └── libmindspore-lite.so    # Dynamic library of the MindSpore Lite inference framework
</pre></div>
</div>
<p>Create a <code class="docutils literal notranslate"><span class="pre">demo</span></code> folder and create the <code class="docutils literal notranslate"><span class="pre">main.cc</span></code> and <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> files.
Create the <code class="docutils literal notranslate"><span class="pre">build</span></code> and <code class="docutils literal notranslate"><span class="pre">model</span></code> directories and place <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> in <code class="docutils literal notranslate"><span class="pre">model</span></code>.
Copy the <code class="docutils literal notranslate"><span class="pre">runtime</span></code> directory in the release package to <code class="docutils literal notranslate"><span class="pre">demo</span></code>. You can retain files in the <code class="docutils literal notranslate"><span class="pre">runtime</span></code> directory or delete library files except the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> and <code class="docutils literal notranslate"><span class="pre">include</span></code> folders.</p>
</li>
<li><p>Build a CMake project.</p>
<p>Open the created <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> file and paste the following content.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span>cmake_minimum_required(VERSION 3.12)  # The CMake version must be 3.12 or later.
project(Demo)  # The project name is Demo.

# The GCC version must be 7.3.0 or later.
if(CMAKE_CXX_COMPILER_ID STREQUAL &quot;GNU&quot; AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.3.0)
    message(FATAL_ERROR &quot;GCC version ${CMAKE_CXX_COMPILER_VERSION} must not be less than 7.3.0&quot;)
endif()

include_directories(${CMAKE_CURRENT_SOURCE_DIR}/runtime/)  # Add the header file search path.

link_directories(${CMAKE_CURRENT_SOURCE_DIR}/runtime/lib)  # Add the library file search path.

add_executable(demo main.cc)  # Build and generate the demo execution program.

# Declare the library to be linked to the demo execution program. mindspore-lite is the dynamic library of the MindSpore Lite inference framework.
target_link_libraries(
        demo
        mindspore-lite
        pthread
        ssp
)
</pre></div>
</div>
<blockquote>
<div><p>If you want to integrate the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.a</span></code> static library, replace <code class="docutils literal notranslate"><span class="pre">mindspore-lite</span></code> with the <code class="docutils literal notranslate"><span class="pre">-Wl,--whole-archive</span> <span class="pre">mindspore-lite</span> <span class="pre">-Wl,--no-whole-archive</span></code> option.</p>
</div></blockquote>
</li>
<li><p>Write code.</p>
<p>Open the created <code class="docutils literal notranslate"><span class="pre">main.cc</span></code> and paste the following content:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;fstream&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;random&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/model.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/context.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/status.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;include/api/types.h&quot;</span>
<span class="k">using</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">MSTensor</span><span class="p">;</span>

<span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="nf">ReadFile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="o">*</span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">file</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;file is nullptr.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">ifstream</span><span class="w"> </span><span class="n">ifs</span><span class="p">(</span><span class="n">file</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ifstream</span><span class="o">::</span><span class="n">in</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ifstream</span><span class="o">::</span><span class="n">binary</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">ifs</span><span class="p">.</span><span class="n">good</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;file: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; is not exist.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">ifs</span><span class="p">.</span><span class="n">is_open</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;file: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; open failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">ifs</span><span class="p">.</span><span class="n">seekg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">end</span><span class="p">);</span>
<span class="w">  </span><span class="o">*</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ifs</span><span class="p">.</span><span class="n">tellg</span><span class="p">();</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="kt">char</span><span class="p">[]</span><span class="o">&gt;</span><span class="w"> </span><span class="n">buf</span><span class="p">(</span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="kt">char</span><span class="p">[</span><span class="o">*</span><span class="n">size</span><span class="p">]);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">buf</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;malloc buf failed, file: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">file</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="n">ifs</span><span class="p">.</span><span class="n">close</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">ifs</span><span class="p">.</span><span class="n">seekg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">beg</span><span class="p">);</span>
<span class="w">  </span><span class="n">ifs</span><span class="p">.</span><span class="n">read</span><span class="p">(</span><span class="n">buf</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="o">*</span><span class="n">size</span><span class="p">);</span>
<span class="w">  </span><span class="n">ifs</span><span class="p">.</span><span class="n">close</span><span class="p">();</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">buf</span><span class="p">.</span><span class="n">release</span><span class="p">();</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">Distribution</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">GenerateRandomData</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Distribution</span><span class="w"> </span><span class="n">distribution</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">mt19937</span><span class="w"> </span><span class="n">random_engine</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">elements_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">);</span>
<span class="w">  </span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">generate_n</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">elements_num</span><span class="p">,</span>
<span class="w">                        </span><span class="p">[</span><span class="o">&amp;</span><span class="n">distribution</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">random_engine</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">distribution</span><span class="p">(</span><span class="n">random_engine</span><span class="p">));</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>

<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Read model file.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">model_path</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;../model/mobilenetv2.ms&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="kt">size_t</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model_buf</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Read model file failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Create and init context, add CPU device info</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New context failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">device_info</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New CPUDeviceInfo failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Create model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;New Model failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Build model</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="w">  </span><span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">build_ret</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Build model error &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Input</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">());</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">input_data</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;MallocData for inTensor failed.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">      </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">GenerateRandomData</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">(),</span><span class="w"> </span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">uniform_real_distribution</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">0.1f</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Predict</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">status</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Inference error.&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Get Output Tensor Data.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">------- print outputs ----------&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tensor</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">outputs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;out tensor name is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">Name</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">out tensor size is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">()</span>
<span class="w">              </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">out tensor elements num is:&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">());</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;output data is:&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;------- print end ----------</span><span class="se">\n</span><span class="s">&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Delete model.</span>
<span class="w">  </span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kSuccess</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The code function is parsed as follows:</p>
<p>(1) Read the model file to the buffer.</p>
<p>Call the <code class="docutils literal notranslate"><span class="pre">ReadFile</span></code> function to read the model file to the <code class="docutils literal notranslate"><span class="pre">model_buf</span></code> memory and use the <code class="docutils literal notranslate"><span class="pre">size</span></code> variable to save the model size.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">model_buf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ReadFile</span><span class="p">(</span><span class="n">model_path</span><span class="p">.</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span><span class="o">&amp;</span><span class="n">size</span><span class="p">);</span>
</pre></div>
</div>
<p>(2) Initialize the context configuration.</p>
<p>The context stores configurations required for model inference, including the operator preference, number of threads, automatic concurrency, and other configurations related to the inference processor.
For details about the context, see “Context” in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/generate/classmindspore_Context.html">API</a>.
When MindSpore Lite loads a model, an object of the <code class="docutils literal notranslate"><span class="pre">Context</span></code> class must be provided. In this example, the <code class="docutils literal notranslate"><span class="pre">context</span></code> object of the <code class="docutils literal notranslate"><span class="pre">Context</span></code> class is applied for.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">Context</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
<p>Then, use the <code class="docutils literal notranslate"><span class="pre">Context::MutableDeviceInfo</span></code> interface to obtain the device management list of the <code class="docutils literal notranslate"><span class="pre">context</span></code> object.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="o">&amp;</span><span class="n">device_list</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">context</span><span class="o">-&gt;</span><span class="n">MutableDeviceInfo</span><span class="p">();</span>
</pre></div>
</div>
<p>In this example, the CPU is used for inference. Therefore, you need to apply for the <code class="docutils literal notranslate"><span class="pre">device_info</span></code> object of the <code class="docutils literal notranslate"><span class="pre">CPUDeviceInfo</span></code> class.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">device_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_shared</span><span class="o">&lt;</span><span class="n">mindspore</span><span class="o">::</span><span class="n">CPUDeviceInfo</span><span class="o">&gt;</span><span class="p">();</span>
</pre></div>
</div>
<p>Because the default CPU settings are used, you can directly add the <code class="docutils literal notranslate"><span class="pre">device_info</span></code> object to the <code class="docutils literal notranslate"><span class="pre">context</span></code> device management list without making any modification.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">device_list</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">device_info</span><span class="p">);</span>
</pre></div>
</div>
<p>(3) Load the model.</p>
<p>Create a <code class="docutils literal notranslate"><span class="pre">Model</span></code> class object <code class="docutils literal notranslate"><span class="pre">model</span></code>. The <code class="docutils literal notranslate"><span class="pre">Model</span></code> class defines the model in MindSpore for computational graph management.
For details about the <code class="docutils literal notranslate"><span class="pre">Model</span></code> class, see <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/generate/classmindspore_Model.html">API</a>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">nothrow</span><span class="p">)</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">Model</span><span class="p">();</span>
</pre></div>
</div>
<p>Call the <code class="docutils literal notranslate"><span class="pre">Build</span></code> API to transfer the model and build the model to a state that can run on the device.
After the model is loaded and built, the parsed model information is recorded in the <code class="docutils literal notranslate"><span class="pre">model</span></code> variable, and the original model file memory <code class="docutils literal notranslate"><span class="pre">model_buf</span></code> can be released.
Because <code class="docutils literal notranslate"><span class="pre">model_buf</span></code> is applied for in <code class="docutils literal notranslate"><span class="pre">char</span></code> array mode, <code class="docutils literal notranslate"><span class="pre">delete[]</span></code> is used to release the memory.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">build_ret</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Build</span><span class="p">(</span><span class="n">model_buf</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">mindspore</span><span class="o">::</span><span class="n">kMindIR</span><span class="p">,</span><span class="w"> </span><span class="n">context</span><span class="p">);</span>
<span class="k">delete</span><span class="p">[](</span><span class="n">model_buf</span><span class="p">);</span>
</pre></div>
</div>
<p>(4) Input data.</p>
<p>Before performing model inference, you need to set the input data for inference.
In this example, the <code class="docutils literal notranslate"><span class="pre">Model.GetInputs</span></code> API is used to obtain all input tensors of a model. The format of a single tensor is <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>.
For details about <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code>, see <code class="docutils literal notranslate"><span class="pre">MSTensor</span></code> in <a class="reference external" href="https://www.mindspore.cn/lite/api/en/r2.3/generate/classmindspore_MSTensor.html">API</a>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">GetInputs</span><span class="p">();</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">MutableData</span></code> API of the tensor can be used to obtain the data memory pointer of the tensor.
In this example, the input to the model is in floating-point format, so the pointer is forcibly converted to a floating-point pointer. You can process the data based on the data format of your model or obtain the data type of the tensor by using the <code class="docutils literal notranslate"><span class="pre">DataType</span></code> API of the tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">());</span>
</pre></div>
</div>
<p>Then, the data to be inferred is transferred to the tensor through the data pointer.
In this example, the input is a randomly generated floating-point number ranging from 0.1 to 1, and the data is evenly distributed.
In actual inference, after reading actual data such as images or audio files, you need to perform algorithm-specific preprocessing and transfer the processed data to the model.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">Distribution</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">GenerateRandomData</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">Distribution</span><span class="w"> </span><span class="n">distribution</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">mt19937</span><span class="w"> </span><span class="n">random_engine</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">elements_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">);</span>
<span class="w">  </span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">generate_n</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="w"> </span><span class="n">elements_num</span><span class="p">,</span>
<span class="w">                       </span><span class="p">[</span><span class="o">&amp;</span><span class="n">distribution</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">random_engine</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">distribution</span><span class="p">(</span><span class="n">random_engine</span><span class="p">));</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>

<span class="p">...</span>

<span class="n">GenerateRandomData</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">DataSize</span><span class="p">(),</span><span class="w"> </span><span class="n">input_data</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">uniform_real_distribution</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">0.1f</span><span class="p">,</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">));</span>
</pre></div>
</div>
<p>(5) Perform inference.</p>
<p>Apply for an array <code class="docutils literal notranslate"><span class="pre">outputs</span></code> for storing the model inference output tensor, call the model inference API <code class="docutils literal notranslate"><span class="pre">Predict</span></code>, and use the input and output tensors as parameters.
After the inference is successful, the output tensor is saved in <code class="docutils literal notranslate"><span class="pre">outputs</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">MSTensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">status</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">model</span><span class="o">-&gt;</span><span class="n">Predict</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">outputs</span><span class="p">);</span>
</pre></div>
</div>
<p>(6) Verify the inference result.</p>
<p>Obtain the data pointer of the output tensor by using <code class="docutils literal notranslate"><span class="pre">MutableData</span></code>.
In this example, it is forcibly converted to a floating-point pointer. You can convert the data type based on the data type of your model or obtain the data type by using the <code class="docutils literal notranslate"><span class="pre">DataType</span></code> API of the tensor.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">out_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="w"> </span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">MutableData</span><span class="p">());</span>
</pre></div>
</div>
<p>In this example, you can view the accuracy of the inference output in the printed result.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">ElementNum</span><span class="p">();</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">out_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; &quot;</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>(7) Release the model object.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">delete</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
</pre></div>
</div>
</li>
<li><p>Perform build.</p>
<p>Go to the <code class="docutils literal notranslate"><span class="pre">build</span></code> directory, enter <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">-G</span> <span class="pre">&quot;CodeBlocks</span> <span class="pre">-</span> <span class="pre">MinGW</span> <span class="pre">Makefiles&quot;</span> <span class="pre">..</span></code> to generate a makefile, and enter <code class="docutils literal notranslate"><span class="pre">cmake</span> <span class="pre">--build</span> <span class="pre">.</span></code> to build the project. After the build is successful, you can obtain the <code class="docutils literal notranslate"><span class="pre">demo</span></code> executable program in the <code class="docutils literal notranslate"><span class="pre">build</span></code> directory.</p>
</li>
<li><p>Run the inference program.</p>
<p>Add the address of the <code class="docutils literal notranslate"><span class="pre">libmindspore-lite.so</span></code> dynamic library to the environment variable PATH.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">set</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>..<span class="se">\r</span>untime<span class="se">\l</span>ib<span class="p">;</span>%PATH%
</pre></div>
</div>
<p>Enter <code class="docutils literal notranslate"><span class="pre">call</span> <span class="pre">demo</span></code> to execute the <code class="docutils literal notranslate"><span class="pre">demo</span></code> program. According to the preceding description, the <code class="docutils literal notranslate"><span class="pre">demo</span></code> program loads the <code class="docutils literal notranslate"><span class="pre">mobilenetv2.ms</span></code> model, transfers the randomly generated input tensor to the model for inference, and prints the value of the output tensor after inference.
If the inference is successful, the following output is displayed. The output tensor contains 1000 values, which is consistent with the understanding of the model obtained in <a class="reference internal" href="#netron-visualization"><span class="std std-doc">Netron Visualization</span></a>. In this example, the input data is evenly distributed from 0.1 to 1 (which can be considered as noise). Therefore, the output has no classification characteristics:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>------- print outputs ----------
out tensor name is:Default/head-MobileNetV2Head/Softmax-op204
out tensor size is:4000
out tensor elements num is:1000
output data is:5.26822e-05 0.000497521 0.000296722 0.000377606 0.000177048 8.02106e-05 0.000212863 0.000422287 0.000273189 0.000234106 0.000998072 0.00423312 0.00204994 0.00124968 0.00294459 0.00139796 0.00111545 0.00065636 0.000809462 0.00153732 0.000621052 0.00224638 0.00127046 0.00187558 0.000420145 0.000150638 0.000266477 0.000438629 0.000187774 0.00054668 0.000212853 0.000921661 0.000127179 0.000565873 0.00100395 0.00030016 0.000282677 0.000358068 0.00215288 0.000477846 0.00107597 0.00065134 0.000722135 0.000807503 0.000631416 0.000432471 0.00125898 0.000255094 8.26058e-05 9.91919e-05 0.000794514 0.00031873 0.000525145 0.000564177 0.000390949 0.000239435 0.000769301 0.000522169 0.000489711 0.00106033 0.00188065 0.00162756 0.000154417 0.000423661 0.00114033 0.000521169 0.00104491 0.000394101 0.000574376 0.00103071 0.000328134 0.00220263 0.000588063 0.00114022 0.000639888 0.00160103 0.000883627 0.00168485 0.00749697 0.00378326 0.00049545 0.000697699 0.00094152 0.000694751 0.000361998 0.00249769 0.00224123 0.00144733 0.000867953 0.000409967 0.000414645 0.000921754 0.00362981 0.000598768 0.00939566 0.000354318 0.0011853 0.000582604 0.000977179 0.000363443 0.000252788 0.000161903 0.000498172 0.000835043 0.000125615 0.000150972 0.000271722 0.000391777 8.49806e-05 0.000175627 0.000255629 0.0104205 0.000473356 0.000470714 0.00154926 3.52034e-05 0.00017297 0.000381467 0.000286569 0.00022002 0.000270967 0.00012511 0.000102305 0.000113712 0.000152496 0.00216914 0.000232594 0.00118621 0.00120123 0.000756038 0.000361149 0.000279887 0.00072076 0.0030916 0.000839053 0.000305989 0.000185089 0.00106419 0.00141358 0.000819862 0.000874739 0.00194274 0.000707348 0.00158608 0.000395842 0.000749171 0.00119562 0.000445385 0.000481742 7.57984e-05 0.000101538 0.000709718 0.000151491 0.00051427 0.000212376 0.000216051 9.55411e-05 0.000147092 0.00030403 9.3476e-05 5.85228e-05 0.000247954 0.000708926 0.00022098 0.000342199 0.000117494 0.000191572 3.63169e-05 0.000411851 0.000342481 0.000239097 0.000764161 0.000259073 0.000524563 0.000426145 0.000111397 0.000177984 8.50417e-05 0.000275155 0.000141314 0.000509691 0.000179604 0.000770131 0.000168981 0.000312896 0.000104055 9.1071e-05 0.000408717 8.05139e-05 0.000312057 0.000296877 0.000172418 0.00024341 0.000300782 0.000146993 0.00109211 0.000191816 8.35939e-05 0.000299942 0.000315375 0.000193755 0.000319056 0.000516599 0.000504943 0.000136374 0.000324095 0.000102209 0.000352826 0.000103771 0.000373529 0.000360807 0.000296265 0.000313525 0.000118756 0.000198175 0.000219075 0.000174439 0.000216093 0.000438399 0.000296872 0.000128021 0.00017442 0.000189079 0.000399597 0.000100693 0.000123358 5.15012e-05 0.000218214 0.000222177 0.000299965 0.000147799 0.000234641 0.000149353 4.5897e-05 0.000133614 0.000225688 0.000322703 0.000510069 0.000426839 0.000150078 6.61004e-05 4.68818e-05 0.000280284 0.000124997 0.000113089 0.000687338 0.000183928 0.000232998 0.00018996 0.00016634 9.61161e-05 0.000261457 7.62777e-05 0.000892919 0.00027851 4.25679e-05 0.00012095 0.000143962 0.000543232 0.00019522 0.000152532 8.21291e-05 5.86343e-05 0.000454828 0.000232324 0.000326869 0.00050617 8.3308e-05 8.23556e-05 7.82488e-05 0.000349937 0.000162254 0.000584313 0.000380654 7.41325e-05 0.000328623 0.00052962 0.000750176 0.000374926 0.000511254 0.000546927 0.000420431 0.000673729 0.000211782 0.00163466 0.000524799 0.000383476 0.000244811 7.51562e-05 6.57744e-05 0.000155914 0.000270638 0.000106245 0.000186127 0.000346968 0.000485479 0.000271254 0.00036631 0.000252093 0.000184659 0.000340458 0.00393658 0.00120467 0.00258523 0.000523741 0.00142551 0.00168216 0.00274844 0.00230136 0.000254464 0.000689839 0.00200172 0.000789165 0.00147548 0.00497233 0.00245074 0.00351014 0.000964297 0.0116707 0.00263743 0.000911238 0.000140097 0.000427111 0.000229297 0.000354368 0.000327572 0.000399973 0.000969767 0.000753985 0.000151906 0.000319341 0.00177747 0.00014731 0.000247144 0.00028714 0.000162237 0.000406454 0.000167767 0.000141812 8.20427e-05 0.000140652 0.000154833 0.000414694 0.000191989 0.00028065 0.000298302 0.000326194 0.000358242 0.000218887 0.000214568 0.000456112 0.000153574 5.4711e-05 0.000176373 0.000716305 6.97331e-05 0.000924458 0.00036906 0.000147747 0.000464726 0.000195069 0.000472077 0.000196377 0.000422707 0.000132992 5.76273e-05 0.000180634 0.000355361 0.000247252 0.000157627 0.000537573 0.00020566 0.000577524 0.00019596 0.000227313 0.000237615 0.000251934 0.000581737 0.000156606 0.000377661 0.000534264 9.59369e-05 0.000165362 0.000174582 7.18626e-05 0.000134693 4.02814e-05 0.000179219 0.000100272 9.8463e-05 0.000262976 0.000178799 0.000224355 8.18936e-05 0.000143329 0.000117873 8.40231e-05 0.000588662 0.000158744 0.00069335 0.000287121 0.000151016 0.00152298 0.00024393 0.000737831 0.00115437 5.96499e-05 0.000118379 0.000228003 0.0041712 5.89845e-05 0.00273432 0.00321251 0.00269996 0.000762481 4.82307e-05 0.000160988 0.00115545 0.0155078 0.00138022 0.0025505 0.000223013 0.000251236 0.000123665 5.52253e-05 0.000267688 0.000453393 0.00029877 0.000429822 0.00099786 0.000183652 0.000397013 0.00108393 0.000333911 0.0008731 0.000275806 0.000101959 0.000920896 0.000532173 0.000526293 0.0006834 0.000935434 0.000351484 0.00198101 0.000158832 0.00025276 0.0309715 0.000236896 0.000507701 7.17417e-05 0.000136413 0.00511946 0.001006 0.00030655 0.000170018 0.00102066 0.000676819 0.00111926 0.00101352 0.00122263 0.000436026 0.000709552 0.00280173 0.000343102 0.000684757 0.00250305 8.5246e-05 8.35988e-05 8.50596e-05 0.000745612 0.000384923 0.000115974 0.000104449 0.00142474 0.000464432 0.00013609 4.29949e-05 0.000410546 0.000318726 8.40787e-05 0.00206693 0.00057538 0.000382494 0.000160234 0.000307552 0.000529971 0.000586405 0.00398225 0.00151492 0.00026454 0.000511842 9.7473e-05 0.000163672 0.000160056 0.000816508 3.00784e-05 0.00037759 0.00014328 8.48268e-05 0.00142338 6.22116e-05 0.000788073 0.00155491 0.00121945 0.000680781 0.000758789 0.000459647 0.00708145 0.00120801 7.03766e-05 0.000364867 0.000123017 0.00420891 0.000513928 0.00123761 0.000267312 0.000333363 0.00122328 0.000298812 0.000238888 0.000615765 8.10465e-05 0.000246716 0.00123949 0.000508113 7.77746e-05 0.000487965 0.000462255 0.000310659 0.000585418 0.00176246 0.000181668 0.000288837 0.000232032 0.00549264 0.000113551 0.000251434 0.000276892 0.000604927 0.00410441 0.000628254 0.000532845 0.00177639 0.000769542 0.000172925 0.00065605 0.0015078 4.19799e-05 0.000255064 0.00488681 0.000521465 0.000326431 0.00111252 0.00235686 0.000651842 8.37604e-05 0.00319951 0.000679279 0.00160411 0.000953606 0.00047153 8.01442e-05 0.00192255 0.0110213 0.000130118 0.00018916 0.00082058 0.000194114 0.000183411 0.000152358 0.000211961 5.22587e-05 0.00303399 0.000128953 0.00159357 0.000101542 5.38654e-05 0.000206161 0.000293241 0.000191215 7.02916e-05 0.000230206 0.000109719 0.000682147 0.000378998 0.000515589 0.000204293 0.00115819 0.00252224 0.00132761 4.51228e-05 0.00333054 0.000486169 0.000733327 0.000177619 9.41916e-05 0.00120342 0.00432701 0.000222835 0.000197637 0.00449768 0.00115172 0.000184445 0.000111001 0.00112382 0.0018688 0.00320062 0.000278918 0.000906152 0.000116432 0.00164653 0.000537722 0.000249092 0.00221725 0.000161599 0.000414339 0.00299422 0.000435541 0.00880695 0.00490311 0.00325869 6.05041e-05 0.00458625 0.00517385 0.00024982 0.000220774 0.0032148 0.000275533 0.00222638 0.00206151 0.000420763 0.00028658 0.0149218 0.000693565 6.89355e-05 0.000175107 0.000611934 0.000185402 0.00048781 0.00104491 0.000305031 0.000719747 0.000464874 0.000902618 0.00710998 0.00028243 0.000266798 0.000557195 0.00018203 0.000165886 0.00432344 0.0018616 0.00081676 0.000688068 0.000116212 0.00375912 0.00011202 0.0119426 0.000395667 0.00134768 0.000107723 8.29395e-05 0.00874447 0.000217795 0.00201653 0.000200428 0.000784866 0.000739253 0.000223862 0.000716373 9.37279e-05 1.64484e-05 0.000103597 0.00134084 0.00208305 6.15101e-05 0.000264137 0.00421874 0.000816694 0.019055 0.000882248 0.0265989 0.000885313 0.00189269 0.000819798 0.000479354 0.000194866 4.39721e-05 0.000374197 0.00102411 0.000391648 0.000144945 0.000320067 0.000943551 6.28455e-05 0.000563089 0.00319211 0.000219879 8.42234e-05 0.000555672 0.00231883 0.0037087 0.000302303 0.000149123 0.000789137 7.45903e-05 0.000133478 0.000470522 0.000542576 0.000413181 0.000967243 0.00134348 0.000439858 0.0010091 0.00714279 0.000202303 0.000809548 8.99185e-05 0.000199892 0.00059308 0.00129059 0.00162076 0.00793667 0.000529655 0.000417269 0.00100714 0.000160703 0.00097642 0.000691081 7.56624e-05 0.000217106 0.00290805 0.000661668 0.00104081 0.000133569 0.000945062 0.00132827 0.000932787 0.00482219 3.9986e-05 0.000903322 0.000455647 0.00143754 0.000103266 0.00367346 0.000897197 0.000118318 0.00149419 0.000865034 0.00126782 0.00090065 0.000132982 0.0039552 0.00210961 0.000428278 0.000123607 0.000284831 2.11637e-05 0.000587767 0.000752392 0.00159891 0.00253384 4.46648e-05 0.00597254 0.00373919 0.000849701 4.3499e-05 0.000935258 0.000311729 0.00719802 0.000368296 0.00284921 0.00317468 0.000813635 0.0011214 0.000610401 0.000484875 0.00417738 0.000496244 9.79432e-05 0.000734274 0.000259079 0.00247699 0.00460816 0.00708891 0.000724271 0.00048205 0.000174656 0.000596118 0.000401012 8.25042e-05 0.000161686 0.00197722 0.000806688 0.00684481 0.000596325 0.00131103 0.000204451 0.00100593 0.00151624 8.50725e-05 0.000122174 0.00021799 0.000259111 0.002961 0.000829398 0.000533044 5.0536e-05 0.000946751 6.78423e-05 0.000485367 0.00306399 0.00523905 0.00123471 0.000224707 0.000101096 0.0014873 0.000104553 0.00355624 0.000205465 0.000169472 5.07939e-05 0.000195914 0.000791247 0.000246651 0.000205654 0.000285258 0.000651622 0.00211643 6.79842e-05 0.000138115 0.00103942 0.000187132 0.000409764 0.00214586 0.000292729 0.00031472 0.000691548 0.000382784 0.000125186 0.00233764 0.000536727 0.000502022 4.95937e-05 0.0264263 0.00477407 0.00376776 0.00014371 0.00137865 0.00109858 0.000563498 0.00261839 0.00397829 0.000242258 0.000141749 0.00157776 0.00031561 0.000136863 0.000277254 0.000887197 5.00407e-05 0.0031923 0.000459011 9.37109e-05 0.000129428 9.72145e-05 0.000116087 5.26294e-05 0.000929531 0.00363911 0.000738978 0.000344878 0.00242673 0.000193775 4.87371e-05 0.0010458 0.00015866 0.000108444 7.05613e-05 0.000979656 0.000203967 0.000434424 0.00147155 0.00623083 0.000167943 0.00654287 0.000231375 0.000144977 7.44322e-05 0.000271412 0.000257479 0.000125951 0.0084965 0.00708424 0.000741149 0.000327848 0.00072125 0.00155309 0.000849641 0.000468936 0.000597561 0.000343363 0.0013401 0.000644772 0.00296955 0.00203899 0.000344333 0.000654109 0.000579819 0.000307663 0.00295462 0.00098848 0.000224191 0.000616008 0.000192669 0.000124413 0.000265971 6.58702e-05 0.00031542 0.000253495 0.000276654 0.00289865 0.000574721 0.000131497 0.000204278 0.000568842 9.16333e-05 0.000257384 0.000243244 0.000443263 0.000109194 0.000139178 0.000500078 0.000198004 0.000107967 0.000169954 0.000123934 0.000258871 9.86871e-05 0.000293493 0.000323952 0.000504283 0.00036182 0.000316952 0.000177675 0.00168864 8.59652e-05 7.83207e-05 0.000429965 0.000663861 0.000506927 0.000768278 0.000391109 0.000419152 0.000638448 0.000214176 0.00016114 0.000515638 0.000709203 0.000154853 0.000343536 0.000326896 0.000130624 7.31075e-05 6.40462e-05 0.000139256 0.000695747 0.000437555 0.00208446 0.000180979 0.000100821 0.000760209 0.000238104 9.52171e-05 0.000459536 0.000388741 0.000291907 0.000577998 0.000439995 9.34035e-05 0.000309451 0.000308385 0.00012725 6.17104e-05 0.000231779 0.000489432 0.00012118 0.000211306 0.000357186 0.000356726 0.000311104 0.000615516 0.000252385 0.000400029 0.000204223 0.000195018 7.65522e-05 0.00028094 0.000303784 0.00186481
------- print end ----------
</pre></div>
</div>
</li>
<li><p>See more detailed integration description.</p>
<p>For details about how to use the C++ API for integration and advanced usage, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_cpp.html">Using C++ Interface to Perform Inference</a>.</p>
<p>For details about how to use the Java API for integration and development, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/use/runtime_java.html">Using Java Interface to Perform Inference</a>.</p>
<p>For details about Android integration and development, see <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/quick_start/quick_start.html">Android Application Development Based on JNI Interface</a>, <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/quick_start/image_segmentation.html">Android Application Development Based on Java Interface</a>, and <a class="reference external" href="https://www.mindspore.cn/lite/docs/en/r2.3/model_lite.html">Model List</a>.</p>
</li>
</ol>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../use/cloud_infer/build.html" class="btn btn-neutral float-left" title="Building Cloud-side MindSpore Lite" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="one_hour_introduction_cloud.html" class="btn btn-neutral float-right" title="Quick Start to Cloud-side Inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright MindSpore.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 
</body>
</html>